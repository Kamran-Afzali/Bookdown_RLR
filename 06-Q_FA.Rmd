# Function Approximation Q-Learning with Linear Models

## Introduction

In reinforcement learning (RL), tabular methods like SARSA and
Q-Learning store a separate Q-value for each state-action pair, which
becomes infeasible in large or continuous state spaces. Function
approximation addresses this by representing the action-value function
$Q(s, a)$ as a parameterized function $Q(s, a; \theta)$, enabling
generalization across states and scalability. This post explores
Q-Learning with linear function approximation, using a 10-state,
2-action environment to demonstrate how it learns policies compared to
tabular methods. We provide mathematical formulations, R code, and
comparisons with tabular Q-Learning, focusing on generalization,
scalability, and practical implications.

## Theoretical Background

Function approximation in RL aims to estimate the action-value function:

$$
Q^\pi(s, a) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty \gamma^t R_{t+1} \mid S_0 = s, A_0 = a \right]
$$

where $\gamma \in [0,1]$ is the discount factor, and $R_{t+1}$ is the
reward at time $t+1$. Instead of storing $Q(s, a)$ in a table, we
approximate it as:

$$
Q(s, a; \theta) = \phi(s, a)^T \theta
$$

where $\phi(s, a)$ is a feature vector for state-action pair $(s, a)$,
and $\theta$ is a parameter vector learned via optimization, typically
stochastic gradient descent (SGD).

### Q-Learning with Function Approximation

Q-Learning with function approximation is an off-policy method that
learns the optimal policy by updating $\theta$ to minimize the temporal
difference (TD) error. The update rule is:

$$
\theta \leftarrow \theta + \alpha \delta \nabla_\theta Q(s, a; \theta)
$$

where $\alpha$ is the learning rate, and the TD error $\delta$ is:

$$
\delta = r + \gamma \max_{a'} Q(s', a'; \theta) - Q(s, a; \theta)
$$

Here, $r$ is the reward, $s'$ is the next state, and
$\max_{a'} Q(s', a'; \theta)$ estimates the value of the next state
assuming the optimal action. For linear function approximation, the
gradient is:

$$
\nabla_\theta Q(s, a; \theta) = \phi(s, a)
$$

Thus, the update becomes:

$$
\theta \leftarrow \theta + \alpha \left( r + \gamma \max_{a'} Q(s', a'; \theta) - Q(s, a; \theta) \right) \phi(s, a)
$$

In our 10-state environment, we use one-hot encoding for $\phi(s, a)$,
mimicking tabular Q-Learning for simplicity but demonstrating the
framework’s potential for generalization with more complex features.

### Comparison with Tabular Q-Learning

Tabular Q-Learning updates a table of Q-values directly:

$$
Q(s, a) \leftarrow Q(s, a) + \alpha \left( r + \gamma \max_{a'} Q(s', a') - Q(s, a) \right)
$$

Function approximation generalizes across states via $\phi(s, a)$,
reducing memory requirements and enabling learning in large or
continuous spaces. However, it introduces approximation errors and
requires careful feature design to ensure convergence.

| **Aspect** | **Tabular Q-Learning** | **Q-Learning with Function Approximation** |
|------------------|-------------------------|------------------------------|
| **Representation** | Table of $Q(s, a)$ values | $Q(s, a; \theta) = \phi(s, a)^T \theta$ |
| **Memory** | $O(|\mathcal{S}| \cdot |\mathcal{A}|)$ | $O(|\theta|)$, depends on feature size |
| **Generalization** | None; state-action specific | Yes; depends on feature design |
| **Scalability** | Poor for large/continuous spaces | Good for large/continuous spaces with proper features |
| **Update Rule** | Direct Q-value update | Parameter update via gradient descent |
| **Convergence** | Guaranteed to optimal $Q^*$ under conditions | Converges to approximation of $Q^*$; depends on features |

## R Implementation

We implement Q-Learning with linear function approximation in a
10-state, 2-action environment, using one-hot encoding for $\phi(s, a)$.
The environment mirrors the previous post, with action 1 yielding a 1.0
reward at the terminal state (state 10) and action 2 yielding 0.5.


```{r}
# Common settings
n_states <- 10
n_actions <- 2
gamma <- 0.9
terminal_state <- n_states

# Environment: transition and reward models
set.seed(42)
transition_model <- array(0, dim = c(n_states, n_actions, n_states))
reward_model <- array(0, dim = c(n_states, n_actions, n_states))

for (s in 1:(n_states - 1)) {
  transition_model[s, 1, s + 1] <- 0.9
  transition_model[s, 1, sample(1:n_states, 1)] <- 0.1
  transition_model[s, 2, sample(1:n_states, 1)] <- 0.8
  transition_model[s, 2, sample(1:n_states, 1)] <- 0.2
  for (s_prime in 1:n_states) {
    reward_model[s, 1, s_prime] <- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1))
    reward_model[s, 2, s_prime] <- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1))
  }
}

transition_model[n_states, , ] <- 0
reward_model[n_states, , ] <- 0

# Sampling function
sample_env <- function(s, a) {
  probs <- transition_model[s, a, ]
  s_prime <- sample(1:n_states, 1, prob = probs)
  reward <- reward_model[s, a, s_prime]
  list(s_prime = s_prime, reward = reward)
}

# Create one-hot features for (state, action) pairs
create_features <- function(s, a, n_states, n_actions) {
  vec <- rep(0, n_states * n_actions)
  index <- (a - 1) * n_states + s
  vec[index] <- 1
  return(vec)
}

# Initialize weights
n_features <- n_states * n_actions
theta <- rep(0, n_features)

# Q-value approximation function
q_hat <- function(s, a, theta) {
  x <- create_features(s, a, n_states, n_actions)
  return(sum(x * theta))
}

# Q-Learning with function approximation
q_learning_fa <- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) {
  theta <- rep(0, n_features)
  rewards <- numeric(episodes)
  
  for (ep in 1:episodes) {
    s <- sample(1:(n_states - 1), 1)
    episode_reward <- 0
    while (TRUE) {
      # Epsilon-greedy action selection
      a <- if (runif(1) < epsilon) {
        sample(1:n_actions, 1)
      } else {
        q_vals <- sapply(1:n_actions, function(a_) q_hat(s, a_, theta))
        which.max(q_vals)
      }
      
      out <- sample_env(s, a)
      s_prime <- out$s_prime
      r <- out$reward
      episode_reward <- episode_reward + r
      
      # Compute TD target and error
      q_current <- q_hat(s, a, theta)
      q_next <- if (s_prime == terminal_state) 0 else max(sapply(1:n_actions, function(a_) q_hat(s_prime, a_, theta)))
      target <- r + gamma * q_next
      error <- target - q_current
      
      # Gradient update
      x <- create_features(s, a, n_states, n_actions)
      theta <- theta + alpha * error * x
      
      if (s_prime == terminal_state) break
      s <- s_prime
    }
    rewards[ep] <- episode_reward
  }
  
  # Derive policy
  policy <- sapply(1:n_states, function(s) {
    if (s == terminal_state) NA else which.max(sapply(1:n_actions, function(a) q_hat(s, a, theta)))
  })
  
  list(theta = theta, policy = policy, rewards = rewards)
}

# Run Q-Learning with function approximation
set.seed(42)
fa_result <- q_learning_fa(episodes = 1000, alpha = 0.1, epsilon = 0.1)
fa_policy <- fa_result$policy
fa_rewards <- fa_result$rewards

# Visualize policy
library(ggplot2)
policy_df <- data.frame(
  State = 1:n_states,
  Policy = fa_policy,
  Algorithm = "Q-Learning FA"
)
policy_df$Policy[n_states] <- NA  # Terminal state

policy_plot <- ggplot(policy_df, aes(x = State, y = Policy)) +
  geom_point(size = 3, color = "deepskyblue") +
  geom_line(na.rm = TRUE, color = "deepskyblue") +
  theme_minimal() +
  labs(title = "Policy from Q-Learning with Function Approximation", x = "State", y = "Action") +
  scale_x_continuous(breaks = 1:n_states) +
  scale_y_continuous(breaks = 1:n_actions, labels = c("Action 1", "Action 2")) +
  theme(legend.position = "none")

# Visualize cumulative rewards
reward_df <- data.frame(
  Episode = 1:1000,
  Reward = cumsum(fa_rewards),
  Algorithm = "Q-Learning FA"
)

policy_plot
```  





Nice — I’ll walk through this **chunk by chunk**, explain what each piece does, point out subtle behaviors/bugs, and finish with a few short, practical fixes and suggestions. I’ll keep it factual and focused.



#### Top-level settings

```r
n_states <- 10
n_actions <- 2
gamma <- 0.9
terminal_state <- n_states
```

* `n_states` and `n_actions`: size of the MDP (10 states, 2 actions).
* `gamma`: discount factor (0.9).
* `terminal_state`: the index of the terminal state (state 10). Many later pieces use this to stop episodes and to set `q_next = 0` for transitions into terminal.



####  Environment: transition and reward models

```r
set.seed(42)
transition_model <- array(0, dim = c(n_states, n_actions, n_states))
reward_model <- array(0, dim = c(n_states, n_actions, n_states))

for (s in 1:(n_states - 1)) {
  transition_model[s, 1, s + 1] <- 0.9
  transition_model[s, 1, sample(1:n_states, 1)] <- 0.1
  transition_model[s, 2, sample(1:n_states, 1)] <- 0.8
  transition_model[s, 2, sample(1:n_states, 1)] <- 0.2
  for (s_prime in 1:n_states) {
    reward_model[s, 1, s_prime] <- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1))
    reward_model[s, 2, s_prime] <- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1))
  }
}

transition_model[n_states, , ] <- 0
reward_model[n_states, , ] <- 0
```

What this builds:

* `transition_model[s, a, s']` is the probability of transitioning from state `s` with action `a` to next state `s'`.
* `reward_model[s, a, s']` is the reward received for that transition.

Important behavioral details and **issues**:

1. **Seeding**: `set.seed(42)` fixes random draws so behavior is reproducible.
2. **Action 1 transitions**:

   * For non-terminal `s` the code assigns `0.9` to go deterministically to `s+1` most of the time.
   * Then it assigns `0.1` to a random state (`sample(1:n_states, 1)`).
   * If the sampled random `s'` equals `s+1` you end up with `transition_model[s,1,s+1] = 0.9 + 0.1 = 1.0`. If it’s different, two entries are non-zero (0.9 and 0.1).
3. **Action 2 transitions — probable bug / unintended overwrite**:

   * The code calls `sample()` twice for action 2, assigning `0.8` and then `0.2` to (possibly) two *different* random indices. That is allowed, but these two `sample()` calls are independent and may pick the same `s'` — if they pick the same index the entry becomes `1.0`.
   * More subtle: because the two `sample()` calls pick indices independently, it’s possible that only one index gets both probabilities or one gets 0.8 and another 0.2. This is probably **not** what the author intended (likely they wanted two different explicitly chosen next states).
4. **Normalization**: `transition_model` rows are *not explicitly normalized*. In R, `sample()` will accept a `prob` vector that does not sum to 1 (it normalizes internally) — so sampling will still work even if rows don’t sum exactly to 1, provided the vector is non-negative and at least one positive entry exists. But for clarity and safety it’s better to ensure each `(s,a,·)` sums to 1.
5. **Rewards**:

   * For action 1: if `s'` is terminal (`s_prime == n_states`) reward = `1.0`, else `0.1 * runif(1)` (random small reward).
   * For action 2: terminal reward `0.5`, else `0.05 * runif(1)` (even smaller).
   * So action 1 gives larger terminal reward than action 2; also non-terminal rewards are noisy.
6. **Terminal state rows zeroed**:

   * `transition_model[n_states,,] <- 0` and `reward_model[n_states,,] <- 0` make terminal state absorbing with no outgoing transitions/rewards. The code later treats terminal specially (stops episodes when reached).



####  Sampling function

```r
sample_env <- function(s, a) {
  probs <- transition_model[s, a, ]
  s_prime <- sample(1:n_states, 1, prob = probs)
  reward <- reward_model[s, a, s_prime]
  list(s_prime = s_prime, reward = reward)
}
```

* Given `(s, a)`, it samples `s'` according to the probability vector `probs` (R will normalize probabilities if they don't sum to 1).
* Returns a list with `s_prime` and `reward = reward_model[s,a,s_prime]`.
* If `probs` is all zeros (possible for terminal state row), `sample()` will error — but the code avoids calling `sample_env` from terminal because episodes end when `s_prime == terminal_state`. Still, careful code should check for zero-prob rows.



####  Feature creation (one-hot for (state,action))

```r
create_features <- function(s, a, n_states, n_actions) {
  vec <- rep(0, n_states * n_actions)
  index <- (a - 1) * n_states + s
  vec[index] <- 1
  return(vec)
}
```

* Creates a one-hot feature vector of length `n_states * n_actions`.
* The mapping `index = (a - 1) * n_states + s` puts action blocks in the feature vector: first `n_states` entries correspond to action 1, next `n_states` to action 2, etc.
* Because features are one-hot, linear function approximation `q(s,a) = x^T theta` is mathematically equivalent to a **tabular** Q-value representation (each `(s,a)` has its own weight in `theta`).



####  Weights and Q-value function

```r
n_features <- n_states * n_actions
theta <- rep(0, n_features)

q_hat <- function(s, a, theta) {
  x <- create_features(s, a, n_states, n_actions)
  return(sum(x * theta))
}
```

* `theta` holds parameters for the linear approximator; initialized at zeros.
* `q_hat(s,a,theta)` computes ( \hat{Q}(s,a) = x^\top \theta ). With one-hot `x` this returns the single `theta` entry corresponding to `(s,a)`.



####  Q-learning with function approximation

```r
q_learning_fa <- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) {
  theta <- rep(0, n_features)
  rewards <- numeric(episodes)
  
  for (ep in 1:episodes) {
    s <- sample(1:(n_states - 1), 1)
    episode_reward <- 0
    while (TRUE) {
      # Epsilon-greedy action selection
      a <- if (runif(1) < epsilon) {
        sample(1:n_actions, 1)
      } else {
        q_vals <- sapply(1:n_actions, function(a_) q_hat(s, a_, theta))
        which.max(q_vals)
      }
      
      out <- sample_env(s, a)
      s_prime <- out$s_prime
      r <- out$reward
      episode_reward <- episode_reward + r
      
      # Compute TD target and error
      q_current <- q_hat(s, a, theta)
      q_next <- if (s_prime == terminal_state) 0 else max(sapply(1:n_actions, function(a_) q_hat(s_prime, a_, theta)))
      target <- r + gamma * q_next
      error <- target - q_current
      
      # Gradient update
      x <- create_features(s, a, n_states, n_actions)
      theta <- theta + alpha * error * x
      
      if (s_prime == terminal_state) break
      s <- s_prime
    }
    rewards[ep] <- episode_reward
  }
  
  # Derive policy
  policy <- sapply(1:n_states, function(s) {
    if (s == terminal_state) NA else which.max(sapply(1:n_actions, function(a) q_hat(s, a, theta)))
  })
  
  list(theta = theta, policy = policy, rewards = rewards)
}
```

Step-by-step explanation:

1. **Initialization**: new `theta` vector and a `rewards` vector to store total reward per episode.
2. **Episode start**: `s <- sample(1:(n_states - 1), 1)` picks a random non-terminal start state for each episode.
3. **Action selection (epsilon-greedy)**:

   * With probability `epsilon` pick a random action.
   * Else compute Q-values `q_hat(s,a,theta)` for all actions and pick the action with maximum Q. Note: `which.max()` returns the index of the first maximum if there’s a tie.
4. **Interact with environment** using `sample_env`.
5. **TD target and error**:

   * `q_current` is the estimate for `(s,a)`.
   * `q_next` is `0` if `s_prime` is terminal, otherwise `max_a' q_hat(s', a')`.
   * `target = r + gamma * q_next`
   * `error = target - q_current`
6. **Gradient (linear FA)** update:

   * For linear Q with features `x`, gradient of `q_hat` w.r.t `theta` is `x`. So the parameter update `theta <- theta + alpha * error * x` is standard semi-gradient Q-learning.
   * Because `x` is a one-hot, only the `(s,a)`-entry in `theta` is changed, identical to a tabular Q-learning update:
     (\theta_{s,a} \leftarrow \theta_{s,a} + \alpha (r + \gamma \max_{a'} Q(s',a') - Q(s,a)).)
7. **Episode termination**: break if `s_prime == terminal_state`; otherwise continue from `s <- s_prime`.
8. **Record episode total reward** in `rewards[ep]`.
9. After all episodes, **derive greedy policy** from final `theta`: for each non-terminal state pick `argmax_a q_hat(s,a,theta)`.



####  Running the learner

```r
set.seed(42)
fa_result <- q_learning_fa(episodes = 1000, alpha = 0.1, epsilon = 0.1)
fa_policy <- fa_result$policy
fa_rewards <- fa_result$rewards
```

* Trains for 1000 episodes using `alpha=0.1` and `epsilon=0.1`.
* Stores learned `policy` and episode rewards.



####  Visualization

```r
library(ggplot2)
policy_df <- data.frame(
  State = 1:n_states,
  Policy = fa_policy,
  Algorithm = "Q-Learning FA"
)
policy_df$Policy[n_states] <- NA  # Terminal state

policy_plot <- ggplot(policy_df, aes(x = State, y = Policy)) +
  geom_point(size = 3, color = "deepskyblue") +
  geom_line(na.rm = TRUE, color = "deepskyblue") +
  theme_minimal() +
  labs(title = "Policy from Q-Learning with Function Approximation", x = "State", y = "Action") +
  scale_x_continuous(breaks = 1:n_states) +
  scale_y_continuous(breaks = 1:n_actions, labels = c("Action 1", "Action 2")) +
  theme(legend.position = "none")

# Visualize cumulative rewards
reward_df <- data.frame(
  Episode = 1:1000,
  Reward = cumsum(fa_rewards),
  Algorithm = "Q-Learning FA"
)

policy_plot
```

* `policy_plot` shows the greedy action (1 or 2) for each state as points/line.
* `reward_df` computes **cumulative** rewards, but note: the code builds `reward_df` but never plots it. Only `policy_plot` is shown at the end. If you want episode-by-episode reward or moving average, you'd need to add a `ggplot` call for `reward_df` (see suggestions below).



####  Overall algorithmic behavior & complexity

* Because features are one-hot, this is function approximation that reduces exactly to **tabular Q-learning**.
* Per update cost: O(n_actions) for computing max over next-state actions. Per episode cost depends on episode length (random here), so total complexity roughly `O(episodes * average_episode_length * n_actions)`.

