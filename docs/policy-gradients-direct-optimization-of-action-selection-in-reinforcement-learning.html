<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 16 Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning | Reinforcement Learning in R</title>
  <meta name="description" content="Chapter 16 Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning | Reinforcement Learning in R" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 16 Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning | Reinforcement Learning in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 16 Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning | Reinforcement Learning in R" />
  
  
  

<meta name="author" content="Kamran Afzalui" />


<meta name="date" content="2025-10-03" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="intrinsic-rewards.html"/>
<link rel="next" href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Reinforcement Learning in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Understanding Reinforcement Learning: From Bandits to Policy Optimization</strong></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-multi-armed-bandit-the-simplest-case"><i class="fa fa-check"></i><b>1.2</b> The Multi-Armed Bandit: The Simplest Case</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#transition-to-markov-decision-processes"><i class="fa fa-check"></i><b>1.3</b> Transition to Markov Decision Processes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#comparing-reinforcement-learning-methods"><i class="fa fa-check"></i><b>1.4</b> Comparing Reinforcement Learning Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#dynamic-programming-model-based-learning"><i class="fa fa-check"></i><b>1.4.1</b> Dynamic Programming: Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#model-free-approaches-monte-carlo-and-td-learning"><i class="fa fa-check"></i><b>1.4.2</b> Model-Free Approaches: Monte Carlo and TD Learning</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#dyna-bridging-model-free-and-model-based-learning"><i class="fa fa-check"></i><b>1.4.3</b> Dyna: Bridging Model-Free and Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#q-learning-and-function-approximation"><i class="fa fa-check"></i><b>1.4.4</b> Q-Learning and Function Approximation</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#policy-gradient-and-actor-critic-methods"><i class="fa fa-check"></i><b>1.4.5</b> Policy Gradient and Actor-Critic Methods</a></li>
<li class="chapter" data-level="1.4.6" data-path="index.html"><a href="index.html#advanced-policy-optimization-techniques"><i class="fa fa-check"></i><b>1.4.6</b> Advanced Policy Optimization Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#further-directions"><i class="fa fa-check"></i><b>1.5</b> Further Directions</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html"><i class="fa fa-check"></i><b>2</b> The Multi-Armed Bandit Problem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#mathematical-formalism"><i class="fa fa-check"></i><b>2.2</b> Mathematical Formalism</a></li>
<li class="chapter" data-level="2.3" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm"><i class="fa fa-check"></i><b>2.3</b> Frequentist Approach: UCB1 Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1"><i class="fa fa-check"></i><b>2.3.1</b> R Code for UCB1</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling"><i class="fa fa-check"></i><b>2.4</b> Bayesian Approach: Thompson Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling"><i class="fa fa-check"></i><b>2.4.1</b> R Code for Thompson Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy"><i class="fa fa-check"></i><b>2.5</b> Epsilon-Greedy Strategy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy"><i class="fa fa-check"></i><b>2.5.1</b> R Code for Epsilon-Greedy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#summary-table"><i class="fa fa-check"></i><b>2.6</b> Summary Table</a></li>
<li class="chapter" data-level="2.7" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html"><i class="fa fa-check"></i><b>3</b> Markov Decision Processes and Dynamic Programming</a>
<ul>
<li class="chapter" data-level="3.1" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#constructing-the-mdp-in-r"><i class="fa fa-check"></i><b>3.2</b> Constructing the MDP in R</a></li>
<li class="chapter" data-level="3.3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#value-iteration-algorithm"><i class="fa fa-check"></i><b>3.3</b> Value Iteration Algorithm</a></li>
<li class="chapter" data-level="3.4" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#evaluation-and-interpretation"><i class="fa fa-check"></i><b>3.4</b> Evaluation and Interpretation</a></li>
<li class="chapter" data-level="3.5" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties-of-value-iteration"><i class="fa fa-check"></i><b>3.5</b> Theoretical Properties of Value Iteration</a></li>
<li class="chapter" data-level="3.6" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-1"><i class="fa fa-check"></i><b>3.6</b> Summary Table</a></li>
<li class="chapter" data-level="3.7" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><i class="fa fa-check"></i><b>4</b> Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#theoretical-background"><i class="fa fa-check"></i><b>4.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#temporal-difference-learning-q-learning"><i class="fa fa-check"></i><b>4.2.1</b> Temporal Difference Learning (Q-Learning)</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#monte-carlo-methods"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.2.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-1-defining-the-environment-in-r"><i class="fa fa-check"></i><b>4.2.3</b> Step 1: Defining the Environment in R</a></li>
<li class="chapter" data-level="4.2.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-2-q-learning-implementation-in-r"><i class="fa fa-check"></i><b>4.2.4</b> Step 2: Q-Learning Implementation in R</a></li>
<li class="chapter" data-level="4.2.5" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-3-monte-carlo-every-visit-implementation"><i class="fa fa-check"></i><b>4.2.5</b> Step 3: Monte Carlo Every-Visit Implementation</a></li>
<li class="chapter" data-level="4.2.6" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-4-simulating-outcome-devaluation"><i class="fa fa-check"></i><b>4.2.6</b> Step 4: Simulating Outcome Devaluation</a></li>
<li class="chapter" data-level="4.2.7" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-5-comparing-policies-before-and-after-devaluation"><i class="fa fa-check"></i><b>4.2.7</b> Step 5: Comparing Policies Before and After Devaluation</a></li>
<li class="chapter" data-level="4.2.8" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-6-visualizing-the-policies"><i class="fa fa-check"></i><b>4.2.8</b> Step 6: Visualizing the Policies</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#interpretation-and-discussion"><i class="fa fa-check"></i><b>4.3</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="4.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#conclusion-2"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><i class="fa fa-check"></i><b>5</b> On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-on-policy"><i class="fa fa-check"></i><b>5.2</b> SARSA (On-Policy)</a></li>
<li class="chapter" data-level="5.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-off-policy"><i class="fa fa-check"></i><b>5.3</b> Q-Learning (Off-Policy)</a></li>
<li class="chapter" data-level="5.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-with-importance-sampling"><i class="fa fa-check"></i><b>5.4</b> Off-Policy Monte Carlo with Importance Sampling</a></li>
<li class="chapter" data-level="5.5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#key-differences"><i class="fa fa-check"></i><b>5.5</b> Key Differences</a></li>
<li class="chapter" data-level="5.6" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#interpretation-and-discussion-1"><i class="fa fa-check"></i><b>5.6</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#conclusion-3"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
<li class="chapter" data-level="5.8" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#comparison-table"><i class="fa fa-check"></i><b>5.8</b> Comparison Table</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><i class="fa fa-check"></i><b>6</b> Function Approximation in Reinforcement Learning: Q-Learning with Linear Models in R</a>
<ul>
<li class="chapter" data-level="6.1" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#theoretical-background-1"><i class="fa fa-check"></i><b>6.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#q-learning-with-function-approximation"><i class="fa fa-check"></i><b>6.2.1</b> Q-Learning with Function Approximation</a></li>
<li class="chapter" data-level="6.2.2" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#comparison-with-tabular-q-learning"><i class="fa fa-check"></i><b>6.2.2</b> Comparison with Tabular Q-Learning</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#r-implementation"><i class="fa fa-check"></i><b>6.3</b> R Implementation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><i class="fa fa-check"></i><b>7</b> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#theoretical-background-2"><i class="fa fa-check"></i><b>7.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation"><i class="fa fa-check"></i><b>7.2.1</b> Q-Learning with Random Forest Approximation</a></li>
<li class="chapter" data-level="7.2.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-engineering-for-tree-based-models"><i class="fa fa-check"></i><b>7.2.2</b> Feature Engineering for Tree-Based Models</a></li>
<li class="chapter" data-level="7.2.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods"><i class="fa fa-check"></i><b>7.2.3</b> Comparison with Previous Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1"><i class="fa fa-check"></i><b>7.3</b> R Implementation</a></li>
<li class="chapter" data-level="7.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights"><i class="fa fa-check"></i><b>7.4</b> Analysis and Insights</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#policy-learning-characteristics"><i class="fa fa-check"></i><b>7.4.1</b> Policy Learning Characteristics</a></li>
<li class="chapter" data-level="7.4.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#computational-considerations"><i class="fa fa-check"></i><b>7.4.2</b> Computational Considerations</a></li>
<li class="chapter" data-level="7.4.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-importance-insights"><i class="fa fa-check"></i><b>7.4.3</b> Feature Importance Insights</a></li>
<li class="chapter" data-level="7.4.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#practical-implications-1"><i class="fa fa-check"></i><b>7.4.4</b> Practical Implications</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-linear-approximation"><i class="fa fa-check"></i><b>7.5</b> Comparison with Linear Approximation</a></li>
<li class="chapter" data-level="7.6" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4"><i class="fa fa-check"></i><b>7.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><i class="fa fa-check"></i><b>8</b> Deep Function Approximation: Q-Learning with Neural Networks in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation"><i class="fa fa-check"></i><b>8.2</b> Theoretical Foundation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity"><i class="fa fa-check"></i><b>8.2.1</b> Universal Approximation and Expressivity</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning"><i class="fa fa-check"></i><b>8.2.2</b> Gradient-Based Learning</a></li>
<li class="chapter" data-level="8.2.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches"><i class="fa fa-check"></i><b>8.2.3</b> Comparison with Previous Approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2"><i class="fa fa-check"></i><b>8.3</b> R Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation"><i class="fa fa-check"></i><b>8.4</b> Analysis and Interpretation</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#learning-dynamics"><i class="fa fa-check"></i><b>8.4.1</b> Learning Dynamics</a></li>
<li class="chapter" data-level="8.4.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#function-representation"><i class="fa fa-check"></i><b>8.4.2</b> Function Representation</a></li>
<li class="chapter" data-level="8.4.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#generalization-properties"><i class="fa fa-check"></i><b>8.4.3</b> Generalization Properties</a></li>
<li class="chapter" data-level="8.4.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#training-stability"><i class="fa fa-check"></i><b>8.4.4</b> Training Stability</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#practical-considerations"><i class="fa fa-check"></i><b>8.5</b> Practical Considerations</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#architecture-selection"><i class="fa fa-check"></i><b>8.5.1</b> Architecture Selection</a></li>
<li class="chapter" data-level="8.5.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#training-frequency"><i class="fa fa-check"></i><b>8.5.2</b> Training Frequency</a></li>
<li class="chapter" data-level="8.5.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#regularization"><i class="fa fa-check"></i><b>8.5.3</b> Regularization</a></li>
<li class="chapter" data-level="8.5.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#initialization-and-convergence"><i class="fa fa-check"></i><b>8.5.4</b> Initialization and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-across-function-approximation-methods"><i class="fa fa-check"></i><b>8.6</b> Comparison Across Function Approximation Methods</a></li>
<li class="chapter" data-level="8.7" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions"><i class="fa fa-check"></i><b>8.7</b> Future Directions</a></li>
<li class="chapter" data-level="8.8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5"><i class="fa fa-check"></i><b>8.8</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html"><i class="fa fa-check"></i><b>9</b> Dyna and DynaQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#theoretical-framework"><i class="fa fa-check"></i><b>9.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#the-dyna-architecture"><i class="fa fa-check"></i><b>9.2.1</b> The Dyna Architecture</a></li>
<li class="chapter" data-level="9.2.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#model-representation"><i class="fa fa-check"></i><b>9.2.2</b> Model Representation</a></li>
<li class="chapter" data-level="9.2.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#convergence-properties"><i class="fa fa-check"></i><b>9.2.3</b> Convergence Properties</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-in-r"><i class="fa fa-check"></i><b>9.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#environment-setup"><i class="fa fa-check"></i><b>9.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="9.3.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#dyna-q-implementation"><i class="fa fa-check"></i><b>9.3.2</b> Dyna-Q Implementation</a></li>
<li class="chapter" data-level="9.3.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#standard-q-learning-for-comparison"><i class="fa fa-check"></i><b>9.3.3</b> Standard Q-Learning for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#experimental-analysis"><i class="fa fa-check"></i><b>9.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#learning-efficiency-comparison"><i class="fa fa-check"></i><b>9.4.1</b> Learning Efficiency Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
<li class="chapter" data-level="9.6" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-considerations-and-conclusion"><i class="fa fa-check"></i><b>9.6</b> Implementation Considerations and Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><i class="fa fa-check"></i><b>10</b> Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#theoretical-framework-1"><i class="fa fa-check"></i><b>10.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#the-exploration-bonus-mechanism"><i class="fa fa-check"></i><b>10.2.1</b> The Exploration Bonus Mechanism</a></li>
<li class="chapter" data-level="10.2.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#complete-dyna-q-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> Complete Dyna-Q+ Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#convergence-and-stability"><i class="fa fa-check"></i><b>10.2.3</b> Convergence and Stability</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#implementation-in-r-1"><i class="fa fa-check"></i><b>10.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#environment-setup-1"><i class="fa fa-check"></i><b>10.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="10.3.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-implementation-1"><i class="fa fa-check"></i><b>10.3.2</b> Dyna-Q+ Implementation</a></li>
<li class="chapter" data-level="10.3.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#standard-dyna-for-comparison"><i class="fa fa-check"></i><b>10.3.3</b> Standard Dyna for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#experimental-analysis-1"><i class="fa fa-check"></i><b>10.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#adaptation-to-environmental-changes"><i class="fa fa-check"></i><b>10.4.1</b> Adaptation to Environmental Changes</a></li>
<li class="chapter" data-level="10.4.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#parameter-sensitivity-analysis"><i class="fa fa-check"></i><b>10.4.2</b> Parameter Sensitivity Analysis</a></li>
<li class="chapter" data-level="10.4.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#exploration-pattern-analysis"><i class="fa fa-check"></i><b>10.4.3</b> Exploration Pattern Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#discussion-and-implementation-considerations"><i class="fa fa-check"></i><b>10.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="10.6" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#conclusion-6"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html"><i class="fa fa-check"></i><b>11</b> Function Approximation And Feature Engineering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#feature-engineering-and-state-representation"><i class="fa fa-check"></i><b>11.1</b> Feature Engineering and State Representation</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-discrimination-vs.-generalization-tradeoff"><i class="fa fa-check"></i><b>11.1.1</b> The Discrimination vs.Â Generalization Tradeoff</a></li>
<li class="chapter" data-level="11.1.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#principles-of-effective-feature-design"><i class="fa fa-check"></i><b>11.1.2</b> Principles of Effective Feature Design</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#mathematical-foundations-of-linear-function-approximation"><i class="fa fa-check"></i><b>11.2</b> Mathematical Foundations of Linear Function Approximation</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#linear-value-functions-and-their-properties"><i class="fa fa-check"></i><b>11.2.1</b> Linear Value Functions and Their Properties</a></li>
<li class="chapter" data-level="11.2.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#temporal-difference-learning-with-linear-approximation"><i class="fa fa-check"></i><b>11.2.2</b> Temporal Difference Learning with Linear Approximation</a></li>
<li class="chapter" data-level="11.2.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-deadly-triad-and-stability-concerns"><i class="fa fa-check"></i><b>11.2.3</b> The Deadly Triad and Stability Concerns</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#classical-basis-function-methods"><i class="fa fa-check"></i><b>11.3</b> Classical Basis Function Methods</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#coarse-coding-overlapping-receptive-fields"><i class="fa fa-check"></i><b>11.3.1</b> Coarse Coding: Overlapping Receptive Fields</a></li>
<li class="chapter" data-level="11.3.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#tile-coding-structured-overlapping-grids"><i class="fa fa-check"></i><b>11.3.2</b> Tile Coding: Structured Overlapping Grids</a></li>
<li class="chapter" data-level="11.3.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#radial-basis-functions-smooth-continuous-features"><i class="fa fa-check"></i><b>11.3.3</b> Radial Basis Functions: Smooth Continuous Features</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#comparative-analysis-and-practical-considerations"><i class="fa fa-check"></i><b>11.4</b> Comparative Analysis and Practical Considerations</a></li>
<li class="chapter" data-level="11.5" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#bridging-classical-and-modern-approaches"><i class="fa fa-check"></i><b>11.5</b> Bridging Classical and Modern Approaches</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html"><i class="fa fa-check"></i><b>12</b> Learning Policies versus Learning Values:</a>
<ul>
<li class="chapter" data-level="12.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#the-two-paradigms-of-reinforcement-learning"><i class="fa fa-check"></i><b>12.1</b> The Two Paradigms of Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#value-based-methods-learning-worth-before-action"><i class="fa fa-check"></i><b>12.1.1</b> Value-Based Methods: Learning Worth Before Action</a></li>
<li class="chapter" data-level="12.1.2" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#policy-based-methods-direct-optimization-of-behavior"><i class="fa fa-check"></i><b>12.1.2</b> Policy-Based Methods: Direct Optimization of Behavior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><i class="fa fa-check"></i><b>13</b> Average Reward in Reinforcement Learning: A Comprehensive Guide</a>
<ul>
<li class="chapter" data-level="13.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-limitations-of-discounted-reward-formulations"><i class="fa fa-check"></i><b>13.1</b> The Limitations of Discounted Reward Formulations</a></li>
<li class="chapter" data-level="13.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-average-reward-alternative-theoretical-foundations"><i class="fa fa-check"></i><b>13.2</b> The Average Reward Alternative: Theoretical Foundations</a></li>
<li class="chapter" data-level="13.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#value-functions-in-the-average-reward-setting"><i class="fa fa-check"></i><b>13.3</b> Value Functions in the Average Reward Setting</a></li>
<li class="chapter" data-level="13.4" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-theory-for-average-reward"><i class="fa fa-check"></i><b>13.4</b> Optimality Theory for Average Reward</a></li>
<li class="chapter" data-level="13.5" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#learning-algorithms-for-average-reward"><i class="fa fa-check"></i><b>13.5</b> Learning Algorithms for Average Reward</a></li>
<li class="chapter" data-level="13.6" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#practical-implementation-server-load-balancing"><i class="fa fa-check"></i><b>13.6</b> Practical Implementation: Server Load Balancing</a></li>
<li class="chapter" data-level="13.7" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#implementation-considerations"><i class="fa fa-check"></i><b>13.7</b> Implementation Considerations</a></li>
<li class="chapter" data-level="13.8" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#when-to-choose-average-reward-over-discounting"><i class="fa fa-check"></i><b>13.8</b> When to Choose Average Reward Over Discounting</a></li>
<li class="chapter" data-level="13.9" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#appendix-a-mathematical-proofs-and-derivations"><i class="fa fa-check"></i><b>13.9</b> Appendix A: Mathematical Proofs and Derivations</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#convergence-of-average-reward-td-learning"><i class="fa fa-check"></i><b>13.9.1</b> Convergence of Average Reward TD Learning</a></li>
<li class="chapter" data-level="13.9.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#policy-gradient-theorem-for-average-reward"><i class="fa fa-check"></i><b>13.9.2</b> Policy Gradient Theorem for Average Reward</a></li>
<li class="chapter" data-level="13.9.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-equations-derivation"><i class="fa fa-check"></i><b>13.9.3</b> Optimality Equations Derivation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="eligibility-traces.html"><a href="eligibility-traces.html"><i class="fa fa-check"></i><b>14</b> Eligibility Traces</a>
<ul>
<li class="chapter" data-level="14.1" data-path="eligibility-traces.html"><a href="eligibility-traces.html#from-one-step-to-multi-step-learning"><i class="fa fa-check"></i><b>14.1</b> From One-Step to Multi-Step Learning</a></li>
<li class="chapter" data-level="14.2" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-mechanics-of-eligibility-traces-a-backward-view"><i class="fa fa-check"></i><b>14.2</b> The Mechanics of Eligibility Traces: A Backward View</a></li>
<li class="chapter" data-level="14.3" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-tdlambda-algorithm-for-prediction"><i class="fa fa-check"></i><b>14.3</b> The TD(<span class="math inline">\(\\lambda\)</span>) Algorithm for Prediction</a></li>
<li class="chapter" data-level="14.4" data-path="eligibility-traces.html"><a href="eligibility-traces.html#control-with-eligibility-traces-average-reward-sarsalambda"><i class="fa fa-check"></i><b>14.4</b> Control with Eligibility Traces: Average Reward Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.5" data-path="eligibility-traces.html"><a href="eligibility-traces.html#practical-implementation-server-load-balancing-with-sarsalambda"><i class="fa fa-check"></i><b>14.5</b> Practical Implementation: Server Load Balancing with Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.6" data-path="eligibility-traces.html"><a href="eligibility-traces.html#computational-and-performance-considerations"><i class="fa fa-check"></i><b>14.6</b> Computational and Performance Considerations</a></li>
<li class="chapter" data-level="14.7" data-path="eligibility-traces.html"><a href="eligibility-traces.html#conclusion-8"><i class="fa fa-check"></i><b>14.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html"><i class="fa fa-check"></i><b>15</b> Intrinsic Rewards</a>
<ul>
<li class="chapter" data-level="15.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#the-sparse-reward-problem-and-its-implications"><i class="fa fa-check"></i><b>15.1</b> The Sparse Reward Problem and Its Implications</a></li>
<li class="chapter" data-level="15.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#mathematical-foundations-of-intrinsic-motivation"><i class="fa fa-check"></i><b>15.2</b> Mathematical Foundations of Intrinsic Motivation</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#curiosity-driven-learning"><i class="fa fa-check"></i><b>15.2.1</b> Curiosity-Driven Learning</a></li>
<li class="chapter" data-level="15.2.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#count-based-exploration"><i class="fa fa-check"></i><b>15.2.2</b> Count-Based Exploration</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#practical-implementation-curiosity-driven-grid-world-navigation"><i class="fa fa-check"></i><b>15.3</b> Practical Implementation: Curiosity-Driven Grid World Navigation</a></li>
<li class="chapter" data-level="15.4" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#advanced-intrinsic-motivation-mechanisms"><i class="fa fa-check"></i><b>15.4</b> Advanced Intrinsic Motivation Mechanisms</a></li>
<li class="chapter" data-level="15.5" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#implementation-considerations-and-practical-guidelines"><i class="fa fa-check"></i><b>15.5</b> Implementation Considerations and Practical Guidelines</a></li>
<li class="chapter" data-level="15.6" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#theoretical-analysis-and-convergence-properties"><i class="fa fa-check"></i><b>15.6</b> Theoretical Analysis and Convergence Properties</a></li>
<li class="chapter" data-level="15.7" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#applications-and-empirical-results"><i class="fa fa-check"></i><b>15.7</b> Applications and Empirical Results</a></li>
<li class="chapter" data-level="15.8" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#limitations-and-future-directions"><i class="fa fa-check"></i><b>15.8</b> Limitations and Future Directions</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>16</b> Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="16.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#introduction-9"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#theoretical-framework-2"><i class="fa fa-check"></i><b>16.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#the-policy-gradient-theorem-1"><i class="fa fa-check"></i><b>16.2.1</b> The Policy Gradient Theorem</a></li>
<li class="chapter" data-level="16.2.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-algorithm"><i class="fa fa-check"></i><b>16.2.2</b> REINFORCE Algorithm</a></li>
<li class="chapter" data-level="16.2.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#baseline-subtraction-and-variance-reduction"><i class="fa fa-check"></i><b>16.2.3</b> Baseline Subtraction and Variance Reduction</a></li>
<li class="chapter" data-level="16.2.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-parameterization"><i class="fa fa-check"></i><b>16.2.4</b> Softmax Policy Parameterization</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#implementation-in-r-2"><i class="fa fa-check"></i><b>16.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#environment-and-feature-representation"><i class="fa fa-check"></i><b>16.3.1</b> Environment and Feature Representation</a></li>
<li class="chapter" data-level="16.3.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-implementation"><i class="fa fa-check"></i><b>16.3.2</b> Softmax Policy Implementation</a></li>
<li class="chapter" data-level="16.3.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-implementation"><i class="fa fa-check"></i><b>16.3.3</b> REINFORCE Implementation</a></li>
<li class="chapter" data-level="16.3.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#actor-critic-implementation"><i class="fa fa-check"></i><b>16.3.4</b> Actor-Critic Implementation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#experimental-analysis-2"><i class="fa fa-check"></i><b>16.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#comparison-of-policy-gradient-variants"><i class="fa fa-check"></i><b>16.4.1</b> Comparison of Policy Gradient Variants</a></li>
<li class="chapter" data-level="16.4.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#learning-rate-sensitivity-analysis"><i class="fa fa-check"></i><b>16.4.2</b> Learning Rate Sensitivity Analysis</a></li>
<li class="chapter" data-level="16.4.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#variance-analysis-of-gradient-estimates"><i class="fa fa-check"></i><b>16.4.3</b> Variance Analysis of Gradient Estimates</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#discussion-and-implementation-considerations-1"><i class="fa fa-check"></i><b>16.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="16.6" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#conclusion-9"><i class="fa fa-check"></i><b>16.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>17</b> Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="17.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#theoretical-framework-3"><i class="fa fa-check"></i><b>17.1</b> Theoretical Framework</a></li>
<li class="chapter" data-level="17.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#implementation-and-comparative-analysis"><i class="fa fa-check"></i><b>17.2</b> Implementation and Comparative Analysis</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#variance-analysis-and-learning-dynamics"><i class="fa fa-check"></i><b>17.2.1</b> Variance Analysis and Learning Dynamics</a></li>
<li class="chapter" data-level="17.2.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#algorithmic-variants-and-extensions"><i class="fa fa-check"></i><b>17.2.2</b> Algorithmic Variants and Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#computational-and-convergence-considerations"><i class="fa fa-check"></i><b>17.3</b> Computational and Convergence Considerations</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#comparative-performance-analysis"><i class="fa fa-check"></i><b>17.3.1</b> Comparative Performance Analysis</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#conclusion-10"><i class="fa fa-check"></i><b>17.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>18</b> Appendix</a>
<ul>
<li class="chapter" data-level="18.1" data-path="appendix.html"><a href="appendix.html#comprehensive-reinforcement-learning-concepts-guide"><i class="fa fa-check"></i><b>18.1</b> Comprehensive Reinforcement Learning Concepts Guide</a></li>
<li class="chapter" data-level="18.2" data-path="appendix.html"><a href="appendix.html#learning-mechanisms"><i class="fa fa-check"></i><b>18.2</b> Learning Mechanisms</a></li>
<li class="chapter" data-level="18.3" data-path="appendix.html"><a href="appendix.html#environment-properties"><i class="fa fa-check"></i><b>18.3</b> Environment Properties</a></li>
<li class="chapter" data-level="18.4" data-path="appendix.html"><a href="appendix.html#learning-paradigms"><i class="fa fa-check"></i><b>18.4</b> Learning Paradigms</a></li>
<li class="chapter" data-level="18.5" data-path="appendix.html"><a href="appendix.html#exploration-strategies"><i class="fa fa-check"></i><b>18.5</b> Exploration Strategies</a></li>
<li class="chapter" data-level="18.6" data-path="appendix.html"><a href="appendix.html#key-algorithms-methods"><i class="fa fa-check"></i><b>18.6</b> Key Algorithms &amp; Methods</a></li>
<li class="chapter" data-level="18.7" data-path="appendix.html"><a href="appendix.html#advanced-concepts"><i class="fa fa-check"></i><b>18.7</b> Advanced Concepts</a></li>
<li class="chapter" data-level="18.8" data-path="appendix.html"><a href="appendix.html#fundamental-equations"><i class="fa fa-check"></i><b>18.8</b> Fundamental Equations</a>
<ul>
<li class="chapter" data-level="18.8.1" data-path="appendix.html"><a href="appendix.html#bellman-equations"><i class="fa fa-check"></i><b>18.8.1</b> <strong>Bellman Equations</strong></a></li>
<li class="chapter" data-level="18.8.2" data-path="appendix.html"><a href="appendix.html#policy-gradient-theorem"><i class="fa fa-check"></i><b>18.8.2</b> <strong>Policy Gradient Theorem</strong></a></li>
<li class="chapter" data-level="18.8.3" data-path="appendix.html"><a href="appendix.html#temporal-difference-error"><i class="fa fa-check"></i><b>18.8.3</b> <strong>Temporal Difference Error</strong></a></li>
</ul></li>
<li class="chapter" data-level="18.9" data-path="appendix.html"><a href="appendix.html#common-challenges-solutions"><i class="fa fa-check"></i><b>18.9</b> Common Challenges &amp; Solutions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning" class="section level1 hasAnchor" number="16">
<h1><span class="header-section-number">Chapter 16</span> Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-9" class="section level2 hasAnchor" number="16.1">
<h2><span class="header-section-number">16.1</span> Introduction<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#introduction-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While value-based methods like Q-learning and Dyna focus on learning the worth of actions to derive optimal policies, they represent an indirect approach to the fundamental problem of reinforcement learning: selecting good actions. Policy gradient methods take a more direct path, explicitly parameterizing and optimizing the policy itself. This shift in perspective opens new possibilities for handling complex action spaces, stochastic optimal policies, and scenarios where the policy structure itself carries important information.</p>
<p>The elegance of policy gradients lies in their conceptual clarity. Rather than learning values and hoping they translate into good action choices, we directly adjust the parameters that govern action selection. This approach proves particularly valuable when dealing with continuous action spaces where discrete value functions become unwieldy, or when the optimal policy is inherently stochastic and cannot be captured by deterministic value-based approaches.</p>
<p>REINFORCE, introduced by Williams in 1992, stands as the foundational policy gradient algorithm. Despite its simplicity, it embodies the core principles that underpin modern policy optimization methods. By treating each episode as a sample from the policyâs trajectory distribution and using the policy gradient theorem to estimate improvement directions, REINFORCE establishes a direct link between observed rewards and parameter updates.</p>
<p>The methodâs relationship to supervised learning is illuminating. Where supervised learning adjusts parameters to match target outputs, policy gradients adjust parameters to increase the probability of actions that led to high rewards. This connection reveals why policy gradients can struggle with high variance: unlike supervised learning with clear targets, reinforcement learning must estimate the value of actions from noisy, delayed rewards.</p>
</div>
<div id="theoretical-framework-2" class="section level2 hasAnchor" number="16.2">
<h2><span class="header-section-number">16.2</span> Theoretical Framework<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#theoretical-framework-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-policy-gradient-theorem-1" class="section level3 hasAnchor" number="16.2.1">
<h3><span class="header-section-number">16.2.1</span> The Policy Gradient Theorem<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#the-policy-gradient-theorem-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Policy gradient methods parameterize the policy as <span class="math inline">\(\pi(a|s, \theta)\)</span>, where <span class="math inline">\(\theta\)</span> represents learnable parameters. The objective is to maximize the expected return under this policy. For episodic tasks, we can express this as maximizing:</p>
<p><span class="math display">\[J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[R(\tau)]\]</span></p>
<p>where <span class="math inline">\(\tau\)</span> represents a trajectory <span class="math inline">\((s_0, a_0, r_1, s_1, a_1, r_2, \ldots, s_{T-1}, a_{T-1}, r_T, s_T)\)</span> and <span class="math inline">\(R(\tau) = \sum_{t=0}^{T-1} \gamma^t r_{t+1}\)</span> is the discounted return.</p>
<p>The fundamental challenge lies in computing the gradient <span class="math inline">\(\nabla_\theta J(\theta)\)</span>. The policy gradient theorem provides the solution:</p>
<p><span class="math display">\[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi(a_t|s_t, \theta) \cdot R_t(\tau)\right]\]</span></p>
<p>where <span class="math inline">\(R_t(\tau) = \sum_{k=t}^{T-1} \gamma^{k-t} r_{k+1}\)</span> is the return from time step <span class="math inline">\(t\)</span> onward.</p>
<p>This formulation reveals several key insights. The gradient is proportional to <span class="math inline">\(\nabla_\theta \log \pi(a_t|s_t, \theta)\)</span>, which increases the probability of actions when the return is positive and decreases it when negative. The magnitude of adjustment scales with the return magnitude, creating a natural weighting mechanism that emphasizes consequential experiences.</p>
</div>
<div id="reinforce-algorithm" class="section level3 hasAnchor" number="16.2.2">
<h3><span class="header-section-number">16.2.2</span> REINFORCE Algorithm<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The REINFORCE algorithm directly implements the policy gradient theorem through Monte Carlo estimation:</p>
<p><strong>For each episode</strong>:
1. Generate trajectory <span class="math inline">\(\tau = (s_0, a_0, r_1, \ldots, s_T)\)</span> following <span class="math inline">\(\pi(\cdot|s, \theta)\)</span>
2. For each time step <span class="math inline">\(t\)</span> in the trajectory:
- Compute return: <span class="math inline">\(R_t = \sum_{k=t}^{T-1} \gamma^{k-t} r_{k+1}\)</span>
- Compute gradient estimate: <span class="math inline">\(\widehat{g}_t = \nabla_\theta \log \pi(a_t|s_t, \theta) \cdot R_t\)</span>
3. Update parameters: <span class="math inline">\(\theta \leftarrow \theta + \alpha \sum_{t=0}^{T-1} \widehat{g}_t\)</span></p>
<p>The algorithmâs elegance comes with a cost: high variance in gradient estimates. Since each trajectory provides only one sample of the policyâs behavior, the gradient estimates can vary dramatically between episodes, leading to unstable learning. This variance stems from two sources: the inherent randomness in trajectory generation and the use of full episode returns, which accumulate noise across multiple time steps.</p>
</div>
<div id="baseline-subtraction-and-variance-reduction" class="section level3 hasAnchor" number="16.2.3">
<h3><span class="header-section-number">16.2.3</span> Baseline Subtraction and Variance Reduction<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#baseline-subtraction-and-variance-reduction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A crucial enhancement to basic REINFORCE involves subtracting a baseline from the returns without introducing bias. The modified gradient becomes:</p>
<p><span class="math display">\[\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi(a_t|s_t, \theta) \cdot (R_t(\tau) - b(s_t))\right]\]</span></p>
<p>The baseline <span class="math inline">\(b(s_t)\)</span> can be any function of the state that does not depend on the action. A natural choice is the state value function <span class="math inline">\(V(s_t)\)</span>, leading to the advantage function <span class="math inline">\(A(s_t, a_t) = R_t(\tau) - V(s_t)\)</span>. This represents how much better the chosen action was compared to the average expected return from that state.</p>
<p>When <span class="math inline">\(V(s_t)\)</span> is approximated by a learned value function, the resulting algorithm becomes Actor-Critic, where the policy (actor) is updated using advantages estimated by the value function (critic). This combination leverages both the direct policy optimization of policy gradients and the sample efficiency of value-based methods.</p>
</div>
<div id="softmax-policy-parameterization" class="section level3 hasAnchor" number="16.2.4">
<h3><span class="header-section-number">16.2.4</span> Softmax Policy Parameterization<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-parameterization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For discrete action spaces, a common parameterization uses the softmax function:</p>
<p><span class="math display">\[\pi(a|s, \theta) = \frac{e^{\theta_a^T \phi(s)}}{\sum_{a&#39;} e^{\theta_{a&#39;}^T \phi(s)}}\]</span></p>
<p>where <span class="math inline">\(\phi(s)\)</span> represents state features and <span class="math inline">\(\theta_a\)</span> are the parameters for action <span class="math inline">\(a\)</span>. The log probability becomes:</p>
<p><span class="math display">\[\log \pi(a|s, \theta) = \theta_a^T \phi(s) - \log \sum_{a&#39;} e^{\theta_{a&#39;}^T \phi(s)}\]</span></p>
<p>The gradient with respect to the parameters for action <span class="math inline">\(a\)</span> is:</p>
<p><span class="math display">\[\nabla_{\theta_a} \log \pi(a|s, \theta) = \phi(s) - \pi(a|s, \theta) \phi(s) = \phi(s)(1 - \pi(a|s, \theta))\]</span></p>
<p>For actions not taken, the gradient is simply <span class="math inline">\(-\pi(a&#39;|s, \theta) \phi(s)\)</span>, which decreases their probabilities proportionally.</p>
</div>
</div>
<div id="implementation-in-r-2" class="section level2 hasAnchor" number="16.3">
<h2><span class="header-section-number">16.3</span> Implementation in R<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#implementation-in-r-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Building upon our previous reinforcement learning implementations, we now develop a policy gradient framework that directly optimizes action selection policies.</p>
<div id="environment-and-feature-representation" class="section level3 hasAnchor" number="16.3.1">
<h3><span class="header-section-number">16.3.1</span> Environment and Feature Representation<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#environment-and-feature-representation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We begin with the same 10-state environment used in our Dyna examples, but now we need to consider how to represent states for policy parameterization:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-1" tabindex="-1"></a><span class="co"># Environment parameters</span></span>
<span id="cb104-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-2" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb104-3"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-3" tabindex="-1"></a>n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb104-4"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-4" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.95</span></span>
<span id="cb104-5"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-5" tabindex="-1"></a>terminal_state <span class="ot">&lt;-</span> n_states</span>
<span id="cb104-6"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-6" tabindex="-1"></a></span>
<span id="cb104-7"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-7" tabindex="-1"></a><span class="co"># Environment setup (consistent with previous examples)</span></span>
<span id="cb104-8"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-8" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb104-9"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-9" tabindex="-1"></a>transition_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb104-10"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-10" tabindex="-1"></a>reward_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb104-11"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-11" tabindex="-1"></a></span>
<span id="cb104-12"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-12" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>)) {</span>
<span id="cb104-13"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-13" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, s <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb104-14"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-14" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb104-15"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-15" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb104-16"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-16" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb104-17"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-17" tabindex="-1"></a>  </span>
<span id="cb104-18"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-18" tabindex="-1"></a>  <span class="cf">for</span> (s_prime <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_states) {</span>
<span id="cb104-19"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-19" tabindex="-1"></a>    reward_model[s, <span class="dv">1</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">1.0</span>, <span class="fl">0.1</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb104-20"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-20" tabindex="-1"></a>    reward_model[s, <span class="dv">2</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">0.5</span>, <span class="fl">0.05</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb104-21"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-21" tabindex="-1"></a>  }</span>
<span id="cb104-22"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-22" tabindex="-1"></a>}</span>
<span id="cb104-23"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-23" tabindex="-1"></a></span>
<span id="cb104-24"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-24" tabindex="-1"></a>transition_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb104-25"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-25" tabindex="-1"></a>reward_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb104-26"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-26" tabindex="-1"></a></span>
<span id="cb104-27"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-27" tabindex="-1"></a><span class="co"># Environment interaction function</span></span>
<span id="cb104-28"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-28" tabindex="-1"></a>sample_env <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a) {</span>
<span id="cb104-29"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-29" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> transition_model[s, a, ]</span>
<span id="cb104-30"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-30" tabindex="-1"></a>  s_prime <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>, <span class="at">prob =</span> probs)</span>
<span id="cb104-31"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-31" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> reward_model[s, a, s_prime]</span>
<span id="cb104-32"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-32" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">s_prime =</span> s_prime, <span class="at">reward =</span> reward)</span>
<span id="cb104-33"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-33" tabindex="-1"></a>}</span>
<span id="cb104-34"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-34" tabindex="-1"></a></span>
<span id="cb104-35"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-35" tabindex="-1"></a><span class="co"># Feature extraction function</span></span>
<span id="cb104-36"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-36" tabindex="-1"></a>extract_features <span class="ot">&lt;-</span> <span class="cf">function</span>(state) {</span>
<span id="cb104-37"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-37" tabindex="-1"></a>  <span class="co"># Simple one-hot encoding</span></span>
<span id="cb104-38"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-38" tabindex="-1"></a>  features <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb104-39"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-39" tabindex="-1"></a>  features[state] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb104-40"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-40" tabindex="-1"></a>  <span class="fu">return</span>(features)</span>
<span id="cb104-41"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb104-41" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="softmax-policy-implementation" class="section level3 hasAnchor" number="16.3.2">
<h3><span class="header-section-number">16.3.2</span> Softmax Policy Implementation<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We implement a softmax policy that maps state features to action probabilities:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-1" tabindex="-1"></a>softmax_policy <span class="ot">&lt;-</span> <span class="cf">function</span>(state, theta) {</span>
<span id="cb105-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-2" tabindex="-1"></a>  features <span class="ot">&lt;-</span> <span class="fu">extract_features</span>(state)</span>
<span id="cb105-3"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-3" tabindex="-1"></a>  </span>
<span id="cb105-4"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-4" tabindex="-1"></a>  <span class="co"># Compute logits for each action</span></span>
<span id="cb105-5"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-5" tabindex="-1"></a>  logits <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_actions)</span>
<span id="cb105-6"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-6" tabindex="-1"></a>  <span class="cf">for</span> (a <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_actions) {</span>
<span id="cb105-7"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-7" tabindex="-1"></a>    <span class="co"># Each action has its own parameter vector</span></span>
<span id="cb105-8"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-8" tabindex="-1"></a>    action_params <span class="ot">&lt;-</span> theta[((a<span class="dv">-1</span>) <span class="sc">*</span> n_states <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(a <span class="sc">*</span> n_states)]</span>
<span id="cb105-9"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-9" tabindex="-1"></a>    logits[a] <span class="ot">&lt;-</span> <span class="fu">sum</span>(action_params <span class="sc">*</span> features)</span>
<span id="cb105-10"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-10" tabindex="-1"></a>  }</span>
<span id="cb105-11"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-11" tabindex="-1"></a>  </span>
<span id="cb105-12"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-12" tabindex="-1"></a>  <span class="co"># Apply softmax</span></span>
<span id="cb105-13"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-13" tabindex="-1"></a>  exp_logits <span class="ot">&lt;-</span> <span class="fu">exp</span>(logits <span class="sc">-</span> <span class="fu">max</span>(logits))  <span class="co"># Subtract max for numerical stability</span></span>
<span id="cb105-14"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-14" tabindex="-1"></a>  probabilities <span class="ot">&lt;-</span> exp_logits <span class="sc">/</span> <span class="fu">sum</span>(exp_logits)</span>
<span id="cb105-15"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-15" tabindex="-1"></a>  </span>
<span id="cb105-16"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-16" tabindex="-1"></a>  <span class="fu">return</span>(probabilities)</span>
<span id="cb105-17"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-17" tabindex="-1"></a>}</span>
<span id="cb105-18"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-18" tabindex="-1"></a></span>
<span id="cb105-19"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-19" tabindex="-1"></a><span class="co"># Sample action from policy</span></span>
<span id="cb105-20"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-20" tabindex="-1"></a>sample_action <span class="ot">&lt;-</span> <span class="cf">function</span>(state, theta) {</span>
<span id="cb105-21"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-21" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> <span class="fu">softmax_policy</span>(state, theta)</span>
<span id="cb105-22"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-22" tabindex="-1"></a>  action <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>, <span class="at">prob =</span> probs)</span>
<span id="cb105-23"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-23" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">action =</span> action, <span class="at">probability =</span> probs[action]))</span>
<span id="cb105-24"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-24" tabindex="-1"></a>}</span>
<span id="cb105-25"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-25" tabindex="-1"></a></span>
<span id="cb105-26"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-26" tabindex="-1"></a><span class="co"># Compute log probability of an action</span></span>
<span id="cb105-27"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-27" tabindex="-1"></a>log_prob <span class="ot">&lt;-</span> <span class="cf">function</span>(state, action, theta) {</span>
<span id="cb105-28"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-28" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> <span class="fu">softmax_policy</span>(state, theta)</span>
<span id="cb105-29"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-29" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">log</span>(probs[action]))</span>
<span id="cb105-30"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-30" tabindex="-1"></a>}</span>
<span id="cb105-31"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-31" tabindex="-1"></a></span>
<span id="cb105-32"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-32" tabindex="-1"></a><span class="co"># Compute gradient of log probability</span></span>
<span id="cb105-33"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-33" tabindex="-1"></a>grad_log_prob <span class="ot">&lt;-</span> <span class="cf">function</span>(state, action, theta) {</span>
<span id="cb105-34"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-34" tabindex="-1"></a>  features <span class="ot">&lt;-</span> <span class="fu">extract_features</span>(state)</span>
<span id="cb105-35"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-35" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> <span class="fu">softmax_policy</span>(state, theta)</span>
<span id="cb105-36"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-36" tabindex="-1"></a>  </span>
<span id="cb105-37"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-37" tabindex="-1"></a>  <span class="co"># Initialize gradient vector</span></span>
<span id="cb105-38"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-38" tabindex="-1"></a>  grad <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, <span class="fu">length</span>(theta))</span>
<span id="cb105-39"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-39" tabindex="-1"></a>  </span>
<span id="cb105-40"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-40" tabindex="-1"></a>  <span class="cf">for</span> (a <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_actions) {</span>
<span id="cb105-41"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-41" tabindex="-1"></a>    param_indices <span class="ot">&lt;-</span> ((a<span class="dv">-1</span>) <span class="sc">*</span> n_states <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">:</span>(a <span class="sc">*</span> n_states)</span>
<span id="cb105-42"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-42" tabindex="-1"></a>    <span class="cf">if</span> (a <span class="sc">==</span> action) {</span>
<span id="cb105-43"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-43" tabindex="-1"></a>      <span class="co"># Chosen action: gradient is features * (1 - probability)</span></span>
<span id="cb105-44"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-44" tabindex="-1"></a>      grad[param_indices] <span class="ot">&lt;-</span> features <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> probs[a])</span>
<span id="cb105-45"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-45" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb105-46"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-46" tabindex="-1"></a>      <span class="co"># Unchosen action: gradient is -features * probability</span></span>
<span id="cb105-47"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-47" tabindex="-1"></a>      grad[param_indices] <span class="ot">&lt;-</span> <span class="sc">-</span>features <span class="sc">*</span> probs[a]</span>
<span id="cb105-48"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-48" tabindex="-1"></a>    }</span>
<span id="cb105-49"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-49" tabindex="-1"></a>  }</span>
<span id="cb105-50"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-50" tabindex="-1"></a>  </span>
<span id="cb105-51"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-51" tabindex="-1"></a>  <span class="fu">return</span>(grad)</span>
<span id="cb105-52"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb105-52" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="reinforce-implementation" class="section level3 hasAnchor" number="16.3.3">
<h3><span class="header-section-number">16.3.3</span> REINFORCE Implementation<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The core REINFORCE algorithm generates episodes, computes returns, and updates policy parameters:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-1" tabindex="-1"></a>reinforce <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha =</span> <span class="fl">0.01</span>, <span class="at">baseline =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb106-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-2" tabindex="-1"></a>  <span class="co"># Initialize policy parameters</span></span>
<span id="cb106-3"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-3" tabindex="-1"></a>  n_params <span class="ot">&lt;-</span> n_states <span class="sc">*</span> n_actions</span>
<span id="cb106-4"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-4" tabindex="-1"></a>  theta <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_params, <span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb106-5"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-5" tabindex="-1"></a>  </span>
<span id="cb106-6"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-6" tabindex="-1"></a>  <span class="co"># Storage for baseline if used</span></span>
<span id="cb106-7"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-7" tabindex="-1"></a>  <span class="cf">if</span> (baseline) {</span>
<span id="cb106-8"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-8" tabindex="-1"></a>    baseline_values <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb106-9"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-9" tabindex="-1"></a>    baseline_counts <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb106-10"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-10" tabindex="-1"></a>  }</span>
<span id="cb106-11"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-11" tabindex="-1"></a>  </span>
<span id="cb106-12"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-12" tabindex="-1"></a>  <span class="co"># Storage for performance tracking</span></span>
<span id="cb106-13"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-13" tabindex="-1"></a>  episode_returns <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb106-14"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-14" tabindex="-1"></a>  episode_lengths <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb106-15"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-15" tabindex="-1"></a>  </span>
<span id="cb106-16"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-16" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb106-17"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-17" tabindex="-1"></a>    <span class="co"># Generate episode</span></span>
<span id="cb106-18"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-18" tabindex="-1"></a>    trajectory <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb106-19"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-19" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb106-20"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-20" tabindex="-1"></a>    step <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb106-21"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-21" tabindex="-1"></a>    </span>
<span id="cb106-22"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-22" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state <span class="sc">&amp;&amp;</span> step <span class="sc">&lt;</span> <span class="dv">100</span>) {  <span class="co"># Prevent infinite episodes</span></span>
<span id="cb106-23"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-23" tabindex="-1"></a>      step <span class="ot">&lt;-</span> step <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb106-24"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-24" tabindex="-1"></a>      </span>
<span id="cb106-25"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-25" tabindex="-1"></a>      <span class="co"># Sample action from current policy</span></span>
<span id="cb106-26"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-26" tabindex="-1"></a>      action_result <span class="ot">&lt;-</span> <span class="fu">sample_action</span>(s, theta)</span>
<span id="cb106-27"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-27" tabindex="-1"></a>      a <span class="ot">&lt;-</span> action_result<span class="sc">$</span>action</span>
<span id="cb106-28"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-28" tabindex="-1"></a>      </span>
<span id="cb106-29"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-29" tabindex="-1"></a>      <span class="co"># Take action and observe outcome</span></span>
<span id="cb106-30"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-30" tabindex="-1"></a>      outcome <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a)</span>
<span id="cb106-31"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-31" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> outcome<span class="sc">$</span>s_prime</span>
<span id="cb106-32"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-32" tabindex="-1"></a>      r <span class="ot">&lt;-</span> outcome<span class="sc">$</span>reward</span>
<span id="cb106-33"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-33" tabindex="-1"></a>      </span>
<span id="cb106-34"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-34" tabindex="-1"></a>      <span class="co"># Store experience</span></span>
<span id="cb106-35"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-35" tabindex="-1"></a>      trajectory[[step]] <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb106-36"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-36" tabindex="-1"></a>        <span class="at">state =</span> s,</span>
<span id="cb106-37"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-37" tabindex="-1"></a>        <span class="at">action =</span> a,</span>
<span id="cb106-38"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-38" tabindex="-1"></a>        <span class="at">reward =</span> r,</span>
<span id="cb106-39"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-39" tabindex="-1"></a>        <span class="at">log_prob =</span> <span class="fu">log_prob</span>(s, a, theta)</span>
<span id="cb106-40"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-40" tabindex="-1"></a>      )</span>
<span id="cb106-41"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-41" tabindex="-1"></a>      </span>
<span id="cb106-42"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-42" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb106-43"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-43" tabindex="-1"></a>    }</span>
<span id="cb106-44"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-44" tabindex="-1"></a>    </span>
<span id="cb106-45"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-45" tabindex="-1"></a>    <span class="co"># Compute returns for each time step</span></span>
<span id="cb106-46"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-46" tabindex="-1"></a>    T <span class="ot">&lt;-</span> <span class="fu">length</span>(trajectory)</span>
<span id="cb106-47"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-47" tabindex="-1"></a>    returns <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb106-48"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-48" tabindex="-1"></a>    G <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb106-49"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-49" tabindex="-1"></a>    </span>
<span id="cb106-50"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-50" tabindex="-1"></a>    <span class="cf">for</span> (t <span class="cf">in</span> T<span class="sc">:</span><span class="dv">1</span>) {</span>
<span id="cb106-51"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-51" tabindex="-1"></a>      G <span class="ot">&lt;-</span> trajectory[[t]]<span class="sc">$</span>reward <span class="sc">+</span> gamma <span class="sc">*</span> G</span>
<span id="cb106-52"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-52" tabindex="-1"></a>      returns[t] <span class="ot">&lt;-</span> G</span>
<span id="cb106-53"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-53" tabindex="-1"></a>    }</span>
<span id="cb106-54"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-54" tabindex="-1"></a>    </span>
<span id="cb106-55"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-55" tabindex="-1"></a>    <span class="co"># Update baseline if used</span></span>
<span id="cb106-56"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-56" tabindex="-1"></a>    <span class="cf">if</span> (baseline <span class="sc">&amp;&amp;</span> T <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb106-57"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-57" tabindex="-1"></a>      <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T) {</span>
<span id="cb106-58"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-58" tabindex="-1"></a>        state <span class="ot">&lt;-</span> trajectory[[t]]<span class="sc">$</span>state</span>
<span id="cb106-59"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-59" tabindex="-1"></a>        baseline_counts[state] <span class="ot">&lt;-</span> baseline_counts[state] <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb106-60"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-60" tabindex="-1"></a>        <span class="co"># Running average update</span></span>
<span id="cb106-61"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-61" tabindex="-1"></a>        baseline_values[state] <span class="ot">&lt;-</span> baseline_values[state] <span class="sc">+</span> </span>
<span id="cb106-62"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-62" tabindex="-1"></a>          (returns[t] <span class="sc">-</span> baseline_values[state]) <span class="sc">/</span> baseline_counts[state]</span>
<span id="cb106-63"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-63" tabindex="-1"></a>      }</span>
<span id="cb106-64"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-64" tabindex="-1"></a>    }</span>
<span id="cb106-65"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-65" tabindex="-1"></a>    </span>
<span id="cb106-66"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-66" tabindex="-1"></a>    <span class="co"># Compute policy gradient and update parameters</span></span>
<span id="cb106-67"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-67" tabindex="-1"></a>    gradient <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_params)</span>
<span id="cb106-68"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-68" tabindex="-1"></a>    </span>
<span id="cb106-69"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-69" tabindex="-1"></a>    <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T) {</span>
<span id="cb106-70"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-70" tabindex="-1"></a>      state <span class="ot">&lt;-</span> trajectory[[t]]<span class="sc">$</span>state</span>
<span id="cb106-71"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-71" tabindex="-1"></a>      action <span class="ot">&lt;-</span> trajectory[[t]]<span class="sc">$</span>action</span>
<span id="cb106-72"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-72" tabindex="-1"></a>      </span>
<span id="cb106-73"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-73" tabindex="-1"></a>      <span class="co"># Advantage estimation</span></span>
<span id="cb106-74"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-74" tabindex="-1"></a>      advantage <span class="ot">&lt;-</span> returns[t]</span>
<span id="cb106-75"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-75" tabindex="-1"></a>      <span class="cf">if</span> (baseline) {</span>
<span id="cb106-76"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-76" tabindex="-1"></a>        advantage <span class="ot">&lt;-</span> advantage <span class="sc">-</span> baseline_values[state]</span>
<span id="cb106-77"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-77" tabindex="-1"></a>      }</span>
<span id="cb106-78"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-78" tabindex="-1"></a>      </span>
<span id="cb106-79"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-79" tabindex="-1"></a>      <span class="co"># Accumulate gradient</span></span>
<span id="cb106-80"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-80" tabindex="-1"></a>      grad_log_pi <span class="ot">&lt;-</span> <span class="fu">grad_log_prob</span>(state, action, theta)</span>
<span id="cb106-81"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-81" tabindex="-1"></a>      gradient <span class="ot">&lt;-</span> gradient <span class="sc">+</span> grad_log_pi <span class="sc">*</span> advantage</span>
<span id="cb106-82"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-82" tabindex="-1"></a>    }</span>
<span id="cb106-83"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-83" tabindex="-1"></a>    </span>
<span id="cb106-84"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-84" tabindex="-1"></a>    <span class="co"># Update parameters</span></span>
<span id="cb106-85"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-85" tabindex="-1"></a>    theta <span class="ot">&lt;-</span> theta <span class="sc">+</span> alpha <span class="sc">*</span> gradient</span>
<span id="cb106-86"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-86" tabindex="-1"></a>    </span>
<span id="cb106-87"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-87" tabindex="-1"></a>    <span class="co"># Record performance</span></span>
<span id="cb106-88"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-88" tabindex="-1"></a>    <span class="cf">if</span> (T <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb106-89"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-89" tabindex="-1"></a>      episode_returns[ep] <span class="ot">&lt;-</span> returns[<span class="dv">1</span>]  <span class="co"># Total episode return</span></span>
<span id="cb106-90"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-90" tabindex="-1"></a>      episode_lengths[ep] <span class="ot">&lt;-</span> T</span>
<span id="cb106-91"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-91" tabindex="-1"></a>    }</span>
<span id="cb106-92"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-92" tabindex="-1"></a>  }</span>
<span id="cb106-93"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-93" tabindex="-1"></a>  </span>
<span id="cb106-94"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-94" tabindex="-1"></a>  result <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb106-95"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-95" tabindex="-1"></a>    <span class="at">theta =</span> theta,</span>
<span id="cb106-96"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-96" tabindex="-1"></a>    <span class="at">episode_returns =</span> episode_returns,</span>
<span id="cb106-97"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-97" tabindex="-1"></a>    <span class="at">episode_lengths =</span> episode_lengths</span>
<span id="cb106-98"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-98" tabindex="-1"></a>  )</span>
<span id="cb106-99"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-99" tabindex="-1"></a>  </span>
<span id="cb106-100"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-100" tabindex="-1"></a>  <span class="cf">if</span> (baseline) {</span>
<span id="cb106-101"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-101" tabindex="-1"></a>    result<span class="sc">$</span>baseline_values <span class="ot">&lt;-</span> baseline_values</span>
<span id="cb106-102"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-102" tabindex="-1"></a>  }</span>
<span id="cb106-103"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-103" tabindex="-1"></a>  </span>
<span id="cb106-104"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-104" tabindex="-1"></a>  <span class="fu">return</span>(result)</span>
<span id="cb106-105"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb106-105" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="actor-critic-implementation" class="section level3 hasAnchor" number="16.3.4">
<h3><span class="header-section-number">16.3.4</span> Actor-Critic Implementation<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#actor-critic-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Building upon REINFORCE, we implement a basic Actor-Critic method that learns both a policy and a value function:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-1" tabindex="-1"></a>actor_critic <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha_actor =</span> <span class="fl">0.01</span>, <span class="at">alpha_critic =</span> <span class="fl">0.1</span>) {</span>
<span id="cb107-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-2" tabindex="-1"></a>  <span class="co"># Initialize policy parameters (actor)</span></span>
<span id="cb107-3"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-3" tabindex="-1"></a>  n_params <span class="ot">&lt;-</span> n_states <span class="sc">*</span> n_actions</span>
<span id="cb107-4"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-4" tabindex="-1"></a>  theta <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_params, <span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb107-5"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-5" tabindex="-1"></a>  </span>
<span id="cb107-6"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-6" tabindex="-1"></a>  <span class="co"># Initialize value function parameters (critic)</span></span>
<span id="cb107-7"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-7" tabindex="-1"></a>  v_weights <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_states, <span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb107-8"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-8" tabindex="-1"></a>  </span>
<span id="cb107-9"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-9" tabindex="-1"></a>  <span class="co"># Performance tracking</span></span>
<span id="cb107-10"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-10" tabindex="-1"></a>  episode_returns <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb107-11"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-11" tabindex="-1"></a>  episode_lengths <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb107-12"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-12" tabindex="-1"></a>  td_errors <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb107-13"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-13" tabindex="-1"></a>  </span>
<span id="cb107-14"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-14" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb107-15"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-15" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb107-16"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-16" tabindex="-1"></a>    step <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb107-17"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-17" tabindex="-1"></a>    episode_td_errors <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb107-18"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-18" tabindex="-1"></a>    </span>
<span id="cb107-19"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-19" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state <span class="sc">&amp;&amp;</span> step <span class="sc">&lt;</span> <span class="dv">100</span>) {</span>
<span id="cb107-20"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-20" tabindex="-1"></a>      step <span class="ot">&lt;-</span> step <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb107-21"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-21" tabindex="-1"></a>      </span>
<span id="cb107-22"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-22" tabindex="-1"></a>      <span class="co"># Current state value</span></span>
<span id="cb107-23"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-23" tabindex="-1"></a>      v_s <span class="ot">&lt;-</span> v_weights[s]</span>
<span id="cb107-24"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-24" tabindex="-1"></a>      </span>
<span id="cb107-25"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-25" tabindex="-1"></a>      <span class="co"># Sample action from policy</span></span>
<span id="cb107-26"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-26" tabindex="-1"></a>      action_result <span class="ot">&lt;-</span> <span class="fu">sample_action</span>(s, theta)</span>
<span id="cb107-27"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-27" tabindex="-1"></a>      a <span class="ot">&lt;-</span> action_result<span class="sc">$</span>action</span>
<span id="cb107-28"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-28" tabindex="-1"></a>      </span>
<span id="cb107-29"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-29" tabindex="-1"></a>      <span class="co"># Take action</span></span>
<span id="cb107-30"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-30" tabindex="-1"></a>      outcome <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a)</span>
<span id="cb107-31"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-31" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> outcome<span class="sc">$</span>s_prime</span>
<span id="cb107-32"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-32" tabindex="-1"></a>      r <span class="ot">&lt;-</span> outcome<span class="sc">$</span>reward</span>
<span id="cb107-33"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-33" tabindex="-1"></a>      </span>
<span id="cb107-34"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-34" tabindex="-1"></a>      <span class="co"># Next state value (0 if terminal)</span></span>
<span id="cb107-35"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-35" tabindex="-1"></a>      v_s_prime <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> terminal_state, <span class="dv">0</span>, v_weights[s_prime])</span>
<span id="cb107-36"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-36" tabindex="-1"></a>      </span>
<span id="cb107-37"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-37" tabindex="-1"></a>      <span class="co"># TD error (this is our advantage estimate)</span></span>
<span id="cb107-38"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-38" tabindex="-1"></a>      td_error <span class="ot">&lt;-</span> r <span class="sc">+</span> gamma <span class="sc">*</span> v_s_prime <span class="sc">-</span> v_s</span>
<span id="cb107-39"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-39" tabindex="-1"></a>      episode_td_errors <span class="ot">&lt;-</span> <span class="fu">c</span>(episode_td_errors, td_error)</span>
<span id="cb107-40"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-40" tabindex="-1"></a>      </span>
<span id="cb107-41"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-41" tabindex="-1"></a>      <span class="co"># Update critic (value function)</span></span>
<span id="cb107-42"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-42" tabindex="-1"></a>      v_weights[s] <span class="ot">&lt;-</span> v_weights[s] <span class="sc">+</span> alpha_critic <span class="sc">*</span> td_error</span>
<span id="cb107-43"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-43" tabindex="-1"></a>      </span>
<span id="cb107-44"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-44" tabindex="-1"></a>      <span class="co"># Update actor (policy)</span></span>
<span id="cb107-45"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-45" tabindex="-1"></a>      grad_log_pi <span class="ot">&lt;-</span> <span class="fu">grad_log_prob</span>(s, a, theta)</span>
<span id="cb107-46"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-46" tabindex="-1"></a>      theta <span class="ot">&lt;-</span> theta <span class="sc">+</span> alpha_actor <span class="sc">*</span> grad_log_pi <span class="sc">*</span> td_error</span>
<span id="cb107-47"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-47" tabindex="-1"></a>      </span>
<span id="cb107-48"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-48" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb107-49"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-49" tabindex="-1"></a>    }</span>
<span id="cb107-50"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-50" tabindex="-1"></a>    </span>
<span id="cb107-51"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-51" tabindex="-1"></a>    <span class="co"># Record episode statistics</span></span>
<span id="cb107-52"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-52" tabindex="-1"></a>    episode_lengths[ep] <span class="ot">&lt;-</span> step</span>
<span id="cb107-53"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-53" tabindex="-1"></a>    td_errors[[ep]] <span class="ot">&lt;-</span> episode_td_errors</span>
<span id="cb107-54"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-54" tabindex="-1"></a>    </span>
<span id="cb107-55"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-55" tabindex="-1"></a>    <span class="co"># Estimate episode return by running a test episode</span></span>
<span id="cb107-56"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-56" tabindex="-1"></a>    <span class="cf">if</span> (ep <span class="sc">%%</span> <span class="dv">50</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb107-57"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-57" tabindex="-1"></a>      test_return <span class="ot">&lt;-</span> <span class="fu">evaluate_policy</span>(theta)</span>
<span id="cb107-58"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-58" tabindex="-1"></a>      episode_returns[ep] <span class="ot">&lt;-</span> test_return</span>
<span id="cb107-59"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-59" tabindex="-1"></a>    }</span>
<span id="cb107-60"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-60" tabindex="-1"></a>  }</span>
<span id="cb107-61"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-61" tabindex="-1"></a>  </span>
<span id="cb107-62"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-62" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb107-63"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-63" tabindex="-1"></a>    <span class="at">theta =</span> theta,</span>
<span id="cb107-64"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-64" tabindex="-1"></a>    <span class="at">v_weights =</span> v_weights,</span>
<span id="cb107-65"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-65" tabindex="-1"></a>    <span class="at">episode_returns =</span> episode_returns,</span>
<span id="cb107-66"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-66" tabindex="-1"></a>    <span class="at">episode_lengths =</span> episode_lengths,</span>
<span id="cb107-67"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-67" tabindex="-1"></a>    <span class="at">td_errors =</span> td_errors</span>
<span id="cb107-68"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-68" tabindex="-1"></a>  ))</span>
<span id="cb107-69"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-69" tabindex="-1"></a>}</span>
<span id="cb107-70"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-70" tabindex="-1"></a></span>
<span id="cb107-71"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-71" tabindex="-1"></a><span class="co"># Helper function to evaluate current policy</span></span>
<span id="cb107-72"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-72" tabindex="-1"></a>evaluate_policy <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, <span class="at">n_episodes =</span> <span class="dv">10</span>) {</span>
<span id="cb107-73"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-73" tabindex="-1"></a>  total_return <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb107-74"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-74" tabindex="-1"></a>  </span>
<span id="cb107-75"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-75" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_episodes) {</span>
<span id="cb107-76"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-76" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb107-77"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-77" tabindex="-1"></a>    episode_return <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb107-78"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-78" tabindex="-1"></a>    step <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb107-79"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-79" tabindex="-1"></a>    </span>
<span id="cb107-80"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-80" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state <span class="sc">&amp;&amp;</span> step <span class="sc">&lt;</span> <span class="dv">100</span>) {</span>
<span id="cb107-81"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-81" tabindex="-1"></a>      step <span class="ot">&lt;-</span> step <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb107-82"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-82" tabindex="-1"></a>      action_result <span class="ot">&lt;-</span> <span class="fu">sample_action</span>(s, theta)</span>
<span id="cb107-83"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-83" tabindex="-1"></a>      outcome <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, action_result<span class="sc">$</span>action)</span>
<span id="cb107-84"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-84" tabindex="-1"></a>      episode_return <span class="ot">&lt;-</span> episode_return <span class="sc">+</span> outcome<span class="sc">$</span>reward <span class="sc">*</span> (gamma <span class="sc">^</span> (step <span class="sc">-</span> <span class="dv">1</span>))</span>
<span id="cb107-85"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-85" tabindex="-1"></a>      s <span class="ot">&lt;-</span> outcome<span class="sc">$</span>s_prime</span>
<span id="cb107-86"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-86" tabindex="-1"></a>    }</span>
<span id="cb107-87"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-87" tabindex="-1"></a>    </span>
<span id="cb107-88"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-88" tabindex="-1"></a>    total_return <span class="ot">&lt;-</span> total_return <span class="sc">+</span> episode_return</span>
<span id="cb107-89"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-89" tabindex="-1"></a>  }</span>
<span id="cb107-90"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-90" tabindex="-1"></a>  </span>
<span id="cb107-91"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-91" tabindex="-1"></a>  <span class="fu">return</span>(total_return <span class="sc">/</span> n_episodes)</span>
<span id="cb107-92"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb107-92" tabindex="-1"></a>}</span></code></pre></div>
</div>
</div>
<div id="experimental-analysis-2" class="section level2 hasAnchor" number="16.4">
<h2><span class="header-section-number">16.4</span> Experimental Analysis<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#experimental-analysis-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="comparison-of-policy-gradient-variants" class="section level3 hasAnchor" number="16.4.1">
<h3><span class="header-section-number">16.4.1</span> Comparison of Policy Gradient Variants<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#comparison-of-policy-gradient-variants" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We compare REINFORCE with and without baselines, and Actor-Critic to understand their relative performance characteristics:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-1" tabindex="-1"></a>comparative_experiment <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb108-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-2" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb108-3"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-3" tabindex="-1"></a>  </span>
<span id="cb108-4"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-4" tabindex="-1"></a>  <span class="co"># Run different variants</span></span>
<span id="cb108-5"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-5" tabindex="-1"></a>  reinforce_basic <span class="ot">&lt;-</span> <span class="fu">reinforce</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha =</span> <span class="fl">0.005</span>, <span class="at">baseline =</span> <span class="cn">FALSE</span>)</span>
<span id="cb108-6"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-6" tabindex="-1"></a>  reinforce_baseline <span class="ot">&lt;-</span> <span class="fu">reinforce</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha =</span> <span class="fl">0.005</span>, <span class="at">baseline =</span> <span class="cn">TRUE</span>)</span>
<span id="cb108-7"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-7" tabindex="-1"></a>  ac_result <span class="ot">&lt;-</span> <span class="fu">actor_critic</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha_actor =</span> <span class="fl">0.005</span>, <span class="at">alpha_critic =</span> <span class="fl">0.01</span>)</span>
<span id="cb108-8"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-8" tabindex="-1"></a>  </span>
<span id="cb108-9"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-9" tabindex="-1"></a>  <span class="co"># Create comparison data frame</span></span>
<span id="cb108-10"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-10" tabindex="-1"></a>  episodes_seq <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span></span>
<span id="cb108-11"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-11" tabindex="-1"></a>  </span>
<span id="cb108-12"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-12" tabindex="-1"></a>  <span class="co"># For fair comparison, evaluate all policies at the same intervals</span></span>
<span id="cb108-13"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-13" tabindex="-1"></a>  eval_episodes <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">50</span>, <span class="dv">1000</span>, <span class="at">by =</span> <span class="dv">50</span>)</span>
<span id="cb108-14"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-14" tabindex="-1"></a>  </span>
<span id="cb108-15"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-15" tabindex="-1"></a>  basic_returns <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">1000</span>)</span>
<span id="cb108-16"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-16" tabindex="-1"></a>  baseline_returns <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">1000</span>)</span>
<span id="cb108-17"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-17" tabindex="-1"></a>  ac_returns <span class="ot">&lt;-</span> ac_result<span class="sc">$</span>episode_returns</span>
<span id="cb108-18"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-18" tabindex="-1"></a>  </span>
<span id="cb108-19"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-19" tabindex="-1"></a>  <span class="co"># Evaluate REINFORCE variants at specified intervals</span></span>
<span id="cb108-20"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-20" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> eval_episodes) {</span>
<span id="cb108-21"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-21" tabindex="-1"></a>    <span class="cf">if</span> (i <span class="sc">&lt;=</span> <span class="fu">length</span>(reinforce_basic<span class="sc">$</span>episode_returns)) {</span>
<span id="cb108-22"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-22" tabindex="-1"></a>      <span class="co"># Use stored episode returns instead of re-evaluating</span></span>
<span id="cb108-23"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-23" tabindex="-1"></a>      basic_returns[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(reinforce_basic<span class="sc">$</span>episode_returns[<span class="fu">max</span>(<span class="dv">1</span>, i<span class="dv">-10</span>)<span class="sc">:</span>i], <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb108-24"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-24" tabindex="-1"></a>      baseline_returns[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(reinforce_baseline<span class="sc">$</span>episode_returns[<span class="fu">max</span>(<span class="dv">1</span>, i<span class="dv">-10</span>)<span class="sc">:</span>i], <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb108-25"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-25" tabindex="-1"></a>    }</span>
<span id="cb108-26"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-26" tabindex="-1"></a>  }</span>
<span id="cb108-27"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-27" tabindex="-1"></a>  </span>
<span id="cb108-28"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-28" tabindex="-1"></a>  <span class="co"># Interpolate for smoother plotting</span></span>
<span id="cb108-29"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-29" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>) {</span>
<span id="cb108-30"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-30" tabindex="-1"></a>    <span class="cf">if</span> (basic_returns[i] <span class="sc">==</span> <span class="dv">0</span> <span class="sc">&amp;&amp;</span> i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb108-31"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-31" tabindex="-1"></a>      basic_returns[i] <span class="ot">&lt;-</span> basic_returns[i<span class="dv">-1</span>]</span>
<span id="cb108-32"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-32" tabindex="-1"></a>      baseline_returns[i] <span class="ot">&lt;-</span> baseline_returns[i<span class="dv">-1</span>]</span>
<span id="cb108-33"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-33" tabindex="-1"></a>    }</span>
<span id="cb108-34"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-34" tabindex="-1"></a>    <span class="cf">if</span> (ac_returns[i] <span class="sc">==</span> <span class="dv">0</span> <span class="sc">&amp;&amp;</span> i <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb108-35"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-35" tabindex="-1"></a>      ac_returns[i] <span class="ot">&lt;-</span> ac_returns[i<span class="dv">-1</span>]</span>
<span id="cb108-36"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-36" tabindex="-1"></a>    }</span>
<span id="cb108-37"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-37" tabindex="-1"></a>  }</span>
<span id="cb108-38"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-38" tabindex="-1"></a>  </span>
<span id="cb108-39"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-39" tabindex="-1"></a>  comparison_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb108-40"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-40" tabindex="-1"></a>    <span class="at">episode =</span> <span class="fu">rep</span>(episodes_seq, <span class="dv">3</span>),</span>
<span id="cb108-41"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-41" tabindex="-1"></a>    <span class="at">algorithm =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;REINFORCE&quot;</span>, <span class="st">&quot;REINFORCE + Baseline&quot;</span>, <span class="st">&quot;Actor-Critic&quot;</span>), </span>
<span id="cb108-42"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-42" tabindex="-1"></a>                   <span class="at">each =</span> <span class="dv">1000</span>),</span>
<span id="cb108-43"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-43" tabindex="-1"></a>    <span class="at">return =</span> <span class="fu">c</span>(basic_returns, baseline_returns, ac_returns)</span>
<span id="cb108-44"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-44" tabindex="-1"></a>  )</span>
<span id="cb108-45"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-45" tabindex="-1"></a>  </span>
<span id="cb108-46"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-46" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb108-47"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-47" tabindex="-1"></a>    <span class="at">data =</span> comparison_data,</span>
<span id="cb108-48"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-48" tabindex="-1"></a>    <span class="at">basic_result =</span> reinforce_basic,</span>
<span id="cb108-49"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-49" tabindex="-1"></a>    <span class="at">baseline_result =</span> reinforce_baseline,</span>
<span id="cb108-50"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-50" tabindex="-1"></a>    <span class="at">ac_result =</span> ac_result</span>
<span id="cb108-51"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-51" tabindex="-1"></a>  ))</span>
<span id="cb108-52"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb108-52" tabindex="-1"></a>}</span></code></pre></div>
<div id="visualizing-policy-gradient-performance" class="section level4 hasAnchor" number="16.4.1.1">
<h4><span class="header-section-number">16.4.1.1</span> Visualizing Policy Gradient Performance<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#visualizing-policy-gradient-performance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Letâs run the comparative experiment and visualize the learning curves:</p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb109-1" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;ggplot2&quot;</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;ggplot2&quot;</span>)</span>
<span id="cb109-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb109-2" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;dplyr&quot;</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;dplyr&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:gridExtra&#39;:
## 
##     combine</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb114-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-2" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb114-3"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-3" tabindex="-1"></a></span>
<span id="cb114-4"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-4" tabindex="-1"></a><span class="co"># Run comparative experiment</span></span>
<span id="cb114-5"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-5" tabindex="-1"></a>experiment_results <span class="ot">&lt;-</span> <span class="fu">comparative_experiment</span>()</span>
<span id="cb114-6"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-6" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> experiment_results<span class="sc">$</span>data</span>
<span id="cb114-7"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-7" tabindex="-1"></a></span>
<span id="cb114-8"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-8" tabindex="-1"></a><span class="co"># Filter out zero values for cleaner visualization</span></span>
<span id="cb114-9"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-9" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> plot_data <span class="sc">%&gt;%</span></span>
<span id="cb114-10"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-10" tabindex="-1"></a>  <span class="fu">filter</span>(return <span class="sc">!=</span> <span class="dv">0</span>) <span class="sc">%&gt;%</span></span>
<span id="cb114-11"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-11" tabindex="-1"></a>  <span class="fu">group_by</span>(algorithm) <span class="sc">%&gt;%</span></span>
<span id="cb114-12"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-12" tabindex="-1"></a>  <span class="fu">arrange</span>(episode) <span class="sc">%&gt;%</span></span>
<span id="cb114-13"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-13" tabindex="-1"></a>  <span class="co"># Apply some smoothing for visualization</span></span>
<span id="cb114-14"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-14" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">smoothed_return =</span> zoo<span class="sc">::</span><span class="fu">rollmean</span>(return, <span class="at">k =</span> <span class="dv">3</span>, <span class="at">fill =</span> <span class="cn">NA</span>, <span class="at">align =</span> <span class="st">&quot;right&quot;</span>))</span>
<span id="cb114-15"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-15" tabindex="-1"></a></span>
<span id="cb114-16"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-16" tabindex="-1"></a><span class="fu">ggplot</span>(plot_data, <span class="fu">aes</span>(<span class="at">x =</span> episode, <span class="at">y =</span> smoothed_return, <span class="at">color =</span> algorithm)) <span class="sc">+</span></span>
<span id="cb114-17"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-17" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="fl">1.2</span>, <span class="at">alpha =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb114-18"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-18" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">span =</span> <span class="fl">0.3</span>, <span class="at">size =</span> <span class="fl">0.8</span>) <span class="sc">+</span></span>
<span id="cb114-19"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-19" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb114-20"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-20" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Policy Gradient Methods Comparison&quot;</span>,</span>
<span id="cb114-21"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-21" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">&quot;Learning curves showing convergence behavior of different policy gradient variants&quot;</span>,</span>
<span id="cb114-22"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-22" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Episode&quot;</span>,</span>
<span id="cb114-23"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-23" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Average Return&quot;</span>,</span>
<span id="cb114-24"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-24" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;Algorithm&quot;</span></span>
<span id="cb114-25"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-25" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb114-26"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-26" tabindex="-1"></a>  <span class="fu">theme_minimal</span>(<span class="at">base_size =</span> <span class="dv">14</span>) <span class="sc">+</span></span>
<span id="cb114-27"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-27" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(</span>
<span id="cb114-28"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-28" tabindex="-1"></a>    <span class="st">&quot;REINFORCE&quot;</span> <span class="ot">=</span> <span class="st">&quot;#e31a1c&quot;</span>,</span>
<span id="cb114-29"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-29" tabindex="-1"></a>    <span class="st">&quot;REINFORCE + Baseline&quot;</span> <span class="ot">=</span> <span class="st">&quot;#1f78b4&quot;</span>, </span>
<span id="cb114-30"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-30" tabindex="-1"></a>    <span class="st">&quot;Actor-Critic&quot;</span> <span class="ot">=</span> <span class="st">&quot;#33a02c&quot;</span></span>
<span id="cb114-31"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-31" tabindex="-1"></a>  )) <span class="sc">+</span></span>
<span id="cb114-32"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-32" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb114-33"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-33" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>,</span>
<span id="cb114-34"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-34" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb114-35"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-35" tabindex="-1"></a>    <span class="at">plot.subtitle =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">&quot;grey30&quot;</span>)</span>
<span id="cb114-36"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb114-36" tabindex="-1"></a>  )</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<pre><code>## Warning: Removed 6 rows containing non-finite outside the scale range (`stat_smooth()`).</code></pre>
<pre><code>## Warning: Removed 6 rows containing missing values or values outside the scale range (`geom_line()`).</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
</div>
</div>
<div id="learning-rate-sensitivity-analysis" class="section level3 hasAnchor" number="16.4.2">
<h3><span class="header-section-number">16.4.2</span> Learning Rate Sensitivity Analysis<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#learning-rate-sensitivity-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Policy gradients are particularly sensitive to learning rates due to their high variance. Letâs analyze this relationship:</p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-1" tabindex="-1"></a>learning_rate_experiment <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb118-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-2" tabindex="-1"></a>  learning_rates <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.001</span>, <span class="fl">0.005</span>, <span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>)</span>
<span id="cb118-3"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-3" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb118-4"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-4" tabindex="-1"></a>  </span>
<span id="cb118-5"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-5" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(learning_rates)) {</span>
<span id="cb118-6"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-6" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">42</span>)  <span class="co"># Consistent starting conditions</span></span>
<span id="cb118-7"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-7" tabindex="-1"></a>    </span>
<span id="cb118-8"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-8" tabindex="-1"></a>    lr <span class="ot">&lt;-</span> learning_rates[i]</span>
<span id="cb118-9"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-9" tabindex="-1"></a>    result <span class="ot">&lt;-</span> <span class="fu">reinforce</span>(<span class="at">episodes =</span> <span class="dv">500</span>, <span class="at">alpha =</span> lr, <span class="at">baseline =</span> <span class="cn">TRUE</span>)</span>
<span id="cb118-10"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-10" tabindex="-1"></a>    </span>
<span id="cb118-11"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-11" tabindex="-1"></a>    <span class="co"># Calculate final performance (average of last 50 episodes)</span></span>
<span id="cb118-12"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-12" tabindex="-1"></a>    final_episodes <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="dv">1</span>, <span class="fu">length</span>(result<span class="sc">$</span>episode_returns) <span class="sc">-</span> <span class="dv">49</span>)<span class="sc">:</span><span class="fu">length</span>(result<span class="sc">$</span>episode_returns)</span>
<span id="cb118-13"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-13" tabindex="-1"></a>    final_performance <span class="ot">&lt;-</span> <span class="fu">mean</span>(result<span class="sc">$</span>episode_returns[final_episodes], <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb118-14"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-14" tabindex="-1"></a>    </span>
<span id="cb118-15"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-15" tabindex="-1"></a>    <span class="co"># Calculate convergence speed (episode where performance first exceeds threshold)</span></span>
<span id="cb118-16"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-16" tabindex="-1"></a>    threshold <span class="ot">&lt;-</span> <span class="fl">0.5</span>  <span class="co"># Arbitrary threshold for &quot;good&quot; performance</span></span>
<span id="cb118-17"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-17" tabindex="-1"></a>    convergence_episode <span class="ot">&lt;-</span> <span class="fu">which</span>(result<span class="sc">$</span>episode_returns <span class="sc">&gt;</span> threshold)[<span class="dv">1</span>]</span>
<span id="cb118-18"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-18" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">is.na</span>(convergence_episode)) convergence_episode <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb118-19"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-19" tabindex="-1"></a>    </span>
<span id="cb118-20"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-20" tabindex="-1"></a>    results[[i]] <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb118-21"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-21" tabindex="-1"></a>      <span class="at">learning_rate =</span> lr,</span>
<span id="cb118-22"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-22" tabindex="-1"></a>      <span class="at">final_performance =</span> final_performance,</span>
<span id="cb118-23"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-23" tabindex="-1"></a>      <span class="at">convergence_episode =</span> convergence_episode,</span>
<span id="cb118-24"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-24" tabindex="-1"></a>      <span class="at">episode_returns =</span> result<span class="sc">$</span>episode_returns</span>
<span id="cb118-25"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-25" tabindex="-1"></a>    )</span>
<span id="cb118-26"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-26" tabindex="-1"></a>  }</span>
<span id="cb118-27"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-27" tabindex="-1"></a>  </span>
<span id="cb118-28"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-28" tabindex="-1"></a>  <span class="fu">return</span>(results)</span>
<span id="cb118-29"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-29" tabindex="-1"></a>}</span>
<span id="cb118-30"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-30" tabindex="-1"></a></span>
<span id="cb118-31"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-31" tabindex="-1"></a><span class="co"># Visualization function for learning rate analysis</span></span>
<span id="cb118-32"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-32" tabindex="-1"></a>plot_learning_rate_analysis <span class="ot">&lt;-</span> <span class="cf">function</span>(lr_results) {</span>
<span id="cb118-33"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-33" tabindex="-1"></a>  <span class="co"># Extract summary statistics</span></span>
<span id="cb118-34"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-34" tabindex="-1"></a>  lr_values <span class="ot">&lt;-</span> <span class="fu">sapply</span>(lr_results, <span class="cf">function</span>(x) x<span class="sc">$</span>learning_rate)</span>
<span id="cb118-35"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-35" tabindex="-1"></a>  final_perfs <span class="ot">&lt;-</span> <span class="fu">sapply</span>(lr_results, <span class="cf">function</span>(x) x<span class="sc">$</span>final_performance)</span>
<span id="cb118-36"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-36" tabindex="-1"></a>  convergence_eps <span class="ot">&lt;-</span> <span class="fu">sapply</span>(lr_results, <span class="cf">function</span>(x) x<span class="sc">$</span>convergence_episode)</span>
<span id="cb118-37"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-37" tabindex="-1"></a>  </span>
<span id="cb118-38"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-38" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">2</span>) <span class="sc">+</span> <span class="fl">0.1</span>)</span>
<span id="cb118-39"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-39" tabindex="-1"></a>  </span>
<span id="cb118-40"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-40" tabindex="-1"></a>  <span class="co"># Plot 1: Final performance vs learning rate</span></span>
<span id="cb118-41"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-41" tabindex="-1"></a>  <span class="fu">plot</span>(lr_values, final_perfs, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&quot;darkblue&quot;</span>,</span>
<span id="cb118-42"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-42" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Learning Rate&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Final Average Return&quot;</span>,</span>
<span id="cb118-43"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-43" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Final Performance vs Learning Rate&quot;</span>,</span>
<span id="cb118-44"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-44" tabindex="-1"></a>       <span class="at">log =</span> <span class="st">&quot;x&quot;</span>)  <span class="co"># Log scale for learning rate</span></span>
<span id="cb118-45"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-45" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb118-46"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-46" tabindex="-1"></a>  </span>
<span id="cb118-47"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-47" tabindex="-1"></a>  <span class="co"># Plot 2: Convergence speed vs learning rate</span></span>
<span id="cb118-48"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-48" tabindex="-1"></a>  <span class="fu">plot</span>(lr_values, convergence_eps, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&quot;darkred&quot;</span>,</span>
<span id="cb118-49"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-49" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Learning Rate&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Episodes to Convergence&quot;</span>,</span>
<span id="cb118-50"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-50" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Convergence Speed vs Learning Rate&quot;</span>,</span>
<span id="cb118-51"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-51" tabindex="-1"></a>       <span class="at">log =</span> <span class="st">&quot;x&quot;</span>)</span>
<span id="cb118-52"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-52" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb118-53"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-53" tabindex="-1"></a>  </span>
<span id="cb118-54"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-54" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb118-55"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb118-55" tabindex="-1"></a>}</span></code></pre></div>
<div id="running-the-learning-rate-analysis" class="section level4 hasAnchor" number="16.4.2.1">
<h4><span class="header-section-number">16.4.2.1</span> Running the Learning Rate Analysis<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#running-the-learning-rate-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-1" tabindex="-1"></a><span class="co"># Run the analysis</span></span>
<span id="cb119-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-2" tabindex="-1"></a>lr_results <span class="ot">&lt;-</span> <span class="fu">learning_rate_experiment</span>()</span>
<span id="cb119-3"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-3" tabindex="-1"></a></span>
<span id="cb119-4"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-4" tabindex="-1"></a><span class="co"># Create summary table</span></span>
<span id="cb119-5"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-5" tabindex="-1"></a>lr_summary <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb119-6"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-6" tabindex="-1"></a>  <span class="at">learning_rate =</span> <span class="fu">sapply</span>(lr_results, <span class="cf">function</span>(x) x<span class="sc">$</span>learning_rate),</span>
<span id="cb119-7"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-7" tabindex="-1"></a>  <span class="at">final_performance =</span> <span class="fu">sapply</span>(lr_results, <span class="cf">function</span>(x) <span class="fu">round</span>(x<span class="sc">$</span>final_performance, <span class="dv">3</span>)),</span>
<span id="cb119-8"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-8" tabindex="-1"></a>  <span class="at">convergence_episode =</span> <span class="fu">sapply</span>(lr_results, <span class="cf">function</span>(x) x<span class="sc">$</span>convergence_episode)</span>
<span id="cb119-9"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-9" tabindex="-1"></a>)</span>
<span id="cb119-10"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-10" tabindex="-1"></a></span>
<span id="cb119-11"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-11" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;knitr&quot;</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;knitr&quot;</span>)</span>
<span id="cb119-12"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-12" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(lr_summary,</span>
<span id="cb119-13"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-13" tabindex="-1"></a>      <span class="at">caption =</span> <span class="st">&quot;Learning Rate Sensitivity Analysis Results&quot;</span>,</span>
<span id="cb119-14"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-14" tabindex="-1"></a>      <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">&quot;Learning Rate&quot;</span>, <span class="st">&quot;Final Performance&quot;</span>, <span class="st">&quot;Episodes to Convergence&quot;</span>),</span>
<span id="cb119-15"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb119-15" tabindex="-1"></a>      <span class="at">align =</span> <span class="st">&#39;c&#39;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-32">Table 16.1: </span>Learning Rate Sensitivity Analysis Results</caption>
<thead>
<tr class="header">
<th align="center">Learning Rate</th>
<th align="center">Final Performance</th>
<th align="center">Episodes to Convergence</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.001</td>
<td align="center">0.869</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">0.005</td>
<td align="center">0.890</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">0.010</td>
<td align="center">0.887</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">0.050</td>
<td align="center">0.946</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">0.100</td>
<td align="center">0.964</td>
<td align="center">1</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb120-1" tabindex="-1"></a><span class="co"># Generate the plots</span></span>
<span id="cb120-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb120-2" tabindex="-1"></a><span class="fu">plot_learning_rate_analysis</span>(lr_results)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-33-1.png" width="672" /></p>
</div>
</div>
<div id="variance-analysis-of-gradient-estimates" class="section level3 hasAnchor" number="16.4.3">
<h3><span class="header-section-number">16.4.3</span> Variance Analysis of Gradient Estimates<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#variance-analysis-of-gradient-estimates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the key challenges in policy gradients is the high variance of gradient estimates. Letâs analyze this:</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-1" tabindex="-1"></a>variance_analysis <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb121-2"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-2" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb121-3"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-3" tabindex="-1"></a>  </span>
<span id="cb121-4"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-4" tabindex="-1"></a>  <span class="co"># Run REINFORCE and track gradient magnitudes</span></span>
<span id="cb121-5"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-5" tabindex="-1"></a>  n_params <span class="ot">&lt;-</span> n_states <span class="sc">*</span> n_actions</span>
<span id="cb121-6"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-6" tabindex="-1"></a>  theta <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_params, <span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb121-7"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-7" tabindex="-1"></a>  </span>
<span id="cb121-8"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-8" tabindex="-1"></a>  gradient_magnitudes <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb121-9"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-9" tabindex="-1"></a>  gradient_variances <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb121-10"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-10" tabindex="-1"></a>  episode_returns <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb121-11"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-11" tabindex="-1"></a>  </span>
<span id="cb121-12"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-12" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>) {  <span class="co"># Shorter run for detailed analysis</span></span>
<span id="cb121-13"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-13" tabindex="-1"></a>    <span class="co"># Generate episode</span></span>
<span id="cb121-14"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-14" tabindex="-1"></a>    trajectory <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb121-15"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-15" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb121-16"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-16" tabindex="-1"></a>    step <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb121-17"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-17" tabindex="-1"></a>    </span>
<span id="cb121-18"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-18" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state <span class="sc">&amp;&amp;</span> step <span class="sc">&lt;</span> <span class="dv">50</span>) {</span>
<span id="cb121-19"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-19" tabindex="-1"></a>      step <span class="ot">&lt;-</span> step <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb121-20"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-20" tabindex="-1"></a>      </span>
<span id="cb121-21"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-21" tabindex="-1"></a>      action_result <span class="ot">&lt;-</span> <span class="fu">sample_action</span>(s, theta)</span>
<span id="cb121-22"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-22" tabindex="-1"></a>      outcome <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, action_result<span class="sc">$</span>action)</span>
<span id="cb121-23"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-23" tabindex="-1"></a>      </span>
<span id="cb121-24"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-24" tabindex="-1"></a>      trajectory[[step]] <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb121-25"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-25" tabindex="-1"></a>        <span class="at">state =</span> s,</span>
<span id="cb121-26"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-26" tabindex="-1"></a>        <span class="at">action =</span> action_result<span class="sc">$</span>action,</span>
<span id="cb121-27"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-27" tabindex="-1"></a>        <span class="at">reward =</span> outcome<span class="sc">$</span>reward</span>
<span id="cb121-28"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-28" tabindex="-1"></a>      )</span>
<span id="cb121-29"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-29" tabindex="-1"></a>      </span>
<span id="cb121-30"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-30" tabindex="-1"></a>      s <span class="ot">&lt;-</span> outcome<span class="sc">$</span>s_prime</span>
<span id="cb121-31"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-31" tabindex="-1"></a>    }</span>
<span id="cb121-32"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-32" tabindex="-1"></a>    </span>
<span id="cb121-33"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-33" tabindex="-1"></a>    <span class="cf">if</span> (step <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb121-34"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-34" tabindex="-1"></a>      <span class="co"># Compute returns</span></span>
<span id="cb121-35"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-35" tabindex="-1"></a>      T <span class="ot">&lt;-</span> <span class="fu">length</span>(trajectory)</span>
<span id="cb121-36"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-36" tabindex="-1"></a>      returns <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb121-37"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-37" tabindex="-1"></a>      G <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb121-38"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-38" tabindex="-1"></a>      </span>
<span id="cb121-39"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-39" tabindex="-1"></a>      <span class="cf">for</span> (t <span class="cf">in</span> T<span class="sc">:</span><span class="dv">1</span>) {</span>
<span id="cb121-40"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-40" tabindex="-1"></a>        G <span class="ot">&lt;-</span> trajectory[[t]]<span class="sc">$</span>reward <span class="sc">+</span> gamma <span class="sc">*</span> G</span>
<span id="cb121-41"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-41" tabindex="-1"></a>        returns[t] <span class="ot">&lt;-</span> G</span>
<span id="cb121-42"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-42" tabindex="-1"></a>      }</span>
<span id="cb121-43"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-43" tabindex="-1"></a>      </span>
<span id="cb121-44"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-44" tabindex="-1"></a>      episode_returns[ep] <span class="ot">&lt;-</span> returns[<span class="dv">1</span>]</span>
<span id="cb121-45"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-45" tabindex="-1"></a>      </span>
<span id="cb121-46"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-46" tabindex="-1"></a>      <span class="co"># Compute gradient for this episode</span></span>
<span id="cb121-47"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-47" tabindex="-1"></a>      gradient <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_params)</span>
<span id="cb121-48"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-48" tabindex="-1"></a>      step_gradients <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb121-49"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-49" tabindex="-1"></a>      </span>
<span id="cb121-50"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-50" tabindex="-1"></a>      <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T) {</span>
<span id="cb121-51"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-51" tabindex="-1"></a>        state <span class="ot">&lt;-</span> trajectory[[t]]<span class="sc">$</span>state</span>
<span id="cb121-52"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-52" tabindex="-1"></a>        action <span class="ot">&lt;-</span> trajectory[[t]]<span class="sc">$</span>action</span>
<span id="cb121-53"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-53" tabindex="-1"></a>        </span>
<span id="cb121-54"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-54" tabindex="-1"></a>        grad_log_pi <span class="ot">&lt;-</span> <span class="fu">grad_log_prob</span>(state, action, theta)</span>
<span id="cb121-55"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-55" tabindex="-1"></a>        step_gradient <span class="ot">&lt;-</span> grad_log_pi <span class="sc">*</span> returns[t]</span>
<span id="cb121-56"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-56" tabindex="-1"></a>        gradient <span class="ot">&lt;-</span> gradient <span class="sc">+</span> step_gradient</span>
<span id="cb121-57"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-57" tabindex="-1"></a>        step_gradients[[t]] <span class="ot">&lt;-</span> step_gradient</span>
<span id="cb121-58"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-58" tabindex="-1"></a>      }</span>
<span id="cb121-59"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-59" tabindex="-1"></a>      </span>
<span id="cb121-60"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-60" tabindex="-1"></a>      gradient_magnitudes[ep] <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>(gradient<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb121-61"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-61" tabindex="-1"></a>      </span>
<span id="cb121-62"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-62" tabindex="-1"></a>      <span class="co"># Store individual step gradients for variance analysis</span></span>
<span id="cb121-63"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-63" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">length</span>(step_gradients) <span class="sc">&gt;</span> <span class="dv">1</span>) {</span>
<span id="cb121-64"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-64" tabindex="-1"></a>        step_gradient_matrix <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, step_gradients)</span>
<span id="cb121-65"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-65" tabindex="-1"></a>        gradient_variances[[ep]] <span class="ot">&lt;-</span> <span class="fu">apply</span>(step_gradient_matrix, <span class="dv">2</span>, var)</span>
<span id="cb121-66"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-66" tabindex="-1"></a>      }</span>
<span id="cb121-67"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-67" tabindex="-1"></a>    }</span>
<span id="cb121-68"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-68" tabindex="-1"></a>  }</span>
<span id="cb121-69"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-69" tabindex="-1"></a>  </span>
<span id="cb121-70"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-70" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb121-71"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-71" tabindex="-1"></a>    <span class="at">gradient_magnitudes =</span> gradient_magnitudes,</span>
<span id="cb121-72"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-72" tabindex="-1"></a>    <span class="at">gradient_variances =</span> gradient_variances,</span>
<span id="cb121-73"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-73" tabindex="-1"></a>    <span class="at">episode_returns =</span> episode_returns</span>
<span id="cb121-74"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-74" tabindex="-1"></a>  ))</span>
<span id="cb121-75"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-75" tabindex="-1"></a>}</span>
<span id="cb121-76"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-76" tabindex="-1"></a></span>
<span id="cb121-77"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-77" tabindex="-1"></a><span class="co"># Visualization of variance patterns</span></span>
<span id="cb121-78"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-78" tabindex="-1"></a>plot_variance_analysis <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb121-79"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-79" tabindex="-1"></a>  var_data <span class="ot">&lt;-</span> <span class="fu">variance_analysis</span>()</span>
<span id="cb121-80"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-80" tabindex="-1"></a>  </span>
<span id="cb121-81"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-81" tabindex="-1"></a>  episodes <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(var_data<span class="sc">$</span>gradient_magnitudes)</span>
<span id="cb121-82"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-82" tabindex="-1"></a>  </span>
<span id="cb121-83"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-83" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>))</span>
<span id="cb121-84"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-84" tabindex="-1"></a>  </span>
<span id="cb121-85"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-85" tabindex="-1"></a>  <span class="co"># Plot gradient magnitudes over episodes</span></span>
<span id="cb121-86"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-86" tabindex="-1"></a>  <span class="fu">plot</span>(episodes, var_data<span class="sc">$</span>gradient_magnitudes, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb121-87"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-87" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Episode&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Gradient Magnitude&quot;</span>,</span>
<span id="cb121-88"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-88" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Gradient Magnitude Evolution&quot;</span>,</span>
<span id="cb121-89"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-89" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb121-90"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-90" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb121-91"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-91" tabindex="-1"></a>  </span>
<span id="cb121-92"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-92" tabindex="-1"></a>  <span class="co"># Plot relationship between returns and gradient magnitudes</span></span>
<span id="cb121-93"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-93" tabindex="-1"></a>  valid_indices <span class="ot">&lt;-</span> <span class="sc">!</span><span class="fu">is.na</span>(var_data<span class="sc">$</span>episode_returns) <span class="sc">&amp;</span> <span class="sc">!</span><span class="fu">is.na</span>(var_data<span class="sc">$</span>gradient_magnitudes)</span>
<span id="cb121-94"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-94" tabindex="-1"></a>  <span class="fu">plot</span>(var_data<span class="sc">$</span>episode_returns[valid_indices], </span>
<span id="cb121-95"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-95" tabindex="-1"></a>       var_data<span class="sc">$</span>gradient_magnitudes[valid_indices],</span>
<span id="cb121-96"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-96" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Episode Return&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Gradient Magnitude&quot;</span>,</span>
<span id="cb121-97"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-97" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Return vs Gradient Magnitude&quot;</span>,</span>
<span id="cb121-98"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-98" tabindex="-1"></a>       <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>)</span>
<span id="cb121-99"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-99" tabindex="-1"></a>  </span>
<span id="cb121-100"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-100" tabindex="-1"></a>  <span class="co"># Add trend line</span></span>
<span id="cb121-101"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-101" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">sum</span>(valid_indices) <span class="sc">&gt;</span> <span class="dv">2</span>) {</span>
<span id="cb121-102"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-102" tabindex="-1"></a>    lm_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(var_data<span class="sc">$</span>gradient_magnitudes[valid_indices] <span class="sc">~</span> </span>
<span id="cb121-103"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-103" tabindex="-1"></a>                var_data<span class="sc">$</span>episode_returns[valid_indices])</span>
<span id="cb121-104"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-104" tabindex="-1"></a>    <span class="fu">abline</span>(lm_fit, <span class="at">col =</span> <span class="st">&quot;darkred&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb121-105"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-105" tabindex="-1"></a>  }</span>
<span id="cb121-106"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-106" tabindex="-1"></a>  </span>
<span id="cb121-107"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-107" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb121-108"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-108" tabindex="-1"></a>  </span>
<span id="cb121-109"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-109" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb121-110"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb121-110" tabindex="-1"></a>}</span></code></pre></div>
<div id="example-analyzing-gradient-variance" class="section level4 hasAnchor" number="16.4.3.1">
<h4><span class="header-section-number">16.4.3.1</span> Example: Analyzing Gradient Variance<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#example-analyzing-gradient-variance" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb122-1"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#cb122-1" tabindex="-1"></a><span class="fu">plot_variance_analysis</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-35-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="discussion-and-implementation-considerations-1" class="section level2 hasAnchor" number="16.5">
<h2><span class="header-section-number">16.5</span> Discussion and Implementation Considerations<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#discussion-and-implementation-considerations-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Policy gradient methods represent a fundamental shift in how we approach reinforcement learning problems. Rather than indirectly deriving policies from value estimates, they optimize the policy directly, offering both conceptual clarity and practical advantages in specific domains. This directness comes with trade-offs that illuminate deeper principles about learning and optimization in stochastic environments.</p>
<p>The high variance inherent in policy gradient estimates reflects a fundamental challenge in reinforcement learning: the credit assignment problem. When an episode yields high rewards, which actions deserve credit? REINFORCEâs approach of crediting all actions in proportion to the total return is statistically unbiased but creates substantial noise in the learning signal. Each episode provides only one sample of the policyâs behavior, and the full episode return conflates the effects of all decisions made during that trajectory.</p>
<p>Baseline subtraction addresses this variance problem through a clever insight: we can subtract any state-dependent function from the returns without introducing bias, since the expectation of the gradient of the log probability times the baseline is zero. The optimal baseline minimizes variance and turns out to be the value function itself, leading naturally to Actor-Critic methods. This connection reveals why Actor-Critic algorithms often outperform pure policy gradient approachesâthey leverage the variance reduction benefits of value function learning while maintaining the policy optimization focus.</p>
<p>The choice of policy parameterization profoundly influences both the optimization landscape and the final solution quality. Softmax policies naturally handle discrete action spaces and provide smooth gradients, but they impose limitations in multi-modal or complex action selection scenarios. The parameterization also determines which policies the algorithm can representâa critical consideration often overlooked in implementation. Linear parameterizations restrict the policy to relatively simple decision boundaries, while neural network parameterizations enable more complex behaviors at the cost of increased optimization difficulty.</p>
<p>Learning rate selection in policy gradients requires more care than in supervised learning due to the non-stationary nature of the optimization problem. As the policy changes, the distribution of states and actions encountered shifts, creating a moving target for the gradient estimates. Too high a learning rate leads to destructive updates that can completely destabilize the policy, while too low a rate results in painfully slow learning. The relationship between learning rate, variance, and convergence speed creates a delicate balancing act that often requires extensive tuning.</p>
<p>Implementation in practice demands attention to numerical stability, particularly in the softmax computation and gradient calculations. The log-sum-exp trick prevents overflow in the softmax calculation, while careful handling of log probabilities avoids underflow issues that can lead to infinite gradients. These seemingly minor implementation details can dramatically affect algorithm performance and reliability.</p>
<p>The temporal structure of episodes introduces additional complexity absent from supervised learning. Early actions in an episode receive credit for all subsequent rewards, even those that may be largely independent of the early decisions. This temporal credit assignment problem becomes more severe in longer episodes, where the connection between individual actions and final outcomes grows increasingly tenuous. Techniques like eligibility traces and n-step returns attempt to address this by providing more immediate feedback, but they introduce additional hyperparameters and computational overhead.</p>
<p>Modern policy gradient methods have evolved sophisticated variance reduction techniques beyond simple baselines. Natural policy gradients account for the geometry of the policy space by incorporating the Fisher information matrix, leading to more stable updates. Trust region methods like TRPO constrain update sizes to prevent catastrophic policy changes. PPO simplifies the trust region approach through clever objective clipping. These advances highlight an important lesson: the basic policy gradient theorem provides a foundation, but practical algorithms require extensive engineering to handle the challenges of high-dimensional, stochastic optimization.</p>
<p>The connection between policy gradients and evolutionary strategies offers an interesting perspective on exploration and optimization. Both approaches treat the policy as a whole object to be improved, but evolutionary methods use population-based search rather than gradient ascent. In high-noise environments or with discontinuous reward functions, evolutionary approaches sometimes outperform gradient-based methods, suggesting that the choice between them depends critically on problem structure.</p>
<p>Policy gradients excel in domains where the optimal policy is inherently stochastic, such as partially observable environments or games requiring mixed strategies. Value-based methods struggle in these scenarios because they typically converge to deterministic policies. The ability to learn stochastic policies also proves valuable in multi-agent settings where unpredictability can be advantageous, and in continuous control problems where exploration through action noise is natural.</p>
<p>The sample efficiency limitations of policy gradients compared to value-based methods reflect the different information each update requires. Q-learning can improve its estimates from any transition, regardless of the policy that generated it. Policy gradients, by contrast, can only learn from on-policy data generated by the current policy. This constraint means that policy gradients often require more environment interactions to achieve the same performance level, though they may converge to better final policies in stochastic or continuous domains.</p>
<p>Debugging policy gradient algorithms presents unique challenges because failure modes can be subtle. Unlike supervised learning where prediction errors are immediately obvious, policy gradient failures may manifest as slow learning, instability, or convergence to suboptimal policies. The stochastic nature of both the environment and the policy makes it difficult to distinguish between bad luck and algorithmic problems. Careful tracking of gradient magnitudes, policy entropy, and learning curves becomes essential for diagnosing issues.</p>
<p>The relationship between exploration and exploitation in policy gradients differs fundamentally from value-based methods. Rather than using explicit exploration strategies like epsilon-greedy, policy gradients explore naturally through the stochasticity of the learned policy. This approach can be more sample efficient when the policy uncertainty aligns with value uncertainty, but it can also lead to insufficient exploration when the policy becomes overly confident too quickly.</p>
<p>Modern implementations often benefit from techniques borrowed from deep learning: gradient clipping prevents explosive updates, batch normalization stabilizes learning, and adaptive learning rates help navigate varying gradient scales across parameters. The integration of policy gradients with deep neural networks has enabled remarkable achievements in complex domains, but it has also introduced new challenges around generalization, catastrophic forgetting, and computational efficiency.</p>
</div>
<div id="conclusion-9" class="section level2 hasAnchor" number="16.6">
<h2><span class="header-section-number">16.6</span> Conclusion<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#conclusion-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Policy gradient methods represent one of the most theoretically elegant approaches to reinforcement learning, directly optimizing the fundamental object of interest: the policy itself. The journey from the basic policy gradient theorem to modern sophisticated algorithms illustrates how simple mathematical insights can evolve into powerful practical tools through careful engineering and theoretical refinement.</p>
<p>The core contribution of policy gradients lies not just in their ability to handle continuous action spaces or learn stochastic policies, but in their conceptual clarity about the learning objective. By framing reinforcement learning as direct policy optimization, these methods provide a natural bridge between classical control theory and modern machine learning. This perspective has proven particularly valuable as the field has moved toward more complex, high-dimensional problems where the clarity of the optimization objective becomes crucial.</p>
<p>The evolution from REINFORCE to Actor-Critic to modern methods like PPO and SAC demonstrates how foundational algorithms serve as stepping stones to more sophisticated approaches. Each advancement addresses specific limitations while preserving the core insights that make policy gradients effective. This progressive refinement exemplifies how algorithmic development in reinforcement learning often proceeds: starting with clear theoretical principles and gradually adding practical enhancements to handle real-world complexities.</p>
<p>The variance challenge that characterizes policy gradients has driven much innovation in the field. The development of baseline subtraction, control variates, and more sophisticated variance reduction techniques has advanced not only policy gradient methods but also our broader understanding of how to learn from delayed, noisy feedback. These techniques have found applications well beyond reinforcement learning, influencing areas like variational inference and neural architecture search.</p>
<p>Looking forward, the principles underlying policy gradients remain relevant even as the field explores new frontiers. The idea of directly optimizing parameterized policies appears in modern approaches to meta-learning, where algorithms learn to adapt their policies quickly to new tasks. The policy gradient theoremâs insights about the relationship between policy changes and performance improvements inform newer methods that aim to learn more efficiently from limited data.</p>
<p>The integration of policy gradients with other learning paradigms continues to yield insights. Combining them with model-based methods addresses sample efficiency concerns while preserving the benefits of direct policy optimization. Integration with imitation learning enables more effective learning from demonstrations. These hybrid approaches suggest that the future of reinforcement learning may lie not in choosing between different paradigms but in thoughtfully combining their complementary strengths.</p>
<p>Perhaps most importantly, policy gradients have helped establish reinforcement learning as a principled approach to sequential decision making under uncertainty. By providing clear mathematical foundations for policy improvement, they have enabled the field to move beyond heuristic approaches toward systematic algorithms with theoretical guarantees. This mathematical rigor has been essential for the fieldâs growth and its increasing influence in areas ranging from robotics to finance to game playing.</p>
<p>The simplicity of the basic policy gradient updateâincrease the probability of actions that led to good outcomesâbelies the sophisticated mathematical machinery required to make this principle work reliably in practice. This tension between conceptual simplicity and implementation complexity characterizes many successful machine learning algorithms. Policy gradients remind us that the most powerful ideas often have elegant theoretical foundations even when their practical realization requires careful attention to numerous technical details.</p>
<p>As reinforcement learning continues to tackle increasingly complex real-world problems, the direct approach of policy gradientsâoptimizing exactly what we care about rather than hoping proxy objectives lead to good policiesâbecomes ever more valuable. The clarity of this approach provides a strong foundation for future developments, whether they involve new architectures, better optimization techniques, or novel applications to emerging domains.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="intrinsic-rewards.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/edit/main/16-Policy_Gradient.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/blob/main/16-Policy_Gradient.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
