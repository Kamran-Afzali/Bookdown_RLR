<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Deep Function Approximation: Q-Learning with Neural Networks in R | Reinforcement Learning in R</title>
  <meta name="description" content="Chapter 8 Deep Function Approximation: Q-Learning with Neural Networks in R | Reinforcement Learning in R" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Deep Function Approximation: Q-Learning with Neural Networks in R | Reinforcement Learning in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Deep Function Approximation: Q-Learning with Neural Networks in R | Reinforcement Learning in R" />
  
  
  

<meta name="author" content="Kamran Afzalui" />


<meta name="date" content="2025-10-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"/>
<link rel="next" href="dyna-and-dynaq.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Reinforcement Learning in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Understanding Reinforcement Learning: From Bandits to Policy Optimization</strong></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-multi-armed-bandit-the-simplest-case"><i class="fa fa-check"></i><b>1.2</b> The Multi-Armed Bandit: The Simplest Case</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#transition-to-markov-decision-processes"><i class="fa fa-check"></i><b>1.3</b> Transition to Markov Decision Processes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#comparing-reinforcement-learning-methods"><i class="fa fa-check"></i><b>1.4</b> Comparing Reinforcement Learning Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#dynamic-programming-model-based-learning"><i class="fa fa-check"></i><b>1.4.1</b> Dynamic Programming: Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#model-free-approaches-monte-carlo-and-td-learning"><i class="fa fa-check"></i><b>1.4.2</b> Model-Free Approaches: Monte Carlo and TD Learning</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#dyna-bridging-model-free-and-model-based-learning"><i class="fa fa-check"></i><b>1.4.3</b> Dyna: Bridging Model-Free and Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#q-learning-and-function-approximation"><i class="fa fa-check"></i><b>1.4.4</b> Q-Learning and Function Approximation</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#policy-gradient-and-actor-critic-methods"><i class="fa fa-check"></i><b>1.4.5</b> Policy Gradient and Actor-Critic Methods</a></li>
<li class="chapter" data-level="1.4.6" data-path="index.html"><a href="index.html#advanced-policy-optimization-techniques"><i class="fa fa-check"></i><b>1.4.6</b> Advanced Policy Optimization Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#further-directions"><i class="fa fa-check"></i><b>1.5</b> Further Directions</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html"><i class="fa fa-check"></i><b>2</b> The Multi-Armed Bandit Problem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#mathematical-formalism"><i class="fa fa-check"></i><b>2.2</b> Mathematical Formalism</a></li>
<li class="chapter" data-level="2.3" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm"><i class="fa fa-check"></i><b>2.3</b> Frequentist Approach: UCB1 Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1"><i class="fa fa-check"></i><b>2.3.1</b> R Code for UCB1</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling"><i class="fa fa-check"></i><b>2.4</b> Bayesian Approach: Thompson Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling"><i class="fa fa-check"></i><b>2.4.1</b> R Code for Thompson Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy"><i class="fa fa-check"></i><b>2.5</b> Epsilon-Greedy Strategy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy"><i class="fa fa-check"></i><b>2.5.1</b> R Code for Epsilon-Greedy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#summary-table"><i class="fa fa-check"></i><b>2.6</b> Summary Table</a></li>
<li class="chapter" data-level="2.7" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html"><i class="fa fa-check"></i><b>3</b> Markov Decision Processes and Dynamic Programming</a>
<ul>
<li class="chapter" data-level="3.1" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#constructing-the-mdp-in-r"><i class="fa fa-check"></i><b>3.2</b> Constructing the MDP in R</a></li>
<li class="chapter" data-level="3.3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#value-iteration-algorithm"><i class="fa fa-check"></i><b>3.3</b> Value Iteration Algorithm</a></li>
<li class="chapter" data-level="3.4" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#evaluation-and-interpretation"><i class="fa fa-check"></i><b>3.4</b> Evaluation and Interpretation</a></li>
<li class="chapter" data-level="3.5" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties-of-value-iteration"><i class="fa fa-check"></i><b>3.5</b> Theoretical Properties of Value Iteration</a></li>
<li class="chapter" data-level="3.6" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-1"><i class="fa fa-check"></i><b>3.6</b> Summary Table</a></li>
<li class="chapter" data-level="3.7" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><i class="fa fa-check"></i><b>4</b> Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#theoretical-background"><i class="fa fa-check"></i><b>4.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#temporal-difference-learning-q-learning"><i class="fa fa-check"></i><b>4.2.1</b> Temporal Difference Learning (Q-Learning)</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#monte-carlo-methods"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.2.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-1-defining-the-environment-in-r"><i class="fa fa-check"></i><b>4.2.3</b> Step 1: Defining the Environment in R</a></li>
<li class="chapter" data-level="4.2.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-2-q-learning-implementation-in-r"><i class="fa fa-check"></i><b>4.2.4</b> Step 2: Q-Learning Implementation in R</a></li>
<li class="chapter" data-level="4.2.5" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-3-monte-carlo-every-visit-implementation"><i class="fa fa-check"></i><b>4.2.5</b> Step 3: Monte Carlo Every-Visit Implementation</a></li>
<li class="chapter" data-level="4.2.6" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-4-simulating-outcome-devaluation"><i class="fa fa-check"></i><b>4.2.6</b> Step 4: Simulating Outcome Devaluation</a></li>
<li class="chapter" data-level="4.2.7" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-5-comparing-policies-before-and-after-devaluation"><i class="fa fa-check"></i><b>4.2.7</b> Step 5: Comparing Policies Before and After Devaluation</a></li>
<li class="chapter" data-level="4.2.8" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-6-visualizing-the-policies"><i class="fa fa-check"></i><b>4.2.8</b> Step 6: Visualizing the Policies</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#interpretation-and-discussion"><i class="fa fa-check"></i><b>4.3</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="4.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#conclusion-2"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><i class="fa fa-check"></i><b>5</b> On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-on-policy"><i class="fa fa-check"></i><b>5.2</b> SARSA (On-Policy)</a></li>
<li class="chapter" data-level="5.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-off-policy"><i class="fa fa-check"></i><b>5.3</b> Q-Learning (Off-Policy)</a></li>
<li class="chapter" data-level="5.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-with-importance-sampling"><i class="fa fa-check"></i><b>5.4</b> Off-Policy Monte Carlo with Importance Sampling</a></li>
<li class="chapter" data-level="5.5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#key-differences"><i class="fa fa-check"></i><b>5.5</b> Key Differences</a></li>
<li class="chapter" data-level="5.6" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#interpretation-and-discussion-1"><i class="fa fa-check"></i><b>5.6</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#conclusion-3"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
<li class="chapter" data-level="5.8" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#comparison-table"><i class="fa fa-check"></i><b>5.8</b> Comparison Table</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html"><i class="fa fa-check"></i><b>6</b> Function Approximation Q-Learning with Linear Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#q-learning-with-function-approximation"><i class="fa fa-check"></i><b>6.1.1</b> Q-Learning with Function Approximation</a></li>
<li class="chapter" data-level="6.1.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#comparison-with-tabular-q-learning"><i class="fa fa-check"></i><b>6.1.2</b> Comparison with Tabular Q-Learning</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#r-implementation"><i class="fa fa-check"></i><b>6.2</b> R Implementation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><i class="fa fa-check"></i><b>7</b> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation"><i class="fa fa-check"></i><b>7.1.1</b> Q-Learning with Random Forest Approximation</a></li>
<li class="chapter" data-level="7.1.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods"><i class="fa fa-check"></i><b>7.1.2</b> Comparison with Previous Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1"><i class="fa fa-check"></i><b>7.2</b> R Implementation</a></li>
<li class="chapter" data-level="7.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights"><i class="fa fa-check"></i><b>7.3</b> Analysis and Insights</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#policy-learning-characteristics"><i class="fa fa-check"></i><b>7.3.1</b> Policy Learning Characteristics</a></li>
<li class="chapter" data-level="7.3.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#computational-considerations"><i class="fa fa-check"></i><b>7.3.2</b> Computational Considerations</a></li>
<li class="chapter" data-level="7.3.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-importance-insights"><i class="fa fa-check"></i><b>7.3.3</b> Feature Importance Insights</a></li>
<li class="chapter" data-level="7.3.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#practical-implications-1"><i class="fa fa-check"></i><b>7.3.4</b> Practical Implications</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-linear-approximation"><i class="fa fa-check"></i><b>7.4</b> Comparison with Linear Approximation</a></li>
<li class="chapter" data-level="7.5" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><i class="fa fa-check"></i><b>8</b> Deep Function Approximation: Q-Learning with Neural Networks in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation"><i class="fa fa-check"></i><b>8.2</b> Theoretical Foundation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity"><i class="fa fa-check"></i><b>8.2.1</b> Universal Approximation and Expressivity</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning"><i class="fa fa-check"></i><b>8.2.2</b> Gradient-Based Learning</a></li>
<li class="chapter" data-level="8.2.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches"><i class="fa fa-check"></i><b>8.2.3</b> Comparison with Previous Approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2"><i class="fa fa-check"></i><b>8.3</b> R Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation"><i class="fa fa-check"></i><b>8.4</b> Analysis and Interpretation</a></li>
<li class="chapter" data-level="8.5" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions"><i class="fa fa-check"></i><b>8.5</b> Future Directions</a></li>
<li class="chapter" data-level="8.6" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5"><i class="fa fa-check"></i><b>8.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html"><i class="fa fa-check"></i><b>9</b> Dyna and DynaQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#theoretical-framework"><i class="fa fa-check"></i><b>9.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#the-dyna-architecture"><i class="fa fa-check"></i><b>9.2.1</b> The Dyna Architecture</a></li>
<li class="chapter" data-level="9.2.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#model-representation"><i class="fa fa-check"></i><b>9.2.2</b> Model Representation</a></li>
<li class="chapter" data-level="9.2.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#convergence-properties"><i class="fa fa-check"></i><b>9.2.3</b> Convergence Properties</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-in-r"><i class="fa fa-check"></i><b>9.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#environment-setup"><i class="fa fa-check"></i><b>9.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="9.3.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#dyna-q-implementation"><i class="fa fa-check"></i><b>9.3.2</b> Dyna-Q Implementation</a></li>
<li class="chapter" data-level="9.3.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#standard-q-learning-for-comparison"><i class="fa fa-check"></i><b>9.3.3</b> Standard Q-Learning for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#experimental-analysis"><i class="fa fa-check"></i><b>9.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#learning-efficiency-comparison"><i class="fa fa-check"></i><b>9.4.1</b> Learning Efficiency Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
<li class="chapter" data-level="9.6" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-considerations-and-conclusion"><i class="fa fa-check"></i><b>9.6</b> Implementation Considerations and Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><i class="fa fa-check"></i><b>10</b> Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#theoretical-framework-1"><i class="fa fa-check"></i><b>10.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#the-exploration-bonus-mechanism"><i class="fa fa-check"></i><b>10.2.1</b> The Exploration Bonus Mechanism</a></li>
<li class="chapter" data-level="10.2.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#complete-dyna-q-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> Complete Dyna-Q+ Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#convergence-and-stability"><i class="fa fa-check"></i><b>10.2.3</b> Convergence and Stability</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#implementation-in-r-1"><i class="fa fa-check"></i><b>10.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#environment-setup-1"><i class="fa fa-check"></i><b>10.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="10.3.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-implementation-1"><i class="fa fa-check"></i><b>10.3.2</b> Dyna-Q+ Implementation</a></li>
<li class="chapter" data-level="10.3.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#standard-dyna-for-comparison"><i class="fa fa-check"></i><b>10.3.3</b> Standard Dyna for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#experimental-analysis-1"><i class="fa fa-check"></i><b>10.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#adaptation-to-environmental-changes"><i class="fa fa-check"></i><b>10.4.1</b> Adaptation to Environmental Changes</a></li>
<li class="chapter" data-level="10.4.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#parameter-sensitivity-analysis"><i class="fa fa-check"></i><b>10.4.2</b> Parameter Sensitivity Analysis</a></li>
<li class="chapter" data-level="10.4.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#exploration-pattern-analysis"><i class="fa fa-check"></i><b>10.4.3</b> Exploration Pattern Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#discussion-and-implementation-considerations"><i class="fa fa-check"></i><b>10.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="10.6" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#conclusion-6"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html"><i class="fa fa-check"></i><b>11</b> Function Approximation And Feature Engineering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#feature-engineering-and-state-representation"><i class="fa fa-check"></i><b>11.1</b> Feature Engineering and State Representation</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-discrimination-vs.-generalization-tradeoff"><i class="fa fa-check"></i><b>11.1.1</b> The Discrimination vs. Generalization Tradeoff</a></li>
<li class="chapter" data-level="11.1.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#principles-of-effective-feature-design"><i class="fa fa-check"></i><b>11.1.2</b> Principles of Effective Feature Design</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#mathematical-foundations-of-linear-function-approximation"><i class="fa fa-check"></i><b>11.2</b> Mathematical Foundations of Linear Function Approximation</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#linear-value-functions-and-their-properties"><i class="fa fa-check"></i><b>11.2.1</b> Linear Value Functions and Their Properties</a></li>
<li class="chapter" data-level="11.2.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#temporal-difference-learning-with-linear-approximation"><i class="fa fa-check"></i><b>11.2.2</b> Temporal Difference Learning with Linear Approximation</a></li>
<li class="chapter" data-level="11.2.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-deadly-triad-and-stability-concerns"><i class="fa fa-check"></i><b>11.2.3</b> The Deadly Triad and Stability Concerns</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#classical-basis-function-methods"><i class="fa fa-check"></i><b>11.3</b> Classical Basis Function Methods</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#coarse-coding-overlapping-receptive-fields"><i class="fa fa-check"></i><b>11.3.1</b> Coarse Coding: Overlapping Receptive Fields</a></li>
<li class="chapter" data-level="11.3.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#tile-coding-structured-overlapping-grids"><i class="fa fa-check"></i><b>11.3.2</b> Tile Coding: Structured Overlapping Grids</a></li>
<li class="chapter" data-level="11.3.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#radial-basis-functions-smooth-continuous-features"><i class="fa fa-check"></i><b>11.3.3</b> Radial Basis Functions: Smooth Continuous Features</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#comparative-analysis-and-practical-considerations"><i class="fa fa-check"></i><b>11.4</b> Comparative Analysis and Practical Considerations</a></li>
<li class="chapter" data-level="11.5" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#bridging-classical-and-modern-approaches"><i class="fa fa-check"></i><b>11.5</b> Bridging Classical and Modern Approaches</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html"><i class="fa fa-check"></i><b>12</b> Learning Policies versus Learning Values:</a>
<ul>
<li class="chapter" data-level="12.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#the-two-paradigms-of-reinforcement-learning"><i class="fa fa-check"></i><b>12.1</b> The Two Paradigms of Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#value-based-methods-learning-worth-before-action"><i class="fa fa-check"></i><b>12.1.1</b> Value-Based Methods: Learning Worth Before Action</a></li>
<li class="chapter" data-level="12.1.2" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#policy-based-methods-direct-optimization-of-behavior"><i class="fa fa-check"></i><b>12.1.2</b> Policy-Based Methods: Direct Optimization of Behavior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><i class="fa fa-check"></i><b>13</b> Average Reward in Reinforcement Learning: A Comprehensive Guide</a>
<ul>
<li class="chapter" data-level="13.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-limitations-of-discounted-reward-formulations"><i class="fa fa-check"></i><b>13.1</b> The Limitations of Discounted Reward Formulations</a></li>
<li class="chapter" data-level="13.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-average-reward-alternative-theoretical-foundations"><i class="fa fa-check"></i><b>13.2</b> The Average Reward Alternative: Theoretical Foundations</a></li>
<li class="chapter" data-level="13.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#value-functions-in-the-average-reward-setting"><i class="fa fa-check"></i><b>13.3</b> Value Functions in the Average Reward Setting</a></li>
<li class="chapter" data-level="13.4" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-theory-for-average-reward"><i class="fa fa-check"></i><b>13.4</b> Optimality Theory for Average Reward</a></li>
<li class="chapter" data-level="13.5" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#learning-algorithms-for-average-reward"><i class="fa fa-check"></i><b>13.5</b> Learning Algorithms for Average Reward</a></li>
<li class="chapter" data-level="13.6" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#practical-implementation-server-load-balancing"><i class="fa fa-check"></i><b>13.6</b> Practical Implementation: Server Load Balancing</a></li>
<li class="chapter" data-level="13.7" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#implementation-considerations"><i class="fa fa-check"></i><b>13.7</b> Implementation Considerations</a></li>
<li class="chapter" data-level="13.8" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#when-to-choose-average-reward-over-discounting"><i class="fa fa-check"></i><b>13.8</b> When to Choose Average Reward Over Discounting</a></li>
<li class="chapter" data-level="13.9" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#appendix-a-mathematical-proofs-and-derivations"><i class="fa fa-check"></i><b>13.9</b> Appendix A: Mathematical Proofs and Derivations</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#convergence-of-average-reward-td-learning"><i class="fa fa-check"></i><b>13.9.1</b> Convergence of Average Reward TD Learning</a></li>
<li class="chapter" data-level="13.9.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#policy-gradient-theorem-for-average-reward"><i class="fa fa-check"></i><b>13.9.2</b> Policy Gradient Theorem for Average Reward</a></li>
<li class="chapter" data-level="13.9.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-equations-derivation"><i class="fa fa-check"></i><b>13.9.3</b> Optimality Equations Derivation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="eligibility-traces.html"><a href="eligibility-traces.html"><i class="fa fa-check"></i><b>14</b> Eligibility Traces</a>
<ul>
<li class="chapter" data-level="14.1" data-path="eligibility-traces.html"><a href="eligibility-traces.html#from-one-step-to-multi-step-learning"><i class="fa fa-check"></i><b>14.1</b> From One-Step to Multi-Step Learning</a></li>
<li class="chapter" data-level="14.2" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-mechanics-of-eligibility-traces-a-backward-view"><i class="fa fa-check"></i><b>14.2</b> The Mechanics of Eligibility Traces: A Backward View</a></li>
<li class="chapter" data-level="14.3" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-tdlambda-algorithm-for-prediction"><i class="fa fa-check"></i><b>14.3</b> The TD(<span class="math inline">\(\\lambda\)</span>) Algorithm for Prediction</a></li>
<li class="chapter" data-level="14.4" data-path="eligibility-traces.html"><a href="eligibility-traces.html#control-with-eligibility-traces-average-reward-sarsalambda"><i class="fa fa-check"></i><b>14.4</b> Control with Eligibility Traces: Average Reward Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.5" data-path="eligibility-traces.html"><a href="eligibility-traces.html#practical-implementation-server-load-balancing-with-sarsalambda"><i class="fa fa-check"></i><b>14.5</b> Practical Implementation: Server Load Balancing with Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.6" data-path="eligibility-traces.html"><a href="eligibility-traces.html#computational-and-performance-considerations"><i class="fa fa-check"></i><b>14.6</b> Computational and Performance Considerations</a></li>
<li class="chapter" data-level="14.7" data-path="eligibility-traces.html"><a href="eligibility-traces.html#conclusion-8"><i class="fa fa-check"></i><b>14.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html"><i class="fa fa-check"></i><b>15</b> Intrinsic Rewards</a>
<ul>
<li class="chapter" data-level="15.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#the-sparse-reward-problem-and-its-implications"><i class="fa fa-check"></i><b>15.1</b> The Sparse Reward Problem and Its Implications</a></li>
<li class="chapter" data-level="15.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#mathematical-foundations-of-intrinsic-motivation"><i class="fa fa-check"></i><b>15.2</b> Mathematical Foundations of Intrinsic Motivation</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#curiosity-driven-learning"><i class="fa fa-check"></i><b>15.2.1</b> Curiosity-Driven Learning</a></li>
<li class="chapter" data-level="15.2.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#count-based-exploration"><i class="fa fa-check"></i><b>15.2.2</b> Count-Based Exploration</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#practical-implementation-curiosity-driven-grid-world-navigation"><i class="fa fa-check"></i><b>15.3</b> Practical Implementation: Curiosity-Driven Grid World Navigation</a></li>
<li class="chapter" data-level="15.4" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#advanced-intrinsic-motivation-mechanisms"><i class="fa fa-check"></i><b>15.4</b> Advanced Intrinsic Motivation Mechanisms</a></li>
<li class="chapter" data-level="15.5" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#implementation-considerations-and-practical-guidelines"><i class="fa fa-check"></i><b>15.5</b> Implementation Considerations and Practical Guidelines</a></li>
<li class="chapter" data-level="15.6" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#theoretical-analysis-and-convergence-properties"><i class="fa fa-check"></i><b>15.6</b> Theoretical Analysis and Convergence Properties</a></li>
<li class="chapter" data-level="15.7" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#applications-and-empirical-results"><i class="fa fa-check"></i><b>15.7</b> Applications and Empirical Results</a></li>
<li class="chapter" data-level="15.8" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#limitations-and-future-directions"><i class="fa fa-check"></i><b>15.8</b> Limitations and Future Directions</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>16</b> Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="16.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#introduction-9"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#theoretical-framework-2"><i class="fa fa-check"></i><b>16.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#the-policy-gradient-theorem-1"><i class="fa fa-check"></i><b>16.2.1</b> The Policy Gradient Theorem</a></li>
<li class="chapter" data-level="16.2.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-algorithm"><i class="fa fa-check"></i><b>16.2.2</b> REINFORCE Algorithm</a></li>
<li class="chapter" data-level="16.2.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#baseline-subtraction-and-variance-reduction"><i class="fa fa-check"></i><b>16.2.3</b> Baseline Subtraction and Variance Reduction</a></li>
<li class="chapter" data-level="16.2.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-parameterization"><i class="fa fa-check"></i><b>16.2.4</b> Softmax Policy Parameterization</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#implementation-in-r-2"><i class="fa fa-check"></i><b>16.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#environment-and-feature-representation"><i class="fa fa-check"></i><b>16.3.1</b> Environment and Feature Representation</a></li>
<li class="chapter" data-level="16.3.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-implementation"><i class="fa fa-check"></i><b>16.3.2</b> Softmax Policy Implementation</a></li>
<li class="chapter" data-level="16.3.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-implementation"><i class="fa fa-check"></i><b>16.3.3</b> REINFORCE Implementation</a></li>
<li class="chapter" data-level="16.3.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#actor-critic-implementation"><i class="fa fa-check"></i><b>16.3.4</b> Actor-Critic Implementation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#experimental-analysis-2"><i class="fa fa-check"></i><b>16.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#comparison-of-policy-gradient-variants"><i class="fa fa-check"></i><b>16.4.1</b> Comparison of Policy Gradient Variants</a></li>
<li class="chapter" data-level="16.4.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#learning-rate-sensitivity-analysis"><i class="fa fa-check"></i><b>16.4.2</b> Learning Rate Sensitivity Analysis</a></li>
<li class="chapter" data-level="16.4.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#variance-analysis-of-gradient-estimates"><i class="fa fa-check"></i><b>16.4.3</b> Variance Analysis of Gradient Estimates</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#discussion-and-implementation-considerations-1"><i class="fa fa-check"></i><b>16.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="16.6" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#conclusion-9"><i class="fa fa-check"></i><b>16.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>17</b> Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="17.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#theoretical-framework-3"><i class="fa fa-check"></i><b>17.1</b> Theoretical Framework</a></li>
<li class="chapter" data-level="17.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#implementation-and-comparative-analysis"><i class="fa fa-check"></i><b>17.2</b> Implementation and Comparative Analysis</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#variance-analysis-and-learning-dynamics"><i class="fa fa-check"></i><b>17.2.1</b> Variance Analysis and Learning Dynamics</a></li>
<li class="chapter" data-level="17.2.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#algorithmic-variants-and-extensions"><i class="fa fa-check"></i><b>17.2.2</b> Algorithmic Variants and Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#computational-and-convergence-considerations"><i class="fa fa-check"></i><b>17.3</b> Computational and Convergence Considerations</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#comparative-performance-analysis"><i class="fa fa-check"></i><b>17.3.1</b> Comparative Performance Analysis</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#conclusion-10"><i class="fa fa-check"></i><b>17.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>18</b> Appendix</a>
<ul>
<li class="chapter" data-level="18.1" data-path="appendix.html"><a href="appendix.html#comprehensive-reinforcement-learning-concepts-guide"><i class="fa fa-check"></i><b>18.1</b> Comprehensive Reinforcement Learning Concepts Guide</a></li>
<li class="chapter" data-level="18.2" data-path="appendix.html"><a href="appendix.html#learning-mechanisms"><i class="fa fa-check"></i><b>18.2</b> Learning Mechanisms</a></li>
<li class="chapter" data-level="18.3" data-path="appendix.html"><a href="appendix.html#environment-properties"><i class="fa fa-check"></i><b>18.3</b> Environment Properties</a></li>
<li class="chapter" data-level="18.4" data-path="appendix.html"><a href="appendix.html#learning-paradigms"><i class="fa fa-check"></i><b>18.4</b> Learning Paradigms</a></li>
<li class="chapter" data-level="18.5" data-path="appendix.html"><a href="appendix.html#exploration-strategies"><i class="fa fa-check"></i><b>18.5</b> Exploration Strategies</a></li>
<li class="chapter" data-level="18.6" data-path="appendix.html"><a href="appendix.html#key-algorithms-methods"><i class="fa fa-check"></i><b>18.6</b> Key Algorithms &amp; Methods</a></li>
<li class="chapter" data-level="18.7" data-path="appendix.html"><a href="appendix.html#advanced-concepts"><i class="fa fa-check"></i><b>18.7</b> Advanced Concepts</a></li>
<li class="chapter" data-level="18.8" data-path="appendix.html"><a href="appendix.html#fundamental-equations"><i class="fa fa-check"></i><b>18.8</b> Fundamental Equations</a>
<ul>
<li class="chapter" data-level="18.8.1" data-path="appendix.html"><a href="appendix.html#bellman-equations"><i class="fa fa-check"></i><b>18.8.1</b> <strong>Bellman Equations</strong></a></li>
<li class="chapter" data-level="18.8.2" data-path="appendix.html"><a href="appendix.html#policy-gradient-theorem"><i class="fa fa-check"></i><b>18.8.2</b> <strong>Policy Gradient Theorem</strong></a></li>
<li class="chapter" data-level="18.8.3" data-path="appendix.html"><a href="appendix.html#temporal-difference-error"><i class="fa fa-check"></i><b>18.8.3</b> <strong>Temporal Difference Error</strong></a></li>
</ul></li>
<li class="chapter" data-level="18.9" data-path="appendix.html"><a href="appendix.html#common-challenges-solutions"><i class="fa fa-check"></i><b>18.9</b> Common Challenges &amp; Solutions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="deep-function-approximation-q-learning-with-neural-networks-in-r" class="section level1 hasAnchor" number="8">
<h1><span class="header-section-number">Chapter 8</span> Deep Function Approximation: Q-Learning with Neural Networks in R<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#deep-function-approximation-q-learning-with-neural-networks-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-6" class="section level2 hasAnchor" number="8.1">
<h2><span class="header-section-number">8.1</span> Introduction<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our exploration of function approximation in reinforcement learning has progressed from linear models to ensemble methods, each offering increasing sophistication in capturing complex relationships between states, actions, and their values. Neural networks represent the natural next step in this evolution, providing the theoretical foundation for modern deep reinforcement learning while maintaining practical implementability in R.</p>
<p>Neural network function approximation transcends the limitations of both linear models and tree-based methods by learning hierarchical feature representations automatically. Where linear models assume additive relationships and Random Forests rely on axis-aligned splits, neural networks can discover arbitrary non-linear transformations of the input space. This capability proves particularly valuable in reinforcement learning, where the optimal action-value function often exhibits complex dependencies that resist simple parametric forms.</p>
<p>This post demonstrates Q-Learning with neural network function approximation using R’s <code>nnet</code> package, continuing our 10-state environment while examining how artificial neural networks learn Q-value approximations. We explore the theoretical foundations, implementation challenges, and practical considerations that distinguish neural network approaches from their predecessors.</p>
</div>
<div id="theoretical-foundation" class="section level2 hasAnchor" number="8.2">
<h2><span class="header-section-number">8.2</span> Theoretical Foundation<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Neural network function approximation replaces our previous parameterizations with a multi-layered composition of non-linear transformations. The action-value function becomes:</p>
<p><span class="math display">\[
Q(s, a; \theta) = f_L(W_L f_{L-1}(W_{L-1} \cdots f_1(W_1 \phi(s, a) + b_1) \cdots + b_{L-1}) + b_L)
\]</span></p>
<p>where <span class="math inline">\(f_i\)</span> represents the activation function at layer <span class="math inline">\(i\)</span>, <span class="math inline">\(W_i\)</span> and <span class="math inline">\(b_i\)</span> are weight matrices and bias vectors, and <span class="math inline">\(\theta = \{W_1, b_1, \ldots, W_L, b_L\}\)</span> encompasses all trainable parameters. This hierarchical structure enables the network to learn increasingly abstract representations of the state-action space.</p>
<div id="universal-approximation-and-expressivity" class="section level3 hasAnchor" number="8.2.1">
<h3><span class="header-section-number">8.2.1</span> Universal Approximation and Expressivity<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The theoretical appeal of neural networks stems from universal approximation theorems, which guarantee that feedforward networks with sufficient hidden units can approximate any continuous function to arbitrary precision. In the context of Q-Learning, this suggests that neural networks can, in principle, represent any action-value function arising from a Markov decision process.</p>
<p>For our implementation, we employ a single hidden layer architecture with sigmoid activation functions:</p>
<p><span class="math display">\[
Q(s, a; \theta) = W_2 \sigma(W_1 \phi(s, a) + b_1) + b_2
\]</span></p>
<p>where <span class="math inline">\(\sigma(z) = \frac{1}{1 + e^{-z}}\)</span> is the sigmoid function, providing the non-linearity necessary for complex function approximation.</p>
</div>
<div id="gradient-based-learning" class="section level3 hasAnchor" number="8.2.2">
<h3><span class="header-section-number">8.2.2</span> Gradient-Based Learning<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Neural network training relies on backpropagation to compute gradients of the temporal difference error with respect to all network parameters. The loss function for a single transition becomes:</p>
<p><span class="math display">\[
L(\theta) = \frac{1}{2}(y - Q(s, a; \theta))^2
\]</span></p>
<p>where <span class="math inline">\(y = r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;; \theta)\)</span> is the TD target. The gradient with respect to parameters <span class="math inline">\(\theta\)</span> follows the chain rule:</p>
<p><span class="math display">\[
\nabla_\theta L(\theta) = (Q(s, a; \theta) - y) \nabla_\theta Q(s, a; \theta)
\]</span></p>
<p>This gradient guides parameter updates through standard optimization algorithms, though the non-convex nature of neural network loss surfaces introduces challenges absent in linear approximation.</p>
</div>
<div id="comparison-with-previous-approaches" class="section level3 hasAnchor" number="8.2.3">
<h3><span class="header-section-number">8.2.3</span> Comparison with Previous Approaches<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Neural networks offer several theoretical advantages over linear and tree-based methods. Unlike linear approximation, they can learn feature interactions without explicit engineering. Unlike Random Forests, they provide smooth function approximations suitable for gradient-based optimization. However, this flexibility comes with increased computational complexity and potential instability during training.</p>
<table style="width:100%;">
<colgroup>
<col width="22%" />
<col width="30%" />
<col width="23%" />
<col width="23%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Characteristic</strong></th>
<th><strong>Linear Approximation</strong></th>
<th><strong>Random Forest</strong></th>
<th><strong>Neural Network</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Function Class</strong></td>
<td>Linear combinations</td>
<td>Piecewise constant</td>
<td>Universal approximators</td>
</tr>
<tr class="even">
<td><strong>Feature Learning</strong></td>
<td>None</td>
<td>Implicit via splits</td>
<td>Explicit representation learning</td>
</tr>
<tr class="odd">
<td><strong>Optimization</strong></td>
<td>Convex (guaranteed convergence)</td>
<td>Non-parametric</td>
<td>Non-convex (local minima)</td>
</tr>
<tr class="even">
<td><strong>Interpretability</strong></td>
<td>High (weight inspection)</td>
<td>Moderate (tree visualization)</td>
<td>Low (distributed representations)</td>
</tr>
<tr class="odd">
<td><strong>Sample Efficiency</strong></td>
<td>High</td>
<td>Moderate</td>
<td>Variable (depends on architecture)</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="r-implementation-2" class="section level2 hasAnchor" number="8.3">
<h2><span class="header-section-number">8.3</span> R Implementation<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our neural network implementation builds upon the established environment while introducing the complexities of gradient-based optimization and network training. The <code>nnet</code> package provides a lightweight implementation suitable for demonstrating core concepts without the overhead of deep learning frameworks.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-1" tabindex="-1"></a><span class="co"># Load required libraries</span></span>
<span id="cb93-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-2" tabindex="-1"></a><span class="fu">library</span>(nnet)</span>
<span id="cb93-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-3" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb93-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-4" tabindex="-1"></a></span>
<span id="cb93-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-5" tabindex="-1"></a><span class="co"># Environment setup (consistent with previous implementations)</span></span>
<span id="cb93-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-6" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb93-7"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-7" tabindex="-1"></a>n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb93-8"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-8" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb93-9"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-9" tabindex="-1"></a>terminal_state <span class="ot">&lt;-</span> n_states</span>
<span id="cb93-10"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-10" tabindex="-1"></a></span>
<span id="cb93-11"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-11" tabindex="-1"></a><span class="co"># Environment: transition and reward models with FIXED probability normalization</span></span>
<span id="cb93-12"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-12" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb93-13"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-13" tabindex="-1"></a>transition_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb93-14"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-14" tabindex="-1"></a>reward_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb93-15"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-15" tabindex="-1"></a></span>
<span id="cb93-16"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-16" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>)) {</span>
<span id="cb93-17"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-17" tabindex="-1"></a>  <span class="co"># Action 1: deterministic to next state with small random component</span></span>
<span id="cb93-18"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-18" tabindex="-1"></a>  next_state <span class="ot">&lt;-</span> s <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb93-19"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-19" tabindex="-1"></a>  random_state <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span>n_states, next_state), <span class="dv">1</span>)</span>
<span id="cb93-20"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-20" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, next_state] <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb93-21"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-21" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, random_state] <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb93-22"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-22" tabindex="-1"></a>  </span>
<span id="cb93-23"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-23" tabindex="-1"></a>  <span class="co"># Action 2: two random transitions</span></span>
<span id="cb93-24"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-24" tabindex="-1"></a>  random_states <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">2</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb93-25"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-25" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, random_states[<span class="dv">1</span>]] <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb93-26"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-26" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, random_states[<span class="dv">2</span>]] <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb93-27"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-27" tabindex="-1"></a>  </span>
<span id="cb93-28"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-28" tabindex="-1"></a>  <span class="co"># Normalize to ensure probabilities sum to 1.0 (safety check)</span></span>
<span id="cb93-29"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-29" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, ] <span class="ot">&lt;-</span> transition_model[s, <span class="dv">1</span>, ] <span class="sc">/</span> <span class="fu">sum</span>(transition_model[s, <span class="dv">1</span>, ])</span>
<span id="cb93-30"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-30" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, ] <span class="ot">&lt;-</span> transition_model[s, <span class="dv">2</span>, ] <span class="sc">/</span> <span class="fu">sum</span>(transition_model[s, <span class="dv">2</span>, ])</span>
<span id="cb93-31"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-31" tabindex="-1"></a>  </span>
<span id="cb93-32"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-32" tabindex="-1"></a>  <span class="co"># Reward model</span></span>
<span id="cb93-33"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-33" tabindex="-1"></a>  <span class="cf">for</span> (s_prime <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_states) {</span>
<span id="cb93-34"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-34" tabindex="-1"></a>    reward_model[s, <span class="dv">1</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">1.0</span>, <span class="fl">0.1</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb93-35"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-35" tabindex="-1"></a>    reward_model[s, <span class="dv">2</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">0.5</span>, <span class="fl">0.05</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb93-36"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-36" tabindex="-1"></a>  }</span>
<span id="cb93-37"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-37" tabindex="-1"></a>}</span>
<span id="cb93-38"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-38" tabindex="-1"></a></span>
<span id="cb93-39"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-39" tabindex="-1"></a><span class="co"># Terminal state has no transitions</span></span>
<span id="cb93-40"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-40" tabindex="-1"></a>transition_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb93-41"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-41" tabindex="-1"></a>reward_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb93-42"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-42" tabindex="-1"></a></span>
<span id="cb93-43"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-43" tabindex="-1"></a><span class="co"># Sampling function</span></span>
<span id="cb93-44"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-44" tabindex="-1"></a>sample_env <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a) {</span>
<span id="cb93-45"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-45" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> transition_model[s, a, ]</span>
<span id="cb93-46"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-46" tabindex="-1"></a>  s_prime <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>, <span class="at">prob =</span> probs)</span>
<span id="cb93-47"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-47" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> reward_model[s, a, s_prime]</span>
<span id="cb93-48"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-48" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">s_prime =</span> s_prime, <span class="at">reward =</span> reward)</span>
<span id="cb93-49"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-49" tabindex="-1"></a>}</span>
<span id="cb93-50"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-50" tabindex="-1"></a></span>
<span id="cb93-51"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-51" tabindex="-1"></a><span class="co"># Feature encoding for neural network input</span></span>
<span id="cb93-52"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-52" tabindex="-1"></a>encode_features <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a, n_states, n_actions) {</span>
<span id="cb93-53"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-53" tabindex="-1"></a>  state_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb93-54"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-54" tabindex="-1"></a>  action_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_actions)</span>
<span id="cb93-55"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-55" tabindex="-1"></a>  state_vec[s] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb93-56"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-56" tabindex="-1"></a>  action_vec[a] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb93-57"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-57" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(state_vec, action_vec))</span>
<span id="cb93-58"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-58" tabindex="-1"></a>}</span>
<span id="cb93-59"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-59" tabindex="-1"></a></span>
<span id="cb93-60"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-60" tabindex="-1"></a>n_features <span class="ot">&lt;-</span> n_states <span class="sc">+</span> n_actions</span>
<span id="cb93-61"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-61" tabindex="-1"></a></span>
<span id="cb93-62"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-62" tabindex="-1"></a><span class="co"># FIXED Q-Learning with neural network function approximation</span></span>
<span id="cb93-63"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-63" tabindex="-1"></a>q_learning_nn <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">epsilon_start =</span> <span class="fl">0.5</span>, <span class="at">epsilon_end =</span> <span class="fl">0.01</span>,</span>
<span id="cb93-64"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-64" tabindex="-1"></a>                          <span class="at">epsilon_decay =</span> <span class="fl">0.995</span>, <span class="at">hidden_size =</span> <span class="dv">10</span>, </span>
<span id="cb93-65"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-65" tabindex="-1"></a>                          <span class="at">retrain_freq =</span> <span class="dv">10</span>, <span class="at">min_samples =</span> <span class="dv">50</span>, <span class="at">max_buffer =</span> <span class="dv">5000</span>,</span>
<span id="cb93-66"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-66" tabindex="-1"></a>                          <span class="at">batch_fraction =</span> <span class="fl">1.0</span>) {</span>
<span id="cb93-67"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-67" tabindex="-1"></a>  </span>
<span id="cb93-68"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-68" tabindex="-1"></a>  <span class="co"># Pre-allocate storage for efficiency (fixed rbind issue)</span></span>
<span id="cb93-69"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-69" tabindex="-1"></a>  max_steps_per_episode <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb93-70"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-70" tabindex="-1"></a>  buffer_size <span class="ot">&lt;-</span> episodes <span class="sc">*</span> max_steps_per_episode</span>
<span id="cb93-71"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-71" tabindex="-1"></a>  q_data_x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> buffer_size, <span class="at">ncol =</span> n_features)</span>
<span id="cb93-72"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-72" tabindex="-1"></a>  q_data_y <span class="ot">&lt;-</span> <span class="fu">numeric</span>(buffer_size)</span>
<span id="cb93-73"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-73" tabindex="-1"></a>  data_idx <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb93-74"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-74" tabindex="-1"></a>  </span>
<span id="cb93-75"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-75" tabindex="-1"></a>  q_model <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb93-76"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-76" tabindex="-1"></a>  rewards <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb93-77"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-77" tabindex="-1"></a>  training_losses <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb93-78"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-78" tabindex="-1"></a>  epsilon <span class="ot">&lt;-</span> epsilon_start</span>
<span id="cb93-79"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-79" tabindex="-1"></a>  </span>
<span id="cb93-80"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-80" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb93-81"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-81" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>), <span class="dv">1</span>)  <span class="co"># Start from non-terminal state</span></span>
<span id="cb93-82"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-82" tabindex="-1"></a>    episode_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb93-83"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-83" tabindex="-1"></a>    steps <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb93-84"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-84" tabindex="-1"></a>    </span>
<span id="cb93-85"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-85" tabindex="-1"></a>    <span class="cf">while</span> (steps <span class="sc">&lt;</span> max_steps_per_episode) {</span>
<span id="cb93-86"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-86" tabindex="-1"></a>      steps <span class="ot">&lt;-</span> steps <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb93-87"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-87" tabindex="-1"></a>      </span>
<span id="cb93-88"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-88" tabindex="-1"></a>      <span class="co"># Predict Q-values for all actions</span></span>
<span id="cb93-89"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-89" tabindex="-1"></a>      q_preds <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb93-90"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-90" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb93-91"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-91" tabindex="-1"></a>        <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(q_model)) {</span>
<span id="cb93-92"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-92" tabindex="-1"></a>          <span class="fu">as.numeric</span>(<span class="fu">predict</span>(q_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x))))</span>
<span id="cb93-93"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-93" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb93-94"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-94" tabindex="-1"></a>          <span class="dv">0</span>  <span class="co"># Initialize to zero instead of random</span></span>
<span id="cb93-95"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-95" tabindex="-1"></a>        }</span>
<span id="cb93-96"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-96" tabindex="-1"></a>      })</span>
<span id="cb93-97"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-97" tabindex="-1"></a>      </span>
<span id="cb93-98"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-98" tabindex="-1"></a>      <span class="co"># Epsilon-greedy action selection with decay</span></span>
<span id="cb93-99"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-99" tabindex="-1"></a>      a <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb93-100"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-100" tabindex="-1"></a>        <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb93-101"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-101" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb93-102"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-102" tabindex="-1"></a>        <span class="fu">which.max</span>(q_preds)</span>
<span id="cb93-103"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-103" tabindex="-1"></a>      }</span>
<span id="cb93-104"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-104" tabindex="-1"></a>      </span>
<span id="cb93-105"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-105" tabindex="-1"></a>      <span class="co"># Take action and observe outcome</span></span>
<span id="cb93-106"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-106" tabindex="-1"></a>      out <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a)</span>
<span id="cb93-107"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-107" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> out<span class="sc">$</span>s_prime</span>
<span id="cb93-108"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-108" tabindex="-1"></a>      r <span class="ot">&lt;-</span> out<span class="sc">$</span>reward</span>
<span id="cb93-109"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-109" tabindex="-1"></a>      episode_reward <span class="ot">&lt;-</span> episode_reward <span class="sc">+</span> r</span>
<span id="cb93-110"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-110" tabindex="-1"></a>      </span>
<span id="cb93-111"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-111" tabindex="-1"></a>      <span class="co"># Compute TD target</span></span>
<span id="cb93-112"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-112" tabindex="-1"></a>      q_next <span class="ot">&lt;-</span> <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) {</span>
<span id="cb93-113"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-113" tabindex="-1"></a>        <span class="dv">0</span></span>
<span id="cb93-114"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-114" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb93-115"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-115" tabindex="-1"></a>        <span class="fu">max</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a_) {</span>
<span id="cb93-116"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-116" tabindex="-1"></a>          x_next <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s_prime, a_, n_states, n_actions)</span>
<span id="cb93-117"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-117" tabindex="-1"></a>          <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(q_model)) {</span>
<span id="cb93-118"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-118" tabindex="-1"></a>            <span class="fu">as.numeric</span>(<span class="fu">predict</span>(q_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x_next))))</span>
<span id="cb93-119"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-119" tabindex="-1"></a>          } <span class="cf">else</span> {</span>
<span id="cb93-120"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-120" tabindex="-1"></a>            <span class="dv">0</span></span>
<span id="cb93-121"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-121" tabindex="-1"></a>          }</span>
<span id="cb93-122"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-122" tabindex="-1"></a>        }))</span>
<span id="cb93-123"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-123" tabindex="-1"></a>      }</span>
<span id="cb93-124"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-124" tabindex="-1"></a>      </span>
<span id="cb93-125"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-125" tabindex="-1"></a>      target <span class="ot">&lt;-</span> r <span class="sc">+</span> gamma <span class="sc">*</span> q_next</span>
<span id="cb93-126"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-126" tabindex="-1"></a>      </span>
<span id="cb93-127"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-127" tabindex="-1"></a>      <span class="co"># Store training example efficiently</span></span>
<span id="cb93-128"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-128" tabindex="-1"></a>      data_idx <span class="ot">&lt;-</span> data_idx <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb93-129"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-129" tabindex="-1"></a>      <span class="cf">if</span> (data_idx <span class="sc">&gt;</span> buffer_size) {</span>
<span id="cb93-130"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-130" tabindex="-1"></a>        <span class="co"># Implement circular buffer to prevent overflow</span></span>
<span id="cb93-131"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-131" tabindex="-1"></a>        data_idx <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb93-132"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-132" tabindex="-1"></a>      }</span>
<span id="cb93-133"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-133" tabindex="-1"></a>      x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb93-134"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-134" tabindex="-1"></a>      q_data_x[data_idx, ] <span class="ot">&lt;-</span> x</span>
<span id="cb93-135"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-135" tabindex="-1"></a>      q_data_y[data_idx] <span class="ot">&lt;-</span> target</span>
<span id="cb93-136"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-136" tabindex="-1"></a>      </span>
<span id="cb93-137"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-137" tabindex="-1"></a>      <span class="co"># Train neural network periodically with proper error handling</span></span>
<span id="cb93-138"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-138" tabindex="-1"></a>      current_data_size <span class="ot">&lt;-</span> <span class="fu">min</span>(data_idx, buffer_size)</span>
<span id="cb93-139"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-139" tabindex="-1"></a>      <span class="cf">if</span> (current_data_size <span class="sc">&gt;=</span> min_samples <span class="sc">&amp;&amp;</span> ep <span class="sc">%%</span> retrain_freq <span class="sc">==</span> <span class="dv">0</span> <span class="sc">&amp;&amp;</span> steps <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb93-140"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-140" tabindex="-1"></a>        <span class="co"># Use experience replay: sample from buffer if too large</span></span>
<span id="cb93-141"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-141" tabindex="-1"></a>        train_size <span class="ot">&lt;-</span> <span class="fu">min</span>(current_data_size, max_buffer)</span>
<span id="cb93-142"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-142" tabindex="-1"></a>        <span class="cf">if</span> (current_data_size <span class="sc">&gt;</span> max_buffer) {</span>
<span id="cb93-143"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-143" tabindex="-1"></a>          sample_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>current_data_size, train_size)</span>
<span id="cb93-144"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-144" tabindex="-1"></a>          train_x <span class="ot">&lt;-</span> q_data_x[sample_idx, , drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb93-145"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-145" tabindex="-1"></a>          train_y <span class="ot">&lt;-</span> q_data_y[sample_idx]</span>
<span id="cb93-146"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-146" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb93-147"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-147" tabindex="-1"></a>          train_x <span class="ot">&lt;-</span> q_data_x[<span class="dv">1</span><span class="sc">:</span>current_data_size, , drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb93-148"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-148" tabindex="-1"></a>          train_y <span class="ot">&lt;-</span> q_data_y[<span class="dv">1</span><span class="sc">:</span>current_data_size]</span>
<span id="cb93-149"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-149" tabindex="-1"></a>        }</span>
<span id="cb93-150"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-150" tabindex="-1"></a>        </span>
<span id="cb93-151"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-151" tabindex="-1"></a>        <span class="co"># Train with error handling</span></span>
<span id="cb93-152"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-152" tabindex="-1"></a>        <span class="fu">tryCatch</span>({</span>
<span id="cb93-153"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-153" tabindex="-1"></a>          q_model <span class="ot">&lt;-</span> <span class="fu">nnet</span>(</span>
<span id="cb93-154"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-154" tabindex="-1"></a>            <span class="at">x =</span> train_x,</span>
<span id="cb93-155"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-155" tabindex="-1"></a>            <span class="at">y =</span> train_y,</span>
<span id="cb93-156"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-156" tabindex="-1"></a>            <span class="at">size =</span> hidden_size,</span>
<span id="cb93-157"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-157" tabindex="-1"></a>            <span class="at">linout =</span> <span class="cn">TRUE</span>,</span>
<span id="cb93-158"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-158" tabindex="-1"></a>            <span class="at">maxit =</span> <span class="dv">200</span>,</span>
<span id="cb93-159"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-159" tabindex="-1"></a>            <span class="at">decay =</span> <span class="fl">0.01</span>,</span>
<span id="cb93-160"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-160" tabindex="-1"></a>            <span class="at">trace =</span> <span class="cn">FALSE</span></span>
<span id="cb93-161"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-161" tabindex="-1"></a>          )</span>
<span id="cb93-162"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-162" tabindex="-1"></a>          </span>
<span id="cb93-163"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-163" tabindex="-1"></a>          <span class="co"># Track training loss on training data</span></span>
<span id="cb93-164"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-164" tabindex="-1"></a>          predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(q_model, <span class="fu">as.data.frame</span>(train_x))</span>
<span id="cb93-165"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-165" tabindex="-1"></a>          mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((predictions <span class="sc">-</span> train_y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb93-166"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-166" tabindex="-1"></a>          training_losses <span class="ot">&lt;-</span> <span class="fu">c</span>(training_losses, mse)</span>
<span id="cb93-167"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-167" tabindex="-1"></a>        }, <span class="at">error =</span> <span class="cf">function</span>(e) {</span>
<span id="cb93-168"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-168" tabindex="-1"></a>          <span class="fu">warning</span>(<span class="fu">paste</span>(<span class="st">&quot;Neural network training failed at episode&quot;</span>, ep, <span class="st">&quot;:&quot;</span>, e<span class="sc">$</span>message))</span>
<span id="cb93-169"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-169" tabindex="-1"></a>        })</span>
<span id="cb93-170"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-170" tabindex="-1"></a>      }</span>
<span id="cb93-171"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-171" tabindex="-1"></a>      </span>
<span id="cb93-172"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-172" tabindex="-1"></a>      <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) <span class="cf">break</span></span>
<span id="cb93-173"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-173" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb93-174"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-174" tabindex="-1"></a>    }</span>
<span id="cb93-175"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-175" tabindex="-1"></a>    </span>
<span id="cb93-176"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-176" tabindex="-1"></a>    rewards[ep] <span class="ot">&lt;-</span> episode_reward</span>
<span id="cb93-177"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-177" tabindex="-1"></a>    </span>
<span id="cb93-178"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-178" tabindex="-1"></a>    <span class="co"># Decay epsilon</span></span>
<span id="cb93-179"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-179" tabindex="-1"></a>    epsilon <span class="ot">&lt;-</span> <span class="fu">max</span>(epsilon_end, epsilon <span class="sc">*</span> epsilon_decay)</span>
<span id="cb93-180"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-180" tabindex="-1"></a>  }</span>
<span id="cb93-181"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-181" tabindex="-1"></a>  </span>
<span id="cb93-182"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-182" tabindex="-1"></a>  <span class="co"># Trim unused buffer space</span></span>
<span id="cb93-183"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-183" tabindex="-1"></a>  actual_size <span class="ot">&lt;-</span> <span class="fu">min</span>(data_idx, buffer_size)</span>
<span id="cb93-184"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-184" tabindex="-1"></a>  q_data_x <span class="ot">&lt;-</span> q_data_x[<span class="dv">1</span><span class="sc">:</span>actual_size, , drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb93-185"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-185" tabindex="-1"></a>  q_data_y <span class="ot">&lt;-</span> q_data_y[<span class="dv">1</span><span class="sc">:</span>actual_size]</span>
<span id="cb93-186"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-186" tabindex="-1"></a>  </span>
<span id="cb93-187"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-187" tabindex="-1"></a>  <span class="co"># Derive final policy</span></span>
<span id="cb93-188"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-188" tabindex="-1"></a>  policy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), <span class="cf">function</span>(s) {</span>
<span id="cb93-189"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-189" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(q_model)) {</span>
<span id="cb93-190"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-190" tabindex="-1"></a>      q_vals <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb93-191"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-191" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb93-192"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-192" tabindex="-1"></a>        <span class="fu">as.numeric</span>(<span class="fu">predict</span>(q_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x))))</span>
<span id="cb93-193"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-193" tabindex="-1"></a>      })</span>
<span id="cb93-194"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-194" tabindex="-1"></a>      <span class="fu">which.max</span>(q_vals)</span>
<span id="cb93-195"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-195" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb93-196"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-196" tabindex="-1"></a>      <span class="dv">1</span>  <span class="co"># Default action</span></span>
<span id="cb93-197"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-197" tabindex="-1"></a>    }</span>
<span id="cb93-198"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-198" tabindex="-1"></a>  })</span>
<span id="cb93-199"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-199" tabindex="-1"></a>  </span>
<span id="cb93-200"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-200" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">model =</span> q_model, <span class="at">policy =</span> <span class="fu">c</span>(policy, <span class="cn">NA</span>), <span class="at">rewards =</span> rewards,</span>
<span id="cb93-201"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-201" tabindex="-1"></a>       <span class="at">training_losses =</span> training_losses,</span>
<span id="cb93-202"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-202" tabindex="-1"></a>       <span class="at">training_data =</span> <span class="fu">list</span>(<span class="at">x =</span> q_data_x, <span class="at">y =</span> q_data_y))</span>
<span id="cb93-203"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-203" tabindex="-1"></a>}</span>
<span id="cb93-204"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-204" tabindex="-1"></a></span>
<span id="cb93-205"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-205" tabindex="-1"></a><span class="co"># Run Q-Learning with neural network approximation</span></span>
<span id="cb93-206"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-206" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb93-207"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-207" tabindex="-1"></a>nn_result <span class="ot">&lt;-</span> <span class="fu">q_learning_nn</span>(</span>
<span id="cb93-208"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-208" tabindex="-1"></a>  <span class="at">episodes =</span> <span class="dv">1000</span>, </span>
<span id="cb93-209"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-209" tabindex="-1"></a>  <span class="at">epsilon_start =</span> <span class="fl">0.5</span>,</span>
<span id="cb93-210"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-210" tabindex="-1"></a>  <span class="at">epsilon_end =</span> <span class="fl">0.01</span>,</span>
<span id="cb93-211"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-211" tabindex="-1"></a>  <span class="at">epsilon_decay =</span> <span class="fl">0.995</span>,</span>
<span id="cb93-212"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-212" tabindex="-1"></a>  <span class="at">hidden_size =</span> <span class="dv">15</span>,</span>
<span id="cb93-213"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-213" tabindex="-1"></a>  <span class="at">retrain_freq =</span> <span class="dv">10</span>,</span>
<span id="cb93-214"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-214" tabindex="-1"></a>  <span class="at">min_samples =</span> <span class="dv">100</span>,</span>
<span id="cb93-215"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-215" tabindex="-1"></a>  <span class="at">max_buffer =</span> <span class="dv">5000</span></span>
<span id="cb93-216"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-216" tabindex="-1"></a>)</span>
<span id="cb93-217"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-217" tabindex="-1"></a>nn_policy <span class="ot">&lt;-</span> nn_result<span class="sc">$</span>policy</span>
<span id="cb93-218"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-218" tabindex="-1"></a>nn_rewards <span class="ot">&lt;-</span> nn_result<span class="sc">$</span>rewards</span>
<span id="cb93-219"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-219" tabindex="-1"></a></span>
<span id="cb93-220"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-220" tabindex="-1"></a><span class="co"># Visualize learned policy</span></span>
<span id="cb93-221"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-221" tabindex="-1"></a>policy_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb93-222"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-222" tabindex="-1"></a>  <span class="at">State =</span> <span class="dv">1</span><span class="sc">:</span>n_states,</span>
<span id="cb93-223"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-223" tabindex="-1"></a>  <span class="at">Policy =</span> nn_policy,</span>
<span id="cb93-224"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-224" tabindex="-1"></a>  <span class="at">Algorithm =</span> <span class="st">&quot;Q-Learning NN&quot;</span></span>
<span id="cb93-225"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-225" tabindex="-1"></a>)</span>
<span id="cb93-226"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-226" tabindex="-1"></a></span>
<span id="cb93-227"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-227" tabindex="-1"></a>policy_plot_nn <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(policy_df[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), ], <span class="fu">aes</span>(<span class="at">x =</span> State, <span class="at">y =</span> Policy)) <span class="sc">+</span></span>
<span id="cb93-228"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-228" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">4</span>, <span class="at">color =</span> <span class="st">&quot;coral&quot;</span>) <span class="sc">+</span></span>
<span id="cb93-229"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-229" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;coral&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb93-230"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-230" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb93-231"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-231" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb93-232"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-232" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Policy from Q-Learning with Neural Network Approximation&quot;</span>,</span>
<span id="cb93-233"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-233" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;State&quot;</span>, </span>
<span id="cb93-234"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-234" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Action&quot;</span></span>
<span id="cb93-235"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-235" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb93-236"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-236" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>n_states) <span class="sc">+</span></span>
<span id="cb93-237"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-237" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>n_actions, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Action 1&quot;</span>, <span class="st">&quot;Action 2&quot;</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">2.5</span>)) <span class="sc">+</span></span>
<span id="cb93-238"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-238" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb93-239"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-239" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb93-240"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-240" tabindex="-1"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb93-241"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-241" tabindex="-1"></a>    <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>)</span>
<span id="cb93-242"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-242" tabindex="-1"></a>  )</span>
<span id="cb93-243"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-243" tabindex="-1"></a></span>
<span id="cb93-244"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-244" tabindex="-1"></a><span class="co"># Learning curve with smoothing</span></span>
<span id="cb93-245"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-245" tabindex="-1"></a>rewards_smooth <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(nn_rewards))</span>
<span id="cb93-246"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-246" tabindex="-1"></a>window_size <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb93-247"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-247" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(nn_rewards)) {</span>
<span id="cb93-248"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-248" tabindex="-1"></a>  start_idx <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="dv">1</span>, i <span class="sc">-</span> window_size <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb93-249"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-249" tabindex="-1"></a>  rewards_smooth[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(nn_rewards[start_idx<span class="sc">:</span>i])</span>
<span id="cb93-250"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-250" tabindex="-1"></a>}</span>
<span id="cb93-251"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-251" tabindex="-1"></a></span>
<span id="cb93-252"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-252" tabindex="-1"></a>reward_df_nn <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb93-253"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-253" tabindex="-1"></a>  <span class="at">Episode =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>,</span>
<span id="cb93-254"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-254" tabindex="-1"></a>  <span class="at">Reward =</span> rewards_smooth,</span>
<span id="cb93-255"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-255" tabindex="-1"></a>  <span class="at">Algorithm =</span> <span class="st">&quot;Q-Learning NN&quot;</span></span>
<span id="cb93-256"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-256" tabindex="-1"></a>)</span>
<span id="cb93-257"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-257" tabindex="-1"></a></span>
<span id="cb93-258"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-258" tabindex="-1"></a>reward_plot_nn <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(reward_df_nn, <span class="fu">aes</span>(<span class="at">x =</span> Episode, <span class="at">y =</span> Reward)) <span class="sc">+</span></span>
<span id="cb93-259"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-259" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;coral&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb93-260"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-260" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb93-261"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-261" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb93-262"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-262" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Learning Curve: Q-Learning with Neural Network (50-episode moving average)&quot;</span>,</span>
<span id="cb93-263"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-263" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Episode&quot;</span>,</span>
<span id="cb93-264"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-264" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Average Reward&quot;</span></span>
<span id="cb93-265"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-265" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb93-266"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-266" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb93-267"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-267" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb93-268"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-268" tabindex="-1"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb93-269"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-269" tabindex="-1"></a>    <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>)</span>
<span id="cb93-270"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-270" tabindex="-1"></a>  )</span>
<span id="cb93-271"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-271" tabindex="-1"></a></span>
<span id="cb93-272"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-272" tabindex="-1"></a><span class="co"># Training loss evolution</span></span>
<span id="cb93-273"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-273" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">length</span>(nn_result<span class="sc">$</span>training_losses) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb93-274"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-274" tabindex="-1"></a>  loss_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb93-275"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-275" tabindex="-1"></a>    <span class="at">Update =</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(nn_result<span class="sc">$</span>training_losses),</span>
<span id="cb93-276"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-276" tabindex="-1"></a>    <span class="at">Loss =</span> nn_result<span class="sc">$</span>training_losses</span>
<span id="cb93-277"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-277" tabindex="-1"></a>  )</span>
<span id="cb93-278"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-278" tabindex="-1"></a>  </span>
<span id="cb93-279"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-279" tabindex="-1"></a>  loss_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(loss_df, <span class="fu">aes</span>(<span class="at">x =</span> Update, <span class="at">y =</span> Loss)) <span class="sc">+</span></span>
<span id="cb93-280"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-280" tabindex="-1"></a>    <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;darkred&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb93-281"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-281" tabindex="-1"></a>    <span class="fu">geom_point</span>(<span class="at">color =</span> <span class="st">&quot;darkred&quot;</span>, <span class="at">size =</span> <span class="dv">2</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb93-282"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-282" tabindex="-1"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb93-283"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-283" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb93-284"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-284" tabindex="-1"></a>      <span class="at">title =</span> <span class="st">&quot;Neural Network Training Loss Evolution&quot;</span>,</span>
<span id="cb93-285"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-285" tabindex="-1"></a>      <span class="at">x =</span> <span class="st">&quot;Training Update&quot;</span>,</span>
<span id="cb93-286"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-286" tabindex="-1"></a>      <span class="at">y =</span> <span class="st">&quot;Mean Squared Error&quot;</span></span>
<span id="cb93-287"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-287" tabindex="-1"></a>    ) <span class="sc">+</span></span>
<span id="cb93-288"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-288" tabindex="-1"></a>    <span class="fu">theme</span>(</span>
<span id="cb93-289"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-289" tabindex="-1"></a>      <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb93-290"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-290" tabindex="-1"></a>      <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb93-291"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-291" tabindex="-1"></a>      <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>)</span>
<span id="cb93-292"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-292" tabindex="-1"></a>    )</span>
<span id="cb93-293"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-293" tabindex="-1"></a>  </span>
<span id="cb93-294"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-294" tabindex="-1"></a>  <span class="fu">print</span>(loss_plot)</span>
<span id="cb93-295"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb93-295" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb94-1" tabindex="-1"></a><span class="co"># Display main plots</span></span>
<span id="cb94-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb94-2" tabindex="-1"></a><span class="fu">print</span>(policy_plot_nn)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-26-2.png" width="672" /></p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb95-1" tabindex="-1"></a><span class="fu">print</span>(reward_plot_nn)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-26-3.png" width="672" /></p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-1" tabindex="-1"></a><span class="co"># Model diagnostics and analysis</span></span>
<span id="cb96-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-2" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(nn_result<span class="sc">$</span>model)) {</span>
<span id="cb96-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-3" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">=== Neural Network Model Summary ===</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-4" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Architecture: Input(&quot;</span>, n_features, <span class="st">&quot;) -&gt; Hidden(&quot;</span>, nn_result<span class="sc">$</span>model<span class="sc">$</span>n[<span class="dv">2</span>], <span class="st">&quot;) -&gt; Output(1)</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-5" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Total parameters:&quot;</span>, <span class="fu">length</span>(nn_result<span class="sc">$</span>model<span class="sc">$</span>wts), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-6" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Training examples collected:&quot;</span>, <span class="fu">nrow</span>(nn_result<span class="sc">$</span>training_data<span class="sc">$</span>x), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-7"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-7" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Number of training updates:&quot;</span>, <span class="fu">length</span>(nn_result<span class="sc">$</span>training_losses), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-8"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-8" tabindex="-1"></a>  </span>
<span id="cb96-9"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-9" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(nn_result<span class="sc">$</span>training_losses) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb96-10"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-10" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">&quot;Final training loss:&quot;</span>, <span class="fu">round</span>(<span class="fu">tail</span>(nn_result<span class="sc">$</span>training_losses, <span class="dv">1</span>), <span class="dv">6</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-11"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-11" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">&quot;Best training loss:&quot;</span>, <span class="fu">round</span>(<span class="fu">min</span>(nn_result<span class="sc">$</span>training_losses), <span class="dv">6</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-12"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-12" tabindex="-1"></a>  }</span>
<span id="cb96-13"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-13" tabindex="-1"></a>  </span>
<span id="cb96-14"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-14" tabindex="-1"></a>  <span class="co"># Weight analysis</span></span>
<span id="cb96-15"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-15" tabindex="-1"></a>  weights <span class="ot">&lt;-</span> nn_result<span class="sc">$</span>model<span class="sc">$</span>wts</span>
<span id="cb96-16"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-16" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">=== Weight Statistics ===</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-17"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-17" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Mean:&quot;</span>, <span class="fu">round</span>(<span class="fu">mean</span>(weights), <span class="dv">4</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-18"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-18" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Standard deviation:&quot;</span>, <span class="fu">round</span>(<span class="fu">sd</span>(weights), <span class="dv">4</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-19"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-19" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Range: [&quot;</span>, <span class="fu">round</span>(<span class="fu">min</span>(weights), <span class="dv">4</span>), <span class="st">&quot;,&quot;</span>, <span class="fu">round</span>(<span class="fu">max</span>(weights), <span class="dv">4</span>), <span class="st">&quot;]</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-20"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-20" tabindex="-1"></a>  </span>
<span id="cb96-21"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-21" tabindex="-1"></a>  <span class="co"># Policy analysis</span></span>
<span id="cb96-22"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-22" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">=== Policy Summary ===</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-23"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-23" tabindex="-1"></a>  action1_count <span class="ot">&lt;-</span> <span class="fu">sum</span>(nn_policy[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)] <span class="sc">==</span> <span class="dv">1</span>, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb96-24"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-24" tabindex="-1"></a>  action2_count <span class="ot">&lt;-</span> <span class="fu">sum</span>(nn_policy[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)] <span class="sc">==</span> <span class="dv">2</span>, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)</span>
<span id="cb96-25"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-25" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Action 1 chosen:&quot;</span>, action1_count, <span class="st">&quot;times</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-26"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-26" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Action 2 chosen:&quot;</span>, action2_count, <span class="st">&quot;times</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-27"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-27" tabindex="-1"></a>  </span>
<span id="cb96-28"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-28" tabindex="-1"></a>  <span class="co"># Final performance</span></span>
<span id="cb96-29"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-29" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">=== Performance Metrics ===</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-30"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-30" tabindex="-1"></a>  final_100_avg <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">tail</span>(nn_rewards, <span class="dv">100</span>))</span>
<span id="cb96-31"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-31" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Average reward (last 100 episodes):&quot;</span>, <span class="fu">round</span>(final_100_avg, <span class="dv">4</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-32"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-32" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Maximum episode reward:&quot;</span>, <span class="fu">round</span>(<span class="fu">max</span>(nn_rewards), <span class="dv">4</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-33"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-33" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Minimum episode reward:&quot;</span>, <span class="fu">round</span>(<span class="fu">min</span>(nn_rewards), <span class="dv">4</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb96-34"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb96-34" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## 
## === Neural Network Model Summary ===
## Architecture: Input( 12 ) -&gt; Hidden( 15 ) -&gt; Output(1)
## Total parameters: 211 
## Training examples collected: 2771 
## Number of training updates: 99 
## Final training loss: 0.024524 
## Best training loss: 0.024524 
## 
## === Weight Statistics ===
## Mean: -0.1505 
## Standard deviation: 0.4385 
## Range: [ -1.7454 , 2.0853 ]
## 
## === Policy Summary ===
## Action 1 chosen: 3 times
## Action 2 chosen: 6 times
## 
## === Performance Metrics ===
## Average reward (last 100 episodes): 0.9508 
## Maximum episode reward: 1.5436 
## Minimum episode reward: 0.5</code></pre>
<p>I’ll explain this code in detail, chunk by chunk.</p>
<p><strong>1. Setup and Environment Configuration</strong></p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb98-1" tabindex="-1"></a><span class="fu">library</span>(nnet)</span>
<span id="cb98-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb98-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb98-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb98-3" tabindex="-1"></a></span>
<span id="cb98-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb98-4" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb98-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb98-5" tabindex="-1"></a>n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb98-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb98-6" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb98-7"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb98-7" tabindex="-1"></a>terminal_state <span class="ot">&lt;-</span> n_states</span></code></pre></div>
<ul>
<li>Loads neural network (<code>nnet</code>) and plotting (<code>ggplot2</code>) libraries</li>
<li>Defines the MDP (Markov Decision Process) parameters:
<ul>
<li><strong>10 states</strong> (states 1-9 are active, state 10 is terminal)</li>
<li><strong>2 possible actions</strong> in each state</li>
<li><strong>γ = 0.9</strong>: discount factor for future rewards (weighs immediate vs future rewards)</li>
<li>Terminal state is state 10 (episode ends here)</li>
</ul></li>
</ul>
<p><strong>2. Building the Environment (Transition &amp; Reward Models)</strong></p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb99-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb99-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb99-2" tabindex="-1"></a>transition_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb99-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb99-3" tabindex="-1"></a>reward_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span></code></pre></div>
<ul>
<li>Sets random seed for reproducibility</li>
<li>Creates 3D arrays to store:
<ul>
<li><strong>Transition probabilities</strong>: P(s’|s,a) - probability of reaching state s’ from state s taking action a</li>
<li><strong>Rewards</strong>: R(s,a,s’) - reward received for transition from s to s’ via action a</li>
</ul></li>
</ul>
<p><strong>Action 1: Deterministic Forward Movement</strong></p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb100-1" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>)) {</span>
<span id="cb100-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb100-2" tabindex="-1"></a>  next_state <span class="ot">&lt;-</span> s <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb100-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb100-3" tabindex="-1"></a>  random_state <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span>n_states, next_state), <span class="dv">1</span>)</span>
<span id="cb100-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb100-4" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, next_state] <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb100-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb100-5" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, random_state] <span class="ot">&lt;-</span> <span class="fl">0.1</span></span></code></pre></div>
<ul>
<li><strong>Action 1</strong> moves forward with 90% probability (s → s+1)</li>
<li>10% chance of jumping to a random state (excluding the next state)</li>
<li><code>setdiff()</code> ensures no overlap between the 90% and 10% transitions (fixes the bug!)</li>
</ul>
<p><strong>Action 2: Random Transitions</strong></p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb101-1" tabindex="-1"></a>  random_states <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">2</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb101-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb101-2" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, random_states[<span class="dv">1</span>]] <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb101-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb101-3" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, random_states[<span class="dv">2</span>]] <span class="ot">&lt;-</span> <span class="fl">0.2</span></span></code></pre></div>
<ul>
<li><strong>Action 2</strong> jumps to two random states</li>
<li>80% probability to first random state</li>
<li>20% probability to second random state</li>
<li><code>replace = FALSE</code> ensures the two states are different</li>
</ul>
<p><strong>Probability Normalization</strong></p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb102-1" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, ] <span class="ot">&lt;-</span> transition_model[s, <span class="dv">1</span>, ] <span class="sc">/</span> <span class="fu">sum</span>(transition_model[s, <span class="dv">1</span>, ])</span>
<span id="cb102-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb102-2" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, ] <span class="ot">&lt;-</span> transition_model[s, <span class="dv">2</span>, ] <span class="sc">/</span> <span class="fu">sum</span>(transition_model[s, <span class="dv">2</span>, ])</span></code></pre></div>
<ul>
<li>Safety check: ensures all probabilities sum to exactly 1.0</li>
<li>Critical for valid probability distributions</li>
</ul>
<p><strong>Reward Structure</strong></p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb103-1" tabindex="-1"></a>  <span class="cf">for</span> (s_prime <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_states) {</span>
<span id="cb103-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb103-2" tabindex="-1"></a>    reward_model[s, <span class="dv">1</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">1.0</span>, <span class="fl">0.1</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb103-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb103-3" tabindex="-1"></a>    reward_model[s, <span class="dv">2</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">0.5</span>, <span class="fl">0.05</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb103-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb103-4" tabindex="-1"></a>  }</span></code></pre></div>
<ul>
<li><strong>Action 1</strong>: Big reward (1.0) for reaching terminal state, small random rewards (0-0.1) otherwise</li>
<li><strong>Action 2</strong>: Medium reward (0.5) for terminal state, tiny random rewards (0-0.05) otherwise</li>
<li>This makes Action 1 more attractive for reaching the goal</li>
</ul>
<p><strong>3. Environment Interaction Function</strong></p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb104-1" tabindex="-1"></a>sample_env <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a) {</span>
<span id="cb104-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb104-2" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> transition_model[s, a, ]</span>
<span id="cb104-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb104-3" tabindex="-1"></a>  s_prime <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>, <span class="at">prob =</span> probs)</span>
<span id="cb104-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb104-4" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> reward_model[s, a, s_prime]</span>
<span id="cb104-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb104-5" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">s_prime =</span> s_prime, <span class="at">reward =</span> reward)</span>
<span id="cb104-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb104-6" tabindex="-1"></a>}</span></code></pre></div>
<ul>
<li>Simulates taking action <code>a</code> in state <code>s</code></li>
<li>Samples next state according to transition probabilities</li>
<li>Returns the next state and reward received</li>
</ul>
<p><strong>4. Feature Encoding</strong></p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb105-1" tabindex="-1"></a>encode_features <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a, n_states, n_actions) {</span>
<span id="cb105-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb105-2" tabindex="-1"></a>  state_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb105-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb105-3" tabindex="-1"></a>  action_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_actions)</span>
<span id="cb105-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb105-4" tabindex="-1"></a>  state_vec[s] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb105-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb105-5" tabindex="-1"></a>  action_vec[a] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb105-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb105-6" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(state_vec, action_vec))</span>
<span id="cb105-7"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb105-7" tabindex="-1"></a>}</span></code></pre></div>
<ul>
<li>Converts (state, action) pair into neural network input</li>
<li>Uses <strong>one-hot encoding</strong>:
<ul>
<li>State 3 → [0,0,1,0,0,0,0,0,0,0]</li>
<li>Action 1 → [1,0]</li>
<li>Combined: [0,0,1,0,0,0,0,0,0,0,1,0] (12 features total)</li>
</ul></li>
</ul>
<p><strong>5. Q-Learning Algorithm with Neural Network</strong></p>
<p><strong>Function Signature</strong></p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb106-1" tabindex="-1"></a>q_learning_nn <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">epsilon_start =</span> <span class="fl">0.5</span>, <span class="at">epsilon_end =</span> <span class="fl">0.01</span>,</span>
<span id="cb106-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb106-2" tabindex="-1"></a>                          <span class="at">epsilon_decay =</span> <span class="fl">0.995</span>, <span class="at">hidden_size =</span> <span class="dv">10</span>, </span>
<span id="cb106-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb106-3" tabindex="-1"></a>                          <span class="at">retrain_freq =</span> <span class="dv">10</span>, <span class="at">min_samples =</span> <span class="dv">50</span>, <span class="at">max_buffer =</span> <span class="dv">5000</span>,</span>
<span id="cb106-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb106-4" tabindex="-1"></a>                          <span class="at">batch_fraction =</span> <span class="fl">1.0</span>)</span></code></pre></div>
<p><strong>Parameters:</strong>
- <code>episodes</code>: Number of learning episodes to run
- <code>epsilon_start/end/decay</code>: Exploration rate (starts high, decays over time)
- <code>hidden_size</code>: Number of neurons in hidden layer
- <code>retrain_freq</code>: Train neural network every N episodes
- <code>min_samples</code>: Minimum data before training starts
- <code>max_buffer</code>: Maximum training examples to keep</p>
<p><strong>Memory Pre-allocation</strong></p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb107-1" tabindex="-1"></a>  max_steps_per_episode <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb107-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb107-2" tabindex="-1"></a>  buffer_size <span class="ot">&lt;-</span> episodes <span class="sc">*</span> max_steps_per_episode</span>
<span id="cb107-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb107-3" tabindex="-1"></a>  q_data_x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> buffer_size, <span class="at">ncol =</span> n_features)</span>
<span id="cb107-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb107-4" tabindex="-1"></a>  q_data_y <span class="ot">&lt;-</span> <span class="fu">numeric</span>(buffer_size)</span>
<span id="cb107-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb107-5" tabindex="-1"></a>  data_idx <span class="ot">&lt;-</span> <span class="dv">0</span></span></code></pre></div>
<ul>
<li>Pre-allocates large matrix for training data (fixes the slow <code>rbind</code> issue!)</li>
<li>Assumes max 100 steps per episode</li>
<li><code>q_data_x</code>: stores input features (state-action pairs)</li>
<li><code>q_data_y</code>: stores target Q-values</li>
<li>Much faster than growing arrays dynamically</li>
</ul>
<p><strong>Episode Loop</strong></p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb108-1" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb108-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb108-2" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>), <span class="dv">1</span>)  <span class="co"># Start from random non-terminal state</span></span>
<span id="cb108-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb108-3" tabindex="-1"></a>    episode_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb108-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb108-4" tabindex="-1"></a>    steps <span class="ot">&lt;-</span> <span class="dv">0</span></span></code></pre></div>
<ul>
<li>Runs 1000 learning episodes</li>
<li>Each episode starts from a random state (not terminal)</li>
<li>Tracks total reward collected</li>
</ul>
<p><strong>Q-Value Prediction</strong></p>
<div class="sourceCode" id="cb109"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb109-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb109-1" tabindex="-1"></a>      q_preds <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb109-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb109-2" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb109-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb109-3" tabindex="-1"></a>        <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(q_model)) {</span>
<span id="cb109-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb109-4" tabindex="-1"></a>          <span class="fu">as.numeric</span>(<span class="fu">predict</span>(q_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x))))</span>
<span id="cb109-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb109-5" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb109-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb109-6" tabindex="-1"></a>          <span class="dv">0</span>  <span class="co"># Initialize to zero instead of random</span></span>
<span id="cb109-7"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb109-7" tabindex="-1"></a>        }</span>
<span id="cb109-8"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb109-8" tabindex="-1"></a>      })</span></code></pre></div>
<ul>
<li>Predicts Q-value for each possible action from current state</li>
<li>If no model trained yet, returns 0 (better than random noise)</li>
<li>Creates input features and feeds them to neural network</li>
</ul>
<p><strong>Epsilon-Greedy Action Selection</strong></p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb110-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb110-1" tabindex="-1"></a>      a <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb110-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb110-2" tabindex="-1"></a>        <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)  <span class="co"># Explore: random action</span></span>
<span id="cb110-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb110-3" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb110-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb110-4" tabindex="-1"></a>        <span class="fu">which.max</span>(q_preds)      <span class="co"># Exploit: best action</span></span>
<span id="cb110-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb110-5" tabindex="-1"></a>      }</span></code></pre></div>
<ul>
<li><strong>Exploration vs Exploitation trade-off</strong></li>
<li>With probability ε: choose random action (explore)</li>
<li>With probability 1-ε: choose best action (exploit)</li>
<li>ε decays over time, shifting from exploration to exploitation</li>
</ul>
<p><strong>Taking Action &amp; Observing Results</strong></p>
<div class="sourceCode" id="cb111"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb111-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb111-1" tabindex="-1"></a>      out <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a)</span>
<span id="cb111-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb111-2" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> out<span class="sc">$</span>s_prime</span>
<span id="cb111-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb111-3" tabindex="-1"></a>      r <span class="ot">&lt;-</span> out<span class="sc">$</span>reward</span>
<span id="cb111-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb111-4" tabindex="-1"></a>      episode_reward <span class="ot">&lt;-</span> episode_reward <span class="sc">+</span> r</span></code></pre></div>
<ul>
<li>Executes chosen action in environment</li>
<li>Observes next state and reward</li>
<li>Accumulates episode reward</li>
</ul>
<p><strong>TD Target Computation</strong></p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb112-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-1" tabindex="-1"></a>      q_next <span class="ot">&lt;-</span> <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) {</span>
<span id="cb112-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-2" tabindex="-1"></a>        <span class="dv">0</span></span>
<span id="cb112-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-3" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb112-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-4" tabindex="-1"></a>        <span class="fu">max</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a_) {</span>
<span id="cb112-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-5" tabindex="-1"></a>          x_next <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s_prime, a_, n_states, n_actions)</span>
<span id="cb112-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-6" tabindex="-1"></a>          <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(q_model)) {</span>
<span id="cb112-7"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-7" tabindex="-1"></a>            <span class="fu">as.numeric</span>(<span class="fu">predict</span>(q_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x_next))))</span>
<span id="cb112-8"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-8" tabindex="-1"></a>          } <span class="cf">else</span> {</span>
<span id="cb112-9"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-9" tabindex="-1"></a>            <span class="dv">0</span></span>
<span id="cb112-10"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-10" tabindex="-1"></a>          }</span>
<span id="cb112-11"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-11" tabindex="-1"></a>        }))</span>
<span id="cb112-12"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-12" tabindex="-1"></a>      }</span>
<span id="cb112-13"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-13" tabindex="-1"></a>      </span>
<span id="cb112-14"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb112-14" tabindex="-1"></a>      target <span class="ot">&lt;-</span> r <span class="sc">+</span> gamma <span class="sc">*</span> q_next</span></code></pre></div>
<ul>
<li>Computes <strong>TD (Temporal Difference) target</strong>: r + γ·max Q(s’,a’)</li>
<li>If terminal state reached: future value = 0</li>
<li>Otherwise: estimates best future Q-value from next state</li>
<li>This is the <strong>Bellman equation</strong> for Q-learning</li>
</ul>
<p><strong>Storing Experience</strong></p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb113-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb113-1" tabindex="-1"></a>      data_idx <span class="ot">&lt;-</span> data_idx <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb113-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb113-2" tabindex="-1"></a>      <span class="cf">if</span> (data_idx <span class="sc">&gt;</span> buffer_size) {</span>
<span id="cb113-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb113-3" tabindex="-1"></a>        data_idx <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co"># Circular buffer</span></span>
<span id="cb113-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb113-4" tabindex="-1"></a>      }</span>
<span id="cb113-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb113-5" tabindex="-1"></a>      x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb113-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb113-6" tabindex="-1"></a>      q_data_x[data_idx, ] <span class="ot">&lt;-</span> x</span>
<span id="cb113-7"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb113-7" tabindex="-1"></a>      q_data_y[data_idx] <span class="ot">&lt;-</span> target</span></code></pre></div>
<ul>
<li>Stores (state, action) → target Q-value pair</li>
<li>Uses circular buffer: when full, overwrites oldest data</li>
<li>This is <strong>experience replay memory</strong></li>
</ul>
<p><strong>Neural Network Training</strong></p>
<div class="sourceCode" id="cb114"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb114-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-1" tabindex="-1"></a>      current_data_size <span class="ot">&lt;-</span> <span class="fu">min</span>(data_idx, buffer_size)</span>
<span id="cb114-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-2" tabindex="-1"></a>      <span class="cf">if</span> (current_data_size <span class="sc">&gt;=</span> min_samples <span class="sc">&amp;&amp;</span> ep <span class="sc">%%</span> retrain_freq <span class="sc">==</span> <span class="dv">0</span> <span class="sc">&amp;&amp;</span> steps <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb114-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-3" tabindex="-1"></a>        train_size <span class="ot">&lt;-</span> <span class="fu">min</span>(current_data_size, max_buffer)</span>
<span id="cb114-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-4" tabindex="-1"></a>        <span class="cf">if</span> (current_data_size <span class="sc">&gt;</span> max_buffer) {</span>
<span id="cb114-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-5" tabindex="-1"></a>          sample_idx <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>current_data_size, train_size)</span>
<span id="cb114-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-6" tabindex="-1"></a>          train_x <span class="ot">&lt;-</span> q_data_x[sample_idx, , drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb114-7"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-7" tabindex="-1"></a>          train_y <span class="ot">&lt;-</span> q_data_y[sample_idx]</span>
<span id="cb114-8"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-8" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb114-9"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-9" tabindex="-1"></a>          train_x <span class="ot">&lt;-</span> q_data_x[<span class="dv">1</span><span class="sc">:</span>current_data_size, , drop <span class="ot">=</span> <span class="cn">FALSE</span>]</span>
<span id="cb114-10"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-10" tabindex="-1"></a>          train_y <span class="ot">&lt;-</span> q_data_y[<span class="dv">1</span><span class="sc">:</span>current_data_size]</span>
<span id="cb114-11"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb114-11" tabindex="-1"></a>        }</span></code></pre></div>
<ul>
<li>Trains neural network every 10 episodes</li>
<li>Only trains once enough data collected (min 100 samples)</li>
<li>If data &gt; 5000 samples: randomly samples batch (experience replay)</li>
<li>Otherwise: uses all collected data</li>
</ul>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb115-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-1" tabindex="-1"></a>        <span class="fu">tryCatch</span>({</span>
<span id="cb115-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-2" tabindex="-1"></a>          q_model <span class="ot">&lt;-</span> <span class="fu">nnet</span>(</span>
<span id="cb115-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-3" tabindex="-1"></a>            <span class="at">x =</span> train_x,</span>
<span id="cb115-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-4" tabindex="-1"></a>            <span class="at">y =</span> train_y,</span>
<span id="cb115-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-5" tabindex="-1"></a>            <span class="at">size =</span> hidden_size,</span>
<span id="cb115-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-6" tabindex="-1"></a>            <span class="at">linout =</span> <span class="cn">TRUE</span>,</span>
<span id="cb115-7"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-7" tabindex="-1"></a>            <span class="at">maxit =</span> <span class="dv">200</span>,</span>
<span id="cb115-8"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-8" tabindex="-1"></a>            <span class="at">decay =</span> <span class="fl">0.01</span>,</span>
<span id="cb115-9"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-9" tabindex="-1"></a>            <span class="at">trace =</span> <span class="cn">FALSE</span></span>
<span id="cb115-10"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-10" tabindex="-1"></a>          )</span>
<span id="cb115-11"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-11" tabindex="-1"></a>          </span>
<span id="cb115-12"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-12" tabindex="-1"></a>          predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(q_model, <span class="fu">as.data.frame</span>(train_x))</span>
<span id="cb115-13"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-13" tabindex="-1"></a>          mse <span class="ot">&lt;-</span> <span class="fu">mean</span>((predictions <span class="sc">-</span> train_y)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb115-14"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-14" tabindex="-1"></a>          training_losses <span class="ot">&lt;-</span> <span class="fu">c</span>(training_losses, mse)</span>
<span id="cb115-15"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-15" tabindex="-1"></a>        }, <span class="at">error =</span> <span class="cf">function</span>(e) {</span>
<span id="cb115-16"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-16" tabindex="-1"></a>          <span class="fu">warning</span>(<span class="fu">paste</span>(<span class="st">&quot;Neural network training failed at episode&quot;</span>, ep, <span class="st">&quot;:&quot;</span>, e<span class="sc">$</span>message))</span>
<span id="cb115-17"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb115-17" tabindex="-1"></a>        })</span></code></pre></div>
<ul>
<li>Trains single-layer neural network:
<ul>
<li>Input: 12 features (state + action one-hot)</li>
<li>Hidden: 15 neurons</li>
<li>Output: 1 value (Q-value)</li>
</ul></li>
<li><code>linout = TRUE</code>: linear output (for regression)</li>
<li><code>maxit = 200</code>: 200 optimization iterations</li>
<li><code>decay = 0.01</code>: weight regularization to prevent overfitting</li>
<li>Error handling: if training fails, just warn and continue</li>
</ul>
<p><strong>Episode Termination</strong></p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb116-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb116-1" tabindex="-1"></a>      <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) <span class="cf">break</span></span>
<span id="cb116-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb116-2" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span></code></pre></div>
<ul>
<li>Ends episode if terminal state reached</li>
<li>Otherwise, continues from new state</li>
</ul>
<p><strong>Epsilon Decay</strong></p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb117-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb117-1" tabindex="-1"></a>    epsilon <span class="ot">&lt;-</span> <span class="fu">max</span>(epsilon_end, epsilon <span class="sc">*</span> epsilon_decay)</span></code></pre></div>
<ul>
<li>After each episode, reduces exploration rate</li>
<li>ε = 0.5 → 0.5×0.995 → 0.4975 → … → 0.01 (minimum)</li>
<li>Gradually shifts from exploration to exploitation</li>
</ul>
<p><strong>6. Policy Extraction</strong></p>
<div class="sourceCode" id="cb118"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb118-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-1" tabindex="-1"></a>  policy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), <span class="cf">function</span>(s) {</span>
<span id="cb118-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-2" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(q_model)) {</span>
<span id="cb118-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-3" tabindex="-1"></a>      q_vals <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb118-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-4" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb118-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-5" tabindex="-1"></a>        <span class="fu">as.numeric</span>(<span class="fu">predict</span>(q_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x))))</span>
<span id="cb118-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-6" tabindex="-1"></a>      })</span>
<span id="cb118-7"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-7" tabindex="-1"></a>      <span class="fu">which.max</span>(q_vals)</span>
<span id="cb118-8"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-8" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb118-9"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-9" tabindex="-1"></a>      <span class="dv">1</span>  <span class="co"># Default action</span></span>
<span id="cb118-10"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-10" tabindex="-1"></a>    }</span>
<span id="cb118-11"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb118-11" tabindex="-1"></a>  })</span></code></pre></div>
<ul>
<li>After training, extracts the learned policy</li>
<li>For each state: predicts Q-value for both actions, chooses best</li>
<li>This is the <strong>greedy policy</strong> (no more exploration)</li>
</ul>
<p><strong>7. Visualization: Policy Plot</strong></p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb119-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb119-1" tabindex="-1"></a>policy_plot_nn <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(policy_df[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), ], <span class="fu">aes</span>(<span class="at">x =</span> State, <span class="at">y =</span> Policy)) <span class="sc">+</span></span>
<span id="cb119-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb119-2" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">4</span>, <span class="at">color =</span> <span class="st">&quot;coral&quot;</span>) <span class="sc">+</span></span>
<span id="cb119-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb119-3" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;coral&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb119-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb119-4" tabindex="-1"></a>  ...</span></code></pre></div>
<ul>
<li>Shows which action the agent chooses in each state</li>
<li>Y-axis: 1 = Action 1, 2 = Action 2</li>
<li>Helps visualize the learned strategy</li>
</ul>
<p><strong>8. Learning Curve</strong></p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb120-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb120-1" tabindex="-1"></a>rewards_smooth <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(nn_rewards))</span>
<span id="cb120-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb120-2" tabindex="-1"></a>window_size <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb120-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb120-3" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(nn_rewards)) {</span>
<span id="cb120-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb120-4" tabindex="-1"></a>  start_idx <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="dv">1</span>, i <span class="sc">-</span> window_size <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb120-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb120-5" tabindex="-1"></a>  rewards_smooth[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(nn_rewards[start_idx<span class="sc">:</span>i])</span>
<span id="cb120-6"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb120-6" tabindex="-1"></a>}</span></code></pre></div>
<ul>
<li>Smooths reward curve using 50-episode moving average</li>
<li>Shows learning progress over time</li>
<li>Should increase as agent learns better policy</li>
</ul>
<p><strong>9. Training Loss Plot</strong></p>
<p>Shows how well the neural network fits the Q-values over time. Lower loss = better approximation.</p>
<p><strong>10. Model Diagnostics</strong></p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb121-1"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb121-1" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(nn_result<span class="sc">$</span>model)) {</span>
<span id="cb121-2"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb121-2" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Architecture: Input(&quot;</span>, n_features, <span class="st">&quot;) -&gt; Hidden(&quot;</span>, nn_result<span class="sc">$</span>model<span class="sc">$</span>n[<span class="dv">2</span>], <span class="st">&quot;) -&gt; Output(1)</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb121-3"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb121-3" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Total parameters:&quot;</span>, <span class="fu">length</span>(nn_result<span class="sc">$</span>model<span class="sc">$</span>wts), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb121-4"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb121-4" tabindex="-1"></a>  ...</span>
<span id="cb121-5"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#cb121-5" tabindex="-1"></a>}</span></code></pre></div>
<ul>
<li>Prints neural network architecture</li>
<li>Shows training statistics</li>
<li>Analyzes learned policy</li>
<li>Reports final performance metrics</li>
</ul>
</div>
<div id="analysis-and-interpretation" class="section level2 hasAnchor" number="8.4">
<h2><span class="header-section-number">8.4</span> Analysis and Interpretation<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Neural network function approximation introduces several unique characteristics compared to linear and tree-based methods. The non-convex optimization landscape means that training can exhibit complex dynamics, including periods of rapid improvement followed by plateaus. The learning curve often shows more volatility than linear methods due to the continuous parameter updates and potential for local minima. Unlike Random Forests that learn piecewise constant approximations, neural networks produce smooth function approximations. This continuity can be advantageous for policy learning, as small changes in state typically result in small changes in Q-values. The hidden layer learns intermediate representations that capture relevant features for action-value estimation.</p>
<p>Neural networks excel at discovering relevant patterns in the state-action space without explicit feature engineering. The hidden units automatically learn combinations of input features that prove useful for Q-value prediction. This automatic feature discovery becomes increasingly valuable as problem complexity grows. The batch retraining approach helps stabilize learning compared to online neural network updates, which can suffer from catastrophic forgetting. However, the periodic retraining introduces discontinuities in the learned function that can temporarily disrupt policy performance.</p>
<p><strong>Practical Considerations</strong></p>
<p>The choice of network architecture significantly impacts performance. Too few hidden units may underfit the true Q-function, while too many can lead to overfitting with limited training data. Our single hidden layer with 10 units provides a reasonable balance for the 10-state environment.</p>
<p>The retraining frequency presents a trade-off between computational efficiency and learning responsiveness. More frequent retraining provides better adaptation to new experience but increases computational cost. The optimal frequency depends on the environment complexity and available computational resources.</p>
<p>Neural networks benefit from regularization techniques to prevent overfitting. Our implementation includes weight decay (L2 regularization) to encourage smaller weights and improve generalization. Other techniques like dropout or early stopping could further enhance performance. Neural network training depends critically on weight initialization and optimization parameters. Poor initialization can trap the network in suboptimal local minima, while inappropriate learning rates can cause divergence or slow convergence.</p>
</div>
<div id="future-directions" class="section level2 hasAnchor" number="8.5">
<h2><span class="header-section-number">8.5</span> Future Directions<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our progression from tabular to linear to ensemble to neural network methods illustrates the evolution of function approximation in reinforcement learning. Each method offers distinct advantages for different problem characteristics.</p>
<p>Tabular methods provide exact representation but fail to scale. Linear methods offer guaranteed convergence and interpretability but assume additive relationships. Random Forests handle non-linearities while maintaining interpretability but produce discontinuous approximations. Neural networks provide universal approximation capabilities and smooth functions but introduce optimization challenges and reduced interpretability.</p>
<p>The choice among methods depends on problem requirements, available data, computational constraints, and interpretability needs. Neural networks shine when function complexity exceeds simpler methods’ capabilities and sufficient training data is available.</p>
<p>This exploration establishes the foundation for more advanced neural network approaches in reinforcement learning. Extensions could include deeper architectures, convolutional networks for spatial problems, recurrent networks for partially observable environments, or modern techniques like attention mechanisms.</p>
<p>The theoretical framework developed here scales naturally to these more complex architectures, with the core principles of temporal difference learning and gradient-based optimization remaining constant while the function approximation capabilities expand dramatically.</p>
</div>
<div id="conclusion-5" class="section level2 hasAnchor" number="8.6">
<h2><span class="header-section-number">8.6</span> Conclusion<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Neural network function approximation represents a significant step toward the sophisticated methods underlying modern deep reinforcement learning. While maintaining the theoretical foundations of Q-Learning, neural networks provide the flexibility to tackle complex environments that challenge simpler approximation methods.</p>
<p>The implementation demonstrates how classical reinforcement learning principles extend naturally to neural network settings, preserving core algorithmic structure while enhancing representational power. This foundation enables practitioners to understand and implement more advanced methods building on these fundamental concepts.</p>
<p>The journey through different function approximation approaches reveals the rich landscape of reinforcement learning methods, each contributing unique insights and capabilities. Neural networks, as universal approximators, provide the theoretical and practical foundation for tackling increasingly complex decision-making problems across diverse domains.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="dyna-and-dynaq.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/edit/main/08-Q_FA_NN.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/blob/main/08-Q_FA_NN.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
