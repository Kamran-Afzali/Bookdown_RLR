<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 The Multi-Armed Bandit Problem | Reinforcement Learning in R</title>
  <meta name="description" content="Chapter 2 The Multi-Armed Bandit Problem | Reinforcement Learning in R" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 The Multi-Armed Bandit Problem | Reinforcement Learning in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 The Multi-Armed Bandit Problem | Reinforcement Learning in R" />
  
  
  

<meta name="author" content="Kamran Afzalui" />


<meta name="date" content="2025-07-23" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="markov-decision-processes-and-dynamic-programming.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Reinforcement Learning in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Understanding Reinforcement Learning: From Bandits to Policy Optimization</strong></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-multi-armed-bandit-the-simplest-case"><i class="fa fa-check"></i><b>1.2</b> The Multi-Armed Bandit: The Simplest Case</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#transition-to-markov-decision-processes"><i class="fa fa-check"></i><b>1.3</b> Transition to Markov Decision Processes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#comparing-reinforcement-learning-methods"><i class="fa fa-check"></i><b>1.4</b> Comparing Reinforcement Learning Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#dynamic-programming-model-based-learning"><i class="fa fa-check"></i><b>1.4.1</b> Dynamic Programming: Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#model-free-approaches-monte-carlo-and-td-learning"><i class="fa fa-check"></i><b>1.4.2</b> Model-Free Approaches: Monte Carlo and TD Learning</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#q-learning-and-function-approximation"><i class="fa fa-check"></i><b>1.4.3</b> Q-Learning and Function Approximation</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#policy-gradient-and-actor-critic-methods"><i class="fa fa-check"></i><b>1.4.4</b> Policy Gradient and Actor-Critic Methods</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#advanced-policy-optimization-techniques"><i class="fa fa-check"></i><b>1.4.5</b> Advanced Policy Optimization Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#conclusion-and-further-directions"><i class="fa fa-check"></i><b>1.5</b> Conclusion and Further Directions</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html"><i class="fa fa-check"></i><b>2</b> The Multi-Armed Bandit Problem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#mathematical-formalism"><i class="fa fa-check"></i><b>2.2</b> Mathematical Formalism</a></li>
<li class="chapter" data-level="2.3" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm"><i class="fa fa-check"></i><b>2.3</b> Frequentist Approach: UCB1 Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1"><i class="fa fa-check"></i><b>2.3.1</b> R Code for UCB1</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling"><i class="fa fa-check"></i><b>2.4</b> Bayesian Approach: Thompson Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling"><i class="fa fa-check"></i><b>2.4.1</b> R Code for Thompson Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy"><i class="fa fa-check"></i><b>2.5</b> Epsilon-Greedy Strategy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy"><i class="fa fa-check"></i><b>2.5.1</b> R Code for Epsilon-Greedy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#summary-table"><i class="fa fa-check"></i><b>2.6</b> Summary Table</a></li>
<li class="chapter" data-level="2.7" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html"><i class="fa fa-check"></i><b>3</b> Markov Decision Processes and Dynamic Programming</a>
<ul>
<li class="chapter" data-level="3.1" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#constructing-the-mdp-in-r"><i class="fa fa-check"></i><b>3.2</b> Constructing the MDP in R</a></li>
<li class="chapter" data-level="3.3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#value-iteration-algorithm"><i class="fa fa-check"></i><b>3.3</b> Value Iteration Algorithm</a></li>
<li class="chapter" data-level="3.4" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#evaluation-and-interpretation"><i class="fa fa-check"></i><b>3.4</b> Evaluation and Interpretation</a></li>
<li class="chapter" data-level="3.5" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties-of-value-iteration"><i class="fa fa-check"></i><b>3.5</b> Theoretical Properties of Value Iteration</a></li>
<li class="chapter" data-level="3.6" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-1"><i class="fa fa-check"></i><b>3.6</b> Summary Table</a></li>
<li class="chapter" data-level="3.7" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><i class="fa fa-check"></i><b>4</b> Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#theoretical-background"><i class="fa fa-check"></i><b>4.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#temporal-difference-learning-q-learning"><i class="fa fa-check"></i><b>4.2.1</b> Temporal Difference Learning (Q-Learning)</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#monte-carlo-methods"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.2.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-1-defining-the-environment-in-r"><i class="fa fa-check"></i><b>4.2.3</b> Step 1: Defining the Environment in R</a></li>
<li class="chapter" data-level="4.2.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-2-q-learning-implementation-in-r"><i class="fa fa-check"></i><b>4.2.4</b> Step 2: Q-Learning Implementation in R</a></li>
<li class="chapter" data-level="4.2.5" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-3-monte-carlo-every-visit-implementation"><i class="fa fa-check"></i><b>4.2.5</b> Step 3: Monte Carlo Every-Visit Implementation</a></li>
<li class="chapter" data-level="4.2.6" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-4-simulating-outcome-devaluation"><i class="fa fa-check"></i><b>4.2.6</b> Step 4: Simulating Outcome Devaluation</a></li>
<li class="chapter" data-level="4.2.7" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-5-comparing-policies-before-and-after-devaluation"><i class="fa fa-check"></i><b>4.2.7</b> Step 5: Comparing Policies Before and After Devaluation</a></li>
<li class="chapter" data-level="4.2.8" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-6-visualizing-the-policies"><i class="fa fa-check"></i><b>4.2.8</b> Step 6: Visualizing the Policies</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#interpretation-and-discussion"><i class="fa fa-check"></i><b>4.3</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="4.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#conclusion-2"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><i class="fa fa-check"></i><b>5</b> On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-on-policy"><i class="fa fa-check"></i><b>5.2</b> SARSA (On-Policy)</a></li>
<li class="chapter" data-level="5.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-off-policy"><i class="fa fa-check"></i><b>5.3</b> Q-Learning (Off-Policy)</a></li>
<li class="chapter" data-level="5.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-with-importance-sampling"><i class="fa fa-check"></i><b>5.4</b> Off-Policy Monte Carlo with Importance Sampling</a></li>
<li class="chapter" data-level="5.5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#key-differences"><i class="fa fa-check"></i><b>5.5</b> Key Differences</a></li>
<li class="chapter" data-level="5.6" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#interpretation-and-discussion-1"><i class="fa fa-check"></i><b>5.6</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#conclusion-3"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
<li class="chapter" data-level="5.8" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#comparison-table"><i class="fa fa-check"></i><b>5.8</b> Comparison Table</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><i class="fa fa-check"></i><b>6</b> Function Approximation in Reinforcement Learning: Q-Learning with Linear Models in R</a>
<ul>
<li class="chapter" data-level="6.1" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#theoretical-background-1"><i class="fa fa-check"></i><b>6.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#q-learning-with-function-approximation"><i class="fa fa-check"></i><b>6.2.1</b> Q-Learning with Function Approximation</a></li>
<li class="chapter" data-level="6.2.2" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#comparison-with-tabular-q-learning"><i class="fa fa-check"></i><b>6.2.2</b> Comparison with Tabular Q-Learning</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#r-implementation"><i class="fa fa-check"></i><b>6.3</b> R Implementation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><i class="fa fa-check"></i><b>7</b> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#theoretical-background-2"><i class="fa fa-check"></i><b>7.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation"><i class="fa fa-check"></i><b>7.2.1</b> Q-Learning with Random Forest Approximation</a></li>
<li class="chapter" data-level="7.2.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-engineering-for-tree-based-models"><i class="fa fa-check"></i><b>7.2.2</b> Feature Engineering for Tree-Based Models</a></li>
<li class="chapter" data-level="7.2.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods"><i class="fa fa-check"></i><b>7.2.3</b> Comparison with Previous Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1"><i class="fa fa-check"></i><b>7.3</b> R Implementation</a></li>
<li class="chapter" data-level="7.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights"><i class="fa fa-check"></i><b>7.4</b> Analysis and Insights</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#policy-learning-characteristics"><i class="fa fa-check"></i><b>7.4.1</b> Policy Learning Characteristics</a></li>
<li class="chapter" data-level="7.4.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#computational-considerations"><i class="fa fa-check"></i><b>7.4.2</b> Computational Considerations</a></li>
<li class="chapter" data-level="7.4.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-importance-insights"><i class="fa fa-check"></i><b>7.4.3</b> Feature Importance Insights</a></li>
<li class="chapter" data-level="7.4.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#practical-implications-1"><i class="fa fa-check"></i><b>7.4.4</b> Practical Implications</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-linear-approximation"><i class="fa fa-check"></i><b>7.5</b> Comparison with Linear Approximation</a></li>
<li class="chapter" data-level="7.6" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4"><i class="fa fa-check"></i><b>7.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><i class="fa fa-check"></i><b>8</b> Deep Function Approximation: Q-Learning with Neural Networks in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation"><i class="fa fa-check"></i><b>8.2</b> Theoretical Foundation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity"><i class="fa fa-check"></i><b>8.2.1</b> Universal Approximation and Expressivity</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning"><i class="fa fa-check"></i><b>8.2.2</b> Gradient-Based Learning</a></li>
<li class="chapter" data-level="8.2.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches"><i class="fa fa-check"></i><b>8.2.3</b> Comparison with Previous Approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2"><i class="fa fa-check"></i><b>8.3</b> R Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation"><i class="fa fa-check"></i><b>8.4</b> Analysis and Interpretation</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#learning-dynamics"><i class="fa fa-check"></i><b>8.4.1</b> Learning Dynamics</a></li>
<li class="chapter" data-level="8.4.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#function-representation"><i class="fa fa-check"></i><b>8.4.2</b> Function Representation</a></li>
<li class="chapter" data-level="8.4.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#generalization-properties"><i class="fa fa-check"></i><b>8.4.3</b> Generalization Properties</a></li>
<li class="chapter" data-level="8.4.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#training-stability"><i class="fa fa-check"></i><b>8.4.4</b> Training Stability</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#practical-considerations"><i class="fa fa-check"></i><b>8.5</b> Practical Considerations</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#architecture-selection"><i class="fa fa-check"></i><b>8.5.1</b> Architecture Selection</a></li>
<li class="chapter" data-level="8.5.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#training-frequency"><i class="fa fa-check"></i><b>8.5.2</b> Training Frequency</a></li>
<li class="chapter" data-level="8.5.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#regularization"><i class="fa fa-check"></i><b>8.5.3</b> Regularization</a></li>
<li class="chapter" data-level="8.5.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#initialization-and-convergence"><i class="fa fa-check"></i><b>8.5.4</b> Initialization and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-across-function-approximation-methods"><i class="fa fa-check"></i><b>8.6</b> Comparison Across Function Approximation Methods</a></li>
<li class="chapter" data-level="8.7" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions"><i class="fa fa-check"></i><b>8.7</b> Future Directions</a></li>
<li class="chapter" data-level="8.8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5"><i class="fa fa-check"></i><b>8.8</b> Conclusion</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-multi-armed-bandit-problem" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> The Multi-Armed Bandit Problem<a href="the-multi-armed-bandit-problem.html#the-multi-armed-bandit-problem" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Introduction<a href="the-multi-armed-bandit-problem.html#introduction" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The multi-armed bandit (MAB) problem is a foundational model in the study of sequential decision-making under uncertainty. Representing the trade-off between exploration (gathering information) and exploitation (maximizing known rewards), MAB problems are central to reinforcement learning, online optimization, and adaptive experimental design. An agent is faced with a choice among multiple options—arms—each producing stochastic rewards with unknown distributions. The objective is to maximize cumulative reward, or equivalently, to minimize the regret incurred by not always choosing the best arm.</p>
<p>This post presents a rigorous treatment of the MAB problem, comparing frequentist and Bayesian approaches. We offer formal mathematical foundations, develop regret bounds, and implement both Upper Confidence Bound (UCB) and Thompson Sampling algorithms in R. A summary table is provided at the end.</p>
</div>
<div id="mathematical-formalism" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Mathematical Formalism<a href="the-multi-armed-bandit-problem.html#mathematical-formalism" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Let <span class="math inline">\(K\)</span> denote the number of arms, and each arm <span class="math inline">\(k \in \{1, \dots, K\}\)</span> has an unknown reward distribution <span class="math inline">\(P_k\)</span>, with mean <span class="math inline">\(\mu_k\)</span>. Define the optimal arm:</p>
<p><span class="math display">\[
k^* = \arg\max_{k} \mu_k.
\]</span></p>
<p>At each time <span class="math inline">\(t \in \{1, \dots, T\}\)</span>, the agent chooses arm <span class="math inline">\(A_t \in \{1, \dots, K\}\)</span> and receives a stochastic reward <span class="math inline">\(R_t \sim P_{A_t}\)</span>. The cumulative expected regret is:</p>
<p><span class="math display">\[
\mathcal{R}(T) = T\mu^* - \mathbb{E}\left[ \sum_{t=1}^T R_t \right] = \sum_{k=1}^K \Delta_k \, \mathbb{E}[N_k(T)],
\]</span></p>
<p>where <span class="math inline">\(\Delta_k = \mu^* - \mu_k\)</span> and <span class="math inline">\(N_k(T)\)</span> is the number of times arm <span class="math inline">\(k\)</span> was played.</p>
</div>
<div id="frequentist-approach-ucb1-algorithm" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Frequentist Approach: UCB1 Algorithm<a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Frequentist methods estimate expected rewards using empirical means. The UCB1 algorithm, based on Hoeffding’s inequality, constructs an upper confidence bound:</p>
<p><span class="math display">\[
A_t = \arg\max_{k} \left[ \hat{\mu}_{k,t} + \sqrt{ \frac{2 \log t}{N_k(t)} } \right].
\]</span></p>
<p>This ensures logarithmic regret in expectation.</p>
<div id="r-code-for-ucb1" class="section level3 hasAnchor" number="2.3.1">
<h3><span class="header-section-number">2.3.1</span> R Code for UCB1<a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="the-multi-armed-bandit-problem.html#cb1-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb1-2"><a href="the-multi-armed-bandit-problem.html#cb1-2" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb1-3"><a href="the-multi-armed-bandit-problem.html#cb1-3" tabindex="-1"></a>T <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb1-4"><a href="the-multi-armed-bandit-problem.html#cb1-4" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.3</span>, <span class="fl">0.5</span>, <span class="fl">0.7</span>)  <span class="co"># true means</span></span>
<span id="cb1-5"><a href="the-multi-armed-bandit-problem.html#cb1-5" tabindex="-1"></a></span>
<span id="cb1-6"><a href="the-multi-armed-bandit-problem.html#cb1-6" tabindex="-1"></a>counts <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, K)</span>
<span id="cb1-7"><a href="the-multi-armed-bandit-problem.html#cb1-7" tabindex="-1"></a>values <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, K)</span>
<span id="cb1-8"><a href="the-multi-armed-bandit-problem.html#cb1-8" tabindex="-1"></a>regret <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb1-9"><a href="the-multi-armed-bandit-problem.html#cb1-9" tabindex="-1"></a></span>
<span id="cb1-10"><a href="the-multi-armed-bandit-problem.html#cb1-10" tabindex="-1"></a><span class="co"># Play each arm once</span></span>
<span id="cb1-11"><a href="the-multi-armed-bandit-problem.html#cb1-11" tabindex="-1"></a><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>K) {</span>
<span id="cb1-12"><a href="the-multi-armed-bandit-problem.html#cb1-12" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, mu[k])</span>
<span id="cb1-13"><a href="the-multi-armed-bandit-problem.html#cb1-13" tabindex="-1"></a>  counts[k] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb1-14"><a href="the-multi-armed-bandit-problem.html#cb1-14" tabindex="-1"></a>  values[k] <span class="ot">&lt;-</span> reward</span>
<span id="cb1-15"><a href="the-multi-armed-bandit-problem.html#cb1-15" tabindex="-1"></a>  regret[k] <span class="ot">&lt;-</span> <span class="fu">max</span>(mu) <span class="sc">-</span> mu[k]</span>
<span id="cb1-16"><a href="the-multi-armed-bandit-problem.html#cb1-16" tabindex="-1"></a>}</span>
<span id="cb1-17"><a href="the-multi-armed-bandit-problem.html#cb1-17" tabindex="-1"></a></span>
<span id="cb1-18"><a href="the-multi-armed-bandit-problem.html#cb1-18" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> (K<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>T) {</span>
<span id="cb1-19"><a href="the-multi-armed-bandit-problem.html#cb1-19" tabindex="-1"></a>  ucb <span class="ot">&lt;-</span> values <span class="sc">+</span> <span class="fu">sqrt</span>(<span class="dv">2</span> <span class="sc">*</span> <span class="fu">log</span>(t) <span class="sc">/</span> counts)</span>
<span id="cb1-20"><a href="the-multi-armed-bandit-problem.html#cb1-20" tabindex="-1"></a>  a <span class="ot">&lt;-</span> <span class="fu">which.max</span>(ucb)</span>
<span id="cb1-21"><a href="the-multi-armed-bandit-problem.html#cb1-21" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, mu[a])</span>
<span id="cb1-22"><a href="the-multi-armed-bandit-problem.html#cb1-22" tabindex="-1"></a>  counts[a] <span class="ot">&lt;-</span> counts[a] <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb1-23"><a href="the-multi-armed-bandit-problem.html#cb1-23" tabindex="-1"></a>  values[a] <span class="ot">&lt;-</span> values[a] <span class="sc">+</span> (reward <span class="sc">-</span> values[a]) <span class="sc">/</span> counts[a]</span>
<span id="cb1-24"><a href="the-multi-armed-bandit-problem.html#cb1-24" tabindex="-1"></a>  regret[t] <span class="ot">&lt;-</span> <span class="fu">max</span>(mu) <span class="sc">-</span> mu[a]</span>
<span id="cb1-25"><a href="the-multi-armed-bandit-problem.html#cb1-25" tabindex="-1"></a>}</span>
<span id="cb1-26"><a href="the-multi-armed-bandit-problem.html#cb1-26" tabindex="-1"></a></span>
<span id="cb1-27"><a href="the-multi-armed-bandit-problem.html#cb1-27" tabindex="-1"></a>cum_regret_ucb <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(regret)</span>
<span id="cb1-28"><a href="the-multi-armed-bandit-problem.html#cb1-28" tabindex="-1"></a><span class="fu">plot</span>(cum_regret_ucb, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, </span>
<span id="cb1-29"><a href="the-multi-armed-bandit-problem.html#cb1-29" tabindex="-1"></a>     <span class="at">ylab =</span> <span class="st">&quot;Cumulative Regret&quot;</span>, <span class="at">xlab =</span> <span class="st">&quot;Time&quot;</span>, <span class="at">main =</span> <span class="st">&quot;UCB1 Regret&quot;</span>)</span></code></pre></div>
</div>
</div>
<div id="bayesian-approach-thompson-sampling" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Bayesian Approach: Thompson Sampling<a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bayesian bandits model reward distributions probabilistically, updating beliefs via Bayes’ rule. For Bernoulli rewards, we assume Beta priors:</p>
<p><span class="math display">\[
\mu_k \sim \text{Beta}(\alpha_k, \beta_k).
\]</span></p>
<p>After observing a reward <span class="math inline">\(r \in \{0, 1\}\)</span>, the posterior update is:</p>
<p><span class="math display">\[
\alpha_k \leftarrow \alpha_k + r, \quad \beta_k \leftarrow \beta_k + 1 - r.
\]</span></p>
<p>The Thompson Sampling algorithm draws a sample <span class="math inline">\(\tilde{\mu}_k \sim \text{Beta}(\alpha_k, \beta_k)\)</span> and selects the arm with the highest sample.</p>
<div id="r-code-for-thompson-sampling" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> R Code for Thompson Sampling<a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="the-multi-armed-bandit-problem.html#cb2-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb2-2"><a href="the-multi-armed-bandit-problem.html#cb2-2" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, K)</span>
<span id="cb2-3"><a href="the-multi-armed-bandit-problem.html#cb2-3" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">1</span>, K)</span>
<span id="cb2-4"><a href="the-multi-armed-bandit-problem.html#cb2-4" tabindex="-1"></a>regret <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb2-5"><a href="the-multi-armed-bandit-problem.html#cb2-5" tabindex="-1"></a></span>
<span id="cb2-6"><a href="the-multi-armed-bandit-problem.html#cb2-6" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T) {</span>
<span id="cb2-7"><a href="the-multi-armed-bandit-problem.html#cb2-7" tabindex="-1"></a>  sampled_means <span class="ot">&lt;-</span> <span class="fu">rbeta</span>(K, alpha, beta)</span>
<span id="cb2-8"><a href="the-multi-armed-bandit-problem.html#cb2-8" tabindex="-1"></a>  a <span class="ot">&lt;-</span> <span class="fu">which.max</span>(sampled_means)</span>
<span id="cb2-9"><a href="the-multi-armed-bandit-problem.html#cb2-9" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, mu[a])</span>
<span id="cb2-10"><a href="the-multi-armed-bandit-problem.html#cb2-10" tabindex="-1"></a>  alpha[a] <span class="ot">&lt;-</span> alpha[a] <span class="sc">+</span> reward</span>
<span id="cb2-11"><a href="the-multi-armed-bandit-problem.html#cb2-11" tabindex="-1"></a>  beta[a] <span class="ot">&lt;-</span> beta[a] <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> reward)</span>
<span id="cb2-12"><a href="the-multi-armed-bandit-problem.html#cb2-12" tabindex="-1"></a>  regret[t] <span class="ot">&lt;-</span> <span class="fu">max</span>(mu) <span class="sc">-</span> mu[a]</span>
<span id="cb2-13"><a href="the-multi-armed-bandit-problem.html#cb2-13" tabindex="-1"></a>}</span>
<span id="cb2-14"><a href="the-multi-armed-bandit-problem.html#cb2-14" tabindex="-1"></a></span>
<span id="cb2-15"><a href="the-multi-armed-bandit-problem.html#cb2-15" tabindex="-1"></a>cum_regret_ts <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(regret)</span>
<span id="cb2-16"><a href="the-multi-armed-bandit-problem.html#cb2-16" tabindex="-1"></a><span class="fu">lines</span>(cum_regret_ts, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb2-17"><a href="the-multi-armed-bandit-problem.html#cb2-17" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;UCB1&quot;</span>, <span class="st">&quot;Thompson Sampling&quot;</span>), </span>
<span id="cb2-18"><a href="the-multi-armed-bandit-problem.html#cb2-18" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
<p>The UCB1 algorithm guarantees a regret bound of:</p>
<p><span class="math display">\[
\mathcal{R}(T) \leq \sum_{k: \Delta_k &gt; 0} \left( \frac{8 \log T}{\Delta_k} + C_k \right),
\]</span></p>
<p>where <span class="math inline">\(C_k\)</span> is a problem-dependent constant. Thompson Sampling achieves comparable performance. Under certain regularity conditions, its Bayesian regret is bounded by:</p>
<p><span class="math display">\[
\mathbb{E}[\mathcal{R}(T)] = O\left( \sqrt{KT \log T} \right),
\]</span></p>
<p>and often outperforms UCB1 in practice due to its adaptive exploration.</p>
</div>
</div>
<div id="epsilon-greedy-strategy" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Epsilon-Greedy Strategy<a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The epsilon-greedy algorithm is a simple and intuitive approach to balancing exploration and exploitation. At each time step, with probability <span class="math inline">\(\epsilon\)</span>, the agent chooses a random arm (exploration), and with probability <span class="math inline">\(1 - \epsilon\)</span>, it selects the arm with the highest empirical mean (exploitation). Let <span class="math inline">\(\hat{\mu}_{k,t}\)</span> denote the empirical mean reward for arm <span class="math inline">\(k\)</span> at time <span class="math inline">\(t\)</span>. Then:</p>
<p><span class="math display">\[
A_t =
\begin{cases}
\text{random choice} &amp; \text{with probability } \epsilon, \\
\arg\max_k \hat{\mu}_{k,t} &amp; \text{with probability } 1 - \epsilon.
\end{cases}
\]</span></p>
<p>While this algorithm is not optimal in the theoretical sense, it often performs well in practice for problems with stationary reward distributions when the exploration rate <span class="math inline">\(\epsilon\)</span> is properly tuned.</p>
<p>Regret under a fixed <span class="math inline">\(\epsilon\)</span> is linear in <span class="math inline">\(T\)</span>, i.e., <span class="math inline">\(\mathcal{R}(T) = O(T)\)</span>, unless <span class="math inline">\(\epsilon\)</span> is decayed over time (e.g., <span class="math inline">\(\epsilon_t = 1/t\)</span>), which introduces a trade-off between convergence speed and variance.</p>
<div id="r-code-for-epsilon-greedy" class="section level3 hasAnchor" number="2.5.1">
<h3><span class="header-section-number">2.5.1</span> R Code for Epsilon-Greedy<a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="the-multi-armed-bandit-problem.html#cb3-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb3-2"><a href="the-multi-armed-bandit-problem.html#cb3-2" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb3-3"><a href="the-multi-armed-bandit-problem.html#cb3-3" tabindex="-1"></a>counts <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, K)</span>
<span id="cb3-4"><a href="the-multi-armed-bandit-problem.html#cb3-4" tabindex="-1"></a>values <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, K)</span>
<span id="cb3-5"><a href="the-multi-armed-bandit-problem.html#cb3-5" tabindex="-1"></a>regret <span class="ot">&lt;-</span> <span class="fu">numeric</span>(T)</span>
<span id="cb3-6"><a href="the-multi-armed-bandit-problem.html#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="the-multi-armed-bandit-problem.html#cb3-7" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>T) {</span>
<span id="cb3-8"><a href="the-multi-armed-bandit-problem.html#cb3-8" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb3-9"><a href="the-multi-armed-bandit-problem.html#cb3-9" tabindex="-1"></a>    a <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>K, <span class="dv">1</span>)  <span class="co"># Exploration</span></span>
<span id="cb3-10"><a href="the-multi-armed-bandit-problem.html#cb3-10" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb3-11"><a href="the-multi-armed-bandit-problem.html#cb3-11" tabindex="-1"></a>    a <span class="ot">&lt;-</span> <span class="fu">which.max</span>(values)  <span class="co"># Exploitation</span></span>
<span id="cb3-12"><a href="the-multi-armed-bandit-problem.html#cb3-12" tabindex="-1"></a>  }</span>
<span id="cb3-13"><a href="the-multi-armed-bandit-problem.html#cb3-13" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(<span class="dv">1</span>, <span class="dv">1</span>, mu[a])</span>
<span id="cb3-14"><a href="the-multi-armed-bandit-problem.html#cb3-14" tabindex="-1"></a>  counts[a] <span class="ot">&lt;-</span> counts[a] <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb3-15"><a href="the-multi-armed-bandit-problem.html#cb3-15" tabindex="-1"></a>  values[a] <span class="ot">&lt;-</span> values[a] <span class="sc">+</span> (reward <span class="sc">-</span> values[a]) <span class="sc">/</span> counts[a]</span>
<span id="cb3-16"><a href="the-multi-armed-bandit-problem.html#cb3-16" tabindex="-1"></a>  regret[t] <span class="ot">&lt;-</span> <span class="fu">max</span>(mu) <span class="sc">-</span> mu[a]</span>
<span id="cb3-17"><a href="the-multi-armed-bandit-problem.html#cb3-17" tabindex="-1"></a>}</span>
<span id="cb3-18"><a href="the-multi-armed-bandit-problem.html#cb3-18" tabindex="-1"></a></span>
<span id="cb3-19"><a href="the-multi-armed-bandit-problem.html#cb3-19" tabindex="-1"></a>cum_regret_eps <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(regret)</span>
<span id="cb3-20"><a href="the-multi-armed-bandit-problem.html#cb3-20" tabindex="-1"></a><span class="fu">lines</span>(cum_regret_eps, <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>)</span>
<span id="cb3-21"><a href="the-multi-armed-bandit-problem.html#cb3-21" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;UCB1&quot;</span>, <span class="st">&quot;Thompson Sampling&quot;</span>, <span class="st">&quot;Epsilon-Greedy&quot;</span>), </span>
<span id="cb3-22"><a href="the-multi-armed-bandit-problem.html#cb3-22" tabindex="-1"></a>       <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;darkgreen&quot;</span>), <span class="at">lwd =</span> <span class="dv">2</span>)</span></code></pre></div>
</div>
</div>
<div id="summary-table" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Summary Table<a href="the-multi-armed-bandit-problem.html#summary-table" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<table style="width:100%;">
<colgroup>
<col width="8%" />
<col width="6%" />
<col width="16%" />
<col width="13%" />
<col width="17%" />
<col width="18%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Method</strong></th>
<th><strong>Paradigm</strong></th>
<th><strong>Assumptions</strong></th>
<th><strong>Exploration Mechanism</strong></th>
<th><strong>Regret Bound</strong></th>
<th><strong>Strengths</strong></th>
<th><strong>Weaknesses</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>UCB1</td>
<td>Frequentist</td>
<td>Stationary, bounded rewards</td>
<td>Upper Confidence Bound</td>
<td><span class="math inline">\(O(\log T)\)</span></td>
<td>Simple, provable guarantees</td>
<td>Conservative, suboptimal in practice</td>
</tr>
<tr class="even">
<td>Thompson Sampling</td>
<td>Bayesian</td>
<td>Prior over reward distributions</td>
<td>Posterior sampling</td>
<td><span class="math inline">\(O(\sqrt{KT})\)</span>, empirically better</td>
<td>Adaptive, efficient with good priors</td>
<td>Sensitive to prior misspecification</td>
</tr>
<tr class="odd">
<td>KL-UCB</td>
<td>Frequentist</td>
<td>Known reward distributions</td>
<td>KL-divergence bounds</td>
<td><span class="math inline">\(O(\log T)\)</span> (tighter)</td>
<td>Distribution-aware</td>
<td>More complex implementation</td>
</tr>
<tr class="even">
<td>Epsilon-Greedy</td>
<td>Heuristic</td>
<td>None</td>
<td>Random exploration</td>
<td><span class="math inline">\(O(T)\)</span> if <span class="math inline">\(\epsilon\)</span> fixed</td>
<td>Very simple</td>
<td>Inefficient long-term</td>
</tr>
</tbody>
</table>
</div>
<div id="conclusion" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Conclusion<a href="the-multi-armed-bandit-problem.html#conclusion" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The multi-armed bandit problem remains an essential model for studying decision-making under uncertainty. While frequentist methods like UCB1 provide rigorous guarantees and conceptual clarity, Bayesian approaches like Thompson Sampling offer greater flexibility and empirical performance. The choice between them hinges on the trade-offs between interpretability, adaptivity, and prior knowledge.</p>
<p>The R implementations provided here allow for practical experimentation and benchmarking. In real-world applications, such as clinical trial design, online recommendations, and adaptive A/B testing, these algorithms offer principled foundations for learning and acting in uncertain environments.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="markov-decision-processes-and-dynamic-programming.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/edit/main/02-MAB.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/blob/main/02-MAB.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
