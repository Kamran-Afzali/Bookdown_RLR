<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R</title>
  <meta name="description" content="Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R" />
  
  
  

<meta name="author" content="Kamran Afzalui" />


<meta name="date" content="2025-12-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="function-approximation-q-learning-with-linear-models.html"/>
<link rel="next" href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Reinforcement Learning in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Understanding Reinforcement Learning: From Bandits to Policy Optimization</strong></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-multi-armed-bandit-the-simplest-case"><i class="fa fa-check"></i><b>1.2</b> The Multi-Armed Bandit: The Simplest Case</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#transition-to-markov-decision-processes"><i class="fa fa-check"></i><b>1.3</b> Transition to Markov Decision Processes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#comparing-reinforcement-learning-methods"><i class="fa fa-check"></i><b>1.4</b> Comparing Reinforcement Learning Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#dynamic-programming-model-based-learning"><i class="fa fa-check"></i><b>1.4.1</b> Dynamic Programming: Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#model-free-approaches-monte-carlo-and-td-learning"><i class="fa fa-check"></i><b>1.4.2</b> Model-Free Approaches: Monte Carlo and TD Learning</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#dyna-bridging-model-free-and-model-based-learning"><i class="fa fa-check"></i><b>1.4.3</b> Dyna: Bridging Model-Free and Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#q-learning-and-function-approximation"><i class="fa fa-check"></i><b>1.4.4</b> Q-Learning and Function Approximation</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#policy-gradient-and-actor-critic-methods"><i class="fa fa-check"></i><b>1.4.5</b> Policy Gradient and Actor-Critic Methods</a></li>
<li class="chapter" data-level="1.4.6" data-path="index.html"><a href="index.html#advanced-policy-optimization-techniques"><i class="fa fa-check"></i><b>1.4.6</b> Advanced Policy Optimization Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#further-directions"><i class="fa fa-check"></i><b>1.5</b> Further Directions</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html"><i class="fa fa-check"></i><b>2</b> The Multi-Armed Bandit Problem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#mathematical-formalism"><i class="fa fa-check"></i><b>2.2</b> Mathematical Formalism</a></li>
<li class="chapter" data-level="2.3" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm"><i class="fa fa-check"></i><b>2.3</b> Frequentist Approach: UCB1 Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1"><i class="fa fa-check"></i><b>2.3.1</b> R Code for UCB1</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling"><i class="fa fa-check"></i><b>2.4</b> Bayesian Approach: Thompson Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling"><i class="fa fa-check"></i><b>2.4.1</b> R Code for Thompson Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy"><i class="fa fa-check"></i><b>2.5</b> Epsilon-Greedy Strategy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy"><i class="fa fa-check"></i><b>2.5.1</b> R Code for Epsilon-Greedy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#summary-table"><i class="fa fa-check"></i><b>2.6</b> Summary Table</a></li>
<li class="chapter" data-level="2.7" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html"><i class="fa fa-check"></i><b>3</b> Markov Decision Processes and Dynamic Programming</a>
<ul>
<li class="chapter" data-level="3.1" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#constructing-the-mdp-in-r"><i class="fa fa-check"></i><b>3.2</b> Constructing the MDP in R</a></li>
<li class="chapter" data-level="3.3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#value-iteration-algorithm"><i class="fa fa-check"></i><b>3.3</b> Value Iteration Algorithm</a></li>
<li class="chapter" data-level="3.4" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#evaluation-and-interpretation"><i class="fa fa-check"></i><b>3.4</b> Evaluation and Interpretation</a></li>
<li class="chapter" data-level="3.5" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties"><i class="fa fa-check"></i><b>3.5</b> Theoretical Properties</a></li>
<li class="chapter" data-level="3.6" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-1"><i class="fa fa-check"></i><b>3.6</b> Summary Table</a></li>
<li class="chapter" data-level="3.7" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
<li class="chapter" data-level="3.8" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties-of-value-iteration"><i class="fa fa-check"></i><b>3.8</b> Theoretical Properties of Value Iteration</a></li>
<li class="chapter" data-level="3.9" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-2"><i class="fa fa-check"></i><b>3.9</b> Summary Table</a></li>
<li class="chapter" data-level="3.10" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-2"><i class="fa fa-check"></i><b>3.10</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><i class="fa fa-check"></i><b>4</b> Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#implementation"><i class="fa fa-check"></i><b>4.2</b> Implementation</a></li>
<li class="chapter" data-level="4.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#interpretation-and-discussion"><i class="fa fa-check"></i><b>4.3</b> Interpretation and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><i class="fa fa-check"></i><b>5</b> On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-on-policy"><i class="fa fa-check"></i><b>5.2</b> SARSA (On-Policy)</a></li>
<li class="chapter" data-level="5.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-off-policy"><i class="fa fa-check"></i><b>5.3</b> Q-Learning (Off-Policy)</a></li>
<li class="chapter" data-level="5.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-with-importance-sampling"><i class="fa fa-check"></i><b>5.4</b> Off-Policy Monte Carlo with Importance Sampling</a></li>
<li class="chapter" data-level="5.5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#environment-and-common-r-components"><i class="fa fa-check"></i><b>5.5</b> Environment and Common R Components</a></li>
<li class="chapter" data-level="5.6" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-in-r"><i class="fa fa-check"></i><b>5.6</b> SARSA in R</a></li>
<li class="chapter" data-level="5.7" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-in-r"><i class="fa fa-check"></i><b>5.7</b> Q-Learning in R</a></li>
<li class="chapter" data-level="5.8" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-in-r"><i class="fa fa-check"></i><b>5.8</b> Off-Policy Monte Carlo in R</a></li>
<li class="chapter" data-level="5.9" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#value-iteration-in-r"><i class="fa fa-check"></i><b>5.9</b> Value Iteration in R</a></li>
<li class="chapter" data-level="5.10" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#running-and-visualizing-the-algorithms-in-r"><i class="fa fa-check"></i><b>5.10</b> Running and Visualizing the Algorithms in R</a></li>
<li class="chapter" data-level="5.11" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#interpretation-and-discussion-1"><i class="fa fa-check"></i><b>5.11</b> Interpretation and Discussion</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#policy-differences"><i class="fa fa-check"></i><b>5.11.1</b> Policy differences</a></li>
<li class="chapter" data-level="5.11.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#devaluation"><i class="fa fa-check"></i><b>5.11.2</b> Devaluation</a></li>
<li class="chapter" data-level="5.11.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#practical-implications"><i class="fa fa-check"></i><b>5.11.3</b> Practical implications</a></li>
<li class="chapter" data-level="5.11.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#experimental-observations"><i class="fa fa-check"></i><b>5.11.4</b> Experimental observations</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#conclusion-3"><i class="fa fa-check"></i><b>5.12</b> Conclusion</a></li>
<li class="chapter" data-level="5.13" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#comparison-table"><i class="fa fa-check"></i><b>5.13</b> Comparison Table</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html"><i class="fa fa-check"></i><b>6</b> Function Approximation Q-Learning with Linear Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#q-learning-with-function-approximation"><i class="fa fa-check"></i><b>6.1.1</b> Q-Learning with Function Approximation</a></li>
<li class="chapter" data-level="6.1.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#comparison-with-tabular-q-learning"><i class="fa fa-check"></i><b>6.1.2</b> Comparison with Tabular Q-Learning</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#r-implementation"><i class="fa fa-check"></i><b>6.2</b> R Implementation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><i class="fa fa-check"></i><b>7</b> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation"><i class="fa fa-check"></i><b>7.1.1</b> Q-Learning with Random Forest Approximation</a></li>
<li class="chapter" data-level="7.1.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods"><i class="fa fa-check"></i><b>7.1.2</b> Comparison with Previous Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1"><i class="fa fa-check"></i><b>7.2</b> R Implementation</a></li>
<li class="chapter" data-level="7.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights"><i class="fa fa-check"></i><b>7.3</b> Analysis and Insights</a></li>
<li class="chapter" data-level="7.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4"><i class="fa fa-check"></i><b>7.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><i class="fa fa-check"></i><b>8</b> Deep Function Approximation: Q-Learning with Neural Networks in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation"><i class="fa fa-check"></i><b>8.2</b> Theoretical Foundation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity"><i class="fa fa-check"></i><b>8.2.1</b> Universal Approximation and Expressivity</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning"><i class="fa fa-check"></i><b>8.2.2</b> Gradient-Based Learning</a></li>
<li class="chapter" data-level="8.2.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches"><i class="fa fa-check"></i><b>8.2.3</b> Comparison with Previous Approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2"><i class="fa fa-check"></i><b>8.3</b> R Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation"><i class="fa fa-check"></i><b>8.4</b> Analysis and Interpretation</a></li>
<li class="chapter" data-level="8.5" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions"><i class="fa fa-check"></i><b>8.5</b> Future Directions</a></li>
<li class="chapter" data-level="8.6" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5"><i class="fa fa-check"></i><b>8.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html"><i class="fa fa-check"></i><b>9</b> Dyna and DynaQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#theoretical-framework"><i class="fa fa-check"></i><b>9.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#the-dyna-architecture"><i class="fa fa-check"></i><b>9.2.1</b> The Dyna Architecture</a></li>
<li class="chapter" data-level="9.2.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#model-representation"><i class="fa fa-check"></i><b>9.2.2</b> Model Representation</a></li>
<li class="chapter" data-level="9.2.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#convergence-properties"><i class="fa fa-check"></i><b>9.2.3</b> Convergence Properties</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-in-r"><i class="fa fa-check"></i><b>9.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#environment-setup"><i class="fa fa-check"></i><b>9.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="9.3.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#dyna-q-implementation"><i class="fa fa-check"></i><b>9.3.2</b> Dyna-Q Implementation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#experimental-analysis"><i class="fa fa-check"></i><b>9.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#learning-efficiency-comparison"><i class="fa fa-check"></i><b>9.4.1</b> Learning Efficiency Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
<li class="chapter" data-level="9.6" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-considerations-and-conclusion"><i class="fa fa-check"></i><b>9.6</b> Implementation Considerations and Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><i class="fa fa-check"></i><b>10</b> Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#theoretical-framework-1"><i class="fa fa-check"></i><b>10.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#the-exploration-bonus-mechanism"><i class="fa fa-check"></i><b>10.2.1</b> The Exploration Bonus Mechanism</a></li>
<li class="chapter" data-level="10.2.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#complete-dyna-q-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> Complete Dyna-Q+ Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#convergence-and-stability"><i class="fa fa-check"></i><b>10.2.3</b> Convergence and Stability</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#implementation-in-r-1"><i class="fa fa-check"></i><b>10.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#environment-setup-1"><i class="fa fa-check"></i><b>10.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="10.3.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-implementation-1"><i class="fa fa-check"></i><b>10.3.2</b> Dyna-Q+ Implementation</a></li>
<li class="chapter" data-level="10.3.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#standard-dyna-for-comparison"><i class="fa fa-check"></i><b>10.3.3</b> Standard Dyna for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#experimental-analysis-1"><i class="fa fa-check"></i><b>10.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#adaptation-to-environmental-changes"><i class="fa fa-check"></i><b>10.4.1</b> Adaptation to Environmental Changes</a></li>
<li class="chapter" data-level="10.4.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#parameter-sensitivity-analysis"><i class="fa fa-check"></i><b>10.4.2</b> Parameter Sensitivity Analysis</a></li>
<li class="chapter" data-level="10.4.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#exploration-pattern-analysis"><i class="fa fa-check"></i><b>10.4.3</b> Exploration Pattern Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#discussion-and-implementation-considerations"><i class="fa fa-check"></i><b>10.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="10.6" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#conclusion-6"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html"><i class="fa fa-check"></i><b>11</b> Function Approximation And Feature Engineering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#feature-engineering-and-state-representation"><i class="fa fa-check"></i><b>11.1</b> Feature Engineering and State Representation</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-discrimination-vs.-generalization-tradeoff"><i class="fa fa-check"></i><b>11.1.1</b> The Discrimination vs.Â Generalization Tradeoff</a></li>
<li class="chapter" data-level="11.1.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#principles-of-effective-feature-design"><i class="fa fa-check"></i><b>11.1.2</b> Principles of Effective Feature Design</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#mathematical-foundations-of-linear-function-approximation"><i class="fa fa-check"></i><b>11.2</b> Mathematical Foundations of Linear Function Approximation</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#linear-value-functions-and-their-properties"><i class="fa fa-check"></i><b>11.2.1</b> Linear Value Functions and Their Properties</a></li>
<li class="chapter" data-level="11.2.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#temporal-difference-learning-with-linear-approximation"><i class="fa fa-check"></i><b>11.2.2</b> Temporal Difference Learning with Linear Approximation</a></li>
<li class="chapter" data-level="11.2.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-deadly-triad-and-stability-concerns"><i class="fa fa-check"></i><b>11.2.3</b> The Deadly Triad and Stability Concerns</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#classical-basis-function-methods"><i class="fa fa-check"></i><b>11.3</b> Classical Basis Function Methods</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#coarse-coding-overlapping-receptive-fields"><i class="fa fa-check"></i><b>11.3.1</b> Coarse Coding: Overlapping Receptive Fields</a></li>
<li class="chapter" data-level="11.3.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#tile-coding-structured-overlapping-grids"><i class="fa fa-check"></i><b>11.3.2</b> Tile Coding: Structured Overlapping Grids</a></li>
<li class="chapter" data-level="11.3.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#radial-basis-functions-smooth-continuous-features"><i class="fa fa-check"></i><b>11.3.3</b> Radial Basis Functions: Smooth Continuous Features</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#comparative-analysis-and-practical-considerations"><i class="fa fa-check"></i><b>11.4</b> Comparative Analysis and Practical Considerations</a></li>
<li class="chapter" data-level="11.5" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#bridging-classical-and-modern-approaches"><i class="fa fa-check"></i><b>11.5</b> Bridging Classical and Modern Approaches</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html"><i class="fa fa-check"></i><b>12</b> Learning Policies versus Learning Values:</a>
<ul>
<li class="chapter" data-level="12.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#the-two-paradigms-of-reinforcement-learning"><i class="fa fa-check"></i><b>12.1</b> The Two Paradigms of Reinforcement Learning</a></li>
<li class="chapter" data-level="12.2" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#value-based-methods-learning-worth-before-action"><i class="fa fa-check"></i><b>12.2</b> Value-Based Methods: Learning Worth Before Action</a></li>
<li class="chapter" data-level="12.3" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#policy-based-methods-direct-optimization-of-behavior"><i class="fa fa-check"></i><b>12.3</b> Policy-Based Methods: Direct Optimization of Behavior</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#the-policy-gradient-theorem"><i class="fa fa-check"></i><b>12.3.1</b> The Policy Gradient Theorem</a></li>
<li class="chapter" data-level="12.3.2" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#variance-reduction-through-baselines"><i class="fa fa-check"></i><b>12.3.2</b> Variance Reduction Through Baselines</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#conclusion-7"><i class="fa fa-check"></i><b>12.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><i class="fa fa-check"></i><b>13</b> Average Reward in Reinforcement Learning: A Comprehensive Guide</a>
<ul>
<li class="chapter" data-level="13.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-limitations-of-discounted-reward-formulations"><i class="fa fa-check"></i><b>13.1</b> The Limitations of Discounted Reward Formulations</a></li>
<li class="chapter" data-level="13.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-average-reward-alternative-theoretical-foundations"><i class="fa fa-check"></i><b>13.2</b> The Average Reward Alternative: Theoretical Foundations</a></li>
<li class="chapter" data-level="13.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#value-functions-in-the-average-reward-setting"><i class="fa fa-check"></i><b>13.3</b> Value Functions in the Average Reward Setting</a></li>
<li class="chapter" data-level="13.4" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-theory-for-average-reward"><i class="fa fa-check"></i><b>13.4</b> Optimality Theory for Average Reward</a></li>
<li class="chapter" data-level="13.5" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#learning-algorithms-for-average-reward"><i class="fa fa-check"></i><b>13.5</b> Learning Algorithms for Average Reward</a></li>
<li class="chapter" data-level="13.6" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#practical-implementation-patient-triage-in-emergency-department"><i class="fa fa-check"></i><b>13.6</b> Practical Implementation: Patient Triage in Emergency Department</a></li>
<li class="chapter" data-level="13.7" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#implementation-considerations"><i class="fa fa-check"></i><b>13.7</b> Implementation Considerations</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#convergence-of-average-reward-td-learning"><i class="fa fa-check"></i><b>13.7.1</b> Convergence of Average Reward TD Learning</a></li>
<li class="chapter" data-level="13.7.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#policy-gradient-theorem-for-average-reward"><i class="fa fa-check"></i><b>13.7.2</b> Policy Gradient Theorem for Average Reward</a></li>
<li class="chapter" data-level="13.7.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-equations-derivation"><i class="fa fa-check"></i><b>13.7.3</b> Optimality Equations Derivation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="eligibility-traces.html"><a href="eligibility-traces.html"><i class="fa fa-check"></i><b>14</b> Eligibility Traces</a>
<ul>
<li class="chapter" data-level="14.1" data-path="eligibility-traces.html"><a href="eligibility-traces.html#from-one-step-to-multi-step-learning"><i class="fa fa-check"></i><b>14.1</b> From One-Step to Multi-Step Learning</a></li>
<li class="chapter" data-level="14.2" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-mechanics-of-eligibility-traces-a-backward-view"><i class="fa fa-check"></i><b>14.2</b> The Mechanics of Eligibility Traces: A Backward View</a></li>
<li class="chapter" data-level="14.3" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-tdlambda-algorithm-for-prediction"><i class="fa fa-check"></i><b>14.3</b> The TD(<span class="math inline">\(`\lambda`\)</span>) Algorithm for Prediction</a></li>
<li class="chapter" data-level="14.4" data-path="eligibility-traces.html"><a href="eligibility-traces.html#control-with-eligibility-traces-average-reward-sarsalambda"><i class="fa fa-check"></i><b>14.4</b> Control with Eligibility Traces: Average Reward Sarsa(<span class="math inline">\(`\lambda`\)</span>)</a></li>
<li class="chapter" data-level="14.5" data-path="eligibility-traces.html"><a href="eligibility-traces.html#practical-implementation-server-load-balancing-with-sarsalambda"><i class="fa fa-check"></i><b>14.5</b> Practical Implementation: Server Load Balancing with Sarsa(<span class="math inline">\(`\lambda`\)</span>)</a></li>
<li class="chapter" data-level="14.6" data-path="eligibility-traces.html"><a href="eligibility-traces.html#computational-and-performance-considerations"><i class="fa fa-check"></i><b>14.6</b> Computational and Performance Considerations</a></li>
<li class="chapter" data-level="14.7" data-path="eligibility-traces.html"><a href="eligibility-traces.html#conclusion-8"><i class="fa fa-check"></i><b>14.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html"><i class="fa fa-check"></i><b>15</b> Intrinsic Rewards</a>
<ul>
<li class="chapter" data-level="15.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#the-sparse-reward-problem-and-its-implications"><i class="fa fa-check"></i><b>15.1</b> The Sparse Reward Problem and Its Implications</a></li>
<li class="chapter" data-level="15.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#mathematical-foundations-of-intrinsic-motivation"><i class="fa fa-check"></i><b>15.2</b> Mathematical Foundations of Intrinsic Motivation</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#curiosity-driven-learning"><i class="fa fa-check"></i><b>15.2.1</b> Curiosity-Driven Learning</a></li>
<li class="chapter" data-level="15.2.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#count-based-exploration"><i class="fa fa-check"></i><b>15.2.2</b> Count-Based Exploration</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#practical-implementation-curiosity-driven-grid-world-navigation"><i class="fa fa-check"></i><b>15.3</b> Practical Implementation: Curiosity-Driven Grid World Navigation</a></li>
<li class="chapter" data-level="15.4" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#advanced-intrinsic-motivation-mechanisms"><i class="fa fa-check"></i><b>15.4</b> Advanced Intrinsic Motivation Mechanisms</a></li>
<li class="chapter" data-level="15.5" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#implementation-considerations-and-practical-guidelines"><i class="fa fa-check"></i><b>15.5</b> Implementation Considerations and Practical Guidelines</a></li>
<li class="chapter" data-level="15.6" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#theoretical-analysis-and-convergence-properties"><i class="fa fa-check"></i><b>15.6</b> Theoretical Analysis and Convergence Properties</a></li>
<li class="chapter" data-level="15.7" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#applications-and-empirical-results"><i class="fa fa-check"></i><b>15.7</b> Applications and Empirical Results</a></li>
<li class="chapter" data-level="15.8" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#limitations-and-future-directions"><i class="fa fa-check"></i><b>15.8</b> Limitations and Future Directions</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>16</b> Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="16.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#introduction-9"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#theoretical-framework-2"><i class="fa fa-check"></i><b>16.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#the-policy-gradient-theorem-1"><i class="fa fa-check"></i><b>16.2.1</b> The Policy Gradient Theorem</a></li>
<li class="chapter" data-level="16.2.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-algorithm"><i class="fa fa-check"></i><b>16.2.2</b> REINFORCE Algorithm</a></li>
<li class="chapter" data-level="16.2.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#baseline-subtraction-and-variance-reduction"><i class="fa fa-check"></i><b>16.2.3</b> Baseline Subtraction and Variance Reduction</a></li>
<li class="chapter" data-level="16.2.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-parameterization"><i class="fa fa-check"></i><b>16.2.4</b> Softmax Policy Parameterization</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#implementation-in-r-2"><i class="fa fa-check"></i><b>16.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#environment-and-feature-representation"><i class="fa fa-check"></i><b>16.3.1</b> Environment and Feature Representation</a></li>
<li class="chapter" data-level="16.3.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-implementation"><i class="fa fa-check"></i><b>16.3.2</b> Softmax Policy Implementation</a></li>
<li class="chapter" data-level="16.3.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-implementation"><i class="fa fa-check"></i><b>16.3.3</b> REINFORCE Implementation</a></li>
<li class="chapter" data-level="16.3.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#actor-critic-implementation"><i class="fa fa-check"></i><b>16.3.4</b> Actor-Critic Implementation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#experimental-analysis-2"><i class="fa fa-check"></i><b>16.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#comparison-of-policy-gradient-variants"><i class="fa fa-check"></i><b>16.4.1</b> Comparison of Policy Gradient Variants</a></li>
<li class="chapter" data-level="16.4.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#learning-rate-sensitivity-analysis"><i class="fa fa-check"></i><b>16.4.2</b> Learning Rate Sensitivity Analysis</a></li>
<li class="chapter" data-level="16.4.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#variance-analysis-of-gradient-estimates"><i class="fa fa-check"></i><b>16.4.3</b> Variance Analysis of Gradient Estimates</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#discussion-and-implementation-considerations-1"><i class="fa fa-check"></i><b>16.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="16.6" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#conclusion-9"><i class="fa fa-check"></i><b>16.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>17</b> Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="17.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#theoretical-framework-3"><i class="fa fa-check"></i><b>17.1</b> Theoretical Framework</a></li>
<li class="chapter" data-level="17.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#implementation-and-comparative-analysis"><i class="fa fa-check"></i><b>17.2</b> Implementation and Comparative Analysis</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#variance-analysis-and-learning-dynamics"><i class="fa fa-check"></i><b>17.2.1</b> Variance Analysis and Learning Dynamics</a></li>
<li class="chapter" data-level="17.2.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#algorithmic-variants-and-extensions"><i class="fa fa-check"></i><b>17.2.2</b> Algorithmic Variants and Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#computational-and-convergence-considerations"><i class="fa fa-check"></i><b>17.3</b> Computational and Convergence Considerations</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#comparative-performance-analysis"><i class="fa fa-check"></i><b>17.3.1</b> Comparative Performance Analysis</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#conclusion-10"><i class="fa fa-check"></i><b>17.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>18</b> Appendix</a>
<ul>
<li class="chapter" data-level="18.1" data-path="appendix.html"><a href="appendix.html#comprehensive-reinforcement-learning-concepts-guide"><i class="fa fa-check"></i><b>18.1</b> Comprehensive Reinforcement Learning Concepts Guide</a></li>
<li class="chapter" data-level="18.2" data-path="appendix.html"><a href="appendix.html#learning-mechanisms"><i class="fa fa-check"></i><b>18.2</b> Learning Mechanisms</a></li>
<li class="chapter" data-level="18.3" data-path="appendix.html"><a href="appendix.html#environment-properties"><i class="fa fa-check"></i><b>18.3</b> Environment Properties</a></li>
<li class="chapter" data-level="18.4" data-path="appendix.html"><a href="appendix.html#learning-paradigms"><i class="fa fa-check"></i><b>18.4</b> Learning Paradigms</a></li>
<li class="chapter" data-level="18.5" data-path="appendix.html"><a href="appendix.html#exploration-strategies"><i class="fa fa-check"></i><b>18.5</b> Exploration Strategies</a></li>
<li class="chapter" data-level="18.6" data-path="appendix.html"><a href="appendix.html#key-algorithms-methods"><i class="fa fa-check"></i><b>18.6</b> Key Algorithms &amp; Methods</a></li>
<li class="chapter" data-level="18.7" data-path="appendix.html"><a href="appendix.html#advanced-concepts"><i class="fa fa-check"></i><b>18.7</b> Advanced Concepts</a></li>
<li class="chapter" data-level="18.8" data-path="appendix.html"><a href="appendix.html#fundamental-equations"><i class="fa fa-check"></i><b>18.8</b> Fundamental Equations</a>
<ul>
<li class="chapter" data-level="18.8.1" data-path="appendix.html"><a href="appendix.html#bellman-equations"><i class="fa fa-check"></i><b>18.8.1</b> <strong>Bellman Equations</strong></a></li>
<li class="chapter" data-level="18.8.2" data-path="appendix.html"><a href="appendix.html#policy-gradient-theorem"><i class="fa fa-check"></i><b>18.8.2</b> <strong>Policy Gradient Theorem</strong></a></li>
<li class="chapter" data-level="18.8.3" data-path="appendix.html"><a href="appendix.html#temporal-difference-error"><i class="fa fa-check"></i><b>18.8.3</b> <strong>Temporal Difference Error</strong></a></li>
</ul></li>
<li class="chapter" data-level="18.9" data-path="appendix.html"><a href="appendix.html#common-challenges-solutions"><i class="fa fa-check"></i><b>18.9</b> Common Challenges &amp; Solutions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-5" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Introduction<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While linear function approximation provides a solid foundation for scaling reinforcement learning beyond tabular methods, it assumes a linear relationship between features and Q-values. Real-world problems often exhibit complex, non-linear patterns that linear models cannot capture effectively. This post extends our previous exploration by implementing Q-Learning with Random Forest function approximation, demonstrating how ensemble methods can learn intricate state-action value relationships while maintaining interpretability and robust generalization.</p>
<p>Random Forests offer several advantages over linear approximation: they handle non-linear relationships naturally, provide built-in feature importance measures, resist overfitting through ensemble averaging, and require minimal hyperparameter tuning. Weâll implement this approach using the same 10-state, 2-action environment, comparing the learned policies and examining the unique characteristics of tree-based function approximation.</p>
<p>Random Forest function approximation replaces the linear parameterization with an ensemble of decision trees. Instead of:</p>
<p><span class="math display">\[
Q(s, a; \theta) = \phi(s, a)^T \theta
\]</span></p>
<p>we now approximate the action-value function as:</p>
<p><span class="math display">\[
Q(s, a) = \frac{1}{B} \sum_{b=1}^{B} T_b(\phi(s, a))
\]</span></p>
<p>where <span class="math inline">\(T_b\)</span> represents the <span class="math inline">\(b\)</span>-th tree in the ensemble, <span class="math inline">\(B\)</span> is the number of trees, and <span class="math inline">\(\phi(s, a)\)</span> is our feature representation. Each tree <span class="math inline">\(T_b\)</span> is trained on a bootstrap sample of the data with random feature subsets at each split, providing natural regularization and variance reduction.</p>
<div id="q-learning-with-random-forest-approximation" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Q-Learning with Random Forest Approximation<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Q-Learning update process with Random Forest approximation involves:</p>
<ol style="list-style-type: decimal">
<li><strong>Experience Collection</strong>: Gather state-action-reward-next state tuples <span class="math inline">\((s, a, r, s&#39;)\)</span></li>
<li><strong>Target Computation</strong>: Calculate TD targets <span class="math inline">\(y = r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;)\)</span></li>
<li><strong>Model Training</strong>: Fit Random Forest regressor to predict <span class="math inline">\(Q(s, a)\)</span> from features <span class="math inline">\(\phi(s, a)\)</span></li>
<li><strong>Policy Update</strong>: Use updated model for epsilon-greedy action selection</li>
</ol>
<p>Unlike linear methods with continuous parameter updates, Random Forest approximation requires periodic model retraining on accumulated experience. This batch-like approach trades computational efficiency for modeling flexibility.</p>
<p>For our implementation, we use a simple concatenation of one-hot encoded state and action vectors:</p>
<p><span class="math display">\[
\phi(s, a) = [e_s^{(state)} \; || \; e_a^{(action)}]
\]</span></p>
<p>where <span class="math inline">\(e_s^{(state)}\)</span> is a one-hot vector for state <span class="math inline">\(s\)</span> and <span class="math inline">\(e_a^{(action)}\)</span> is a one-hot vector for action <span class="math inline">\(a\)</span>. This encoding allows trees to learn complex interactions between states and actions while maintaining interpretability.</p>
</div>
<div id="comparison-with-previous-methods" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Comparison with Previous Methods<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="10%" />
<col width="21%" />
<col width="30%" />
<col width="37%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>Tabular Q-Learning</strong></th>
<th><strong>Linear Function Approximation</strong></th>
<th><strong>Random Forest Function Approximation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Model Complexity</strong></td>
<td>None; direct storage</td>
<td>Linear combinations</td>
<td>Non-linear ensemble</td>
</tr>
<tr class="even">
<td><strong>Feature Interactions</strong></td>
<td>Implicit</td>
<td>None (unless engineered)</td>
<td>Automatic discovery</td>
</tr>
<tr class="odd">
<td><strong>Interpretability</strong></td>
<td>Full</td>
<td>Moderate (weights)</td>
<td>High (tree structures)</td>
</tr>
<tr class="even">
<td><strong>Training</strong></td>
<td>Online updates</td>
<td>Gradient descent</td>
<td>Batch retraining</td>
</tr>
<tr class="odd">
<td><strong>Overfitting Risk</strong></td>
<td>None</td>
<td>Low</td>
<td>Low (ensemble averaging)</td>
</tr>
<tr class="even">
<td><strong>Computational Cost</strong></td>
<td><span class="math inline">\(O(1)\)</span> lookup</td>
<td><span class="math inline">\(O(d)\)</span> linear algebra</td>
<td><span class="math inline">\(O(B \cdot \log n)\)</span> prediction</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="r-implementation-1" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> R Implementation<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our implementation builds upon the previous environment while introducing Random Forest-based Q-value approximation. The key innovation lies in accumulating training examples and periodically retraining the forest to incorporate new experience.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-1" tabindex="-1"></a><span class="co"># Load required libraries</span></span>
<span id="cb61-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-2" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb61-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-3" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb61-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-4" tabindex="-1"></a><span class="fu">library</span>(gridExtra)</span>
<span id="cb61-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-5" tabindex="-1"></a></span>
<span id="cb61-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-6" tabindex="-1"></a><span class="co"># Set global seed for reproducibility</span></span>
<span id="cb61-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-7" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb61-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-8" tabindex="-1"></a></span>
<span id="cb61-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-9" tabindex="-1"></a><span class="co"># Environment setup</span></span>
<span id="cb61-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-10" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb61-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-11" tabindex="-1"></a>n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb61-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-12" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb61-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-13" tabindex="-1"></a>terminal_state <span class="ot">&lt;-</span> n_states</span>
<span id="cb61-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-14" tabindex="-1"></a></span>
<span id="cb61-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-15" tabindex="-1"></a><span class="co"># Build valid transition and reward models</span></span>
<span id="cb61-16"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-16" tabindex="-1"></a>transition_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb61-17"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-17" tabindex="-1"></a>reward_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb61-18"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-18" tabindex="-1"></a></span>
<span id="cb61-19"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-19" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>)) {</span>
<span id="cb61-20"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-20" tabindex="-1"></a>  <span class="cf">for</span> (a <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_actions) {</span>
<span id="cb61-21"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-21" tabindex="-1"></a>    <span class="co"># Ensure valid, normalized probability distributions</span></span>
<span id="cb61-22"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-22" tabindex="-1"></a>    <span class="co"># Action 1: mostly forward, with small random transition</span></span>
<span id="cb61-23"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-23" tabindex="-1"></a>    <span class="cf">if</span> (a <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb61-24"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-24" tabindex="-1"></a>      next_state <span class="ot">&lt;-</span> <span class="fu">min</span>(s <span class="sc">+</span> <span class="dv">1</span>, n_states <span class="sc">-</span> <span class="dv">1</span>)  <span class="co"># Don&#39;t randomly jump to terminal</span></span>
<span id="cb61-25"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-25" tabindex="-1"></a>      random_state <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), next_state), <span class="dv">1</span>)</span>
<span id="cb61-26"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-26" tabindex="-1"></a>      transition_model[s, <span class="dv">1</span>, next_state] <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb61-27"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-27" tabindex="-1"></a>      transition_model[s, <span class="dv">1</span>, random_state] <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb61-28"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-28" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb61-29"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-29" tabindex="-1"></a>      <span class="co"># Action 2: two random non-terminal states</span></span>
<span id="cb61-30"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-30" tabindex="-1"></a>      random_states <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), <span class="dv">2</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb61-31"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-31" tabindex="-1"></a>      transition_model[s, <span class="dv">2</span>, random_states[<span class="dv">1</span>]] <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb61-32"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-32" tabindex="-1"></a>      transition_model[s, <span class="dv">2</span>, random_states[<span class="dv">2</span>]] <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb61-33"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-33" tabindex="-1"></a>    }</span>
<span id="cb61-34"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-34" tabindex="-1"></a>    </span>
<span id="cb61-35"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-35" tabindex="-1"></a>    <span class="co"># Assign rewards</span></span>
<span id="cb61-36"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-36" tabindex="-1"></a>    <span class="cf">for</span> (s_prime <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_states) {</span>
<span id="cb61-37"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-37" tabindex="-1"></a>      <span class="cf">if</span> (transition_model[s, a, s_prime] <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb61-38"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-38" tabindex="-1"></a>        reward_model[s, a, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(</span>
<span id="cb61-39"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-39" tabindex="-1"></a>          s_prime <span class="sc">==</span> n_states, </span>
<span id="cb61-40"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-40" tabindex="-1"></a>          <span class="fu">ifelse</span>(a <span class="sc">==</span> <span class="dv">1</span>, <span class="fl">1.0</span>, <span class="fl">0.5</span>),  <span class="co"># Terminal reward</span></span>
<span id="cb61-41"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-41" tabindex="-1"></a>          <span class="fu">runif</span>(<span class="dv">1</span>, <span class="dv">0</span>, <span class="fu">ifelse</span>(a <span class="sc">==</span> <span class="dv">1</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>))  <span class="co"># Step reward</span></span>
<span id="cb61-42"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-42" tabindex="-1"></a>        )</span>
<span id="cb61-43"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-43" tabindex="-1"></a>      }</span>
<span id="cb61-44"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-44" tabindex="-1"></a>    }</span>
<span id="cb61-45"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-45" tabindex="-1"></a>  }</span>
<span id="cb61-46"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-46" tabindex="-1"></a>}</span>
<span id="cb61-47"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-47" tabindex="-1"></a></span>
<span id="cb61-48"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-48" tabindex="-1"></a><span class="co"># Terminal state: no transitions</span></span>
<span id="cb61-49"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-49" tabindex="-1"></a>transition_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb61-50"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-50" tabindex="-1"></a>reward_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb61-51"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-51" tabindex="-1"></a></span>
<span id="cb61-52"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-52" tabindex="-1"></a><span class="co"># Verify transition probabilities sum to 1</span></span>
<span id="cb61-53"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-53" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)) {</span>
<span id="cb61-54"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-54" tabindex="-1"></a>  <span class="cf">for</span> (a <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_actions) {</span>
<span id="cb61-55"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-55" tabindex="-1"></a>    prob_sum <span class="ot">&lt;-</span> <span class="fu">sum</span>(transition_model[s, a, ])</span>
<span id="cb61-56"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-56" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">abs</span>(prob_sum <span class="sc">-</span> <span class="fl">1.0</span>) <span class="sc">&gt;</span> <span class="fl">1e-6</span>) {</span>
<span id="cb61-57"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-57" tabindex="-1"></a>      <span class="fu">warning</span>(<span class="fu">sprintf</span>(<span class="st">&quot;State %d, Action %d: probabilities sum to %.3f&quot;</span>, s, a, prob_sum))</span>
<span id="cb61-58"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-58" tabindex="-1"></a>    }</span>
<span id="cb61-59"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-59" tabindex="-1"></a>  }</span>
<span id="cb61-60"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-60" tabindex="-1"></a>}</span>
<span id="cb61-61"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-61" tabindex="-1"></a></span>
<span id="cb61-62"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-62" tabindex="-1"></a><span class="co"># Sampling function</span></span>
<span id="cb61-63"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-63" tabindex="-1"></a>sample_env <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a) {</span>
<span id="cb61-64"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-64" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> transition_model[s, a, ]</span>
<span id="cb61-65"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-65" tabindex="-1"></a>  s_prime <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>, <span class="at">prob =</span> probs)</span>
<span id="cb61-66"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-66" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> reward_model[s, a, s_prime]</span>
<span id="cb61-67"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-67" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">s_prime =</span> s_prime, <span class="at">reward =</span> reward)</span>
<span id="cb61-68"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-68" tabindex="-1"></a>}</span>
<span id="cb61-69"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-69" tabindex="-1"></a></span>
<span id="cb61-70"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-70" tabindex="-1"></a><span class="co"># Feature encoding</span></span>
<span id="cb61-71"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-71" tabindex="-1"></a>encode_features <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a, n_states, n_actions) {</span>
<span id="cb61-72"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-72" tabindex="-1"></a>  state_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb61-73"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-73" tabindex="-1"></a>  action_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_actions)</span>
<span id="cb61-74"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-74" tabindex="-1"></a>  state_vec[s] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb61-75"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-75" tabindex="-1"></a>  action_vec[a] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb61-76"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-76" tabindex="-1"></a>  <span class="fu">c</span>(state_vec, action_vec)</span>
<span id="cb61-77"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-77" tabindex="-1"></a>}</span>
<span id="cb61-78"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-78" tabindex="-1"></a></span>
<span id="cb61-79"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-79" tabindex="-1"></a>n_features <span class="ot">&lt;-</span> n_states <span class="sc">+</span> n_actions</span>
<span id="cb61-80"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-80" tabindex="-1"></a></span>
<span id="cb61-81"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-81" tabindex="-1"></a><span class="co"># Experience replay buffer</span></span>
<span id="cb61-82"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-82" tabindex="-1"></a>ExperienceBuffer <span class="ot">&lt;-</span> <span class="fu">setRefClass</span>(</span>
<span id="cb61-83"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-83" tabindex="-1"></a>  <span class="st">&quot;ExperienceBuffer&quot;</span>,</span>
<span id="cb61-84"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-84" tabindex="-1"></a>  <span class="at">fields =</span> <span class="fu">list</span>(</span>
<span id="cb61-85"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-85" tabindex="-1"></a>    <span class="at">capacity =</span> <span class="st">&quot;numeric&quot;</span>,</span>
<span id="cb61-86"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-86" tabindex="-1"></a>    <span class="at">buffer =</span> <span class="st">&quot;list&quot;</span>,</span>
<span id="cb61-87"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-87" tabindex="-1"></a>    <span class="at">position =</span> <span class="st">&quot;numeric&quot;</span></span>
<span id="cb61-88"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-88" tabindex="-1"></a>  ),</span>
<span id="cb61-89"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-89" tabindex="-1"></a>  <span class="at">methods =</span> <span class="fu">list</span>(</span>
<span id="cb61-90"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-90" tabindex="-1"></a>    <span class="at">initialize =</span> <span class="cf">function</span>(<span class="at">cap =</span> <span class="dv">5000</span>) {</span>
<span id="cb61-91"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-91" tabindex="-1"></a>      capacity <span class="ot">&lt;&lt;-</span> cap</span>
<span id="cb61-92"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-92" tabindex="-1"></a>      buffer <span class="ot">&lt;&lt;-</span> <span class="fu">vector</span>(<span class="st">&quot;list&quot;</span>, capacity)</span>
<span id="cb61-93"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-93" tabindex="-1"></a>      position <span class="ot">&lt;&lt;-</span> <span class="dv">0</span></span>
<span id="cb61-94"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-94" tabindex="-1"></a>    },</span>
<span id="cb61-95"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-95" tabindex="-1"></a>    <span class="at">add =</span> <span class="cf">function</span>(experience) {</span>
<span id="cb61-96"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-96" tabindex="-1"></a>      idx <span class="ot">&lt;-</span> (position <span class="sc">%%</span> capacity) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb61-97"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-97" tabindex="-1"></a>      buffer[[idx]] <span class="ot">&lt;&lt;-</span> experience</span>
<span id="cb61-98"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-98" tabindex="-1"></a>      position <span class="ot">&lt;&lt;-</span> position <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb61-99"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-99" tabindex="-1"></a>    },</span>
<span id="cb61-100"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-100" tabindex="-1"></a>    <span class="at">sample =</span> <span class="cf">function</span>(batch_size) {</span>
<span id="cb61-101"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-101" tabindex="-1"></a>      valid_size <span class="ot">&lt;-</span> <span class="fu">min</span>(position, capacity)</span>
<span id="cb61-102"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-102" tabindex="-1"></a>      <span class="cf">if</span> (valid_size <span class="sc">&lt;</span> batch_size) {</span>
<span id="cb61-103"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-103" tabindex="-1"></a>        indices <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span>valid_size</span>
<span id="cb61-104"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-104" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb61-105"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-105" tabindex="-1"></a>        indices <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>valid_size, batch_size, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb61-106"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-106" tabindex="-1"></a>      }</span>
<span id="cb61-107"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-107" tabindex="-1"></a>      buffer[indices]</span>
<span id="cb61-108"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-108" tabindex="-1"></a>    },</span>
<span id="cb61-109"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-109" tabindex="-1"></a>    <span class="at">size =</span> <span class="cf">function</span>() {</span>
<span id="cb61-110"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-110" tabindex="-1"></a>      <span class="fu">min</span>(position, capacity)</span>
<span id="cb61-111"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-111" tabindex="-1"></a>    }</span>
<span id="cb61-112"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-112" tabindex="-1"></a>  )</span>
<span id="cb61-113"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-113" tabindex="-1"></a>)</span>
<span id="cb61-114"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-114" tabindex="-1"></a></span>
<span id="cb61-115"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-115" tabindex="-1"></a><span class="co"># Tabular Q-Learning (baseline)</span></span>
<span id="cb61-116"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-116" tabindex="-1"></a>q_learning_tabular <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>) {</span>
<span id="cb61-117"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-117" tabindex="-1"></a>  Q <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> n_states, <span class="at">ncol =</span> n_actions)</span>
<span id="cb61-118"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-118" tabindex="-1"></a>  rewards_history <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb61-119"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-119" tabindex="-1"></a>  </span>
<span id="cb61-120"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-120" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb61-121"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-121" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>), <span class="dv">1</span>)</span>
<span id="cb61-122"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-122" tabindex="-1"></a>    episode_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb61-123"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-123" tabindex="-1"></a>    </span>
<span id="cb61-124"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-124" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state) {</span>
<span id="cb61-125"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-125" tabindex="-1"></a>      <span class="co"># Epsilon-greedy action selection</span></span>
<span id="cb61-126"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-126" tabindex="-1"></a>      a <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb61-127"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-127" tabindex="-1"></a>        <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb61-128"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-128" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb61-129"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-129" tabindex="-1"></a>        <span class="fu">which.max</span>(Q[s, ])</span>
<span id="cb61-130"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-130" tabindex="-1"></a>      }</span>
<span id="cb61-131"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-131" tabindex="-1"></a>      </span>
<span id="cb61-132"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-132" tabindex="-1"></a>      <span class="co"># Take action</span></span>
<span id="cb61-133"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-133" tabindex="-1"></a>      out <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a)</span>
<span id="cb61-134"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-134" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> out<span class="sc">$</span>s_prime</span>
<span id="cb61-135"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-135" tabindex="-1"></a>      r <span class="ot">&lt;-</span> out<span class="sc">$</span>reward</span>
<span id="cb61-136"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-136" tabindex="-1"></a>      episode_reward <span class="ot">&lt;-</span> episode_reward <span class="sc">+</span> r</span>
<span id="cb61-137"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-137" tabindex="-1"></a>      </span>
<span id="cb61-138"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-138" tabindex="-1"></a>      <span class="co"># Q-learning update</span></span>
<span id="cb61-139"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-139" tabindex="-1"></a>      max_q_next <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> terminal_state, <span class="dv">0</span>, <span class="fu">max</span>(Q[s_prime, ]))</span>
<span id="cb61-140"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-140" tabindex="-1"></a>      td_target <span class="ot">&lt;-</span> r <span class="sc">+</span> gamma <span class="sc">*</span> max_q_next</span>
<span id="cb61-141"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-141" tabindex="-1"></a>      Q[s, a] <span class="ot">&lt;-</span> Q[s, a] <span class="sc">+</span> alpha <span class="sc">*</span> (td_target <span class="sc">-</span> Q[s, a])</span>
<span id="cb61-142"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-142" tabindex="-1"></a>      </span>
<span id="cb61-143"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-143" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb61-144"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-144" tabindex="-1"></a>    }</span>
<span id="cb61-145"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-145" tabindex="-1"></a>    </span>
<span id="cb61-146"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-146" tabindex="-1"></a>    rewards_history[ep] <span class="ot">&lt;-</span> episode_reward</span>
<span id="cb61-147"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-147" tabindex="-1"></a>  }</span>
<span id="cb61-148"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-148" tabindex="-1"></a>  </span>
<span id="cb61-149"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-149" tabindex="-1"></a>  <span class="co"># Extract policy</span></span>
<span id="cb61-150"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-150" tabindex="-1"></a>  policy <span class="ot">&lt;-</span> <span class="fu">apply</span>(Q[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), ], <span class="dv">1</span>, which.max)</span>
<span id="cb61-151"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-151" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">Q =</span> Q, <span class="at">policy =</span> <span class="fu">c</span>(policy, <span class="cn">NA</span>), <span class="at">rewards =</span> rewards_history)</span>
<span id="cb61-152"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-152" tabindex="-1"></a>}</span>
<span id="cb61-153"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-153" tabindex="-1"></a></span>
<span id="cb61-154"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-154" tabindex="-1"></a><span class="co"># Q-Learning with Random Forest (Fixed)</span></span>
<span id="cb61-155"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-155" tabindex="-1"></a>q_learning_rf <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>, <span class="at">buffer_capacity =</span> <span class="dv">5000</span>,</span>
<span id="cb61-156"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-156" tabindex="-1"></a>                         <span class="at">batch_size =</span> <span class="dv">128</span>, <span class="at">retrain_freq =</span> <span class="dv">50</span>, <span class="at">update_target_freq =</span> <span class="dv">100</span>) {</span>
<span id="cb61-157"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-157" tabindex="-1"></a>  </span>
<span id="cb61-158"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-158" tabindex="-1"></a>  <span class="co"># Initialize experience buffer</span></span>
<span id="cb61-159"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-159" tabindex="-1"></a>  replay_buffer <span class="ot">&lt;-</span> ExperienceBuffer<span class="sc">$</span><span class="fu">new</span>(buffer_capacity)</span>
<span id="cb61-160"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-160" tabindex="-1"></a>  </span>
<span id="cb61-161"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-161" tabindex="-1"></a>  <span class="co"># Initialize models (current and target)</span></span>
<span id="cb61-162"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-162" tabindex="-1"></a>  current_model <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb61-163"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-163" tabindex="-1"></a>  target_model <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb61-164"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-164" tabindex="-1"></a>  </span>
<span id="cb61-165"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-165" tabindex="-1"></a>  rewards_history <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb61-166"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-166" tabindex="-1"></a>  td_errors <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb61-167"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-167" tabindex="-1"></a>  model_updates <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb61-168"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-168" tabindex="-1"></a>  </span>
<span id="cb61-169"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-169" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb61-170"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-170" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>), <span class="dv">1</span>)</span>
<span id="cb61-171"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-171" tabindex="-1"></a>    episode_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb61-172"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-172" tabindex="-1"></a>    episode_td_error <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb61-173"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-173" tabindex="-1"></a>    steps <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb61-174"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-174" tabindex="-1"></a>    </span>
<span id="cb61-175"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-175" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state) {</span>
<span id="cb61-176"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-176" tabindex="-1"></a>      <span class="co"># Predict Q-values using current model</span></span>
<span id="cb61-177"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-177" tabindex="-1"></a>      q_values <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb61-178"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-178" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb61-179"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-179" tabindex="-1"></a>        <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(current_model)) {</span>
<span id="cb61-180"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-180" tabindex="-1"></a>          <span class="fu">predict</span>(current_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x)))</span>
<span id="cb61-181"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-181" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb61-182"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-182" tabindex="-1"></a>          <span class="dv">0</span>  <span class="co"># Initialize to zero</span></span>
<span id="cb61-183"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-183" tabindex="-1"></a>        }</span>
<span id="cb61-184"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-184" tabindex="-1"></a>      })</span>
<span id="cb61-185"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-185" tabindex="-1"></a>      </span>
<span id="cb61-186"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-186" tabindex="-1"></a>      <span class="co"># Epsilon-greedy action selection</span></span>
<span id="cb61-187"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-187" tabindex="-1"></a>      a <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb61-188"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-188" tabindex="-1"></a>        <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb61-189"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-189" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb61-190"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-190" tabindex="-1"></a>        <span class="fu">which.max</span>(q_values)</span>
<span id="cb61-191"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-191" tabindex="-1"></a>      }</span>
<span id="cb61-192"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-192" tabindex="-1"></a>      </span>
<span id="cb61-193"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-193" tabindex="-1"></a>      <span class="co"># Take action</span></span>
<span id="cb61-194"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-194" tabindex="-1"></a>      out <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a)</span>
<span id="cb61-195"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-195" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> out<span class="sc">$</span>s_prime</span>
<span id="cb61-196"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-196" tabindex="-1"></a>      r <span class="ot">&lt;-</span> out<span class="sc">$</span>reward</span>
<span id="cb61-197"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-197" tabindex="-1"></a>      episode_reward <span class="ot">&lt;-</span> episode_reward <span class="sc">+</span> r</span>
<span id="cb61-198"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-198" tabindex="-1"></a>      steps <span class="ot">&lt;-</span> steps <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb61-199"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-199" tabindex="-1"></a>      </span>
<span id="cb61-200"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-200" tabindex="-1"></a>      <span class="co"># Store experience</span></span>
<span id="cb61-201"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-201" tabindex="-1"></a>      replay_buffer<span class="sc">$</span><span class="fu">add</span>(<span class="fu">list</span>(<span class="at">s =</span> s, <span class="at">a =</span> a, <span class="at">r =</span> r, <span class="at">s_prime =</span> s_prime))</span>
<span id="cb61-202"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-202" tabindex="-1"></a>      </span>
<span id="cb61-203"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-203" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb61-204"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-204" tabindex="-1"></a>    }</span>
<span id="cb61-205"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-205" tabindex="-1"></a>    </span>
<span id="cb61-206"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-206" tabindex="-1"></a>    rewards_history[ep] <span class="ot">&lt;-</span> episode_reward</span>
<span id="cb61-207"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-207" tabindex="-1"></a>    </span>
<span id="cb61-208"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-208" tabindex="-1"></a>    <span class="co"># Train model if enough samples</span></span>
<span id="cb61-209"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-209" tabindex="-1"></a>    <span class="cf">if</span> (replay_buffer<span class="sc">$</span><span class="fu">size</span>() <span class="sc">&gt;=</span> batch_size <span class="sc">&amp;&amp;</span> ep <span class="sc">%%</span> retrain_freq <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb61-210"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-210" tabindex="-1"></a>      <span class="co"># Sample batch from replay buffer</span></span>
<span id="cb61-211"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-211" tabindex="-1"></a>      batch <span class="ot">&lt;-</span> replay_buffer<span class="sc">$</span><span class="fu">sample</span>(batch_size)</span>
<span id="cb61-212"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-212" tabindex="-1"></a>      </span>
<span id="cb61-213"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-213" tabindex="-1"></a>      <span class="co"># Prepare training data</span></span>
<span id="cb61-214"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-214" tabindex="-1"></a>      train_x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> batch_size, <span class="at">ncol =</span> n_features)</span>
<span id="cb61-215"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-215" tabindex="-1"></a>      train_y <span class="ot">&lt;-</span> <span class="fu">numeric</span>(batch_size)</span>
<span id="cb61-216"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-216" tabindex="-1"></a>      </span>
<span id="cb61-217"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-217" tabindex="-1"></a>      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>batch_size) {</span>
<span id="cb61-218"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-218" tabindex="-1"></a>        exp <span class="ot">&lt;-</span> batch[[i]]</span>
<span id="cb61-219"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-219" tabindex="-1"></a>        </span>
<span id="cb61-220"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-220" tabindex="-1"></a>        <span class="co"># Use target model for computing Q-values (DQN-style)</span></span>
<span id="cb61-221"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-221" tabindex="-1"></a>        model_to_use <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(target_model)) target_model <span class="cf">else</span> current_model</span>
<span id="cb61-222"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-222" tabindex="-1"></a>        </span>
<span id="cb61-223"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-223" tabindex="-1"></a>        <span class="co"># Compute TD target</span></span>
<span id="cb61-224"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-224" tabindex="-1"></a>        <span class="cf">if</span> (exp<span class="sc">$</span>s_prime <span class="sc">==</span> terminal_state) {</span>
<span id="cb61-225"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-225" tabindex="-1"></a>          target_q <span class="ot">&lt;-</span> exp<span class="sc">$</span>r</span>
<span id="cb61-226"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-226" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb61-227"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-227" tabindex="-1"></a>          <span class="co"># Get max Q-value from target network</span></span>
<span id="cb61-228"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-228" tabindex="-1"></a>          q_next <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a_next) {</span>
<span id="cb61-229"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-229" tabindex="-1"></a>            x_next <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(exp<span class="sc">$</span>s_prime, a_next, n_states, n_actions)</span>
<span id="cb61-230"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-230" tabindex="-1"></a>            <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(model_to_use)) {</span>
<span id="cb61-231"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-231" tabindex="-1"></a>              <span class="fu">predict</span>(model_to_use, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x_next)))</span>
<span id="cb61-232"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-232" tabindex="-1"></a>            } <span class="cf">else</span> {</span>
<span id="cb61-233"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-233" tabindex="-1"></a>              <span class="dv">0</span></span>
<span id="cb61-234"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-234" tabindex="-1"></a>            }</span>
<span id="cb61-235"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-235" tabindex="-1"></a>          })</span>
<span id="cb61-236"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-236" tabindex="-1"></a>          target_q <span class="ot">&lt;-</span> exp<span class="sc">$</span>r <span class="sc">+</span> gamma <span class="sc">*</span> <span class="fu">max</span>(q_next)</span>
<span id="cb61-237"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-237" tabindex="-1"></a>        }</span>
<span id="cb61-238"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-238" tabindex="-1"></a>        </span>
<span id="cb61-239"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-239" tabindex="-1"></a>        train_x[i, ] <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(exp<span class="sc">$</span>s, exp<span class="sc">$</span>a, n_states, n_actions)</span>
<span id="cb61-240"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-240" tabindex="-1"></a>        train_y[i] <span class="ot">&lt;-</span> target_q</span>
<span id="cb61-241"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-241" tabindex="-1"></a>      }</span>
<span id="cb61-242"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-242" tabindex="-1"></a>      </span>
<span id="cb61-243"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-243" tabindex="-1"></a>      <span class="co"># Train Random Forest</span></span>
<span id="cb61-244"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-244" tabindex="-1"></a>      current_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(</span>
<span id="cb61-245"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-245" tabindex="-1"></a>        <span class="at">x =</span> <span class="fu">as.data.frame</span>(train_x),</span>
<span id="cb61-246"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-246" tabindex="-1"></a>        <span class="at">y =</span> train_y,</span>
<span id="cb61-247"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-247" tabindex="-1"></a>        <span class="at">ntree =</span> <span class="dv">50</span>,  <span class="co"># Reduced for efficiency</span></span>
<span id="cb61-248"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-248" tabindex="-1"></a>        <span class="at">nodesize =</span> <span class="dv">10</span>,</span>
<span id="cb61-249"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-249" tabindex="-1"></a>        <span class="at">mtry =</span> <span class="fu">max</span>(<span class="dv">1</span>, <span class="fu">floor</span>(n_features <span class="sc">/</span> <span class="dv">3</span>)),</span>
<span id="cb61-250"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-250" tabindex="-1"></a>        <span class="at">keep.forest =</span> <span class="cn">TRUE</span></span>
<span id="cb61-251"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-251" tabindex="-1"></a>      )</span>
<span id="cb61-252"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-252" tabindex="-1"></a>      </span>
<span id="cb61-253"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-253" tabindex="-1"></a>      model_updates <span class="ot">&lt;-</span> model_updates <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb61-254"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-254" tabindex="-1"></a>      </span>
<span id="cb61-255"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-255" tabindex="-1"></a>      <span class="co"># Update target model periodically</span></span>
<span id="cb61-256"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-256" tabindex="-1"></a>      <span class="cf">if</span> (model_updates <span class="sc">%%</span> (update_target_freq <span class="sc">/</span> retrain_freq) <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb61-257"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-257" tabindex="-1"></a>        target_model <span class="ot">&lt;-</span> current_model</span>
<span id="cb61-258"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-258" tabindex="-1"></a>      }</span>
<span id="cb61-259"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-259" tabindex="-1"></a>      </span>
<span id="cb61-260"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-260" tabindex="-1"></a>      <span class="co"># Track TD error</span></span>
<span id="cb61-261"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-261" tabindex="-1"></a>      predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(current_model, <span class="fu">as.data.frame</span>(train_x))</span>
<span id="cb61-262"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-262" tabindex="-1"></a>      td_errors[ep] <span class="ot">&lt;-</span> <span class="fu">mean</span>(<span class="fu">abs</span>(predictions <span class="sc">-</span> train_y))</span>
<span id="cb61-263"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-263" tabindex="-1"></a>    }</span>
<span id="cb61-264"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-264" tabindex="-1"></a>  }</span>
<span id="cb61-265"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-265" tabindex="-1"></a>  </span>
<span id="cb61-266"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-266" tabindex="-1"></a>  <span class="co"># Extract final policy</span></span>
<span id="cb61-267"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-267" tabindex="-1"></a>  policy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), <span class="cf">function</span>(s) {</span>
<span id="cb61-268"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-268" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(current_model)) {</span>
<span id="cb61-269"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-269" tabindex="-1"></a>      q_vals <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb61-270"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-270" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb61-271"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-271" tabindex="-1"></a>        <span class="fu">predict</span>(current_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x)))</span>
<span id="cb61-272"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-272" tabindex="-1"></a>      })</span>
<span id="cb61-273"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-273" tabindex="-1"></a>      <span class="fu">which.max</span>(q_vals)</span>
<span id="cb61-274"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-274" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb61-275"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-275" tabindex="-1"></a>      <span class="dv">1</span></span>
<span id="cb61-276"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-276" tabindex="-1"></a>    }</span>
<span id="cb61-277"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-277" tabindex="-1"></a>  })</span>
<span id="cb61-278"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-278" tabindex="-1"></a>  </span>
<span id="cb61-279"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-279" tabindex="-1"></a>  <span class="fu">list</span>(</span>
<span id="cb61-280"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-280" tabindex="-1"></a>    <span class="at">model =</span> current_model,</span>
<span id="cb61-281"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-281" tabindex="-1"></a>    <span class="at">policy =</span> <span class="fu">c</span>(policy, <span class="cn">NA</span>),</span>
<span id="cb61-282"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-282" tabindex="-1"></a>    <span class="at">rewards =</span> rewards_history,</span>
<span id="cb61-283"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-283" tabindex="-1"></a>    <span class="at">td_errors =</span> td_errors,</span>
<span id="cb61-284"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-284" tabindex="-1"></a>    <span class="at">buffer =</span> replay_buffer</span>
<span id="cb61-285"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-285" tabindex="-1"></a>  )</span>
<span id="cb61-286"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-286" tabindex="-1"></a>}</span>
<span id="cb61-287"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-287" tabindex="-1"></a></span>
<span id="cb61-288"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-288" tabindex="-1"></a><span class="co"># Run both algorithms</span></span>
<span id="cb61-289"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-289" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Running Tabular Q-Learning...</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-290"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-290" tabindex="-1"></a>tabular_result <span class="ot">&lt;-</span> <span class="fu">q_learning_tabular</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>)</span>
<span id="cb61-291"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-291" tabindex="-1"></a></span>
<span id="cb61-292"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-292" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Running Q-Learning with Random Forest...</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-293"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-293" tabindex="-1"></a>rf_result <span class="ot">&lt;-</span> <span class="fu">q_learning_rf</span>(</span>
<span id="cb61-294"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-294" tabindex="-1"></a>  <span class="at">episodes =</span> <span class="dv">1000</span>, </span>
<span id="cb61-295"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-295" tabindex="-1"></a>  <span class="at">epsilon =</span> <span class="fl">0.1</span>, </span>
<span id="cb61-296"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-296" tabindex="-1"></a>  <span class="at">buffer_capacity =</span> <span class="dv">5000</span>,</span>
<span id="cb61-297"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-297" tabindex="-1"></a>  <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb61-298"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-298" tabindex="-1"></a>  <span class="at">retrain_freq =</span> <span class="dv">50</span>,</span>
<span id="cb61-299"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-299" tabindex="-1"></a>  <span class="at">update_target_freq =</span> <span class="dv">100</span></span>
<span id="cb61-300"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-300" tabindex="-1"></a>)</span>
<span id="cb61-301"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-301" tabindex="-1"></a></span>
<span id="cb61-302"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-302" tabindex="-1"></a><span class="co"># Compute moving average (properly)</span></span>
<span id="cb61-303"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-303" tabindex="-1"></a>moving_average <span class="ot">&lt;-</span> <span class="cf">function</span>(x, <span class="at">window =</span> <span class="dv">50</span>) {</span>
<span id="cb61-304"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-304" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb61-305"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-305" tabindex="-1"></a>  result <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n)</span>
<span id="cb61-306"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-306" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb61-307"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-307" tabindex="-1"></a>    start_idx <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="dv">1</span>, i <span class="sc">-</span> window <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb61-308"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-308" tabindex="-1"></a>    result[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(x[start_idx<span class="sc">:</span>i])</span>
<span id="cb61-309"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-309" tabindex="-1"></a>  }</span>
<span id="cb61-310"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-310" tabindex="-1"></a>  result</span>
<span id="cb61-311"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-311" tabindex="-1"></a>}</span>
<span id="cb61-312"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-312" tabindex="-1"></a></span>
<span id="cb61-313"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-313" tabindex="-1"></a>tabular_smooth <span class="ot">&lt;-</span> <span class="fu">moving_average</span>(tabular_result<span class="sc">$</span>rewards, <span class="dv">50</span>)</span>
<span id="cb61-314"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-314" tabindex="-1"></a>rf_smooth <span class="ot">&lt;-</span> <span class="fu">moving_average</span>(rf_result<span class="sc">$</span>rewards, <span class="dv">50</span>)</span>
<span id="cb61-315"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-315" tabindex="-1"></a></span>
<span id="cb61-316"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-316" tabindex="-1"></a><span class="co"># Create comparison dataframe</span></span>
<span id="cb61-317"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-317" tabindex="-1"></a>reward_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb61-318"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-318" tabindex="-1"></a>  <span class="at">Episode =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>, <span class="dv">2</span>),</span>
<span id="cb61-319"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-319" tabindex="-1"></a>  <span class="at">Reward =</span> <span class="fu">c</span>(tabular_smooth, rf_smooth),</span>
<span id="cb61-320"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-320" tabindex="-1"></a>  <span class="at">Algorithm =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Tabular Q-Learning&quot;</span>, <span class="st">&quot;Q-Learning + RF&quot;</span>), <span class="at">each =</span> <span class="dv">1000</span>)</span>
<span id="cb61-321"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-321" tabindex="-1"></a>)</span>
<span id="cb61-322"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-322" tabindex="-1"></a></span>
<span id="cb61-323"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-323" tabindex="-1"></a><span class="co"># Plot 1: Learning curves comparison</span></span>
<span id="cb61-324"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-324" tabindex="-1"></a>reward_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(reward_df, <span class="fu">aes</span>(<span class="at">x =</span> Episode, <span class="at">y =</span> Reward, <span class="at">color =</span> Algorithm)) <span class="sc">+</span></span>
<span id="cb61-325"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-325" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb61-326"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-326" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;Tabular Q-Learning&quot;</span> <span class="ot">=</span> <span class="st">&quot;blue&quot;</span>, </span>
<span id="cb61-327"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-327" tabindex="-1"></a>                                 <span class="st">&quot;Q-Learning + RF&quot;</span> <span class="ot">=</span> <span class="st">&quot;forestgreen&quot;</span>)) <span class="sc">+</span></span>
<span id="cb61-328"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-328" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb61-329"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-329" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb61-330"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-330" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Learning Curves Comparison (50-episode moving average)&quot;</span>,</span>
<span id="cb61-331"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-331" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Episode&quot;</span>,</span>
<span id="cb61-332"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-332" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Average Reward&quot;</span></span>
<span id="cb61-333"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-333" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb61-334"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-334" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb61-335"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-335" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb61-336"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-336" tabindex="-1"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb61-337"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-337" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span></span>
<span id="cb61-338"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-338" tabindex="-1"></a>  )</span>
<span id="cb61-339"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-339" tabindex="-1"></a></span>
<span id="cb61-340"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-340" tabindex="-1"></a><span class="co"># Plot 2: Policy comparison</span></span>
<span id="cb61-341"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-341" tabindex="-1"></a>policy_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb61-342"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-342" tabindex="-1"></a>  <span class="at">State =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), <span class="dv">2</span>),</span>
<span id="cb61-343"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-343" tabindex="-1"></a>  <span class="at">Action =</span> <span class="fu">c</span>(tabular_result<span class="sc">$</span>policy[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)], </span>
<span id="cb61-344"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-344" tabindex="-1"></a>             rf_result<span class="sc">$</span>policy[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)]),</span>
<span id="cb61-345"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-345" tabindex="-1"></a>  <span class="at">Algorithm =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Tabular Q-Learning&quot;</span>, <span class="st">&quot;Q-Learning + RF&quot;</span>), </span>
<span id="cb61-346"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-346" tabindex="-1"></a>                  <span class="at">each =</span> n_states<span class="dv">-1</span>)</span>
<span id="cb61-347"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-347" tabindex="-1"></a>)</span>
<span id="cb61-348"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-348" tabindex="-1"></a></span>
<span id="cb61-349"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-349" tabindex="-1"></a>policy_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(policy_df, <span class="fu">aes</span>(<span class="at">x =</span> State, <span class="at">y =</span> Action, <span class="at">color =</span> Algorithm, <span class="at">shape =</span> Algorithm)) <span class="sc">+</span></span>
<span id="cb61-350"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-350" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb61-351"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-351" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="dv">1</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb61-352"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-352" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;Tabular Q-Learning&quot;</span> <span class="ot">=</span> <span class="st">&quot;blue&quot;</span>, </span>
<span id="cb61-353"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-353" tabindex="-1"></a>                                 <span class="st">&quot;Q-Learning + RF&quot;</span> <span class="ot">=</span> <span class="st">&quot;forestgreen&quot;</span>)) <span class="sc">+</span></span>
<span id="cb61-354"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-354" tabindex="-1"></a>  <span class="fu">scale_shape_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="dv">16</span>, <span class="dv">17</span>)) <span class="sc">+</span></span>
<span id="cb61-355"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-355" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Action 1&quot;</span>, <span class="st">&quot;Action 2&quot;</span>), </span>
<span id="cb61-356"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-356" tabindex="-1"></a>                     <span class="at">limits =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">2.5</span>)) <span class="sc">+</span></span>
<span id="cb61-357"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-357" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)) <span class="sc">+</span></span>
<span id="cb61-358"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-358" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb61-359"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-359" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb61-360"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-360" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Learned Policies Comparison&quot;</span>,</span>
<span id="cb61-361"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-361" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;State&quot;</span>,</span>
<span id="cb61-362"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-362" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Action&quot;</span></span>
<span id="cb61-363"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-363" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb61-364"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-364" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb61-365"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-365" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb61-366"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-366" tabindex="-1"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb61-367"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-367" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span></span>
<span id="cb61-368"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-368" tabindex="-1"></a>  )</span>
<span id="cb61-369"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-369" tabindex="-1"></a></span>
<span id="cb61-370"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-370" tabindex="-1"></a><span class="co"># Display plots</span></span>
<span id="cb61-371"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-371" tabindex="-1"></a><span class="fu">print</span>(reward_plot)</span>
<span id="cb61-372"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-372" tabindex="-1"></a><span class="fu">print</span>(policy_plot)</span>
<span id="cb61-373"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-373" tabindex="-1"></a></span>
<span id="cb61-374"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-374" tabindex="-1"></a><span class="co"># Feature importance (if model exists)</span></span>
<span id="cb61-375"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-375" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_result<span class="sc">$</span>model)) {</span>
<span id="cb61-376"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-376" tabindex="-1"></a>  importance_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb61-377"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-377" tabindex="-1"></a>    <span class="at">Feature =</span> <span class="fu">c</span>(<span class="fu">paste0</span>(<span class="st">&quot;S&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_states), <span class="fu">paste0</span>(<span class="st">&quot;A&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_actions)),</span>
<span id="cb61-378"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-378" tabindex="-1"></a>    <span class="at">Importance =</span> <span class="fu">importance</span>(rf_result<span class="sc">$</span>model)[, <span class="dv">1</span>]</span>
<span id="cb61-379"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-379" tabindex="-1"></a>  )</span>
<span id="cb61-380"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-380" tabindex="-1"></a>  </span>
<span id="cb61-381"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-381" tabindex="-1"></a>  importance_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(importance_df, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Feature, Importance), <span class="at">y =</span> Importance)) <span class="sc">+</span></span>
<span id="cb61-382"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-382" tabindex="-1"></a>    <span class="fu">geom_col</span>(<span class="at">fill =</span> <span class="st">&quot;forestgreen&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb61-383"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-383" tabindex="-1"></a>    <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb61-384"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-384" tabindex="-1"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb61-385"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-385" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb61-386"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-386" tabindex="-1"></a>      <span class="at">title =</span> <span class="st">&quot;Feature Importance in Random Forest Q-Function&quot;</span>,</span>
<span id="cb61-387"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-387" tabindex="-1"></a>      <span class="at">x =</span> <span class="st">&quot;Feature&quot;</span>,</span>
<span id="cb61-388"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-388" tabindex="-1"></a>      <span class="at">y =</span> <span class="st">&quot;Importance (IncNodePurity)&quot;</span></span>
<span id="cb61-389"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-389" tabindex="-1"></a>    ) <span class="sc">+</span></span>
<span id="cb61-390"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-390" tabindex="-1"></a>    <span class="fu">theme</span>(</span>
<span id="cb61-391"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-391" tabindex="-1"></a>      <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb61-392"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-392" tabindex="-1"></a>      <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>)</span>
<span id="cb61-393"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-393" tabindex="-1"></a>    )</span>
<span id="cb61-394"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-394" tabindex="-1"></a>  </span>
<span id="cb61-395"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-395" tabindex="-1"></a>  <span class="fu">print</span>(importance_plot)</span>
<span id="cb61-396"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-396" tabindex="-1"></a>}</span>
<span id="cb61-397"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-397" tabindex="-1"></a></span>
<span id="cb61-398"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-398" tabindex="-1"></a><span class="co"># Summary statistics</span></span>
<span id="cb61-399"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-399" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">=== Summary Statistics ===</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-400"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-400" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Tabular Q-Learning:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-401"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-401" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;  Final 100-episode avg reward:&quot;</span>, <span class="fu">mean</span>(<span class="fu">tail</span>(tabular_result<span class="sc">$</span>rewards, <span class="dv">100</span>)), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-402"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-402" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;  Best policy reward:&quot;</span>, <span class="fu">max</span>(tabular_smooth), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-403"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-403" tabindex="-1"></a></span>
<span id="cb61-404"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-404" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Q-Learning with Random Forest:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-405"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-405" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;  Final 100-episode avg reward:&quot;</span>, <span class="fu">mean</span>(<span class="fu">tail</span>(rf_result<span class="sc">$</span>rewards, <span class="dv">100</span>)), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-406"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-406" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;  Best policy reward:&quot;</span>, <span class="fu">max</span>(rf_smooth), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-407"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-407" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;  Experience buffer size:&quot;</span>, rf_result<span class="sc">$</span>buffer<span class="sc">$</span><span class="fu">size</span>(), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-408"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-408" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_result<span class="sc">$</span>model)) {</span>
<span id="cb61-409"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-409" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;  RF model trees:&quot;</span>, rf_result<span class="sc">$</span>model<span class="sc">$</span>ntree, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-410"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-410" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;  RF OOB MSE:&quot;</span>, <span class="fu">mean</span>(rf_result<span class="sc">$</span>model<span class="sc">$</span>mse), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb61-411"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-411" tabindex="-1"></a>}</span>
<span id="cb61-412"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-412" tabindex="-1"></a></span>
<span id="cb61-413"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-413" tabindex="-1"></a><span class="co"># Policy agreement</span></span>
<span id="cb61-414"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-414" tabindex="-1"></a>policy_agreement <span class="ot">&lt;-</span> <span class="fu">sum</span>(tabular_result<span class="sc">$</span>policy[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)] <span class="sc">==</span> </span>
<span id="cb61-415"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-415" tabindex="-1"></a>                        rf_result<span class="sc">$</span>policy[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)]) <span class="sc">/</span> (n_states<span class="dv">-1</span>)</span>
<span id="cb61-416"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-416" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Policy agreement:&quot;</span>, <span class="fu">round</span>(policy_agreement <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">1</span>), <span class="st">&quot;%</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<p><strong>Environment Setup</strong></p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb62-1" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb62-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb62-2" tabindex="-1"></a>n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb62-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb62-3" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb62-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb62-4" tabindex="-1"></a>terminal_state <span class="ot">&lt;-</span> n_states</span></code></pre></div>
<p>This chunk defines the basic parameters of the reinforcement learning problem.</p>
<ul>
<li><code>n_states &lt;- 10</code>: The âworldâ or âgameâ has 10 distinct states.</li>
<li><code>n_actions &lt;- 2</code>: In any state, the agent can choose between 2 possible actions.</li>
<li><code>gamma &lt;- 0.9</code>: This is the <strong>discount factor</strong>. It determines how much the agent values future rewards. A value of 0.9 means a reward received in the next step is worth 90% of a reward received now.</li>
<li><code>terminal_state &lt;- n_states</code>: State 10 is designated as the âendâ state. When the agent reaches this state, the âepisodeâ (one attempt) is over.</li>
</ul>
<p><strong>Environment Model Definition</strong></p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-1" tabindex="-1"></a>transition_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb63-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-2" tabindex="-1"></a>reward_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb63-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-3" tabindex="-1"></a></span>
<span id="cb63-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-4" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>)) {</span>
<span id="cb63-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-5" tabindex="-1"></a>  <span class="cf">for</span> (a <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_actions) {</span>
<span id="cb63-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-6" tabindex="-1"></a>    <span class="cf">if</span> (a <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb63-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-7" tabindex="-1"></a>      next_state <span class="ot">&lt;-</span> <span class="fu">min</span>(s <span class="sc">+</span> <span class="dv">1</span>, n_states <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb63-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-8" tabindex="-1"></a>      random_state <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">setdiff</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), next_state), <span class="dv">1</span>)</span>
<span id="cb63-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-9" tabindex="-1"></a>      transition_model[s, <span class="dv">1</span>, next_state] <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb63-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-10" tabindex="-1"></a>      transition_model[s, <span class="dv">1</span>, random_state] <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb63-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-11" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb63-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-12" tabindex="-1"></a>      random_states <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), <span class="dv">2</span>, <span class="at">replace =</span> <span class="cn">FALSE</span>)</span>
<span id="cb63-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-13" tabindex="-1"></a>      transition_model[s, <span class="dv">2</span>, random_states[<span class="dv">1</span>]] <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb63-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-14" tabindex="-1"></a>      transition_model[s, <span class="dv">2</span>, random_states[<span class="dv">2</span>]] <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb63-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-15" tabindex="-1"></a>    }</span>
<span id="cb63-16"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-16" tabindex="-1"></a>    <span class="co"># ... reward assignment ...</span></span>
<span id="cb63-17"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-17" tabindex="-1"></a>  }</span>
<span id="cb63-18"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb63-18" tabindex="-1"></a>}</span></code></pre></div>
<p>This chunk builds the ârulesâ of the environment with <strong>critical fixes</strong>:</p>
<ul>
<li><strong>Fixed Probability Distribution</strong>:
<ul>
<li><strong>Action 1</strong>: 90% chance to move to the next state, 10% to a <em>different</em> random non-terminal state (using <code>setdiff</code> to prevent overwriting)</li>
<li><strong>Action 2</strong>: 80% to one random state, 20% to another <em>different</em> random state (using <code>replace = FALSE</code>)</li>
<li>This ensures probabilities <strong>always sum to exactly 1.0</strong> for each (state, action) pair</li>
</ul></li>
<li><strong>No Invalid Terminal Transitions</strong>: Random states are sampled from <code>1:(n_states-1)</code>, excluding the terminal state</li>
<li><strong>Verification</strong>: The code includes a check that prints warnings if any probabilities donât sum to 1.0</li>
</ul>
<p><strong>Sampling Function</strong></p>
<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb64-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb64-1" tabindex="-1"></a>sample_env <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a) {</span>
<span id="cb64-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb64-2" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> transition_model[s, a, ]</span>
<span id="cb64-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb64-3" tabindex="-1"></a>  s_prime <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>, <span class="at">prob =</span> probs)</span>
<span id="cb64-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb64-4" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> reward_model[s, a, s_prime]</span>
<span id="cb64-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb64-5" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">s_prime =</span> s_prime, <span class="at">reward =</span> reward)</span>
<span id="cb64-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb64-6" tabindex="-1"></a>}</span></code></pre></div>
<p>This function acts as the <strong>simulator</strong>. The Q-learning agent doesnât get to see the full <code>transition_model</code>. Instead, it gives a state (<code>s</code>) and action (<code>a</code>) to this function, which:</p>
<ol style="list-style-type: decimal">
<li>Looks up the probabilities for all possible next states</li>
<li>Samples one next state based on those probabilities</li>
<li>Returns the next state and corresponding reward</li>
</ol>
<p><strong>Feature Encoding</strong></p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb65-1" tabindex="-1"></a>encode_features <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a, n_states, n_actions) {</span>
<span id="cb65-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb65-2" tabindex="-1"></a>  state_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb65-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb65-3" tabindex="-1"></a>  action_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_actions)</span>
<span id="cb65-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb65-4" tabindex="-1"></a>  state_vec[s] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb65-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb65-5" tabindex="-1"></a>  action_vec[a] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb65-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb65-6" tabindex="-1"></a>  <span class="fu">c</span>(state_vec, action_vec)</span>
<span id="cb65-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb65-7" tabindex="-1"></a>}</span></code></pre></div>
<p>This performs <strong>one-hot encoding</strong> to convert states and actions into numerical features:</p>
<ul>
<li>Creates a vector of 10 zeros, puts a 1 at position <code>s</code> (e.g., state 3 â <code>[0,0,1,0,0,0,0,0,0,0]</code>)</li>
<li>Creates a vector of 2 zeros, puts a 1 at position <code>a</code> (e.g., action 2 â <code>[0,1]</code>)</li>
<li>Combines them into a 12-dimensional feature vector for the Random Forest</li>
</ul>
<p><strong>Experience Replay Buffer</strong></p>
<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb66-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb66-1" tabindex="-1"></a>ExperienceBuffer <span class="ot">&lt;-</span> <span class="fu">setRefClass</span>(</span>
<span id="cb66-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb66-2" tabindex="-1"></a>  <span class="st">&quot;ExperienceBuffer&quot;</span>,</span>
<span id="cb66-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb66-3" tabindex="-1"></a>  <span class="at">fields =</span> <span class="fu">list</span>(<span class="at">capacity =</span> <span class="st">&quot;numeric&quot;</span>, <span class="at">buffer =</span> <span class="st">&quot;list&quot;</span>, <span class="at">position =</span> <span class="st">&quot;numeric&quot;</span>),</span>
<span id="cb66-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb66-4" tabindex="-1"></a>  <span class="at">methods =</span> <span class="fu">list</span>(</span>
<span id="cb66-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb66-5" tabindex="-1"></a>    <span class="at">add =</span> <span class="cf">function</span>(experience) { ... },</span>
<span id="cb66-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb66-6" tabindex="-1"></a>    <span class="at">sample =</span> <span class="cf">function</span>(batch_size) { ... },</span>
<span id="cb66-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb66-7" tabindex="-1"></a>    <span class="at">size =</span> <span class="cf">function</span>() { ... }</span>
<span id="cb66-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb66-8" tabindex="-1"></a>  )</span>
<span id="cb66-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb66-9" tabindex="-1"></a>)</span></code></pre></div>
<ul>
<li><strong>Circular Buffer</strong>: Stores up to 5000 experiences (state, action, reward, next_state)</li>
<li><strong>Prevents Catastrophic Forgetting</strong>: By sampling randomly from past experiences rather than learning only from the most recent ones</li>
<li><strong>Breaks Temporal Correlation</strong>: Random sampling decorrelates sequential experiences, improving learning stability</li>
<li><strong>Methods</strong>:
<ul>
<li><code>add()</code>: Stores a new experience, overwriting old ones when full</li>
<li><code>sample()</code>: Returns a random batch of experiences for training</li>
<li><code>size()</code>: Returns current number of stored experiences</li>
</ul></li>
</ul>
<p><strong>Tabular Q-Learning Baseline</strong></p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-1" tabindex="-1"></a>q_learning_tabular <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>) {</span>
<span id="cb67-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-2" tabindex="-1"></a>  Q <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> n_states, <span class="at">ncol =</span> n_actions)</span>
<span id="cb67-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-3" tabindex="-1"></a>  <span class="co"># ... standard Q-learning update ...</span></span>
<span id="cb67-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-4" tabindex="-1"></a>  Q[s, a] <span class="ot">&lt;-</span> Q[s, a] <span class="sc">+</span> alpha <span class="sc">*</span> (td_target <span class="sc">-</span> Q[s, a])</span>
<span id="cb67-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-5" tabindex="-1"></a>  <span class="co"># ...</span></span>
<span id="cb67-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-6" tabindex="-1"></a>}</span></code></pre></div>
<p>This implements <strong>standard tabular Q-learning</strong> as a baseline:</p>
<ul>
<li>Maintains a 10Ã2 table of Q-values</li>
<li>Uses the classical update rule: Q(s,a) â Q(s,a) + Î±[r + Î³ max Q(sâ,aâ) - Q(s,a)]</li>
<li>Provides a <strong>gold standard</strong> to compare against the Random Forest approximation</li>
<li>Should converge to the optimal policy on this small problem</li>
</ul>
<p><strong>Q-Learning with Random Forest</strong></p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb68-1" tabindex="-1"></a>q_learning_rf <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>, </span>
<span id="cb68-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb68-2" tabindex="-1"></a>                         <span class="at">buffer_capacity =</span> <span class="dv">5000</span>, <span class="at">batch_size =</span> <span class="dv">128</span>,</span>
<span id="cb68-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb68-3" tabindex="-1"></a>                         <span class="at">retrain_freq =</span> <span class="dv">50</span>, <span class="at">update_target_freq =</span> <span class="dv">100</span>) {</span></code></pre></div>
<p>The main function now has <strong>improved hyperparameters</strong>:</p>
<ul>
<li><code>buffer_capacity = 5000</code>: Size of experience replay buffer</li>
<li><code>batch_size = 128</code>: Number of experiences to sample for each training update</li>
<li><code>retrain_freq = 50</code>: Retrain model every 50 episodes (less frequent, more stable)</li>
<li><code>update_target_freq = 100</code>: Update target network every 100 episodes</li>
</ul>
<p><strong>Initialization</strong></p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-1" tabindex="-1"></a>  replay_buffer <span class="ot">&lt;-</span> ExperienceBuffer<span class="sc">$</span><span class="fu">new</span>(buffer_capacity)</span>
<span id="cb69-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-2" tabindex="-1"></a>  current_model <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb69-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-3" tabindex="-1"></a>  target_model <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb69-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-4" tabindex="-1"></a>  rewards_history <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb69-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-5" tabindex="-1"></a>  td_errors <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span></code></pre></div>
<ul>
<li><code>replay_buffer</code>: Stores experiences for replay</li>
<li><code>current_model</code>: The Q-function being actively trained</li>
<li><code>target_model</code>: A frozen copy used for computing TD targets (DQN-style)</li>
<li><code>td_errors</code>: Tracks learning progress by monitoring prediction errors</li>
</ul>
<p><strong>Episode Loop</strong></p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-1" tabindex="-1"></a><span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb70-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-2" tabindex="-1"></a>  s <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>), <span class="dv">1</span>)</span>
<span id="cb70-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-3" tabindex="-1"></a>  episode_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb70-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-4" tabindex="-1"></a>  </span>
<span id="cb70-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-5" tabindex="-1"></a>  <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state) {</span>
<span id="cb70-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-6" tabindex="-1"></a>    <span class="co"># ... one step ...</span></span>
<span id="cb70-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-7" tabindex="-1"></a>    replay_buffer<span class="sc">$</span><span class="fu">add</span>(<span class="fu">list</span>(<span class="at">s =</span> s, <span class="at">a =</span> a, <span class="at">r =</span> r, <span class="at">s_prime =</span> s_prime))</span>
<span id="cb70-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-8" tabindex="-1"></a>    s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb70-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-9" tabindex="-1"></a>  }</span></code></pre></div>
<p>For each episode:</p>
<ol style="list-style-type: decimal">
<li>Start in a random non-terminal state</li>
<li>Take steps until reaching the terminal state</li>
<li><strong>Store each experience</strong> in the replay buffer (not directly used for training)</li>
<li>Accumulate episode reward</li>
</ol>
<p><strong>Action Selection</strong></p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-1" tabindex="-1"></a>  q_values <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb71-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-2" tabindex="-1"></a>    x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb71-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-3" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(current_model)) {</span>
<span id="cb71-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-4" tabindex="-1"></a>      <span class="fu">predict</span>(current_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x)))</span>
<span id="cb71-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-5" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb71-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-6" tabindex="-1"></a>      <span class="dv">0</span></span>
<span id="cb71-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-7" tabindex="-1"></a>    }</span>
<span id="cb71-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-8" tabindex="-1"></a>  })</span>
<span id="cb71-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-9" tabindex="-1"></a>  </span>
<span id="cb71-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-10" tabindex="-1"></a>  a <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb71-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-11" tabindex="-1"></a>    <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb71-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-12" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb71-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-13" tabindex="-1"></a>    <span class="fu">which.max</span>(q_values)</span>
<span id="cb71-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb71-14" tabindex="-1"></a>  }</span></code></pre></div>
<p><strong>Epsilon-greedy action selection</strong>:</p>
<ul>
<li>Predict Q-values for all actions using the current model</li>
<li>With probability Îµ (10%), explore by choosing randomly</li>
<li>With probability 1-Îµ (90%), exploit by choosing the action with highest Q-value</li>
</ul>
<p><strong>Training with Experience Replay</strong></p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-1" tabindex="-1"></a><span class="cf">if</span> (replay_buffer<span class="sc">$</span><span class="fu">size</span>() <span class="sc">&gt;=</span> batch_size <span class="sc">&amp;&amp;</span> ep <span class="sc">%%</span> retrain_freq <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb72-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-2" tabindex="-1"></a>  batch <span class="ot">&lt;-</span> replay_buffer<span class="sc">$</span><span class="fu">sample</span>(batch_size)</span>
<span id="cb72-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-3" tabindex="-1"></a>  </span>
<span id="cb72-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-4" tabindex="-1"></a>  train_x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> batch_size, <span class="at">ncol =</span> n_features)</span>
<span id="cb72-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-5" tabindex="-1"></a>  train_y <span class="ot">&lt;-</span> <span class="fu">numeric</span>(batch_size)</span>
<span id="cb72-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-6" tabindex="-1"></a>  </span>
<span id="cb72-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-7" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>batch_size) {</span>
<span id="cb72-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-8" tabindex="-1"></a>    exp <span class="ot">&lt;-</span> batch[[i]]</span>
<span id="cb72-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-9" tabindex="-1"></a>    model_to_use <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(target_model)) target_model <span class="cf">else</span> current_model</span>
<span id="cb72-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-10" tabindex="-1"></a>    </span>
<span id="cb72-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-11" tabindex="-1"></a>    <span class="cf">if</span> (exp<span class="sc">$</span>s_prime <span class="sc">==</span> terminal_state) {</span>
<span id="cb72-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-12" tabindex="-1"></a>      target_q <span class="ot">&lt;-</span> exp<span class="sc">$</span>r</span>
<span id="cb72-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-13" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb72-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-14" tabindex="-1"></a>      q_next <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a_next) {</span>
<span id="cb72-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-15" tabindex="-1"></a>        x_next <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(exp<span class="sc">$</span>s_prime, a_next, n_states, n_actions)</span>
<span id="cb72-16"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-16" tabindex="-1"></a>        <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(model_to_use)) {</span>
<span id="cb72-17"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-17" tabindex="-1"></a>          <span class="fu">predict</span>(model_to_use, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x_next)))</span>
<span id="cb72-18"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-18" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb72-19"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-19" tabindex="-1"></a>          <span class="dv">0</span></span>
<span id="cb72-20"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-20" tabindex="-1"></a>        }</span>
<span id="cb72-21"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-21" tabindex="-1"></a>      })</span>
<span id="cb72-22"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-22" tabindex="-1"></a>      target_q <span class="ot">&lt;-</span> exp<span class="sc">$</span>r <span class="sc">+</span> gamma <span class="sc">*</span> <span class="fu">max</span>(q_next)</span>
<span id="cb72-23"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-23" tabindex="-1"></a>    }</span>
<span id="cb72-24"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-24" tabindex="-1"></a>    </span>
<span id="cb72-25"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-25" tabindex="-1"></a>    train_x[i, ] <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(exp<span class="sc">$</span>s, exp<span class="sc">$</span>a, n_states, n_actions)</span>
<span id="cb72-26"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-26" tabindex="-1"></a>    train_y[i] <span class="ot">&lt;-</span> target_q</span>
<span id="cb72-27"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-27" tabindex="-1"></a>  }</span>
<span id="cb72-28"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-28" tabindex="-1"></a>  </span>
<span id="cb72-29"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-29" tabindex="-1"></a>  current_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(...)</span>
<span id="cb72-30"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-30" tabindex="-1"></a>}</span></code></pre></div>
<p><strong>Random Forest Training</strong></p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb73-1" tabindex="-1"></a>current_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(</span>
<span id="cb73-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb73-2" tabindex="-1"></a>  <span class="at">x =</span> <span class="fu">as.data.frame</span>(train_x),</span>
<span id="cb73-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb73-3" tabindex="-1"></a>  <span class="at">y =</span> train_y,</span>
<span id="cb73-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb73-4" tabindex="-1"></a>  <span class="at">ntree =</span> <span class="dv">50</span>,          <span class="co"># Reduced from 100 for efficiency</span></span>
<span id="cb73-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb73-5" tabindex="-1"></a>  <span class="at">nodesize =</span> <span class="dv">10</span>,       <span class="co"># Increased from 5 to prevent overfitting</span></span>
<span id="cb73-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb73-6" tabindex="-1"></a>  <span class="at">mtry =</span> <span class="fu">max</span>(<span class="dv">1</span>, <span class="fu">floor</span>(n_features <span class="sc">/</span> <span class="dv">3</span>)),</span>
<span id="cb73-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb73-7" tabindex="-1"></a>  <span class="at">keep.forest =</span> <span class="cn">TRUE</span></span>
<span id="cb73-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb73-8" tabindex="-1"></a>)</span></code></pre></div>
<p><strong>Improved hyperparameters</strong>:</p>
<ul>
<li><code>ntree = 50</code>: Fewer trees (faster training, still accurate)</li>
<li><code>nodesize = 10</code>: Larger leaf nodes (less overfitting on small batches)</li>
<li>These settings balance accuracy with computational efficiency</li>
</ul>
<p><strong>Policy Extraction</strong></p>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-1" tabindex="-1"></a>policy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), <span class="cf">function</span>(s) {</span>
<span id="cb74-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-2" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(current_model)) {</span>
<span id="cb74-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-3" tabindex="-1"></a>    q_vals <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb74-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-4" tabindex="-1"></a>      x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb74-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-5" tabindex="-1"></a>      <span class="fu">predict</span>(current_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x)))</span>
<span id="cb74-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-6" tabindex="-1"></a>    })</span>
<span id="cb74-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-7" tabindex="-1"></a>    <span class="fu">which.max</span>(q_vals)</span>
<span id="cb74-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-8" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb74-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-9" tabindex="-1"></a>    <span class="dv">1</span></span>
<span id="cb74-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-10" tabindex="-1"></a>  }</span>
<span id="cb74-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-11" tabindex="-1"></a>})</span></code></pre></div>
<p>After all episodes, extract the final learned policy:</p>
<ul>
<li>For each state, predict Q-values for both actions</li>
<li>Select the action with highest Q-value</li>
<li>This creates a lookup table: âin state s, take action aâ</li>
</ul>
<p><strong>Visualization and Comparison</strong></p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb75-1" tabindex="-1"></a>moving_average <span class="ot">&lt;-</span> <span class="cf">function</span>(x, <span class="at">window =</span> <span class="dv">50</span>) {</span>
<span id="cb75-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb75-2" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">length</span>(x)</span>
<span id="cb75-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb75-3" tabindex="-1"></a>  result <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n)</span>
<span id="cb75-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb75-4" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n) {</span>
<span id="cb75-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb75-5" tabindex="-1"></a>    start_idx <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="dv">1</span>, i <span class="sc">-</span> window <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb75-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb75-6" tabindex="-1"></a>    result[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(x[start_idx<span class="sc">:</span>i])</span>
<span id="cb75-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb75-7" tabindex="-1"></a>  }</span>
<span id="cb75-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb75-8" tabindex="-1"></a>  result</span>
<span id="cb75-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb75-9" tabindex="-1"></a>}</span></code></pre></div>
<p><strong>Properly implements</strong> a moving average:</p>
<ul>
<li>For episode i, averages rewards from episodes [i-49, i]</li>
<li>Early episodes (i &lt; 50) use expanding windows</li>
<li>This smooths noisy reward data to show learning trends</li>
</ul>
<p><strong>Learning Curves Comparison</strong></p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb76-1" tabindex="-1"></a>reward_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb76-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb76-2" tabindex="-1"></a>  <span class="at">Episode =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>, <span class="dv">2</span>),</span>
<span id="cb76-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb76-3" tabindex="-1"></a>  <span class="at">Reward =</span> <span class="fu">c</span>(tabular_smooth, rf_smooth),</span>
<span id="cb76-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb76-4" tabindex="-1"></a>  <span class="at">Algorithm =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Tabular Q-Learning&quot;</span>, <span class="st">&quot;Q-Learning + RF&quot;</span>), <span class="at">each =</span> <span class="dv">1000</span>)</span>
<span id="cb76-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb76-5" tabindex="-1"></a>)</span>
<span id="cb76-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb76-6" tabindex="-1"></a></span>
<span id="cb76-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb76-7" tabindex="-1"></a>reward_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(reward_df, <span class="fu">aes</span>(<span class="at">x =</span> Episode, <span class="at">y =</span> Reward, <span class="at">color =</span> Algorithm)) <span class="sc">+</span></span>
<span id="cb76-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb76-8" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span> ...</span></code></pre></div>
<p>Creates a <strong>side-by-side comparison</strong> plot:</p>
<ul>
<li>Blue line: Tabular Q-learning (baseline)</li>
<li>Green line: Random Forest Q-learning</li>
<li>Shows which algorithm learns faster and reaches higher rewards</li>
<li>Allows visual assessment of whether function approximation helps or hurts</li>
</ul>
<p><strong>Policy Compariso</strong></p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb77-1" tabindex="-1"></a>policy_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(policy_df, <span class="fu">aes</span>(<span class="at">x =</span> State, <span class="at">y =</span> Action, </span>
<span id="cb77-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb77-2" tabindex="-1"></a>                                     <span class="at">color =</span> Algorithm, <span class="at">shape =</span> Algorithm)) <span class="sc">+</span></span>
<span id="cb77-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb77-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">4</span>) <span class="sc">+</span></span>
<span id="cb77-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb77-4" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="dv">1</span>, <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span> ...</span></code></pre></div>
<p>Visualizes the final policies learned by both algorithms:</p>
<ul>
<li>Shows which action (1 or 2) each algorithm chooses for each state</li>
<li>Points and lines make it easy to see agreements and disagreements</li>
<li>Ideal: both algorithms should converge to similar policies</li>
</ul>
<p><strong>Feature Importance</strong></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-1" tabindex="-1"></a>importance_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb78-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-2" tabindex="-1"></a>  <span class="at">Feature =</span> <span class="fu">c</span>(<span class="fu">paste0</span>(<span class="st">&quot;S&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_states), <span class="fu">paste0</span>(<span class="st">&quot;A&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_actions)),</span>
<span id="cb78-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-3" tabindex="-1"></a>  <span class="at">Importance =</span> <span class="fu">importance</span>(rf_result<span class="sc">$</span>model)[, <span class="dv">1</span>]</span>
<span id="cb78-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-4" tabindex="-1"></a>)</span>
<span id="cb78-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-5" tabindex="-1"></a></span>
<span id="cb78-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-6" tabindex="-1"></a>importance_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(importance_df, </span>
<span id="cb78-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-7" tabindex="-1"></a>                         <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Feature, Importance), <span class="at">y =</span> Importance)) <span class="sc">+</span></span>
<span id="cb78-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-8" tabindex="-1"></a>  <span class="fu">geom_col</span>(<span class="at">fill =</span> <span class="st">&quot;forestgreen&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb78-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-9" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span> ...</span></code></pre></div>
<p>Shows which features the Random Forest found most important:</p>
<ul>
<li>Higher bars = more important for predicting Q-values</li>
<li>Typically, states near the goal (State 9, State 10) will be most important</li>
<li>Action features show which action provides more informative signals</li>
<li>Provides interpretability: âwhat does the model care about?â</li>
</ul>
<p><strong>Summary Statistics</strong></p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">=== Summary Statistics ===</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb79-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Tabular Q-Learning:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb79-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-3" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;  Final 100-episode avg reward:&quot;</span>, <span class="fu">mean</span>(<span class="fu">tail</span>(tabular_result<span class="sc">$</span>rewards, <span class="dv">100</span>)), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb79-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-4" tabindex="-1"></a></span>
<span id="cb79-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-5" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Q-Learning with Random Forest:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb79-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-6" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;  Final 100-episode avg reward:&quot;</span>, <span class="fu">mean</span>(<span class="fu">tail</span>(rf_result<span class="sc">$</span>rewards, <span class="dv">100</span>)), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb79-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-7" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;  Experience buffer size:&quot;</span>, rf_result<span class="sc">$</span>buffer<span class="sc">$</span><span class="fu">size</span>(), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb79-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-8" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;  RF OOB MSE:&quot;</span>, <span class="fu">mean</span>(rf_result<span class="sc">$</span>model<span class="sc">$</span>mse), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb79-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-9" tabindex="-1"></a></span>
<span id="cb79-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-10" tabindex="-1"></a>policy_agreement <span class="ot">&lt;-</span> <span class="fu">sum</span>(tabular_result<span class="sc">$</span>policy[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)] <span class="sc">==</span> </span>
<span id="cb79-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-11" tabindex="-1"></a>                        rf_result<span class="sc">$</span>policy[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)]) <span class="sc">/</span> (n_states<span class="dv">-1</span>)</span>
<span id="cb79-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-12" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Policy agreement:&quot;</span>, <span class="fu">round</span>(policy_agreement <span class="sc">*</span> <span class="dv">100</span>, <span class="dv">1</span>), <span class="st">&quot;%</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<p>Provides quantitative comparison:</p>
<ul>
<li><strong>Final avg reward</strong>: Performance of each algorithm (higher is better)</li>
<li><strong>Buffer size</strong>: How many experiences were collected</li>
<li><strong>OOB MSE</strong>: Random Forestâs internal error estimate (lower is better)</li>
<li><strong>Policy agreement</strong>: Percentage of states where both algorithms chose the same action
<ul>
<li>100% = perfect agreement (both found the same optimal policy)</li>
<li>Lower values suggest one algorithm failed to converge or the problem has multiple optimal solutions</li>
</ul></li>
</ul>
</div>
<div id="analysis-and-insights" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Analysis and Insights<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random Forest function approximation exhibits several characteristics that distinguish it from linear methods. Trees can capture non-linear decision boundaries, enabling the model to learn state-action relationships that linear approaches cannot represent. The random feature sampling at each split performs automatic feature selection, focusing computational resources on the most informative variables. Ensemble averaging across multiple trees reduces overfitting and provides stable predictions across different training samples. Individual trees maintain interpretable decision paths that show how Q-values are estimated for specific state-action pairs.</p>
<p>The batch retraining approach creates distinct computational trade-offs that affect implementation decisions. Training frequency must balance responsiveness against computational cost, as more frequent updates improve adaptation but require additional processing time. Trees need sufficient data to learn meaningful patterns, which can slow initial learning compared to methods that update continuously. Memory requirements increase over time as training examples accumulate, requiring careful management of historical data.</p>
<p>Random Forest methods naturally generate feature importance measures that reveal which states and actions most influence Q-value predictions. This interpretability provides diagnostic capabilities for understanding learning issues and analyzing policy decisions. The feature ranking can guide state representation choices and help identify redundant or irrelevant variables in the problem formulation.</p>
<p>Random Forest function approximation occupies a position between simple linear models and neural networks in terms of complexity and capability. The method handles larger state spaces more effectively than tabular approaches while remaining computationally tractable. It captures non-linear patterns without requiring extensive feature engineering or domain expertise. The approach shows less sensitivity to hyperparameter choices compared to neural networks while maintaining stability across different problem instances. The inherent interpretability provides insights into the decision-making process that can be valuable for debugging and analysis.</p>
<p>Random Forest methods demonstrate several advantages and trade-offs when compared to linear function approximation. The tree-based approach excels at pattern recognition, learning state-action relationships that linear models cannot capture due to their representational limitations. However, initial learning proceeds more slowly as trees require sufficient data to construct meaningful decision boundaries. Computational costs are higher due to periodic retraining requirements, contrasting with the continuous gradient updates used in linear methods. Generalization performance tends to be superior, as ensemble averaging provides natural regularization that reduces overfitting tendencies.</p>
</div>
<div id="conclusion-4" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Conclusion<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random Forest function approximation extends linear methods by offering enhanced modeling flexibility while preserving interpretability characteristics. The approach performs particularly well in environments with non-linear state-action relationships and provides regularization through ensemble averaging.</p>
<p>Several key observations emerge from this analysis. Non-linear function approximation can capture patterns that linear models miss, enabling better policy learning in complex environments. Batch learning approaches require careful consideration of training frequency and sample requirements to balance performance with computational efficiency. Feature importance analysis provides insights into learned policies that can guide problem formulation and debugging efforts. Tree-based methods offer an interpretable alternative to neural network approaches while maintaining theoretical foundations.</p>
<p>This exploration demonstrates how ensemble methods can enhance reinforcement learning without abandoning the established principles of Q-Learning. Future work could investigate online tree learning algorithms that avoid batch retraining requirements, adaptive schedules that optimize training frequency based on performance metrics, or hybrid approaches that combine strengths from different function approximation methods.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="function-approximation-q-learning-with-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/edit/main/07-Q_FA_RF.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/blob/main/07-Q_FA_RF.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
