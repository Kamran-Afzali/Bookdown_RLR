<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R</title>
  <meta name="description" content="Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R" />
  
  
  

<meta name="author" content="Kamran Afzalui" />


<meta name="date" content="2025-08-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"/>
<link rel="next" href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Reinforcement Learning in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Understanding Reinforcement Learning: From Bandits to Policy Optimization</strong></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-multi-armed-bandit-the-simplest-case"><i class="fa fa-check"></i><b>1.2</b> The Multi-Armed Bandit: The Simplest Case</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#transition-to-markov-decision-processes"><i class="fa fa-check"></i><b>1.3</b> Transition to Markov Decision Processes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#comparing-reinforcement-learning-methods"><i class="fa fa-check"></i><b>1.4</b> Comparing Reinforcement Learning Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#dynamic-programming-model-based-learning"><i class="fa fa-check"></i><b>1.4.1</b> Dynamic Programming: Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#model-free-approaches-monte-carlo-and-td-learning"><i class="fa fa-check"></i><b>1.4.2</b> Model-Free Approaches: Monte Carlo and TD Learning</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#dyna-bridging-model-free-and-model-based-learning"><i class="fa fa-check"></i><b>1.4.3</b> Dyna: Bridging Model-Free and Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#q-learning-and-function-approximation"><i class="fa fa-check"></i><b>1.4.4</b> Q-Learning and Function Approximation</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#policy-gradient-and-actor-critic-methods"><i class="fa fa-check"></i><b>1.4.5</b> Policy Gradient and Actor-Critic Methods</a></li>
<li class="chapter" data-level="1.4.6" data-path="index.html"><a href="index.html#advanced-policy-optimization-techniques"><i class="fa fa-check"></i><b>1.4.6</b> Advanced Policy Optimization Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#further-directions"><i class="fa fa-check"></i><b>1.5</b> Further Directions</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html"><i class="fa fa-check"></i><b>2</b> The Multi-Armed Bandit Problem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#mathematical-formalism"><i class="fa fa-check"></i><b>2.2</b> Mathematical Formalism</a></li>
<li class="chapter" data-level="2.3" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm"><i class="fa fa-check"></i><b>2.3</b> Frequentist Approach: UCB1 Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1"><i class="fa fa-check"></i><b>2.3.1</b> R Code for UCB1</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling"><i class="fa fa-check"></i><b>2.4</b> Bayesian Approach: Thompson Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling"><i class="fa fa-check"></i><b>2.4.1</b> R Code for Thompson Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy"><i class="fa fa-check"></i><b>2.5</b> Epsilon-Greedy Strategy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy"><i class="fa fa-check"></i><b>2.5.1</b> R Code for Epsilon-Greedy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#summary-table"><i class="fa fa-check"></i><b>2.6</b> Summary Table</a></li>
<li class="chapter" data-level="2.7" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html"><i class="fa fa-check"></i><b>3</b> Markov Decision Processes and Dynamic Programming</a>
<ul>
<li class="chapter" data-level="3.1" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#constructing-the-mdp-in-r"><i class="fa fa-check"></i><b>3.2</b> Constructing the MDP in R</a></li>
<li class="chapter" data-level="3.3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#value-iteration-algorithm"><i class="fa fa-check"></i><b>3.3</b> Value Iteration Algorithm</a></li>
<li class="chapter" data-level="3.4" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#evaluation-and-interpretation"><i class="fa fa-check"></i><b>3.4</b> Evaluation and Interpretation</a></li>
<li class="chapter" data-level="3.5" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties-of-value-iteration"><i class="fa fa-check"></i><b>3.5</b> Theoretical Properties of Value Iteration</a></li>
<li class="chapter" data-level="3.6" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-1"><i class="fa fa-check"></i><b>3.6</b> Summary Table</a></li>
<li class="chapter" data-level="3.7" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><i class="fa fa-check"></i><b>4</b> Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#theoretical-background"><i class="fa fa-check"></i><b>4.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#temporal-difference-learning-q-learning"><i class="fa fa-check"></i><b>4.2.1</b> Temporal Difference Learning (Q-Learning)</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#monte-carlo-methods"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.2.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-1-defining-the-environment-in-r"><i class="fa fa-check"></i><b>4.2.3</b> Step 1: Defining the Environment in R</a></li>
<li class="chapter" data-level="4.2.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-2-q-learning-implementation-in-r"><i class="fa fa-check"></i><b>4.2.4</b> Step 2: Q-Learning Implementation in R</a></li>
<li class="chapter" data-level="4.2.5" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-3-monte-carlo-every-visit-implementation"><i class="fa fa-check"></i><b>4.2.5</b> Step 3: Monte Carlo Every-Visit Implementation</a></li>
<li class="chapter" data-level="4.2.6" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-4-simulating-outcome-devaluation"><i class="fa fa-check"></i><b>4.2.6</b> Step 4: Simulating Outcome Devaluation</a></li>
<li class="chapter" data-level="4.2.7" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-5-comparing-policies-before-and-after-devaluation"><i class="fa fa-check"></i><b>4.2.7</b> Step 5: Comparing Policies Before and After Devaluation</a></li>
<li class="chapter" data-level="4.2.8" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-6-visualizing-the-policies"><i class="fa fa-check"></i><b>4.2.8</b> Step 6: Visualizing the Policies</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#interpretation-and-discussion"><i class="fa fa-check"></i><b>4.3</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="4.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#conclusion-2"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><i class="fa fa-check"></i><b>5</b> On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-on-policy"><i class="fa fa-check"></i><b>5.2</b> SARSA (On-Policy)</a></li>
<li class="chapter" data-level="5.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-off-policy"><i class="fa fa-check"></i><b>5.3</b> Q-Learning (Off-Policy)</a></li>
<li class="chapter" data-level="5.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-with-importance-sampling"><i class="fa fa-check"></i><b>5.4</b> Off-Policy Monte Carlo with Importance Sampling</a></li>
<li class="chapter" data-level="5.5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#key-differences"><i class="fa fa-check"></i><b>5.5</b> Key Differences</a></li>
<li class="chapter" data-level="5.6" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#interpretation-and-discussion-1"><i class="fa fa-check"></i><b>5.6</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#conclusion-3"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
<li class="chapter" data-level="5.8" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#comparison-table"><i class="fa fa-check"></i><b>5.8</b> Comparison Table</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><i class="fa fa-check"></i><b>6</b> Function Approximation in Reinforcement Learning: Q-Learning with Linear Models in R</a>
<ul>
<li class="chapter" data-level="6.1" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#theoretical-background-1"><i class="fa fa-check"></i><b>6.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#q-learning-with-function-approximation"><i class="fa fa-check"></i><b>6.2.1</b> Q-Learning with Function Approximation</a></li>
<li class="chapter" data-level="6.2.2" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#comparison-with-tabular-q-learning"><i class="fa fa-check"></i><b>6.2.2</b> Comparison with Tabular Q-Learning</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html"><a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html#r-implementation"><i class="fa fa-check"></i><b>6.3</b> R Implementation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><i class="fa fa-check"></i><b>7</b> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#theoretical-background-2"><i class="fa fa-check"></i><b>7.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation"><i class="fa fa-check"></i><b>7.2.1</b> Q-Learning with Random Forest Approximation</a></li>
<li class="chapter" data-level="7.2.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-engineering-for-tree-based-models"><i class="fa fa-check"></i><b>7.2.2</b> Feature Engineering for Tree-Based Models</a></li>
<li class="chapter" data-level="7.2.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods"><i class="fa fa-check"></i><b>7.2.3</b> Comparison with Previous Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1"><i class="fa fa-check"></i><b>7.3</b> R Implementation</a></li>
<li class="chapter" data-level="7.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights"><i class="fa fa-check"></i><b>7.4</b> Analysis and Insights</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#policy-learning-characteristics"><i class="fa fa-check"></i><b>7.4.1</b> Policy Learning Characteristics</a></li>
<li class="chapter" data-level="7.4.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#computational-considerations"><i class="fa fa-check"></i><b>7.4.2</b> Computational Considerations</a></li>
<li class="chapter" data-level="7.4.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-importance-insights"><i class="fa fa-check"></i><b>7.4.3</b> Feature Importance Insights</a></li>
<li class="chapter" data-level="7.4.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#practical-implications-1"><i class="fa fa-check"></i><b>7.4.4</b> Practical Implications</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-linear-approximation"><i class="fa fa-check"></i><b>7.5</b> Comparison with Linear Approximation</a></li>
<li class="chapter" data-level="7.6" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4"><i class="fa fa-check"></i><b>7.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><i class="fa fa-check"></i><b>8</b> Deep Function Approximation: Q-Learning with Neural Networks in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation"><i class="fa fa-check"></i><b>8.2</b> Theoretical Foundation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity"><i class="fa fa-check"></i><b>8.2.1</b> Universal Approximation and Expressivity</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning"><i class="fa fa-check"></i><b>8.2.2</b> Gradient-Based Learning</a></li>
<li class="chapter" data-level="8.2.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches"><i class="fa fa-check"></i><b>8.2.3</b> Comparison with Previous Approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2"><i class="fa fa-check"></i><b>8.3</b> R Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation"><i class="fa fa-check"></i><b>8.4</b> Analysis and Interpretation</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#learning-dynamics"><i class="fa fa-check"></i><b>8.4.1</b> Learning Dynamics</a></li>
<li class="chapter" data-level="8.4.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#function-representation"><i class="fa fa-check"></i><b>8.4.2</b> Function Representation</a></li>
<li class="chapter" data-level="8.4.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#generalization-properties"><i class="fa fa-check"></i><b>8.4.3</b> Generalization Properties</a></li>
<li class="chapter" data-level="8.4.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#training-stability"><i class="fa fa-check"></i><b>8.4.4</b> Training Stability</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#practical-considerations"><i class="fa fa-check"></i><b>8.5</b> Practical Considerations</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#architecture-selection"><i class="fa fa-check"></i><b>8.5.1</b> Architecture Selection</a></li>
<li class="chapter" data-level="8.5.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#training-frequency"><i class="fa fa-check"></i><b>8.5.2</b> Training Frequency</a></li>
<li class="chapter" data-level="8.5.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#regularization"><i class="fa fa-check"></i><b>8.5.3</b> Regularization</a></li>
<li class="chapter" data-level="8.5.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#initialization-and-convergence"><i class="fa fa-check"></i><b>8.5.4</b> Initialization and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-across-function-approximation-methods"><i class="fa fa-check"></i><b>8.6</b> Comparison Across Function Approximation Methods</a></li>
<li class="chapter" data-level="8.7" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions"><i class="fa fa-check"></i><b>8.7</b> Future Directions</a></li>
<li class="chapter" data-level="8.8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5"><i class="fa fa-check"></i><b>8.8</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html"><i class="fa fa-check"></i><b>9</b> Dyna and DynaQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#theoretical-framework"><i class="fa fa-check"></i><b>9.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#the-dyna-architecture"><i class="fa fa-check"></i><b>9.2.1</b> The Dyna Architecture</a></li>
<li class="chapter" data-level="9.2.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#model-representation"><i class="fa fa-check"></i><b>9.2.2</b> Model Representation</a></li>
<li class="chapter" data-level="9.2.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#convergence-properties"><i class="fa fa-check"></i><b>9.2.3</b> Convergence Properties</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-in-r"><i class="fa fa-check"></i><b>9.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#environment-setup"><i class="fa fa-check"></i><b>9.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="9.3.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#dyna-q-implementation"><i class="fa fa-check"></i><b>9.3.2</b> Dyna-Q Implementation</a></li>
<li class="chapter" data-level="9.3.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#standard-q-learning-for-comparison"><i class="fa fa-check"></i><b>9.3.3</b> Standard Q-Learning for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#experimental-analysis"><i class="fa fa-check"></i><b>9.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#learning-efficiency-comparison"><i class="fa fa-check"></i><b>9.4.1</b> Learning Efficiency Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
<li class="chapter" data-level="9.6" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-considerations-and-conclusion"><i class="fa fa-check"></i><b>9.6</b> Implementation Considerations and Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><i class="fa fa-check"></i><b>10</b> Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#theoretical-framework-1"><i class="fa fa-check"></i><b>10.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#the-exploration-bonus-mechanism"><i class="fa fa-check"></i><b>10.2.1</b> The Exploration Bonus Mechanism</a></li>
<li class="chapter" data-level="10.2.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#complete-dyna-q-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> Complete Dyna-Q+ Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#convergence-and-stability"><i class="fa fa-check"></i><b>10.2.3</b> Convergence and Stability</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#implementation-in-r-1"><i class="fa fa-check"></i><b>10.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#environment-setup-1"><i class="fa fa-check"></i><b>10.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="10.3.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-implementation-1"><i class="fa fa-check"></i><b>10.3.2</b> Dyna-Q+ Implementation</a></li>
<li class="chapter" data-level="10.3.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#standard-dyna-for-comparison"><i class="fa fa-check"></i><b>10.3.3</b> Standard Dyna for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#experimental-analysis-1"><i class="fa fa-check"></i><b>10.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#adaptation-to-environmental-changes"><i class="fa fa-check"></i><b>10.4.1</b> Adaptation to Environmental Changes</a></li>
<li class="chapter" data-level="10.4.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#parameter-sensitivity-analysis"><i class="fa fa-check"></i><b>10.4.2</b> Parameter Sensitivity Analysis</a></li>
<li class="chapter" data-level="10.4.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#exploration-pattern-analysis"><i class="fa fa-check"></i><b>10.4.3</b> Exploration Pattern Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#discussion-and-implementation-considerations"><i class="fa fa-check"></i><b>10.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="10.6" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#conclusion-6"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>11</b> Appendix</a>
<ul>
<li class="chapter" data-level="11.1" data-path="appendix.html"><a href="appendix.html#comprehensive-reinforcement-learning-concepts-guide"><i class="fa fa-check"></i><b>11.1</b> Comprehensive Reinforcement Learning Concepts Guide</a></li>
<li class="chapter" data-level="11.2" data-path="appendix.html"><a href="appendix.html#learning-mechanisms"><i class="fa fa-check"></i><b>11.2</b> Learning Mechanisms</a></li>
<li class="chapter" data-level="11.3" data-path="appendix.html"><a href="appendix.html#environment-properties"><i class="fa fa-check"></i><b>11.3</b> Environment Properties</a></li>
<li class="chapter" data-level="11.4" data-path="appendix.html"><a href="appendix.html#learning-paradigms"><i class="fa fa-check"></i><b>11.4</b> Learning Paradigms</a></li>
<li class="chapter" data-level="11.5" data-path="appendix.html"><a href="appendix.html#exploration-strategies"><i class="fa fa-check"></i><b>11.5</b> Exploration Strategies</a></li>
<li class="chapter" data-level="11.6" data-path="appendix.html"><a href="appendix.html#key-algorithms-methods"><i class="fa fa-check"></i><b>11.6</b> Key Algorithms &amp; Methods</a></li>
<li class="chapter" data-level="11.7" data-path="appendix.html"><a href="appendix.html#advanced-concepts"><i class="fa fa-check"></i><b>11.7</b> Advanced Concepts</a></li>
<li class="chapter" data-level="11.8" data-path="appendix.html"><a href="appendix.html#fundamental-equations"><i class="fa fa-check"></i><b>11.8</b> Fundamental Equations</a>
<ul>
<li class="chapter" data-level="11.8.1" data-path="appendix.html"><a href="appendix.html#bellman-equations"><i class="fa fa-check"></i><b>11.8.1</b> <strong>Bellman Equations</strong></a></li>
<li class="chapter" data-level="11.8.2" data-path="appendix.html"><a href="appendix.html#policy-gradient-theorem"><i class="fa fa-check"></i><b>11.8.2</b> <strong>Policy Gradient Theorem</strong></a></li>
<li class="chapter" data-level="11.8.3" data-path="appendix.html"><a href="appendix.html#temporal-difference-error"><i class="fa fa-check"></i><b>11.8.3</b> <strong>Temporal Difference Error</strong></a></li>
</ul></li>
<li class="chapter" data-level="11.9" data-path="appendix.html"><a href="appendix.html#common-challenges-solutions"><i class="fa fa-check"></i><b>11.9</b> Common Challenges &amp; Solutions</a></li>
<li class="chapter" data-level="11.10" data-path="appendix.html"><a href="appendix.html#function-approximation-fundamentals-in-reinforcement-learning"><i class="fa fa-check"></i><b>11.10</b> Function Approximation Fundamentals in Reinforcement Learning</a></li>
<li class="chapter" data-level="11.11" data-path="appendix.html"><a href="appendix.html#feature-engineering-and-state-representation"><i class="fa fa-check"></i><b>11.11</b> Feature Engineering and State Representation</a>
<ul>
<li class="chapter" data-level="11.11.1" data-path="appendix.html"><a href="appendix.html#the-discrimination-vs.-generalization-tradeoff"><i class="fa fa-check"></i><b>11.11.1</b> The Discrimination vs. Generalization Tradeoff</a></li>
<li class="chapter" data-level="11.11.2" data-path="appendix.html"><a href="appendix.html#principles-of-feature-extraction"><i class="fa fa-check"></i><b>11.11.2</b> Principles of Feature Extraction</a></li>
</ul></li>
<li class="chapter" data-level="11.12" data-path="appendix.html"><a href="appendix.html#mathematical-foundations-of-linear-function-approximation"><i class="fa fa-check"></i><b>11.12</b> Mathematical Foundations of Linear Function Approximation</a>
<ul>
<li class="chapter" data-level="11.12.1" data-path="appendix.html"><a href="appendix.html#linear-value-functions"><i class="fa fa-check"></i><b>11.12.1</b> Linear Value Functions</a></li>
<li class="chapter" data-level="11.12.2" data-path="appendix.html"><a href="appendix.html#temporal-difference-td-learning-with-function-approximation"><i class="fa fa-check"></i><b>11.12.2</b> Temporal Difference (TD) Learning with Function Approximation</a></li>
<li class="chapter" data-level="11.12.3" data-path="appendix.html"><a href="appendix.html#convergence-and-the-deadly-triad"><i class="fa fa-check"></i><b>11.12.3</b> Convergence and the “Deadly Triad”</a></li>
</ul></li>
<li class="chapter" data-level="11.13" data-path="appendix.html"><a href="appendix.html#classical-basis-function-methods"><i class="fa fa-check"></i><b>11.13</b> Classical Basis Function Methods</a>
<ul>
<li class="chapter" data-level="11.13.1" data-path="appendix.html"><a href="appendix.html#coarse-coding"><i class="fa fa-check"></i><b>11.13.1</b> Coarse Coding</a></li>
<li class="chapter" data-level="11.13.2" data-path="appendix.html"><a href="appendix.html#tile-coding"><i class="fa fa-check"></i><b>11.13.2</b> Tile Coding</a></li>
<li class="chapter" data-level="11.13.3" data-path="appendix.html"><a href="appendix.html#radial-basis-functions-rbfs"><i class="fa fa-check"></i><b>11.13.3</b> Radial Basis Functions (RBFs)</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-5" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Introduction<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While linear function approximation provides a solid foundation for scaling reinforcement learning beyond tabular methods, it assumes a linear relationship between features and Q-values. Real-world problems often exhibit complex, non-linear patterns that linear models cannot capture effectively. This post extends our previous exploration by implementing Q-Learning with Random Forest function approximation, demonstrating how ensemble methods can learn intricate state-action value relationships while maintaining interpretability and robust generalization.</p>
<p>Random Forests offer several advantages over linear approximation: they handle non-linear relationships naturally, provide built-in feature importance measures, resist overfitting through ensemble averaging, and require minimal hyperparameter tuning. We’ll implement this approach using the same 10-state, 2-action environment, comparing the learned policies and examining the unique characteristics of tree-based function approximation.</p>
</div>
<div id="theoretical-background-2" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Theoretical Background<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#theoretical-background-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random Forest function approximation replaces the linear parameterization with an ensemble of decision trees. Instead of:</p>
<p><span class="math display">\[
Q(s, a; \theta) = \phi(s, a)^T \theta
\]</span></p>
<p>we now approximate the action-value function as:</p>
<p><span class="math display">\[
Q(s, a) = \frac{1}{B} \sum_{b=1}^{B} T_b(\phi(s, a))
\]</span></p>
<p>where <span class="math inline">\(T_b\)</span> represents the <span class="math inline">\(b\)</span>-th tree in the ensemble, <span class="math inline">\(B\)</span> is the number of trees, and <span class="math inline">\(\phi(s, a)\)</span> is our feature representation. Each tree <span class="math inline">\(T_b\)</span> is trained on a bootstrap sample of the data with random feature subsets at each split, providing natural regularization and variance reduction.</p>
<div id="q-learning-with-random-forest-approximation" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Q-Learning with Random Forest Approximation<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Q-Learning update process with Random Forest approximation involves:</p>
<ol style="list-style-type: decimal">
<li><strong>Experience Collection</strong>: Gather state-action-reward-next state tuples <span class="math inline">\((s, a, r, s&#39;)\)</span></li>
<li><strong>Target Computation</strong>: Calculate TD targets <span class="math inline">\(y = r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;)\)</span></li>
<li><strong>Model Training</strong>: Fit Random Forest regressor to predict <span class="math inline">\(Q(s, a)\)</span> from features <span class="math inline">\(\phi(s, a)\)</span></li>
<li><strong>Policy Update</strong>: Use updated model for epsilon-greedy action selection</li>
</ol>
<p>Unlike linear methods with continuous parameter updates, Random Forest approximation requires periodic model retraining on accumulated experience. This batch-like approach trades computational efficiency for modeling flexibility.</p>
</div>
<div id="feature-engineering-for-tree-based-models" class="section level3 hasAnchor" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> Feature Engineering for Tree-Based Models<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-engineering-for-tree-based-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For our implementation, we use a simple concatenation of one-hot encoded state and action vectors:</p>
<p><span class="math display">\[
\phi(s, a) = [e_s^{(state)} \; || \; e_a^{(action)}]
\]</span></p>
<p>where <span class="math inline">\(e_s^{(state)}\)</span> is a one-hot vector for state <span class="math inline">\(s\)</span> and <span class="math inline">\(e_a^{(action)}\)</span> is a one-hot vector for action <span class="math inline">\(a\)</span>. This encoding allows trees to learn complex interactions between states and actions while maintaining interpretability.</p>
</div>
<div id="comparison-with-previous-methods" class="section level3 hasAnchor" number="7.2.3">
<h3><span class="header-section-number">7.2.3</span> Comparison with Previous Methods<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="10%" />
<col width="21%" />
<col width="30%" />
<col width="37%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>Tabular Q-Learning</strong></th>
<th><strong>Linear Function Approximation</strong></th>
<th><strong>Random Forest Function Approximation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Model Complexity</strong></td>
<td>None; direct storage</td>
<td>Linear combinations</td>
<td>Non-linear ensemble</td>
</tr>
<tr class="even">
<td><strong>Feature Interactions</strong></td>
<td>Implicit</td>
<td>None (unless engineered)</td>
<td>Automatic discovery</td>
</tr>
<tr class="odd">
<td><strong>Interpretability</strong></td>
<td>Full</td>
<td>Moderate (weights)</td>
<td>High (tree structures)</td>
</tr>
<tr class="even">
<td><strong>Training</strong></td>
<td>Online updates</td>
<td>Gradient descent</td>
<td>Batch retraining</td>
</tr>
<tr class="odd">
<td><strong>Overfitting Risk</strong></td>
<td>None</td>
<td>Low</td>
<td>Low (ensemble averaging)</td>
</tr>
<tr class="even">
<td><strong>Computational Cost</strong></td>
<td><span class="math inline">\(O(1)\)</span> lookup</td>
<td><span class="math inline">\(O(d)\)</span> linear algebra</td>
<td><span class="math inline">\(O(B \cdot \log n)\)</span> prediction</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="r-implementation-1" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> R Implementation<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our implementation builds upon the previous environment while introducing Random Forest-based Q-value approximation. The key innovation lies in accumulating training examples and periodically retraining the forest to incorporate new experience.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb19-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-1" tabindex="-1"></a><span class="co"># Load required libraries</span></span>
<span id="cb19-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-2" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb19-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-3" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb19-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-4" tabindex="-1"></a></span>
<span id="cb19-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-5" tabindex="-1"></a><span class="co"># Environment setup (same as previous implementation)</span></span>
<span id="cb19-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-6" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb19-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-7" tabindex="-1"></a>n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb19-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-8" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb19-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-9" tabindex="-1"></a>terminal_state <span class="ot">&lt;-</span> n_states</span>
<span id="cb19-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-10" tabindex="-1"></a></span>
<span id="cb19-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-11" tabindex="-1"></a><span class="co"># Environment: transition and reward models</span></span>
<span id="cb19-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-12" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb19-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-13" tabindex="-1"></a>transition_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb19-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-14" tabindex="-1"></a>reward_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb19-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-15" tabindex="-1"></a></span>
<span id="cb19-16"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-16" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>)) {</span>
<span id="cb19-17"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-17" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, s <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb19-18"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-18" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb19-19"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-19" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb19-20"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-20" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb19-21"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-21" tabindex="-1"></a>  <span class="cf">for</span> (s_prime <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_states) {</span>
<span id="cb19-22"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-22" tabindex="-1"></a>    reward_model[s, <span class="dv">1</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">1.0</span>, <span class="fl">0.1</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb19-23"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-23" tabindex="-1"></a>    reward_model[s, <span class="dv">2</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">0.5</span>, <span class="fl">0.05</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb19-24"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-24" tabindex="-1"></a>  }</span>
<span id="cb19-25"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-25" tabindex="-1"></a>}</span>
<span id="cb19-26"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-26" tabindex="-1"></a></span>
<span id="cb19-27"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-27" tabindex="-1"></a>transition_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb19-28"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-28" tabindex="-1"></a>reward_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb19-29"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-29" tabindex="-1"></a></span>
<span id="cb19-30"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-30" tabindex="-1"></a><span class="co"># Sampling function</span></span>
<span id="cb19-31"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-31" tabindex="-1"></a>sample_env <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a) {</span>
<span id="cb19-32"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-32" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> transition_model[s, a, ]</span>
<span id="cb19-33"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-33" tabindex="-1"></a>  s_prime <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>, <span class="at">prob =</span> probs)</span>
<span id="cb19-34"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-34" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> reward_model[s, a, s_prime]</span>
<span id="cb19-35"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-35" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">s_prime =</span> s_prime, <span class="at">reward =</span> reward)</span>
<span id="cb19-36"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-36" tabindex="-1"></a>}</span>
<span id="cb19-37"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-37" tabindex="-1"></a></span>
<span id="cb19-38"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-38" tabindex="-1"></a><span class="co"># Feature encoding for Random Forest</span></span>
<span id="cb19-39"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-39" tabindex="-1"></a>encode_features <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a, n_states, n_actions) {</span>
<span id="cb19-40"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-40" tabindex="-1"></a>  state_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb19-41"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-41" tabindex="-1"></a>  action_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_actions)</span>
<span id="cb19-42"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-42" tabindex="-1"></a>  state_vec[s] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb19-43"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-43" tabindex="-1"></a>  action_vec[a] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb19-44"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-44" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(state_vec, action_vec))</span>
<span id="cb19-45"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-45" tabindex="-1"></a>}</span>
<span id="cb19-46"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-46" tabindex="-1"></a></span>
<span id="cb19-47"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-47" tabindex="-1"></a>n_features <span class="ot">&lt;-</span> n_states <span class="sc">+</span> n_actions</span>
<span id="cb19-48"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-48" tabindex="-1"></a></span>
<span id="cb19-49"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-49" tabindex="-1"></a><span class="co"># Q-Learning with Random Forest function approximation</span></span>
<span id="cb19-50"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-50" tabindex="-1"></a>q_learning_rf <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>, <span class="at">retrain_freq =</span> <span class="dv">10</span>, <span class="at">min_samples =</span> <span class="dv">50</span>) {</span>
<span id="cb19-51"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-51" tabindex="-1"></a>  <span class="co"># Initialize training data storage</span></span>
<span id="cb19-52"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-52" tabindex="-1"></a>  rf_data_x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> <span class="dv">0</span>, <span class="at">ncol =</span> n_features)</span>
<span id="cb19-53"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-53" tabindex="-1"></a>  rf_data_y <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">0</span>)</span>
<span id="cb19-54"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-54" tabindex="-1"></a>  rf_model <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb19-55"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-55" tabindex="-1"></a>  rewards <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb19-56"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-56" tabindex="-1"></a>  </span>
<span id="cb19-57"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-57" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb19-58"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-58" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>), <span class="dv">1</span>)  <span class="co"># Start from non-terminal state</span></span>
<span id="cb19-59"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-59" tabindex="-1"></a>    episode_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb19-60"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-60" tabindex="-1"></a>    </span>
<span id="cb19-61"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-61" tabindex="-1"></a>    <span class="cf">while</span> (<span class="cn">TRUE</span>) {</span>
<span id="cb19-62"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-62" tabindex="-1"></a>      <span class="co"># Predict Q-values for all actions</span></span>
<span id="cb19-63"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-63" tabindex="-1"></a>      q_preds <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb19-64"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-64" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb19-65"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-65" tabindex="-1"></a>        <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_model)) {</span>
<span id="cb19-66"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-66" tabindex="-1"></a>          <span class="fu">predict</span>(rf_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x)))</span>
<span id="cb19-67"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-67" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb19-68"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-68" tabindex="-1"></a>          <span class="fu">runif</span>(<span class="dv">1</span>)  <span class="co"># Random initialization</span></span>
<span id="cb19-69"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-69" tabindex="-1"></a>        }</span>
<span id="cb19-70"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-70" tabindex="-1"></a>      })</span>
<span id="cb19-71"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-71" tabindex="-1"></a>      </span>
<span id="cb19-72"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-72" tabindex="-1"></a>      <span class="co"># Epsilon-greedy action selection</span></span>
<span id="cb19-73"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-73" tabindex="-1"></a>      a <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb19-74"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-74" tabindex="-1"></a>        <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb19-75"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-75" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb19-76"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-76" tabindex="-1"></a>        <span class="fu">which.max</span>(q_preds)</span>
<span id="cb19-77"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-77" tabindex="-1"></a>      }</span>
<span id="cb19-78"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-78" tabindex="-1"></a>      </span>
<span id="cb19-79"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-79" tabindex="-1"></a>      <span class="co"># Take action and observe outcome</span></span>
<span id="cb19-80"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-80" tabindex="-1"></a>      out <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a)</span>
<span id="cb19-81"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-81" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> out<span class="sc">$</span>s_prime</span>
<span id="cb19-82"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-82" tabindex="-1"></a>      r <span class="ot">&lt;-</span> out<span class="sc">$</span>reward</span>
<span id="cb19-83"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-83" tabindex="-1"></a>      episode_reward <span class="ot">&lt;-</span> episode_reward <span class="sc">+</span> r</span>
<span id="cb19-84"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-84" tabindex="-1"></a>      </span>
<span id="cb19-85"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-85" tabindex="-1"></a>      <span class="co"># Compute TD target</span></span>
<span id="cb19-86"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-86" tabindex="-1"></a>      q_next <span class="ot">&lt;-</span> <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) {</span>
<span id="cb19-87"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-87" tabindex="-1"></a>        <span class="dv">0</span></span>
<span id="cb19-88"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-88" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb19-89"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-89" tabindex="-1"></a>        <span class="fu">max</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a_) {</span>
<span id="cb19-90"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-90" tabindex="-1"></a>          x_next <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s_prime, a_, n_states, n_actions)</span>
<span id="cb19-91"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-91" tabindex="-1"></a>          <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_model)) {</span>
<span id="cb19-92"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-92" tabindex="-1"></a>            <span class="fu">predict</span>(rf_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x_next)))</span>
<span id="cb19-93"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-93" tabindex="-1"></a>          } <span class="cf">else</span> {</span>
<span id="cb19-94"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-94" tabindex="-1"></a>            <span class="dv">0</span></span>
<span id="cb19-95"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-95" tabindex="-1"></a>          }</span>
<span id="cb19-96"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-96" tabindex="-1"></a>        }))</span>
<span id="cb19-97"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-97" tabindex="-1"></a>      }</span>
<span id="cb19-98"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-98" tabindex="-1"></a>      </span>
<span id="cb19-99"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-99" tabindex="-1"></a>      target <span class="ot">&lt;-</span> r <span class="sc">+</span> gamma <span class="sc">*</span> q_next</span>
<span id="cb19-100"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-100" tabindex="-1"></a>      </span>
<span id="cb19-101"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-101" tabindex="-1"></a>      <span class="co"># Store training example</span></span>
<span id="cb19-102"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-102" tabindex="-1"></a>      x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb19-103"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-103" tabindex="-1"></a>      rf_data_x <span class="ot">&lt;-</span> <span class="fu">rbind</span>(rf_data_x, x)</span>
<span id="cb19-104"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-104" tabindex="-1"></a>      rf_data_y <span class="ot">&lt;-</span> <span class="fu">c</span>(rf_data_y, target)</span>
<span id="cb19-105"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-105" tabindex="-1"></a>      </span>
<span id="cb19-106"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-106" tabindex="-1"></a>      <span class="co"># Retrain Random Forest periodically</span></span>
<span id="cb19-107"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-107" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">nrow</span>(rf_data_x) <span class="sc">&gt;=</span> min_samples <span class="sc">&amp;&amp;</span> ep <span class="sc">%%</span> retrain_freq <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb19-108"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-108" tabindex="-1"></a>        rf_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(</span>
<span id="cb19-109"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-109" tabindex="-1"></a>          <span class="at">x =</span> <span class="fu">as.data.frame</span>(rf_data_x),</span>
<span id="cb19-110"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-110" tabindex="-1"></a>          <span class="at">y =</span> rf_data_y,</span>
<span id="cb19-111"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-111" tabindex="-1"></a>          <span class="at">ntree =</span> <span class="dv">100</span>,</span>
<span id="cb19-112"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-112" tabindex="-1"></a>          <span class="at">nodesize =</span> <span class="dv">5</span>,</span>
<span id="cb19-113"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-113" tabindex="-1"></a>          <span class="at">mtry =</span> <span class="fu">max</span>(<span class="dv">1</span>, <span class="fu">floor</span>(n_features <span class="sc">/</span> <span class="dv">3</span>))</span>
<span id="cb19-114"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-114" tabindex="-1"></a>        )</span>
<span id="cb19-115"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-115" tabindex="-1"></a>      }</span>
<span id="cb19-116"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-116" tabindex="-1"></a>      </span>
<span id="cb19-117"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-117" tabindex="-1"></a>      <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) <span class="cf">break</span></span>
<span id="cb19-118"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-118" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb19-119"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-119" tabindex="-1"></a>    }</span>
<span id="cb19-120"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-120" tabindex="-1"></a>    </span>
<span id="cb19-121"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-121" tabindex="-1"></a>    rewards[ep] <span class="ot">&lt;-</span> episode_reward</span>
<span id="cb19-122"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-122" tabindex="-1"></a>  }</span>
<span id="cb19-123"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-123" tabindex="-1"></a>  </span>
<span id="cb19-124"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-124" tabindex="-1"></a>  <span class="co"># Derive final policy</span></span>
<span id="cb19-125"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-125" tabindex="-1"></a>  policy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), <span class="cf">function</span>(s) {</span>
<span id="cb19-126"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-126" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_model)) {</span>
<span id="cb19-127"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-127" tabindex="-1"></a>      q_vals <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb19-128"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-128" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb19-129"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-129" tabindex="-1"></a>        <span class="fu">predict</span>(rf_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x)))</span>
<span id="cb19-130"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-130" tabindex="-1"></a>      })</span>
<span id="cb19-131"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-131" tabindex="-1"></a>      <span class="fu">which.max</span>(q_vals)</span>
<span id="cb19-132"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-132" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb19-133"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-133" tabindex="-1"></a>      <span class="dv">1</span>  <span class="co"># Default action</span></span>
<span id="cb19-134"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-134" tabindex="-1"></a>    }</span>
<span id="cb19-135"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-135" tabindex="-1"></a>  })</span>
<span id="cb19-136"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-136" tabindex="-1"></a>  </span>
<span id="cb19-137"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-137" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">model =</span> rf_model, <span class="at">policy =</span> <span class="fu">c</span>(policy, <span class="cn">NA</span>), <span class="at">rewards =</span> rewards, </span>
<span id="cb19-138"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-138" tabindex="-1"></a>       <span class="at">training_data =</span> <span class="fu">list</span>(<span class="at">x =</span> rf_data_x, <span class="at">y =</span> rf_data_y))</span>
<span id="cb19-139"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-139" tabindex="-1"></a>}</span>
<span id="cb19-140"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-140" tabindex="-1"></a></span>
<span id="cb19-141"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-141" tabindex="-1"></a><span class="co"># Run Q-Learning with Random Forest approximation</span></span>
<span id="cb19-142"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-142" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb19-143"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-143" tabindex="-1"></a>rf_result <span class="ot">&lt;-</span> <span class="fu">q_learning_rf</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>, <span class="at">retrain_freq =</span> <span class="dv">10</span>)</span>
<span id="cb19-144"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-144" tabindex="-1"></a>rf_policy <span class="ot">&lt;-</span> rf_result<span class="sc">$</span>policy</span>
<span id="cb19-145"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-145" tabindex="-1"></a>rf_rewards <span class="ot">&lt;-</span> rf_result<span class="sc">$</span>rewards</span>
<span id="cb19-146"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-146" tabindex="-1"></a></span>
<span id="cb19-147"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-147" tabindex="-1"></a><span class="co"># Create policy visualization</span></span>
<span id="cb19-148"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-148" tabindex="-1"></a>policy_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb19-149"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-149" tabindex="-1"></a>  <span class="at">State =</span> <span class="dv">1</span><span class="sc">:</span>n_states,</span>
<span id="cb19-150"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-150" tabindex="-1"></a>  <span class="at">Policy =</span> rf_policy,</span>
<span id="cb19-151"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-151" tabindex="-1"></a>  <span class="at">Algorithm =</span> <span class="st">&quot;Q-Learning RF&quot;</span></span>
<span id="cb19-152"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-152" tabindex="-1"></a>)</span>
<span id="cb19-153"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-153" tabindex="-1"></a></span>
<span id="cb19-154"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-154" tabindex="-1"></a>policy_plot_rf <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(policy_df[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), ], <span class="fu">aes</span>(<span class="at">x =</span> State, <span class="at">y =</span> Policy)) <span class="sc">+</span></span>
<span id="cb19-155"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-155" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">4</span>, <span class="at">color =</span> <span class="st">&quot;forestgreen&quot;</span>) <span class="sc">+</span></span>
<span id="cb19-156"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-156" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;forestgreen&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb19-157"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-157" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb19-158"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-158" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb19-159"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-159" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Policy from Q-Learning with Random Forest Approximation&quot;</span>,</span>
<span id="cb19-160"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-160" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;State&quot;</span>, </span>
<span id="cb19-161"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-161" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Action&quot;</span></span>
<span id="cb19-162"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-162" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb19-163"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-163" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>n_states) <span class="sc">+</span></span>
<span id="cb19-164"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-164" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>n_actions, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Action 1&quot;</span>, <span class="st">&quot;Action 2&quot;</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">2.5</span>)) <span class="sc">+</span></span>
<span id="cb19-165"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-165" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb19-166"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-166" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb19-167"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-167" tabindex="-1"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb19-168"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-168" tabindex="-1"></a>    <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>)</span>
<span id="cb19-169"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-169" tabindex="-1"></a>  )</span>
<span id="cb19-170"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-170" tabindex="-1"></a></span>
<span id="cb19-171"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-171" tabindex="-1"></a><span class="co"># Compare cumulative rewards with moving average</span></span>
<span id="cb19-172"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-172" tabindex="-1"></a>rewards_smooth <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(rf_rewards))</span>
<span id="cb19-173"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-173" tabindex="-1"></a>window_size <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb19-174"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-174" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(rf_rewards)) {</span>
<span id="cb19-175"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-175" tabindex="-1"></a>  start_idx <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="dv">1</span>, i <span class="sc">-</span> window_size <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb19-176"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-176" tabindex="-1"></a>  rewards_smooth[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(rf_rewards[start_idx<span class="sc">:</span>i])</span>
<span id="cb19-177"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-177" tabindex="-1"></a>}</span>
<span id="cb19-178"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-178" tabindex="-1"></a></span>
<span id="cb19-179"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-179" tabindex="-1"></a>reward_df_rf <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb19-180"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-180" tabindex="-1"></a>  <span class="at">Episode =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>,</span>
<span id="cb19-181"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-181" tabindex="-1"></a>  <span class="at">Reward =</span> rewards_smooth,</span>
<span id="cb19-182"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-182" tabindex="-1"></a>  <span class="at">Algorithm =</span> <span class="st">&quot;Q-Learning RF&quot;</span></span>
<span id="cb19-183"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-183" tabindex="-1"></a>)</span>
<span id="cb19-184"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-184" tabindex="-1"></a></span>
<span id="cb19-185"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-185" tabindex="-1"></a>reward_plot_rf <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(reward_df_rf, <span class="fu">aes</span>(<span class="at">x =</span> Episode, <span class="at">y =</span> Reward)) <span class="sc">+</span></span>
<span id="cb19-186"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-186" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;forestgreen&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb19-187"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-187" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb19-188"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-188" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb19-189"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-189" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Learning Curve: Q-Learning with Random Forest (50-episode moving average)&quot;</span>,</span>
<span id="cb19-190"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-190" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Episode&quot;</span>,</span>
<span id="cb19-191"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-191" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Average Reward&quot;</span></span>
<span id="cb19-192"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-192" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb19-193"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-193" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb19-194"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-194" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb19-195"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-195" tabindex="-1"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb19-196"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-196" tabindex="-1"></a>    <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>)</span>
<span id="cb19-197"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-197" tabindex="-1"></a>  )</span>
<span id="cb19-198"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-198" tabindex="-1"></a></span>
<span id="cb19-199"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-199" tabindex="-1"></a><span class="co"># Display plots</span></span>
<span id="cb19-200"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-200" tabindex="-1"></a><span class="fu">print</span>(policy_plot_rf)</span>
<span id="cb19-201"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-201" tabindex="-1"></a><span class="fu">print</span>(reward_plot_rf)</span>
<span id="cb19-202"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-202" tabindex="-1"></a></span>
<span id="cb19-203"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-203" tabindex="-1"></a><span class="co"># Feature importance analysis</span></span>
<span id="cb19-204"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-204" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_result<span class="sc">$</span>model)) {</span>
<span id="cb19-205"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-205" tabindex="-1"></a>  importance_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb19-206"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-206" tabindex="-1"></a>    <span class="at">Feature =</span> <span class="fu">c</span>(<span class="fu">paste</span>(<span class="st">&quot;State&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_states), <span class="fu">paste</span>(<span class="st">&quot;Action&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_actions)),</span>
<span id="cb19-207"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-207" tabindex="-1"></a>    <span class="at">Importance =</span> <span class="fu">importance</span>(rf_result<span class="sc">$</span>model)[, <span class="dv">1</span>]</span>
<span id="cb19-208"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-208" tabindex="-1"></a>  )</span>
<span id="cb19-209"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-209" tabindex="-1"></a>  </span>
<span id="cb19-210"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-210" tabindex="-1"></a>  importance_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(importance_df, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Feature, Importance), <span class="at">y =</span> Importance)) <span class="sc">+</span></span>
<span id="cb19-211"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-211" tabindex="-1"></a>    <span class="fu">geom_col</span>(<span class="at">fill =</span> <span class="st">&quot;forestgreen&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb19-212"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-212" tabindex="-1"></a>    <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb19-213"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-213" tabindex="-1"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb19-214"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-214" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb19-215"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-215" tabindex="-1"></a>      <span class="at">title =</span> <span class="st">&quot;Feature Importance in Random Forest Q-Function&quot;</span>,</span>
<span id="cb19-216"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-216" tabindex="-1"></a>      <span class="at">x =</span> <span class="st">&quot;Feature&quot;</span>,</span>
<span id="cb19-217"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-217" tabindex="-1"></a>      <span class="at">y =</span> <span class="st">&quot;Importance (Mean Decrease in MSE)&quot;</span></span>
<span id="cb19-218"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-218" tabindex="-1"></a>    ) <span class="sc">+</span></span>
<span id="cb19-219"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-219" tabindex="-1"></a>    <span class="fu">theme</span>(</span>
<span id="cb19-220"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-220" tabindex="-1"></a>      <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb19-221"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-221" tabindex="-1"></a>      <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb19-222"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-222" tabindex="-1"></a>      <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>)</span>
<span id="cb19-223"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-223" tabindex="-1"></a>    )</span>
<span id="cb19-224"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-224" tabindex="-1"></a>  </span>
<span id="cb19-225"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-225" tabindex="-1"></a>  <span class="fu">print</span>(importance_plot)</span>
<span id="cb19-226"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-226" tabindex="-1"></a>}</span>
<span id="cb19-227"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-227" tabindex="-1"></a></span>
<span id="cb19-228"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-228" tabindex="-1"></a><span class="co"># Model diagnostics</span></span>
<span id="cb19-229"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-229" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Random Forest Model Summary:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb19-230"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-230" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Number of trees:&quot;</span>, rf_result<span class="sc">$</span>model<span class="sc">$</span>ntree, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb19-231"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-231" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Training examples:&quot;</span>, <span class="fu">nrow</span>(rf_result<span class="sc">$</span>training_data<span class="sc">$</span>x), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb19-232"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb19-232" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Final OOB error:&quot;</span>, <span class="fu">tail</span>(rf_result<span class="sc">$</span>model<span class="sc">$</span>mse, <span class="dv">1</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
</div>
<div id="analysis-and-insights" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Analysis and Insights<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="policy-learning-characteristics" class="section level3 hasAnchor" number="7.4.1">
<h3><span class="header-section-number">7.4.1</span> Policy Learning Characteristics<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#policy-learning-characteristics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random Forest function approximation exhibits several characteristics that distinguish it from linear methods. Trees can capture non-linear decision boundaries, enabling the model to learn state-action relationships that linear approaches cannot represent. The random feature sampling at each split performs automatic feature selection, focusing computational resources on the most informative variables. Ensemble averaging across multiple trees reduces overfitting and provides stable predictions across different training samples. Individual trees maintain interpretable decision paths that show how Q-values are estimated for specific state-action pairs.</p>
</div>
<div id="computational-considerations" class="section level3 hasAnchor" number="7.4.2">
<h3><span class="header-section-number">7.4.2</span> Computational Considerations<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#computational-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The batch retraining approach creates distinct computational trade-offs that affect implementation decisions. Training frequency must balance responsiveness against computational cost, as more frequent updates improve adaptation but require additional processing time. Trees need sufficient data to learn meaningful patterns, which can slow initial learning compared to methods that update continuously. Memory requirements increase over time as training examples accumulate, requiring careful management of historical data.</p>
</div>
<div id="feature-importance-insights" class="section level3 hasAnchor" number="7.4.3">
<h3><span class="header-section-number">7.4.3</span> Feature Importance Insights<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-importance-insights" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random Forest methods naturally generate feature importance measures that reveal which states and actions most influence Q-value predictions. This interpretability provides diagnostic capabilities for understanding learning issues and analyzing policy decisions. The feature ranking can guide state representation choices and help identify redundant or irrelevant variables in the problem formulation.</p>
</div>
<div id="practical-implications-1" class="section level3 hasAnchor" number="7.4.4">
<h3><span class="header-section-number">7.4.4</span> Practical Implications<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#practical-implications-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random Forest function approximation occupies a position between simple linear models and neural networks in terms of complexity and capability. The method handles larger state spaces more effectively than tabular approaches while remaining computationally tractable. It captures non-linear patterns without requiring extensive feature engineering or domain expertise. The approach shows less sensitivity to hyperparameter choices compared to neural networks while maintaining stability across different problem instances. The inherent interpretability provides insights into the decision-making process that can be valuable for debugging and analysis.</p>
</div>
</div>
<div id="comparison-with-linear-approximation" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Comparison with Linear Approximation<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-linear-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random Forest methods demonstrate several advantages and trade-offs when compared to linear function approximation. The tree-based approach excels at pattern recognition, learning state-action relationships that linear models cannot capture due to their representational limitations. However, initial learning proceeds more slowly as trees require sufficient data to construct meaningful decision boundaries. Computational costs are higher due to periodic retraining requirements, contrasting with the continuous gradient updates used in linear methods. Generalization performance tends to be superior, as ensemble averaging provides natural regularization that reduces overfitting tendencies.</p>
</div>
<div id="conclusion-4" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> Conclusion<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random Forest function approximation extends linear methods by offering enhanced modeling flexibility while preserving interpretability characteristics. The approach performs particularly well in environments with non-linear state-action relationships and provides regularization through ensemble averaging.</p>
<p>Several key observations emerge from this analysis. Non-linear function approximation can capture patterns that linear models miss, enabling better policy learning in complex environments. Batch learning approaches require careful consideration of training frequency and sample requirements to balance performance with computational efficiency. Feature importance analysis provides insights into learned policies that can guide problem formulation and debugging efforts. Tree-based methods offer an interpretable alternative to neural network approaches while maintaining theoretical foundations.</p>
<p>This exploration demonstrates how ensemble methods can enhance reinforcement learning without abandoning the established principles of Q-Learning. Future work could investigate online tree learning algorithms that avoid batch retraining requirements, adaptive schedules that optimize training frequency based on performance metrics, or hybrid approaches that combine strengths from different function approximation methods.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/edit/main/07-Q_FA_RF.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/blob/main/07-Q_FA_RF.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
