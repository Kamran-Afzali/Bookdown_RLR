<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R</title>
  <meta name="description" content="Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R | Reinforcement Learning in R" />
  
  
  

<meta name="author" content="Kamran Afzalui" />


<meta name="date" content="2025-10-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="function-approximation-q-learning-with-linear-models.html"/>
<link rel="next" href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Reinforcement Learning in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Understanding Reinforcement Learning: From Bandits to Policy Optimization</strong></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-multi-armed-bandit-the-simplest-case"><i class="fa fa-check"></i><b>1.2</b> The Multi-Armed Bandit: The Simplest Case</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#transition-to-markov-decision-processes"><i class="fa fa-check"></i><b>1.3</b> Transition to Markov Decision Processes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#comparing-reinforcement-learning-methods"><i class="fa fa-check"></i><b>1.4</b> Comparing Reinforcement Learning Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#dynamic-programming-model-based-learning"><i class="fa fa-check"></i><b>1.4.1</b> Dynamic Programming: Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#model-free-approaches-monte-carlo-and-td-learning"><i class="fa fa-check"></i><b>1.4.2</b> Model-Free Approaches: Monte Carlo and TD Learning</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#dyna-bridging-model-free-and-model-based-learning"><i class="fa fa-check"></i><b>1.4.3</b> Dyna: Bridging Model-Free and Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#q-learning-and-function-approximation"><i class="fa fa-check"></i><b>1.4.4</b> Q-Learning and Function Approximation</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#policy-gradient-and-actor-critic-methods"><i class="fa fa-check"></i><b>1.4.5</b> Policy Gradient and Actor-Critic Methods</a></li>
<li class="chapter" data-level="1.4.6" data-path="index.html"><a href="index.html#advanced-policy-optimization-techniques"><i class="fa fa-check"></i><b>1.4.6</b> Advanced Policy Optimization Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#further-directions"><i class="fa fa-check"></i><b>1.5</b> Further Directions</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html"><i class="fa fa-check"></i><b>2</b> The Multi-Armed Bandit Problem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#mathematical-formalism"><i class="fa fa-check"></i><b>2.2</b> Mathematical Formalism</a></li>
<li class="chapter" data-level="2.3" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm"><i class="fa fa-check"></i><b>2.3</b> Frequentist Approach: UCB1 Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1"><i class="fa fa-check"></i><b>2.3.1</b> R Code for UCB1</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling"><i class="fa fa-check"></i><b>2.4</b> Bayesian Approach: Thompson Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling"><i class="fa fa-check"></i><b>2.4.1</b> R Code for Thompson Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy"><i class="fa fa-check"></i><b>2.5</b> Epsilon-Greedy Strategy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy"><i class="fa fa-check"></i><b>2.5.1</b> R Code for Epsilon-Greedy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#summary-table"><i class="fa fa-check"></i><b>2.6</b> Summary Table</a></li>
<li class="chapter" data-level="2.7" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html"><i class="fa fa-check"></i><b>3</b> Markov Decision Processes and Dynamic Programming</a>
<ul>
<li class="chapter" data-level="3.1" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#constructing-the-mdp-in-r"><i class="fa fa-check"></i><b>3.2</b> Constructing the MDP in R</a></li>
<li class="chapter" data-level="3.3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#value-iteration-algorithm"><i class="fa fa-check"></i><b>3.3</b> Value Iteration Algorithm</a></li>
<li class="chapter" data-level="3.4" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#evaluation-and-interpretation"><i class="fa fa-check"></i><b>3.4</b> Evaluation and Interpretation</a></li>
<li class="chapter" data-level="3.5" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties-of-value-iteration"><i class="fa fa-check"></i><b>3.5</b> Theoretical Properties of Value Iteration</a></li>
<li class="chapter" data-level="3.6" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-1"><i class="fa fa-check"></i><b>3.6</b> Summary Table</a></li>
<li class="chapter" data-level="3.7" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><i class="fa fa-check"></i><b>4</b> Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#theoretical-background"><i class="fa fa-check"></i><b>4.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#temporal-difference-learning-q-learning"><i class="fa fa-check"></i><b>4.2.1</b> Temporal Difference Learning (Q-Learning)</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#monte-carlo-methods"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.2.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-1-defining-the-environment-in-r"><i class="fa fa-check"></i><b>4.2.3</b> Step 1: Defining the Environment in R</a></li>
<li class="chapter" data-level="4.2.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-2-q-learning-implementation-in-r"><i class="fa fa-check"></i><b>4.2.4</b> Step 2: Q-Learning Implementation in R</a></li>
<li class="chapter" data-level="4.2.5" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-3-monte-carlo-every-visit-implementation"><i class="fa fa-check"></i><b>4.2.5</b> Step 3: Monte Carlo Every-Visit Implementation</a></li>
<li class="chapter" data-level="4.2.6" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-4-simulating-outcome-devaluation"><i class="fa fa-check"></i><b>4.2.6</b> Step 4: Simulating Outcome Devaluation</a></li>
<li class="chapter" data-level="4.2.7" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-5-comparing-policies-before-and-after-devaluation"><i class="fa fa-check"></i><b>4.2.7</b> Step 5: Comparing Policies Before and After Devaluation</a></li>
<li class="chapter" data-level="4.2.8" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-6-visualizing-the-policies"><i class="fa fa-check"></i><b>4.2.8</b> Step 6: Visualizing the Policies</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#interpretation-and-discussion"><i class="fa fa-check"></i><b>4.3</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="4.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#conclusion-2"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><i class="fa fa-check"></i><b>5</b> On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-on-policy"><i class="fa fa-check"></i><b>5.2</b> SARSA (On-Policy)</a></li>
<li class="chapter" data-level="5.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-off-policy"><i class="fa fa-check"></i><b>5.3</b> Q-Learning (Off-Policy)</a></li>
<li class="chapter" data-level="5.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-with-importance-sampling"><i class="fa fa-check"></i><b>5.4</b> Off-Policy Monte Carlo with Importance Sampling</a></li>
<li class="chapter" data-level="5.5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#key-differences"><i class="fa fa-check"></i><b>5.5</b> Key Differences</a></li>
<li class="chapter" data-level="5.6" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#interpretation-and-discussion-1"><i class="fa fa-check"></i><b>5.6</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#conclusion-3"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
<li class="chapter" data-level="5.8" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#comparison-table"><i class="fa fa-check"></i><b>5.8</b> Comparison Table</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html"><i class="fa fa-check"></i><b>6</b> Function Approximation Q-Learning with Linear Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#q-learning-with-function-approximation"><i class="fa fa-check"></i><b>6.1.1</b> Q-Learning with Function Approximation</a></li>
<li class="chapter" data-level="6.1.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#comparison-with-tabular-q-learning"><i class="fa fa-check"></i><b>6.1.2</b> Comparison with Tabular Q-Learning</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#r-implementation"><i class="fa fa-check"></i><b>6.2</b> R Implementation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><i class="fa fa-check"></i><b>7</b> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation"><i class="fa fa-check"></i><b>7.1.1</b> Q-Learning with Random Forest Approximation</a></li>
<li class="chapter" data-level="7.1.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods"><i class="fa fa-check"></i><b>7.1.2</b> Comparison with Previous Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1"><i class="fa fa-check"></i><b>7.2</b> R Implementation</a></li>
<li class="chapter" data-level="7.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights"><i class="fa fa-check"></i><b>7.3</b> Analysis and Insights</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#policy-learning-characteristics"><i class="fa fa-check"></i><b>7.3.1</b> Policy Learning Characteristics</a></li>
<li class="chapter" data-level="7.3.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#computational-considerations"><i class="fa fa-check"></i><b>7.3.2</b> Computational Considerations</a></li>
<li class="chapter" data-level="7.3.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-importance-insights"><i class="fa fa-check"></i><b>7.3.3</b> Feature Importance Insights</a></li>
<li class="chapter" data-level="7.3.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#practical-implications-1"><i class="fa fa-check"></i><b>7.3.4</b> Practical Implications</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-linear-approximation"><i class="fa fa-check"></i><b>7.4</b> Comparison with Linear Approximation</a></li>
<li class="chapter" data-level="7.5" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><i class="fa fa-check"></i><b>8</b> Deep Function Approximation: Q-Learning with Neural Networks in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation"><i class="fa fa-check"></i><b>8.2</b> Theoretical Foundation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity"><i class="fa fa-check"></i><b>8.2.1</b> Universal Approximation and Expressivity</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning"><i class="fa fa-check"></i><b>8.2.2</b> Gradient-Based Learning</a></li>
<li class="chapter" data-level="8.2.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches"><i class="fa fa-check"></i><b>8.2.3</b> Comparison with Previous Approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2"><i class="fa fa-check"></i><b>8.3</b> R Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation"><i class="fa fa-check"></i><b>8.4</b> Analysis and Interpretation</a></li>
<li class="chapter" data-level="8.5" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions"><i class="fa fa-check"></i><b>8.5</b> Future Directions</a></li>
<li class="chapter" data-level="8.6" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5"><i class="fa fa-check"></i><b>8.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html"><i class="fa fa-check"></i><b>9</b> Dyna and DynaQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#theoretical-framework"><i class="fa fa-check"></i><b>9.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#the-dyna-architecture"><i class="fa fa-check"></i><b>9.2.1</b> The Dyna Architecture</a></li>
<li class="chapter" data-level="9.2.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#model-representation"><i class="fa fa-check"></i><b>9.2.2</b> Model Representation</a></li>
<li class="chapter" data-level="9.2.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#convergence-properties"><i class="fa fa-check"></i><b>9.2.3</b> Convergence Properties</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-in-r"><i class="fa fa-check"></i><b>9.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#environment-setup"><i class="fa fa-check"></i><b>9.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="9.3.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#dyna-q-implementation"><i class="fa fa-check"></i><b>9.3.2</b> Dyna-Q Implementation</a></li>
<li class="chapter" data-level="9.3.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#standard-q-learning-for-comparison"><i class="fa fa-check"></i><b>9.3.3</b> Standard Q-Learning for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#experimental-analysis"><i class="fa fa-check"></i><b>9.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#learning-efficiency-comparison"><i class="fa fa-check"></i><b>9.4.1</b> Learning Efficiency Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
<li class="chapter" data-level="9.6" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-considerations-and-conclusion"><i class="fa fa-check"></i><b>9.6</b> Implementation Considerations and Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><i class="fa fa-check"></i><b>10</b> Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#theoretical-framework-1"><i class="fa fa-check"></i><b>10.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#the-exploration-bonus-mechanism"><i class="fa fa-check"></i><b>10.2.1</b> The Exploration Bonus Mechanism</a></li>
<li class="chapter" data-level="10.2.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#complete-dyna-q-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> Complete Dyna-Q+ Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#convergence-and-stability"><i class="fa fa-check"></i><b>10.2.3</b> Convergence and Stability</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#implementation-in-r-1"><i class="fa fa-check"></i><b>10.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#environment-setup-1"><i class="fa fa-check"></i><b>10.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="10.3.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-implementation-1"><i class="fa fa-check"></i><b>10.3.2</b> Dyna-Q+ Implementation</a></li>
<li class="chapter" data-level="10.3.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#standard-dyna-for-comparison"><i class="fa fa-check"></i><b>10.3.3</b> Standard Dyna for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#experimental-analysis-1"><i class="fa fa-check"></i><b>10.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#adaptation-to-environmental-changes"><i class="fa fa-check"></i><b>10.4.1</b> Adaptation to Environmental Changes</a></li>
<li class="chapter" data-level="10.4.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#parameter-sensitivity-analysis"><i class="fa fa-check"></i><b>10.4.2</b> Parameter Sensitivity Analysis</a></li>
<li class="chapter" data-level="10.4.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#exploration-pattern-analysis"><i class="fa fa-check"></i><b>10.4.3</b> Exploration Pattern Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#discussion-and-implementation-considerations"><i class="fa fa-check"></i><b>10.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="10.6" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#conclusion-6"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html"><i class="fa fa-check"></i><b>11</b> Function Approximation And Feature Engineering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#feature-engineering-and-state-representation"><i class="fa fa-check"></i><b>11.1</b> Feature Engineering and State Representation</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-discrimination-vs.-generalization-tradeoff"><i class="fa fa-check"></i><b>11.1.1</b> The Discrimination vs.Â Generalization Tradeoff</a></li>
<li class="chapter" data-level="11.1.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#principles-of-effective-feature-design"><i class="fa fa-check"></i><b>11.1.2</b> Principles of Effective Feature Design</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#mathematical-foundations-of-linear-function-approximation"><i class="fa fa-check"></i><b>11.2</b> Mathematical Foundations of Linear Function Approximation</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#linear-value-functions-and-their-properties"><i class="fa fa-check"></i><b>11.2.1</b> Linear Value Functions and Their Properties</a></li>
<li class="chapter" data-level="11.2.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#temporal-difference-learning-with-linear-approximation"><i class="fa fa-check"></i><b>11.2.2</b> Temporal Difference Learning with Linear Approximation</a></li>
<li class="chapter" data-level="11.2.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-deadly-triad-and-stability-concerns"><i class="fa fa-check"></i><b>11.2.3</b> The Deadly Triad and Stability Concerns</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#classical-basis-function-methods"><i class="fa fa-check"></i><b>11.3</b> Classical Basis Function Methods</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#coarse-coding-overlapping-receptive-fields"><i class="fa fa-check"></i><b>11.3.1</b> Coarse Coding: Overlapping Receptive Fields</a></li>
<li class="chapter" data-level="11.3.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#tile-coding-structured-overlapping-grids"><i class="fa fa-check"></i><b>11.3.2</b> Tile Coding: Structured Overlapping Grids</a></li>
<li class="chapter" data-level="11.3.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#radial-basis-functions-smooth-continuous-features"><i class="fa fa-check"></i><b>11.3.3</b> Radial Basis Functions: Smooth Continuous Features</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#comparative-analysis-and-practical-considerations"><i class="fa fa-check"></i><b>11.4</b> Comparative Analysis and Practical Considerations</a></li>
<li class="chapter" data-level="11.5" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#bridging-classical-and-modern-approaches"><i class="fa fa-check"></i><b>11.5</b> Bridging Classical and Modern Approaches</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html"><i class="fa fa-check"></i><b>12</b> Learning Policies versus Learning Values:</a>
<ul>
<li class="chapter" data-level="12.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#the-two-paradigms-of-reinforcement-learning"><i class="fa fa-check"></i><b>12.1</b> The Two Paradigms of Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#value-based-methods-learning-worth-before-action"><i class="fa fa-check"></i><b>12.1.1</b> Value-Based Methods: Learning Worth Before Action</a></li>
<li class="chapter" data-level="12.1.2" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#policy-based-methods-direct-optimization-of-behavior"><i class="fa fa-check"></i><b>12.1.2</b> Policy-Based Methods: Direct Optimization of Behavior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><i class="fa fa-check"></i><b>13</b> Average Reward in Reinforcement Learning: A Comprehensive Guide</a>
<ul>
<li class="chapter" data-level="13.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-limitations-of-discounted-reward-formulations"><i class="fa fa-check"></i><b>13.1</b> The Limitations of Discounted Reward Formulations</a></li>
<li class="chapter" data-level="13.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-average-reward-alternative-theoretical-foundations"><i class="fa fa-check"></i><b>13.2</b> The Average Reward Alternative: Theoretical Foundations</a></li>
<li class="chapter" data-level="13.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#value-functions-in-the-average-reward-setting"><i class="fa fa-check"></i><b>13.3</b> Value Functions in the Average Reward Setting</a></li>
<li class="chapter" data-level="13.4" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-theory-for-average-reward"><i class="fa fa-check"></i><b>13.4</b> Optimality Theory for Average Reward</a></li>
<li class="chapter" data-level="13.5" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#learning-algorithms-for-average-reward"><i class="fa fa-check"></i><b>13.5</b> Learning Algorithms for Average Reward</a></li>
<li class="chapter" data-level="13.6" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#practical-implementation-server-load-balancing"><i class="fa fa-check"></i><b>13.6</b> Practical Implementation: Server Load Balancing</a></li>
<li class="chapter" data-level="13.7" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#implementation-considerations"><i class="fa fa-check"></i><b>13.7</b> Implementation Considerations</a></li>
<li class="chapter" data-level="13.8" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#when-to-choose-average-reward-over-discounting"><i class="fa fa-check"></i><b>13.8</b> When to Choose Average Reward Over Discounting</a></li>
<li class="chapter" data-level="13.9" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#appendix-a-mathematical-proofs-and-derivations"><i class="fa fa-check"></i><b>13.9</b> Appendix A: Mathematical Proofs and Derivations</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#convergence-of-average-reward-td-learning"><i class="fa fa-check"></i><b>13.9.1</b> Convergence of Average Reward TD Learning</a></li>
<li class="chapter" data-level="13.9.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#policy-gradient-theorem-for-average-reward"><i class="fa fa-check"></i><b>13.9.2</b> Policy Gradient Theorem for Average Reward</a></li>
<li class="chapter" data-level="13.9.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-equations-derivation"><i class="fa fa-check"></i><b>13.9.3</b> Optimality Equations Derivation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="eligibility-traces.html"><a href="eligibility-traces.html"><i class="fa fa-check"></i><b>14</b> Eligibility Traces</a>
<ul>
<li class="chapter" data-level="14.1" data-path="eligibility-traces.html"><a href="eligibility-traces.html#from-one-step-to-multi-step-learning"><i class="fa fa-check"></i><b>14.1</b> From One-Step to Multi-Step Learning</a></li>
<li class="chapter" data-level="14.2" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-mechanics-of-eligibility-traces-a-backward-view"><i class="fa fa-check"></i><b>14.2</b> The Mechanics of Eligibility Traces: A Backward View</a></li>
<li class="chapter" data-level="14.3" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-tdlambda-algorithm-for-prediction"><i class="fa fa-check"></i><b>14.3</b> The TD(<span class="math inline">\(\\lambda\)</span>) Algorithm for Prediction</a></li>
<li class="chapter" data-level="14.4" data-path="eligibility-traces.html"><a href="eligibility-traces.html#control-with-eligibility-traces-average-reward-sarsalambda"><i class="fa fa-check"></i><b>14.4</b> Control with Eligibility Traces: Average Reward Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.5" data-path="eligibility-traces.html"><a href="eligibility-traces.html#practical-implementation-server-load-balancing-with-sarsalambda"><i class="fa fa-check"></i><b>14.5</b> Practical Implementation: Server Load Balancing with Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.6" data-path="eligibility-traces.html"><a href="eligibility-traces.html#computational-and-performance-considerations"><i class="fa fa-check"></i><b>14.6</b> Computational and Performance Considerations</a></li>
<li class="chapter" data-level="14.7" data-path="eligibility-traces.html"><a href="eligibility-traces.html#conclusion-8"><i class="fa fa-check"></i><b>14.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html"><i class="fa fa-check"></i><b>15</b> Intrinsic Rewards</a>
<ul>
<li class="chapter" data-level="15.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#the-sparse-reward-problem-and-its-implications"><i class="fa fa-check"></i><b>15.1</b> The Sparse Reward Problem and Its Implications</a></li>
<li class="chapter" data-level="15.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#mathematical-foundations-of-intrinsic-motivation"><i class="fa fa-check"></i><b>15.2</b> Mathematical Foundations of Intrinsic Motivation</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#curiosity-driven-learning"><i class="fa fa-check"></i><b>15.2.1</b> Curiosity-Driven Learning</a></li>
<li class="chapter" data-level="15.2.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#count-based-exploration"><i class="fa fa-check"></i><b>15.2.2</b> Count-Based Exploration</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#practical-implementation-curiosity-driven-grid-world-navigation"><i class="fa fa-check"></i><b>15.3</b> Practical Implementation: Curiosity-Driven Grid World Navigation</a></li>
<li class="chapter" data-level="15.4" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#advanced-intrinsic-motivation-mechanisms"><i class="fa fa-check"></i><b>15.4</b> Advanced Intrinsic Motivation Mechanisms</a></li>
<li class="chapter" data-level="15.5" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#implementation-considerations-and-practical-guidelines"><i class="fa fa-check"></i><b>15.5</b> Implementation Considerations and Practical Guidelines</a></li>
<li class="chapter" data-level="15.6" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#theoretical-analysis-and-convergence-properties"><i class="fa fa-check"></i><b>15.6</b> Theoretical Analysis and Convergence Properties</a></li>
<li class="chapter" data-level="15.7" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#applications-and-empirical-results"><i class="fa fa-check"></i><b>15.7</b> Applications and Empirical Results</a></li>
<li class="chapter" data-level="15.8" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#limitations-and-future-directions"><i class="fa fa-check"></i><b>15.8</b> Limitations and Future Directions</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>16</b> Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="16.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#introduction-9"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#theoretical-framework-2"><i class="fa fa-check"></i><b>16.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#the-policy-gradient-theorem-1"><i class="fa fa-check"></i><b>16.2.1</b> The Policy Gradient Theorem</a></li>
<li class="chapter" data-level="16.2.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-algorithm"><i class="fa fa-check"></i><b>16.2.2</b> REINFORCE Algorithm</a></li>
<li class="chapter" data-level="16.2.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#baseline-subtraction-and-variance-reduction"><i class="fa fa-check"></i><b>16.2.3</b> Baseline Subtraction and Variance Reduction</a></li>
<li class="chapter" data-level="16.2.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-parameterization"><i class="fa fa-check"></i><b>16.2.4</b> Softmax Policy Parameterization</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#implementation-in-r-2"><i class="fa fa-check"></i><b>16.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#environment-and-feature-representation"><i class="fa fa-check"></i><b>16.3.1</b> Environment and Feature Representation</a></li>
<li class="chapter" data-level="16.3.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-implementation"><i class="fa fa-check"></i><b>16.3.2</b> Softmax Policy Implementation</a></li>
<li class="chapter" data-level="16.3.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-implementation"><i class="fa fa-check"></i><b>16.3.3</b> REINFORCE Implementation</a></li>
<li class="chapter" data-level="16.3.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#actor-critic-implementation"><i class="fa fa-check"></i><b>16.3.4</b> Actor-Critic Implementation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#experimental-analysis-2"><i class="fa fa-check"></i><b>16.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#comparison-of-policy-gradient-variants"><i class="fa fa-check"></i><b>16.4.1</b> Comparison of Policy Gradient Variants</a></li>
<li class="chapter" data-level="16.4.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#learning-rate-sensitivity-analysis"><i class="fa fa-check"></i><b>16.4.2</b> Learning Rate Sensitivity Analysis</a></li>
<li class="chapter" data-level="16.4.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#variance-analysis-of-gradient-estimates"><i class="fa fa-check"></i><b>16.4.3</b> Variance Analysis of Gradient Estimates</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#discussion-and-implementation-considerations-1"><i class="fa fa-check"></i><b>16.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="16.6" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#conclusion-9"><i class="fa fa-check"></i><b>16.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>17</b> Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="17.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#theoretical-framework-3"><i class="fa fa-check"></i><b>17.1</b> Theoretical Framework</a></li>
<li class="chapter" data-level="17.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#implementation-and-comparative-analysis"><i class="fa fa-check"></i><b>17.2</b> Implementation and Comparative Analysis</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#variance-analysis-and-learning-dynamics"><i class="fa fa-check"></i><b>17.2.1</b> Variance Analysis and Learning Dynamics</a></li>
<li class="chapter" data-level="17.2.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#algorithmic-variants-and-extensions"><i class="fa fa-check"></i><b>17.2.2</b> Algorithmic Variants and Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#computational-and-convergence-considerations"><i class="fa fa-check"></i><b>17.3</b> Computational and Convergence Considerations</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#comparative-performance-analysis"><i class="fa fa-check"></i><b>17.3.1</b> Comparative Performance Analysis</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#conclusion-10"><i class="fa fa-check"></i><b>17.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>18</b> Appendix</a>
<ul>
<li class="chapter" data-level="18.1" data-path="appendix.html"><a href="appendix.html#comprehensive-reinforcement-learning-concepts-guide"><i class="fa fa-check"></i><b>18.1</b> Comprehensive Reinforcement Learning Concepts Guide</a></li>
<li class="chapter" data-level="18.2" data-path="appendix.html"><a href="appendix.html#learning-mechanisms"><i class="fa fa-check"></i><b>18.2</b> Learning Mechanisms</a></li>
<li class="chapter" data-level="18.3" data-path="appendix.html"><a href="appendix.html#environment-properties"><i class="fa fa-check"></i><b>18.3</b> Environment Properties</a></li>
<li class="chapter" data-level="18.4" data-path="appendix.html"><a href="appendix.html#learning-paradigms"><i class="fa fa-check"></i><b>18.4</b> Learning Paradigms</a></li>
<li class="chapter" data-level="18.5" data-path="appendix.html"><a href="appendix.html#exploration-strategies"><i class="fa fa-check"></i><b>18.5</b> Exploration Strategies</a></li>
<li class="chapter" data-level="18.6" data-path="appendix.html"><a href="appendix.html#key-algorithms-methods"><i class="fa fa-check"></i><b>18.6</b> Key Algorithms &amp; Methods</a></li>
<li class="chapter" data-level="18.7" data-path="appendix.html"><a href="appendix.html#advanced-concepts"><i class="fa fa-check"></i><b>18.7</b> Advanced Concepts</a></li>
<li class="chapter" data-level="18.8" data-path="appendix.html"><a href="appendix.html#fundamental-equations"><i class="fa fa-check"></i><b>18.8</b> Fundamental Equations</a>
<ul>
<li class="chapter" data-level="18.8.1" data-path="appendix.html"><a href="appendix.html#bellman-equations"><i class="fa fa-check"></i><b>18.8.1</b> <strong>Bellman Equations</strong></a></li>
<li class="chapter" data-level="18.8.2" data-path="appendix.html"><a href="appendix.html#policy-gradient-theorem"><i class="fa fa-check"></i><b>18.8.2</b> <strong>Policy Gradient Theorem</strong></a></li>
<li class="chapter" data-level="18.8.3" data-path="appendix.html"><a href="appendix.html#temporal-difference-error"><i class="fa fa-check"></i><b>18.8.3</b> <strong>Temporal Difference Error</strong></a></li>
</ul></li>
<li class="chapter" data-level="18.9" data-path="appendix.html"><a href="appendix.html#common-challenges-solutions"><i class="fa fa-check"></i><b>18.9</b> Common Challenges &amp; Solutions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-5" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Introduction<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While linear function approximation provides a solid foundation for scaling reinforcement learning beyond tabular methods, it assumes a linear relationship between features and Q-values. Real-world problems often exhibit complex, non-linear patterns that linear models cannot capture effectively. This post extends our previous exploration by implementing Q-Learning with Random Forest function approximation, demonstrating how ensemble methods can learn intricate state-action value relationships while maintaining interpretability and robust generalization.</p>
<p>Random Forests offer several advantages over linear approximation: they handle non-linear relationships naturally, provide built-in feature importance measures, resist overfitting through ensemble averaging, and require minimal hyperparameter tuning. Weâll implement this approach using the same 10-state, 2-action environment, comparing the learned policies and examining the unique characteristics of tree-based function approximation.</p>
<p>Random Forest function approximation replaces the linear parameterization with an ensemble of decision trees. Instead of:</p>
<p><span class="math display">\[
Q(s, a; \theta) = \phi(s, a)^T \theta
\]</span></p>
<p>we now approximate the action-value function as:</p>
<p><span class="math display">\[
Q(s, a) = \frac{1}{B} \sum_{b=1}^{B} T_b(\phi(s, a))
\]</span></p>
<p>where <span class="math inline">\(T_b\)</span> represents the <span class="math inline">\(b\)</span>-th tree in the ensemble, <span class="math inline">\(B\)</span> is the number of trees, and <span class="math inline">\(\phi(s, a)\)</span> is our feature representation. Each tree <span class="math inline">\(T_b\)</span> is trained on a bootstrap sample of the data with random feature subsets at each split, providing natural regularization and variance reduction.</p>
<div id="q-learning-with-random-forest-approximation" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Q-Learning with Random Forest Approximation<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Q-Learning update process with Random Forest approximation involves:</p>
<ol style="list-style-type: decimal">
<li><strong>Experience Collection</strong>: Gather state-action-reward-next state tuples <span class="math inline">\((s, a, r, s&#39;)\)</span></li>
<li><strong>Target Computation</strong>: Calculate TD targets <span class="math inline">\(y = r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;)\)</span></li>
<li><strong>Model Training</strong>: Fit Random Forest regressor to predict <span class="math inline">\(Q(s, a)\)</span> from features <span class="math inline">\(\phi(s, a)\)</span></li>
<li><strong>Policy Update</strong>: Use updated model for epsilon-greedy action selection</li>
</ol>
<p>Unlike linear methods with continuous parameter updates, Random Forest approximation requires periodic model retraining on accumulated experience. This batch-like approach trades computational efficiency for modeling flexibility.</p>
<p>For our implementation, we use a simple concatenation of one-hot encoded state and action vectors:</p>
<p><span class="math display">\[
\phi(s, a) = [e_s^{(state)} \; || \; e_a^{(action)}]
\]</span></p>
<p>where <span class="math inline">\(e_s^{(state)}\)</span> is a one-hot vector for state <span class="math inline">\(s\)</span> and <span class="math inline">\(e_a^{(action)}\)</span> is a one-hot vector for action <span class="math inline">\(a\)</span>. This encoding allows trees to learn complex interactions between states and actions while maintaining interpretability.</p>
</div>
<div id="comparison-with-previous-methods" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Comparison with Previous Methods<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<table>
<colgroup>
<col width="10%" />
<col width="21%" />
<col width="30%" />
<col width="37%" />
</colgroup>
<thead>
<tr class="header">
<th><strong>Aspect</strong></th>
<th><strong>Tabular Q-Learning</strong></th>
<th><strong>Linear Function Approximation</strong></th>
<th><strong>Random Forest Function Approximation</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Model Complexity</strong></td>
<td>None; direct storage</td>
<td>Linear combinations</td>
<td>Non-linear ensemble</td>
</tr>
<tr class="even">
<td><strong>Feature Interactions</strong></td>
<td>Implicit</td>
<td>None (unless engineered)</td>
<td>Automatic discovery</td>
</tr>
<tr class="odd">
<td><strong>Interpretability</strong></td>
<td>Full</td>
<td>Moderate (weights)</td>
<td>High (tree structures)</td>
</tr>
<tr class="even">
<td><strong>Training</strong></td>
<td>Online updates</td>
<td>Gradient descent</td>
<td>Batch retraining</td>
</tr>
<tr class="odd">
<td><strong>Overfitting Risk</strong></td>
<td>None</td>
<td>Low</td>
<td>Low (ensemble averaging)</td>
</tr>
<tr class="even">
<td><strong>Computational Cost</strong></td>
<td><span class="math inline">\(O(1)\)</span> lookup</td>
<td><span class="math inline">\(O(d)\)</span> linear algebra</td>
<td><span class="math inline">\(O(B \cdot \log n)\)</span> prediction</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="r-implementation-1" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> R Implementation<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our implementation builds upon the previous environment while introducing Random Forest-based Q-value approximation. The key innovation lies in accumulating training examples and periodically retraining the forest to incorporate new experience.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-1" tabindex="-1"></a><span class="co"># Load required libraries</span></span>
<span id="cb61-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb61-2" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span></code></pre></div>
<pre><code>## randomForest 4.7-1.2</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:gridExtra&#39;:
## 
##     combine</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-1" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb67-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-2" tabindex="-1"></a></span>
<span id="cb67-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-3" tabindex="-1"></a><span class="co"># Environment setup (same as previous implementation)</span></span>
<span id="cb67-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-4" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb67-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-5" tabindex="-1"></a>n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb67-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-6" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb67-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-7" tabindex="-1"></a>terminal_state <span class="ot">&lt;-</span> n_states</span>
<span id="cb67-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-8" tabindex="-1"></a></span>
<span id="cb67-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-9" tabindex="-1"></a><span class="co"># Environment: transition and reward models</span></span>
<span id="cb67-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-10" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb67-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-11" tabindex="-1"></a>transition_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb67-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-12" tabindex="-1"></a>reward_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb67-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-13" tabindex="-1"></a></span>
<span id="cb67-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-14" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>)) {</span>
<span id="cb67-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-15" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, s <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb67-16"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-16" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb67-17"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-17" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb67-18"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-18" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb67-19"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-19" tabindex="-1"></a>  <span class="cf">for</span> (s_prime <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_states) {</span>
<span id="cb67-20"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-20" tabindex="-1"></a>    reward_model[s, <span class="dv">1</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">1.0</span>, <span class="fl">0.1</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb67-21"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-21" tabindex="-1"></a>    reward_model[s, <span class="dv">2</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">0.5</span>, <span class="fl">0.05</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb67-22"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-22" tabindex="-1"></a>  }</span>
<span id="cb67-23"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-23" tabindex="-1"></a>}</span>
<span id="cb67-24"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-24" tabindex="-1"></a></span>
<span id="cb67-25"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-25" tabindex="-1"></a>transition_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb67-26"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-26" tabindex="-1"></a>reward_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb67-27"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-27" tabindex="-1"></a></span>
<span id="cb67-28"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-28" tabindex="-1"></a><span class="co"># Sampling function</span></span>
<span id="cb67-29"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-29" tabindex="-1"></a>sample_env <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a) {</span>
<span id="cb67-30"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-30" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> transition_model[s, a, ]</span>
<span id="cb67-31"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-31" tabindex="-1"></a>  s_prime <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>, <span class="at">prob =</span> probs)</span>
<span id="cb67-32"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-32" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> reward_model[s, a, s_prime]</span>
<span id="cb67-33"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-33" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">s_prime =</span> s_prime, <span class="at">reward =</span> reward)</span>
<span id="cb67-34"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-34" tabindex="-1"></a>}</span>
<span id="cb67-35"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-35" tabindex="-1"></a></span>
<span id="cb67-36"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-36" tabindex="-1"></a><span class="co"># Feature encoding for Random Forest</span></span>
<span id="cb67-37"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-37" tabindex="-1"></a>encode_features <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a, n_states, n_actions) {</span>
<span id="cb67-38"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-38" tabindex="-1"></a>  state_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb67-39"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-39" tabindex="-1"></a>  action_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_actions)</span>
<span id="cb67-40"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-40" tabindex="-1"></a>  state_vec[s] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb67-41"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-41" tabindex="-1"></a>  action_vec[a] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb67-42"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-42" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(state_vec, action_vec))</span>
<span id="cb67-43"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-43" tabindex="-1"></a>}</span>
<span id="cb67-44"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-44" tabindex="-1"></a></span>
<span id="cb67-45"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-45" tabindex="-1"></a>n_features <span class="ot">&lt;-</span> n_states <span class="sc">+</span> n_actions</span>
<span id="cb67-46"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-46" tabindex="-1"></a></span>
<span id="cb67-47"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-47" tabindex="-1"></a><span class="co"># Q-Learning with Random Forest function approximation</span></span>
<span id="cb67-48"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-48" tabindex="-1"></a>q_learning_rf <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>, <span class="at">retrain_freq =</span> <span class="dv">10</span>, <span class="at">min_samples =</span> <span class="dv">50</span>) {</span>
<span id="cb67-49"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-49" tabindex="-1"></a>  <span class="co"># Initialize training data storage</span></span>
<span id="cb67-50"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-50" tabindex="-1"></a>  rf_data_x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> <span class="dv">0</span>, <span class="at">ncol =</span> n_features)</span>
<span id="cb67-51"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-51" tabindex="-1"></a>  rf_data_y <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">0</span>)</span>
<span id="cb67-52"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-52" tabindex="-1"></a>  rf_model <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb67-53"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-53" tabindex="-1"></a>  rewards <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb67-54"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-54" tabindex="-1"></a>  </span>
<span id="cb67-55"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-55" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb67-56"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-56" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>), <span class="dv">1</span>)  <span class="co"># Start from non-terminal state</span></span>
<span id="cb67-57"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-57" tabindex="-1"></a>    episode_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb67-58"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-58" tabindex="-1"></a>    </span>
<span id="cb67-59"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-59" tabindex="-1"></a>    <span class="cf">while</span> (<span class="cn">TRUE</span>) {</span>
<span id="cb67-60"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-60" tabindex="-1"></a>      <span class="co"># Predict Q-values for all actions</span></span>
<span id="cb67-61"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-61" tabindex="-1"></a>      q_preds <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb67-62"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-62" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb67-63"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-63" tabindex="-1"></a>        <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_model)) {</span>
<span id="cb67-64"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-64" tabindex="-1"></a>          <span class="fu">predict</span>(rf_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x)))</span>
<span id="cb67-65"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-65" tabindex="-1"></a>        } <span class="cf">else</span> {</span>
<span id="cb67-66"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-66" tabindex="-1"></a>          <span class="fu">runif</span>(<span class="dv">1</span>)  <span class="co"># Random initialization</span></span>
<span id="cb67-67"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-67" tabindex="-1"></a>        }</span>
<span id="cb67-68"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-68" tabindex="-1"></a>      })</span>
<span id="cb67-69"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-69" tabindex="-1"></a>      </span>
<span id="cb67-70"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-70" tabindex="-1"></a>      <span class="co"># Epsilon-greedy action selection</span></span>
<span id="cb67-71"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-71" tabindex="-1"></a>      a <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb67-72"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-72" tabindex="-1"></a>        <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb67-73"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-73" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb67-74"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-74" tabindex="-1"></a>        <span class="fu">which.max</span>(q_preds)</span>
<span id="cb67-75"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-75" tabindex="-1"></a>      }</span>
<span id="cb67-76"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-76" tabindex="-1"></a>      </span>
<span id="cb67-77"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-77" tabindex="-1"></a>      <span class="co"># Take action and observe outcome</span></span>
<span id="cb67-78"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-78" tabindex="-1"></a>      out <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a)</span>
<span id="cb67-79"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-79" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> out<span class="sc">$</span>s_prime</span>
<span id="cb67-80"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-80" tabindex="-1"></a>      r <span class="ot">&lt;-</span> out<span class="sc">$</span>reward</span>
<span id="cb67-81"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-81" tabindex="-1"></a>      episode_reward <span class="ot">&lt;-</span> episode_reward <span class="sc">+</span> r</span>
<span id="cb67-82"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-82" tabindex="-1"></a>      </span>
<span id="cb67-83"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-83" tabindex="-1"></a>      <span class="co"># Compute TD target</span></span>
<span id="cb67-84"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-84" tabindex="-1"></a>      q_next <span class="ot">&lt;-</span> <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) {</span>
<span id="cb67-85"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-85" tabindex="-1"></a>        <span class="dv">0</span></span>
<span id="cb67-86"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-86" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb67-87"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-87" tabindex="-1"></a>        <span class="fu">max</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a_) {</span>
<span id="cb67-88"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-88" tabindex="-1"></a>          x_next <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s_prime, a_, n_states, n_actions)</span>
<span id="cb67-89"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-89" tabindex="-1"></a>          <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_model)) {</span>
<span id="cb67-90"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-90" tabindex="-1"></a>            <span class="fu">predict</span>(rf_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x_next)))</span>
<span id="cb67-91"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-91" tabindex="-1"></a>          } <span class="cf">else</span> {</span>
<span id="cb67-92"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-92" tabindex="-1"></a>            <span class="dv">0</span></span>
<span id="cb67-93"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-93" tabindex="-1"></a>          }</span>
<span id="cb67-94"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-94" tabindex="-1"></a>        }))</span>
<span id="cb67-95"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-95" tabindex="-1"></a>      }</span>
<span id="cb67-96"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-96" tabindex="-1"></a>      </span>
<span id="cb67-97"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-97" tabindex="-1"></a>      target <span class="ot">&lt;-</span> r <span class="sc">+</span> gamma <span class="sc">*</span> q_next</span>
<span id="cb67-98"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-98" tabindex="-1"></a>      </span>
<span id="cb67-99"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-99" tabindex="-1"></a>      <span class="co"># Store training example</span></span>
<span id="cb67-100"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-100" tabindex="-1"></a>      x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb67-101"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-101" tabindex="-1"></a>      rf_data_x <span class="ot">&lt;-</span> <span class="fu">rbind</span>(rf_data_x, x)</span>
<span id="cb67-102"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-102" tabindex="-1"></a>      rf_data_y <span class="ot">&lt;-</span> <span class="fu">c</span>(rf_data_y, target)</span>
<span id="cb67-103"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-103" tabindex="-1"></a>      </span>
<span id="cb67-104"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-104" tabindex="-1"></a>      <span class="co"># Retrain Random Forest periodically</span></span>
<span id="cb67-105"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-105" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">nrow</span>(rf_data_x) <span class="sc">&gt;=</span> min_samples <span class="sc">&amp;&amp;</span> ep <span class="sc">%%</span> retrain_freq <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb67-106"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-106" tabindex="-1"></a>        rf_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(</span>
<span id="cb67-107"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-107" tabindex="-1"></a>          <span class="at">x =</span> <span class="fu">as.data.frame</span>(rf_data_x),</span>
<span id="cb67-108"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-108" tabindex="-1"></a>          <span class="at">y =</span> rf_data_y,</span>
<span id="cb67-109"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-109" tabindex="-1"></a>          <span class="at">ntree =</span> <span class="dv">100</span>,</span>
<span id="cb67-110"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-110" tabindex="-1"></a>          <span class="at">nodesize =</span> <span class="dv">5</span>,</span>
<span id="cb67-111"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-111" tabindex="-1"></a>          <span class="at">mtry =</span> <span class="fu">max</span>(<span class="dv">1</span>, <span class="fu">floor</span>(n_features <span class="sc">/</span> <span class="dv">3</span>))</span>
<span id="cb67-112"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-112" tabindex="-1"></a>        )</span>
<span id="cb67-113"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-113" tabindex="-1"></a>      }</span>
<span id="cb67-114"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-114" tabindex="-1"></a>      </span>
<span id="cb67-115"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-115" tabindex="-1"></a>      <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) <span class="cf">break</span></span>
<span id="cb67-116"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-116" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb67-117"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-117" tabindex="-1"></a>    }</span>
<span id="cb67-118"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-118" tabindex="-1"></a>    </span>
<span id="cb67-119"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-119" tabindex="-1"></a>    rewards[ep] <span class="ot">&lt;-</span> episode_reward</span>
<span id="cb67-120"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-120" tabindex="-1"></a>  }</span>
<span id="cb67-121"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-121" tabindex="-1"></a>  </span>
<span id="cb67-122"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-122" tabindex="-1"></a>  <span class="co"># Derive final policy</span></span>
<span id="cb67-123"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-123" tabindex="-1"></a>  policy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), <span class="cf">function</span>(s) {</span>
<span id="cb67-124"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-124" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_model)) {</span>
<span id="cb67-125"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-125" tabindex="-1"></a>      q_vals <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb67-126"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-126" tabindex="-1"></a>        x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb67-127"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-127" tabindex="-1"></a>        <span class="fu">predict</span>(rf_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x)))</span>
<span id="cb67-128"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-128" tabindex="-1"></a>      })</span>
<span id="cb67-129"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-129" tabindex="-1"></a>      <span class="fu">which.max</span>(q_vals)</span>
<span id="cb67-130"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-130" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb67-131"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-131" tabindex="-1"></a>      <span class="dv">1</span>  <span class="co"># Default action</span></span>
<span id="cb67-132"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-132" tabindex="-1"></a>    }</span>
<span id="cb67-133"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-133" tabindex="-1"></a>  })</span>
<span id="cb67-134"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-134" tabindex="-1"></a>  </span>
<span id="cb67-135"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-135" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">model =</span> rf_model, <span class="at">policy =</span> <span class="fu">c</span>(policy, <span class="cn">NA</span>), <span class="at">rewards =</span> rewards, </span>
<span id="cb67-136"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-136" tabindex="-1"></a>       <span class="at">training_data =</span> <span class="fu">list</span>(<span class="at">x =</span> rf_data_x, <span class="at">y =</span> rf_data_y))</span>
<span id="cb67-137"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-137" tabindex="-1"></a>}</span>
<span id="cb67-138"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-138" tabindex="-1"></a></span>
<span id="cb67-139"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-139" tabindex="-1"></a><span class="co"># Run Q-Learning with Random Forest approximation</span></span>
<span id="cb67-140"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-140" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb67-141"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-141" tabindex="-1"></a>rf_result <span class="ot">&lt;-</span> <span class="fu">q_learning_rf</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>, <span class="at">retrain_freq =</span> <span class="dv">10</span>)</span>
<span id="cb67-142"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-142" tabindex="-1"></a>rf_policy <span class="ot">&lt;-</span> rf_result<span class="sc">$</span>policy</span>
<span id="cb67-143"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-143" tabindex="-1"></a>rf_rewards <span class="ot">&lt;-</span> rf_result<span class="sc">$</span>rewards</span>
<span id="cb67-144"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-144" tabindex="-1"></a></span>
<span id="cb67-145"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-145" tabindex="-1"></a><span class="co"># Create policy visualization</span></span>
<span id="cb67-146"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-146" tabindex="-1"></a>policy_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb67-147"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-147" tabindex="-1"></a>  <span class="at">State =</span> <span class="dv">1</span><span class="sc">:</span>n_states,</span>
<span id="cb67-148"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-148" tabindex="-1"></a>  <span class="at">Policy =</span> rf_policy,</span>
<span id="cb67-149"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-149" tabindex="-1"></a>  <span class="at">Algorithm =</span> <span class="st">&quot;Q-Learning RF&quot;</span></span>
<span id="cb67-150"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-150" tabindex="-1"></a>)</span>
<span id="cb67-151"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-151" tabindex="-1"></a></span>
<span id="cb67-152"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-152" tabindex="-1"></a>policy_plot_rf <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(policy_df[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), ], <span class="fu">aes</span>(<span class="at">x =</span> State, <span class="at">y =</span> Policy)) <span class="sc">+</span></span>
<span id="cb67-153"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-153" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="dv">4</span>, <span class="at">color =</span> <span class="st">&quot;forestgreen&quot;</span>) <span class="sc">+</span></span>
<span id="cb67-154"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-154" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;forestgreen&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb67-155"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-155" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb67-156"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-156" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb67-157"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-157" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Policy from Q-Learning with Random Forest Approximation&quot;</span>,</span>
<span id="cb67-158"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-158" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;State&quot;</span>, </span>
<span id="cb67-159"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-159" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Action&quot;</span></span>
<span id="cb67-160"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-160" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb67-161"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-161" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>n_states) <span class="sc">+</span></span>
<span id="cb67-162"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-162" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>n_actions, <span class="at">labels =</span> <span class="fu">c</span>(<span class="st">&quot;Action 1&quot;</span>, <span class="st">&quot;Action 2&quot;</span>), <span class="at">limits =</span> <span class="fu">c</span>(<span class="fl">0.5</span>, <span class="fl">2.5</span>)) <span class="sc">+</span></span>
<span id="cb67-163"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-163" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb67-164"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-164" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb67-165"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-165" tabindex="-1"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb67-166"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-166" tabindex="-1"></a>    <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>)</span>
<span id="cb67-167"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-167" tabindex="-1"></a>  )</span>
<span id="cb67-168"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-168" tabindex="-1"></a></span>
<span id="cb67-169"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-169" tabindex="-1"></a><span class="co"># Compare cumulative rewards with moving average</span></span>
<span id="cb67-170"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-170" tabindex="-1"></a>rewards_smooth <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(rf_rewards))</span>
<span id="cb67-171"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-171" tabindex="-1"></a>window_size <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb67-172"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-172" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(rf_rewards)) {</span>
<span id="cb67-173"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-173" tabindex="-1"></a>  start_idx <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="dv">1</span>, i <span class="sc">-</span> window_size <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb67-174"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-174" tabindex="-1"></a>  rewards_smooth[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(rf_rewards[start_idx<span class="sc">:</span>i])</span>
<span id="cb67-175"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-175" tabindex="-1"></a>}</span>
<span id="cb67-176"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-176" tabindex="-1"></a></span>
<span id="cb67-177"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-177" tabindex="-1"></a>reward_df_rf <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb67-178"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-178" tabindex="-1"></a>  <span class="at">Episode =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>,</span>
<span id="cb67-179"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-179" tabindex="-1"></a>  <span class="at">Reward =</span> rewards_smooth,</span>
<span id="cb67-180"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-180" tabindex="-1"></a>  <span class="at">Algorithm =</span> <span class="st">&quot;Q-Learning RF&quot;</span></span>
<span id="cb67-181"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-181" tabindex="-1"></a>)</span>
<span id="cb67-182"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-182" tabindex="-1"></a></span>
<span id="cb67-183"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-183" tabindex="-1"></a>reward_plot_rf <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(reward_df_rf, <span class="fu">aes</span>(<span class="at">x =</span> Episode, <span class="at">y =</span> Reward)) <span class="sc">+</span></span>
<span id="cb67-184"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-184" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">&quot;forestgreen&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb67-185"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-185" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb67-186"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-186" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb67-187"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-187" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Learning Curve: Q-Learning with Random Forest (50-episode moving average)&quot;</span>,</span>
<span id="cb67-188"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-188" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Episode&quot;</span>,</span>
<span id="cb67-189"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-189" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Average Reward&quot;</span></span>
<span id="cb67-190"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-190" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb67-191"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-191" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb67-192"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-192" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb67-193"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-193" tabindex="-1"></a>    <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb67-194"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-194" tabindex="-1"></a>    <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>)</span>
<span id="cb67-195"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-195" tabindex="-1"></a>  )</span>
<span id="cb67-196"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-196" tabindex="-1"></a></span>
<span id="cb67-197"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-197" tabindex="-1"></a><span class="co"># Display plots</span></span>
<span id="cb67-198"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb67-198" tabindex="-1"></a><span class="fu">print</span>(policy_plot_rf)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-25-1.png" width="672" /></p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb68-1" tabindex="-1"></a><span class="fu">print</span>(reward_plot_rf)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-25-2.png" width="672" /></p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-1" tabindex="-1"></a><span class="co"># Feature importance analysis</span></span>
<span id="cb69-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-2" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_result<span class="sc">$</span>model)) {</span>
<span id="cb69-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-3" tabindex="-1"></a>  importance_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb69-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-4" tabindex="-1"></a>    <span class="at">Feature =</span> <span class="fu">c</span>(<span class="fu">paste</span>(<span class="st">&quot;State&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_states), <span class="fu">paste</span>(<span class="st">&quot;Action&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_actions)),</span>
<span id="cb69-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-5" tabindex="-1"></a>    <span class="at">Importance =</span> <span class="fu">importance</span>(rf_result<span class="sc">$</span>model)[, <span class="dv">1</span>]</span>
<span id="cb69-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-6" tabindex="-1"></a>  )</span>
<span id="cb69-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-7" tabindex="-1"></a>  </span>
<span id="cb69-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-8" tabindex="-1"></a>  importance_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(importance_df, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Feature, Importance), <span class="at">y =</span> Importance)) <span class="sc">+</span></span>
<span id="cb69-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-9" tabindex="-1"></a>    <span class="fu">geom_col</span>(<span class="at">fill =</span> <span class="st">&quot;forestgreen&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb69-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-10" tabindex="-1"></a>    <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb69-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-11" tabindex="-1"></a>    <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb69-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-12" tabindex="-1"></a>    <span class="fu">labs</span>(</span>
<span id="cb69-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-13" tabindex="-1"></a>      <span class="at">title =</span> <span class="st">&quot;Feature Importance in Random Forest Q-Function&quot;</span>,</span>
<span id="cb69-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-14" tabindex="-1"></a>      <span class="at">x =</span> <span class="st">&quot;Feature&quot;</span>,</span>
<span id="cb69-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-15" tabindex="-1"></a>      <span class="at">y =</span> <span class="st">&quot;Importance (Mean Decrease in MSE)&quot;</span></span>
<span id="cb69-16"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-16" tabindex="-1"></a>    ) <span class="sc">+</span></span>
<span id="cb69-17"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-17" tabindex="-1"></a>    <span class="fu">theme</span>(</span>
<span id="cb69-18"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-18" tabindex="-1"></a>      <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">14</span>, <span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb69-19"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-19" tabindex="-1"></a>      <span class="at">axis.title =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">12</span>),</span>
<span id="cb69-20"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-20" tabindex="-1"></a>      <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size =</span> <span class="dv">10</span>)</span>
<span id="cb69-21"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-21" tabindex="-1"></a>    )</span>
<span id="cb69-22"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-22" tabindex="-1"></a>  </span>
<span id="cb69-23"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-23" tabindex="-1"></a>  <span class="fu">print</span>(importance_plot)</span>
<span id="cb69-24"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb69-24" tabindex="-1"></a>}</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-25-3.png" width="672" /></p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-1" tabindex="-1"></a><span class="co"># Model diagnostics</span></span>
<span id="cb70-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb70-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Random Forest Model Summary:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Random Forest Model Summary:</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb72-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Number of trees:&quot;</span>, rf_result<span class="sc">$</span>model<span class="sc">$</span>ntree, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Number of trees: 100</code></pre>
<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb74-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb74-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Training examples:&quot;</span>, <span class="fu">nrow</span>(rf_result<span class="sc">$</span>training_data<span class="sc">$</span>x), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Training examples: 3040</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb76-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb76-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Final OOB error:&quot;</span>, <span class="fu">tail</span>(rf_result<span class="sc">$</span>model<span class="sc">$</span>mse, <span class="dv">1</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Final OOB error: 0.02695277</code></pre>
<p><strong>Environment Setup</strong></p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb78-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-1" tabindex="-1"></a><span class="co"># Environment setup (same as previous implementation)</span></span>
<span id="cb78-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-2" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb78-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-3" tabindex="-1"></a>n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb78-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-4" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb78-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb78-5" tabindex="-1"></a>terminal_state <span class="ot">&lt;-</span> n_states</span></code></pre></div>
<p>This chunk defines the basic parameters of the reinforcement learning problem.</p>
<ul>
<li><code>n_states &lt;- 10</code>: The âworldâ or âgameâ has 10 distinct states.</li>
<li><code>n_actions &lt;- 2</code>: In any state, the agent can choose between 2 possible actions.</li>
<li><code>gamma &lt;- 0.9</code>: This is the <strong>discount factor</strong>. It determines how much the agent values future rewards. A value of 0.9 means a reward received in the next step is worth 90% of a reward received now.</li>
<li><code>terminal_state &lt;- n_states</code>: State 10 is designated as the âendâ state. When the agent reaches this state, the âepisodeâ (one attempt) is over.</li>
</ul>
<p><strong>Environment Model Definition</strong></p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-1" tabindex="-1"></a><span class="co"># Environment: transition and reward models</span></span>
<span id="cb79-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb79-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-3" tabindex="-1"></a>transition_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb79-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-4" tabindex="-1"></a>reward_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb79-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-5" tabindex="-1"></a></span>
<span id="cb79-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-6" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>)) {</span>
<span id="cb79-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-7" tabindex="-1"></a>Â  transition_model[s, <span class="dv">1</span>, s <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb79-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-8" tabindex="-1"></a>Â  transition_model[s, <span class="dv">1</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb79-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-9" tabindex="-1"></a>Â  transition_model[s, <span class="dv">2</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb79-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-10" tabindex="-1"></a>Â  transition_model[s, <span class="dv">2</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb79-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-11" tabindex="-1"></a>Â  <span class="cf">for</span> (s_prime <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_states) {</span>
<span id="cb79-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-12" tabindex="-1"></a>Â  Â  reward_model[s, <span class="dv">1</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">1.0</span>, <span class="fl">0.1</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb79-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-13" tabindex="-1"></a>Â  Â  reward_model[s, <span class="dv">2</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">0.5</span>, <span class="fl">0.05</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb79-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-14" tabindex="-1"></a>Â  }</span>
<span id="cb79-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-15" tabindex="-1"></a>}</span>
<span id="cb79-16"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-16" tabindex="-1"></a></span>
<span id="cb79-17"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-17" tabindex="-1"></a>transition_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb79-18"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb79-18" tabindex="-1"></a>reward_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span></code></pre></div>
<p>This chunk builds the ârulesâ of the environment, which the agent will interact with.</p>
<ul>
<li><code>set.seed(42)</code>: Ensures that the random elements (like which state to jump to) are the same every time the code is run, making the results reproducible.</li>
<li><code>transition_model</code> &amp; <code>reward_model</code>: These 3D arrays define the environment.
<ul>
<li><code>transition_model[s, a, s_prime]</code> stores the <em>probability</em> of moving to <code>s_prime</code> (next state) if you take action <code>a</code> in state <code>s</code>.</li>
<li><code>reward_model[s, a, s_prime]</code> stores the <em>reward</em> you get for that specific transition.</li>
</ul></li>
<li><strong>The Loop</strong>: This populates the arrays.
<ul>
<li><strong>Action 1</strong>: Has a 90% chance of moving to the next sequential state (<code>s + 1</code>) and a 10% chance of moving to a random state.</li>
<li><strong>Action 2</strong>: Is highly random, with an 80% chance of moving to one random state and a 20% chance of moving to another.</li>
<li><strong>Rewards</strong>: The agent gets a large reward (1.0 or 0.5) if it lands in the terminal state (<code>s_prime == n_states</code>). Otherwise, it gets a small, random reward.</li>
</ul></li>
<li><strong>Terminal State</strong>: The last two lines ensure that if the agent is in the terminal state (10), all actions lead nowhere (<code>0</code> probability) and give no reward (<code>0</code> reward).</li>
</ul>
<p><strong>Sampling Function</strong></p>
<div class="sourceCode" id="cb80"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb80-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb80-1" tabindex="-1"></a><span class="co"># Sampling function</span></span>
<span id="cb80-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb80-2" tabindex="-1"></a>sample_env <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a) {</span>
<span id="cb80-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb80-3" tabindex="-1"></a>Â  probs <span class="ot">&lt;-</span> transition_model[s, a, ]</span>
<span id="cb80-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb80-4" tabindex="-1"></a>Â  s_prime <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>, <span class="at">prob =</span> probs)</span>
<span id="cb80-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb80-5" tabindex="-1"></a>Â  reward <span class="ot">&lt;-</span> reward_model[s, a, s_prime]</span>
<span id="cb80-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb80-6" tabindex="-1"></a>Â  <span class="fu">list</span>(<span class="at">s_prime =</span> s_prime, <span class="at">reward =</span> reward)</span>
<span id="cb80-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb80-7" tabindex="-1"></a>}</span></code></pre></div>
<p>This function acts as the <strong>simulator</strong>. The Q-learning agent doesnât get to see the full <code>transition_model</code>. Instead, it just gives a state (<code>s</code>) and an action (<code>a</code>) to this function. The function then:</p>
<ol style="list-style-type: decimal">
<li>Looks up the probabilities for all possible next states (<code>probs</code>).</li>
<li>Uses <code>sample()</code> to pick one next state (<code>s_prime</code>) based on those probabilities.</li>
<li>Finds the reward for that <em>actual</em> transition.</li>
<li>Returns the <code>s_prime</code> and <code>reward</code> to the agent.</li>
</ol>
<p><strong>Feature Encoding</strong></p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb81-1" tabindex="-1"></a><span class="co"># Feature encoding for Random Forest</span></span>
<span id="cb81-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb81-2" tabindex="-1"></a>encode_features <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a, n_states, n_actions) {</span>
<span id="cb81-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb81-3" tabindex="-1"></a>Â  state_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb81-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb81-4" tabindex="-1"></a>Â  action_vec <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_actions)</span>
<span id="cb81-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb81-5" tabindex="-1"></a>Â  state_vec[s] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb81-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb81-6" tabindex="-1"></a>Â  action_vec[a] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb81-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb81-7" tabindex="-1"></a>Â  <span class="fu">return</span>(<span class="fu">c</span>(state_vec, action_vec))</span>
<span id="cb81-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb81-8" tabindex="-1"></a>}</span>
<span id="cb81-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb81-9" tabindex="-1"></a></span>
<span id="cb81-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb81-10" tabindex="-1"></a>n_features <span class="ot">&lt;-</span> n_states <span class="sc">+</span> n_actions</span></code></pre></div>
<p>This is a critical step for using a Random Forest. The model doesnât understand âstate 5â as a concept. It needs numerical features.</p>
<ul>
<li>This function performs <strong>one-hot encoding</strong>.</li>
<li>It creates a vector of <code>n_states</code> (10) zeros and puts a <code>1</code> at the position corresponding to the current state <code>s</code>. (e.g., state 3 becomes <code>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0]</code>).</li>
<li>It does the same for the action <code>a</code>. (e.g., action 2 becomes <code>[0, 1]</code>).</li>
<li>It combines them into a single feature vector of length 12 (10+2). This vector is what the Random Forest will use as input.</li>
</ul>
<p><strong>Q-Learning Function Definition</strong></p>
<div class="sourceCode" id="cb82"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb82-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb82-1" tabindex="-1"></a><span class="co"># Q-Learning with Random Forest function approximation</span></span>
<span id="cb82-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb82-2" tabindex="-1"></a>q_learning_rf <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>, <span class="at">retrain_freq =</span> <span class="dv">10</span>, <span class="at">min_samples =</span> <span class="dv">50</span>) {</span>
<span id="cb82-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb82-3" tabindex="-1"></a>  <span class="co"># ... (function body) ...</span></span>
<span id="cb82-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb82-4" tabindex="-1"></a>}</span></code></pre></div>
<p>This defines the main function that runs the entire learning process.</p>
<ul>
<li><code>episodes = 1000</code>: The agent will run 1000 full âgamesâ or âattempts.â</li>
<li><code>epsilon = 0.1</code>: This controls the <strong>epsilon-greedy</strong> strategy. 10% of the time, the agent will choose a <em>random</em> action (<strong>exploration</strong>). 90% of the time, it will choose the action it <em>thinks</em> is best (<strong>exploitation</strong>).</li>
<li><code>retrain_freq = 10</code>: Training a Random Forest is computationally more expensive than updating a table cell. So, the model is only re-trained every 10 episodes.</li>
<li><code>min_samples = 50</code>: The model wonât be trained at all until at least 50 (state, action) experiences have been collected.</li>
</ul>
<p><strong>Inside <code>q_learning_rf</code>: Initialization</strong></p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb83-1" tabindex="-1"></a>Â  <span class="co"># Initialize training data storage</span></span>
<span id="cb83-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb83-2" tabindex="-1"></a>Â  rf_data_x <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="at">nrow =</span> <span class="dv">0</span>, <span class="at">ncol =</span> n_features)</span>
<span id="cb83-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb83-3" tabindex="-1"></a>Â  rf_data_y <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="dv">0</span>)</span>
<span id="cb83-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb83-4" tabindex="-1"></a>Â  rf_model <span class="ot">&lt;-</span> <span class="cn">NULL</span></span>
<span id="cb83-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb83-5" tabindex="-1"></a>Â  rewards <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span></code></pre></div>
<p>Inside the function, these variables are set up to store data during learning.</p>
<ul>
<li><code>rf_data_x</code>: An empty matrix that will store all the feature vectors (from <code>encode_features</code>). This is the âXâ or input data for the model.</li>
<li><code>rf_data_y</code>: An empty vector that will store the âtargetâ Q-values. This is the âYâ or output data the model tries to predict.</li>
<li><code>rf_model</code>: This variable will hold the actual Random Forest model object. It starts as <code>NULL</code> (empty).</li>
<li><code>rewards</code>: A vector to store the total reward from each of the 1000 episodes, used for plotting the learning curve.</li>
</ul>
<p><strong>Inside <code>q_learning_rf</code>: Main Loop (Episodes)</strong></p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb84-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-1" tabindex="-1"></a>Â  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb84-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-2" tabindex="-1"></a>Â  Â  s <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>), <span class="dv">1</span>)Â  <span class="co"># Start from non-terminal state</span></span>
<span id="cb84-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-3" tabindex="-1"></a>Â  Â  episode_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb84-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-4" tabindex="-1"></a>Â  Â Â </span>
<span id="cb84-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-5" tabindex="-1"></a>Â  Â  <span class="cf">while</span> (<span class="cn">TRUE</span>) {</span>
<span id="cb84-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-6" tabindex="-1"></a>      <span class="co"># ... (logic for one step) ...</span></span>
<span id="cb84-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-7" tabindex="-1"></a>Â  Â  Â  <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) <span class="cf">break</span></span>
<span id="cb84-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-8" tabindex="-1"></a>Â  Â  Â  s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb84-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-9" tabindex="-1"></a>Â  Â  }</span>
<span id="cb84-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-10" tabindex="-1"></a>Â  Â Â </span>
<span id="cb84-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-11" tabindex="-1"></a>Â  Â  rewards[ep] <span class="ot">&lt;-</span> episode_reward</span>
<span id="cb84-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb84-12" tabindex="-1"></a>Â  }</span></code></pre></div>
<p>This is the main logic.</p>
<ul>
<li>The outer <code>for</code> loop runs for 1000 episodes.</li>
<li><code>s &lt;- sample(...)</code>: At the start of each episode, the agent is placed in a random, non-terminal state.</li>
<li>The inner <code>while (TRUE)</code> loop represents a single episode. It runs step-by-step until the agent hits the terminal state.</li>
<li><code>if (s_prime == terminal_state) break</code>: When the agent reaches the end state, the <code>while</code> loop breaks, and the episode ends.</li>
<li><code>s &lt;- s_prime</code>: The agentâs new state becomes its current state for the next iteration of the <code>while</code> loop.</li>
<li><code>rewards[ep] &lt;- episode_reward</code>: After the episode ends, the total reward collected is stored.</li>
</ul>
<p><strong>Inside <code>while</code> loop: Predict Q-Values &amp; Select Action</strong></p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-1" tabindex="-1"></a>Â  Â  Â  <span class="co"># Predict Q-values for all actions</span></span>
<span id="cb85-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-2" tabindex="-1"></a>Â  Â  Â  q_preds <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb85-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-3" tabindex="-1"></a>Â  Â  Â  Â  x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb85-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-4" tabindex="-1"></a>Â  Â  Â  Â  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_model)) {</span>
<span id="cb85-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-5" tabindex="-1"></a>Â  Â  Â  Â  Â  <span class="fu">predict</span>(rf_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x)))</span>
<span id="cb85-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-6" tabindex="-1"></a>Â  Â  Â  Â  } <span class="cf">else</span> {</span>
<span id="cb85-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-7" tabindex="-1"></a>Â  Â  Â  Â  Â  <span class="fu">runif</span>(<span class="dv">1</span>)Â  <span class="co"># Random initialization</span></span>
<span id="cb85-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-8" tabindex="-1"></a>Â  Â  Â  Â  }</span>
<span id="cb85-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-9" tabindex="-1"></a>Â  Â  Â  })</span>
<span id="cb85-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-10" tabindex="-1"></a>Â  Â  Â Â </span>
<span id="cb85-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-11" tabindex="-1"></a>Â  Â  Â  <span class="co"># Epsilon-greedy action selection</span></span>
<span id="cb85-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-12" tabindex="-1"></a>Â  Â  Â  a <span class="ot">&lt;-</span> <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb85-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-13" tabindex="-1"></a>Â  Â  Â  Â  <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb85-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-14" tabindex="-1"></a>Â  Â  Â  } <span class="cf">else</span> {</span>
<span id="cb85-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-15" tabindex="-1"></a>Â  Â  Â  Â  <span class="fu">which.max</span>(q_preds)</span>
<span id="cb85-16"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb85-16" tabindex="-1"></a>Â  Â  Â  }</span></code></pre></div>
<p>This is the agentâs decision-making process.</p>
<ol style="list-style-type: decimal">
<li><strong>Predict</strong>: The agent needs to know the Q-value for all possible actions (1 and 2) from its current state <code>s</code>.
<ul>
<li><code>sapply</code> runs the prediction for each action.</li>
<li>If the <code>rf_model</code> exists, it uses <code>predict()</code> to get the Q-value.</li>
<li>If the model hasnât been trained yet (<code>is.null</code>), it just returns a random value.</li>
<li><code>q_preds</code> becomes a vector like <code>[0.7, 0.4]</code>, the predicted values for (s, a=1) and (s, a=2).</li>
</ul></li>
<li><strong>Act (Epsilon-Greedy)</strong>:
<ul>
<li><code>if (runif(1) &lt; epsilon)</code>: If a random number is less than 0.1, the agent <strong>explores</strong> by picking a random action.</li>
<li><code>else</code>: Otherwise (90% of the time), the agent <strong>exploits</strong> by picking the action with the highest predicted Q-value (<code>which.max(q_preds)</code>).</li>
</ul></li>
</ol>
<p><strong>Inside <code>while</code> loop: Take Step &amp; Compute TD Target</strong></p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-1" tabindex="-1"></a>Â  Â  Â  <span class="co"># Take action and observe outcome</span></span>
<span id="cb86-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-2" tabindex="-1"></a>Â  Â  Â  out <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a)</span>
<span id="cb86-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-3" tabindex="-1"></a>Â  Â  Â  s_prime <span class="ot">&lt;-</span> out<span class="sc">$</span>s_prime</span>
<span id="cb86-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-4" tabindex="-1"></a>Â  Â  Â  r <span class="ot">&lt;-</span> out<span class="sc">$</span>reward</span>
<span id="cb86-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-5" tabindex="-1"></a>Â  Â  Â  episode_reward <span class="ot">&lt;-</span> episode_reward <span class="sc">+</span> r</span>
<span id="cb86-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-6" tabindex="-1"></a>Â  Â  Â Â </span>
<span id="cb86-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-7" tabindex="-1"></a>Â  Â  Â  <span class="co"># Compute TD target</span></span>
<span id="cb86-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-8" tabindex="-1"></a>Â  Â  Â  q_next <span class="ot">&lt;-</span> <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) {</span>
<span id="cb86-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-9" tabindex="-1"></a>Â  Â  Â  Â  <span class="dv">0</span></span>
<span id="cb86-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-10" tabindex="-1"></a>Â  Â  Â  } <span class="cf">else</span> {</span>
<span id="cb86-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-11" tabindex="-1"></a>Â  Â  Â  Â  <span class="fu">max</span>(<span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a_) {</span>
<span id="cb86-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-12" tabindex="-1"></a>Â  Â  Â  Â  Â  x_next <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s_prime, a_, n_states, n_actions)</span>
<span id="cb86-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-13" tabindex="-1"></a>Â  Â  Â  Â  Â  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_model)) {</span>
<span id="cb86-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-14" tabindex="-1"></a>Â  Â  Â  Â  Â  Â  <span class="fu">predict</span>(rf_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x_next)))</span>
<span id="cb86-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-15" tabindex="-1"></a>Â  Â  Â  Â  Â  } <span class="cf">else</span> {</span>
<span id="cb86-16"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-16" tabindex="-1"></a>Â  Â  Â  Â  Â  Â  <span class="dv">0</span></span>
<span id="cb86-17"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-17" tabindex="-1"></a>Â  Â  Â  Â  Â  }</span>
<span id="cb86-18"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-18" tabindex="-1"></a>Â  Â  Â  Â  }))</span>
<span id="cb86-19"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-19" tabindex="-1"></a>Â  Â  Â  }</span>
<span id="cb86-20"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-20" tabindex="-1"></a>Â  Â  Â Â </span>
<span id="cb86-21"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb86-21" tabindex="-1"></a>Â  Â  Â  target <span class="ot">&lt;-</span> r <span class="sc">+</span> gamma <span class="sc">*</span> q_next</span></code></pre></div>
<p>This is the core âlearningâ part of Q-Learning.</p>
<ol style="list-style-type: decimal">
<li><strong>Take Step</strong>: The agent calls <code>sample_env()</code> with its chosen action <code>a</code>. It receives the next state <code>s_prime</code> and the reward <code>r</code>.</li>
<li><strong>Compute TD Target</strong>: The agent calculates what the Q-value for <code>(s, a)</code> <em>should have been</em>. This is the <strong>TD Target</strong>.
<ul>
<li>First, it finds the best possible Q-value it can get from the <em>next state</em> <code>s_prime</code>. This is <code>q_next</code>.</li>
<li><code>q_next</code> is 0 if <code>s_prime</code> is the end.</li>
<li>Otherwise, itâs the <code>max()</code> value the RF model predicts for <code>s_prime</code> (checking all possible next actions <code>a_</code>). This is the <span class="math inline">\(\max_{a&#39;} Q(s&#39;, a&#39;)\)</span> part of the Bellman equation.</li>
<li><strong><code>target &lt;- r + gamma * q_next</code></strong>: The target is the immediate reward <code>r</code> plus the discounted value of the best future reward (<code>gamma * q_next</code>). This is the <span class="math inline">\(r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;)\)</span> value.</li>
</ul></li>
</ol>
<p><strong>Inside <code>while</code> loop: Store Data &amp; Retrain Model</strong></p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb87-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-1" tabindex="-1"></a>Â  Â  Â  <span class="co"># Store training example</span></span>
<span id="cb87-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-2" tabindex="-1"></a>Â  Â  Â  x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb87-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-3" tabindex="-1"></a>Â  Â  Â  rf_data_x <span class="ot">&lt;-</span> <span class="fu">rbind</span>(rf_data_x, x)</span>
<span id="cb87-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-4" tabindex="-1"></a>Â  Â  Â  rf_data_y <span class="ot">&lt;-</span> <span class="fu">c</span>(rf_data_y, target)</span>
<span id="cb87-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-5" tabindex="-1"></a>Â  Â  Â Â </span>
<span id="cb87-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-6" tabindex="-1"></a>Â  Â  Â  <span class="co"># Retrain Random Forest periodically</span></span>
<span id="cb87-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-7" tabindex="-1"></a>Â  Â  Â  <span class="cf">if</span> (<span class="fu">nrow</span>(rf_data_x) <span class="sc">&gt;=</span> min_samples <span class="sc">&amp;&amp;</span> ep <span class="sc">%%</span> retrain_freq <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb87-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-8" tabindex="-1"></a>Â  Â  Â  Â  rf_model <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(</span>
<span id="cb87-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-9" tabindex="-1"></a>Â  Â  Â  Â  Â  <span class="at">x =</span> <span class="fu">as.data.frame</span>(rf_data_x),</span>
<span id="cb87-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-10" tabindex="-1"></a>Â  Â  Â  Â  Â  <span class="at">y =</span> rf_data_y,</span>
<span id="cb87-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-11" tabindex="-1"></a>Â  Â  Â  Â  Â  <span class="at">ntree =</span> <span class="dv">100</span>,</span>
<span id="cb87-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-12" tabindex="-1"></a>Â  Â  Â  Â  Â  <span class="at">nodesize =</span> <span class="dv">5</span>,</span>
<span id="cb87-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-13" tabindex="-1"></a>Â  Â  Â  Â  Â  <span class="at">mtry =</span> <span class="fu">max</span>(<span class="dv">1</span>, <span class="fu">floor</span>(n_features <span class="sc">/</span> <span class="dv">3</span>))</span>
<span id="cb87-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-14" tabindex="-1"></a>Â  Â  Â  Â  )</span>
<span id="cb87-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb87-15" tabindex="-1"></a>Â  Â  Â  }</span></code></pre></div>
<ol style="list-style-type: decimal">
<li><strong>Store Data</strong>: The agent now has a complete training sample:
<ul>
<li><strong>Input (X)</strong>: The encoded state-action pair <code>x</code> that it <em>took</em>.</li>
<li><strong>Output (Y)</strong>: The <code>target</code> value it just calculated.</li>
<li>It adds this <code>(x, target)</code> pair to its training dataset (<code>rf_data_x</code>, <code>rf_data_y</code>).</li>
</ul></li>
<li><strong>Retrain</strong>: It checks if itâs time to re-train the model.
<ul>
<li>If it has enough samples (<code>&gt;= min_samples</code>) AND the episode number is a multiple of <code>retrain_freq</code> (e.g., 10, 20, 30â¦).</li>
<li>It trains a <em>new</em> <code>randomForest</code> model using <em>all</em> the data it has collected so far. This new <code>rf_model</code> will be used for predictions in the next episodes, replacing the old one.</li>
</ul></li>
</ol>
<p><strong>Inside <code>q_learning_rf</code>: Derive Policy &amp; Return</strong></p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-1" tabindex="-1"></a>Â  <span class="co"># Derive final policy</span></span>
<span id="cb88-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-2" tabindex="-1"></a>Â  policy <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), <span class="cf">function</span>(s) {</span>
<span id="cb88-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-3" tabindex="-1"></a>Â  Â  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_model)) {</span>
<span id="cb88-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-4" tabindex="-1"></a>Â  Â  Â  q_vals <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="cf">function</span>(a) {</span>
<span id="cb88-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-5" tabindex="-1"></a>Â  Â  Â  Â  x <span class="ot">&lt;-</span> <span class="fu">encode_features</span>(s, a, n_states, n_actions)</span>
<span id="cb88-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-6" tabindex="-1"></a>Â  Â  Â  Â  <span class="fu">predict</span>(rf_model, <span class="fu">as.data.frame</span>(<span class="fu">t</span>(x)))</span>
<span id="cb88-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-7" tabindex="-1"></a>Â  Â  Â  })</span>
<span id="cb88-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-8" tabindex="-1"></a>Â  Â  Â  <span class="fu">which.max</span>(q_vals)</span>
<span id="cb88-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-9" tabindex="-1"></a>Â  Â  } <span class="cf">else</span> {</span>
<span id="cb88-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-10" tabindex="-1"></a>Â  Â  Â  <span class="dv">1</span>Â  <span class="co"># Default action</span></span>
<span id="cb88-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-11" tabindex="-1"></a>Â  Â  }</span>
<span id="cb88-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-12" tabindex="-1"></a>Â  })</span>
<span id="cb88-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-13" tabindex="-1"></a>Â Â </span>
<span id="cb88-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-14" tabindex="-1"></a>Â  <span class="fu">list</span>(<span class="at">model =</span> rf_model, <span class="at">policy =</span> <span class="fu">c</span>(policy, <span class="cn">NA</span>), <span class="at">rewards =</span> rewards,Â </span>
<span id="cb88-15"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb88-15" tabindex="-1"></a>Â  Â  Â  Â <span class="at">training_data =</span> <span class="fu">list</span>(<span class="at">x =</span> rf_data_x, <span class="at">y =</span> rf_data_y))</span></code></pre></div>
<p>After the <code>for</code> loop (all 1000 episodes) is finished:</p>
<ol style="list-style-type: decimal">
<li><strong>Derive Policy</strong>: The code extracts the final âbestâ policy.
<ul>
<li>It loops through every state <code>s</code> (from 1 to 9).</li>
<li>For each state, it asks the <em>final</em> <code>rf_model</code> to predict the Q-values for both actions.</li>
<li>It saves the action that gives the highest Q-value (<code>which.max(q_vals)</code>).</li>
<li>This <code>policy</code> vector is the agentâs learned ârulebook.â</li>
</ul></li>
<li><strong>Return</strong>: The function returns a <code>list</code> containing all the important results: the final model, the policy, the history of rewards, and the full training dataset.</li>
</ol>
<p><strong>Run Algorithm &amp; Create Policy Plot</strong></p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb89-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-1" tabindex="-1"></a><span class="co"># Run Q-Learning with Random Forest approximation</span></span>
<span id="cb89-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb89-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-3" tabindex="-1"></a>rf_result <span class="ot">&lt;-</span> <span class="fu">q_learning_rf</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>, <span class="at">retrain_freq =</span> <span class="dv">10</span>)</span>
<span id="cb89-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-4" tabindex="-1"></a>rf_policy <span class="ot">&lt;-</span> rf_result<span class="sc">$</span>policy</span>
<span id="cb89-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-5" tabindex="-1"></a>rf_rewards <span class="ot">&lt;-</span> rf_result<span class="sc">$</span>rewards</span>
<span id="cb89-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-6" tabindex="-1"></a></span>
<span id="cb89-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-7" tabindex="-1"></a><span class="co"># Create policy visualization</span></span>
<span id="cb89-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-8" tabindex="-1"></a>policy_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(...)</span>
<span id="cb89-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-9" tabindex="-1"></a>policy_plot_rf <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(policy_df[<span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>), ], <span class="fu">aes</span>(<span class="at">x =</span> State, <span class="at">y =</span> Policy)) <span class="sc">+</span></span>
<span id="cb89-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-10" tabindex="-1"></a>Â  <span class="co"># ... ggplot settings ...</span></span>
<span id="cb89-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-11" tabindex="-1"></a>Â  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Policy from Q-Learning with Random Forest Approximation&quot;</span>)</span>
<span id="cb89-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-12" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb89-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb89-13" tabindex="-1"></a><span class="fu">print</span>(policy_plot_rf)</span></code></pre></div>
<ul>
<li>The code calls the <code>q_learning_rf</code> function to run the simulation and stores the output in <code>rf_result</code>.</li>
<li>It then extracts the <code>policy</code> and <code>rewards</code> for plotting.</li>
<li>The <code>ggplot</code> code builds a plot to visualize the policy, showing the preferred action (1 or 2) for each state (1-9).</li>
</ul>
<p><strong>Create Reward Plot (Learning Curve)</strong></p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-1" tabindex="-1"></a><span class="co"># Compare cumulative rewards with moving average</span></span>
<span id="cb90-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-2" tabindex="-1"></a>rewards_smooth <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(rf_rewards))</span>
<span id="cb90-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-3" tabindex="-1"></a>window_size <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb90-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-4" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(rf_rewards)) {</span>
<span id="cb90-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-5" tabindex="-1"></a>Â  start_idx <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="dv">1</span>, i <span class="sc">-</span> window_size <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb90-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-6" tabindex="-1"></a>Â  rewards_smooth[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(rf_rewards[start_idx<span class="sc">:</span>i])</span>
<span id="cb90-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-7" tabindex="-1"></a>}</span>
<span id="cb90-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-8" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb90-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-9" tabindex="-1"></a>reward_plot_rf <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(reward_df_rf, <span class="fu">aes</span>(<span class="at">x =</span> Episode, <span class="at">y =</span> Reward)) <span class="sc">+</span></span>
<span id="cb90-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-10" tabindex="-1"></a>Â  <span class="co"># ... ggplot settings ...</span></span>
<span id="cb90-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-11" tabindex="-1"></a>Â  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Learning Curve: Q-Learning with Random Forest (50-episode moving average)&quot;</span>)</span>
<span id="cb90-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-12" tabindex="-1"></a><span class="co"># ...</span></span>
<span id="cb90-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb90-13" tabindex="-1"></a><span class="fu">print</span>(reward_plot_rf)</span></code></pre></div>
<ul>
<li>The raw reward-per-episode data (<code>rf_rewards</code>) can be very ânoisyâ (it jumps around a lot).</li>
<li>This chunk calculates a <strong>moving average</strong> (with a <code>window_size</code> of 50) to smooth out the data and show the agentâs learning <em>trend</em>.</li>
<li>The <code>ggplot</code> code then plots this smoothed reward over the 1000 episodes. An upward-sloping line indicates the agent successfully learned to get more rewards over time.</li>
</ul>
<p><strong>Feature Importance Analysis</strong></p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-1" tabindex="-1"></a><span class="co"># Feature importance analysis</span></span>
<span id="cb91-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-2" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(rf_result<span class="sc">$</span>model)) {</span>
<span id="cb91-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-3" tabindex="-1"></a>Â  importance_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb91-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-4" tabindex="-1"></a>Â  Â  <span class="at">Feature =</span> <span class="fu">c</span>(<span class="fu">paste</span>(<span class="st">&quot;State&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_states), <span class="fu">paste</span>(<span class="st">&quot;Action&quot;</span>, <span class="dv">1</span><span class="sc">:</span>n_actions)),</span>
<span id="cb91-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-5" tabindex="-1"></a>Â  Â  <span class="at">Importance =</span> <span class="fu">importance</span>(rf_result<span class="sc">$</span>model)[, <span class="dv">1</span>]</span>
<span id="cb91-6"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-6" tabindex="-1"></a>Â  )</span>
<span id="cb91-7"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-7" tabindex="-1"></a>Â Â </span>
<span id="cb91-8"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-8" tabindex="-1"></a>Â  importance_plot <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(importance_df, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Feature, Importance), <span class="at">y =</span> Importance)) <span class="sc">+</span></span>
<span id="cb91-9"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-9" tabindex="-1"></a>Â  Â  <span class="fu">geom_col</span>(<span class="at">fill =</span> <span class="st">&quot;forestgreen&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb91-10"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-10" tabindex="-1"></a>Â  Â  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb91-11"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-11" tabindex="-1"></a>Â  Â  <span class="co"># ... ggplot settings ...</span></span>
<span id="cb91-12"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-12" tabindex="-1"></a>Â  Â  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Feature Importance in Random Forest Q-Function&quot;</span>)</span>
<span id="cb91-13"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-13" tabindex="-1"></a>Â  <span class="fu">print</span>(importance_plot)</span>
<span id="cb91-14"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb91-14" tabindex="-1"></a>}</span></code></pre></div>
<ul>
<li>This is an advantage of using a Random Forest. We can inspect the trained model to see which features it found most <em>important</em> for predicting the Q-value.</li>
<li><code>importance(rf_result$model)</code> extracts these scores.</li>
<li>The code then creates a horizontal bar chart (<code>geom_col</code> + <code>coord_flip</code>) to display the importance of each feature (e.g., âState 9â, âState 1â, âAction 2â, etc.). This can provide insight into the environment, for example, by showing that being in states near the goal (State 10) is very important.</li>
</ul>
<p><strong>Model Diagnostics</strong></p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb92-1"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb92-1" tabindex="-1"></a><span class="co"># Model diagnostics</span></span>
<span id="cb92-2"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb92-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Random Forest Model Summary:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb92-3"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb92-3" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Number of trees:&quot;</span>, rf_result<span class="sc">$</span>model<span class="sc">$</span>ntree, <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb92-4"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb92-4" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Training examples:&quot;</span>, <span class="fu">nrow</span>(rf_result<span class="sc">$</span>training_data<span class="sc">$</span>x), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb92-5"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#cb92-5" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Final OOB error:&quot;</span>, <span class="fu">tail</span>(rf_result<span class="sc">$</span>model<span class="sc">$</span>mse, <span class="dv">1</span>), <span class="st">&quot;</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<ul>
<li>This final chunk prints some summary statistics about the final <code>rf_model</code>.</li>
<li><code>ntree</code>: The number of trees in the forest (100).</li>
<li><code>Training examples</code>: The total number of (s, a) steps taken across all 1000 episodes.</li>
<li><code>Final OOB error</code>: (Out-of-Bag Mean Squared Error). This is an internal metric from the Random Forest that estimates its prediction error. A lower value is better.</li>
</ul>
</div>
<div id="analysis-and-insights" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Analysis and Insights<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="policy-learning-characteristics" class="section level3 hasAnchor" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> Policy Learning Characteristics<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#policy-learning-characteristics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random Forest function approximation exhibits several characteristics that distinguish it from linear methods. Trees can capture non-linear decision boundaries, enabling the model to learn state-action relationships that linear approaches cannot represent. The random feature sampling at each split performs automatic feature selection, focusing computational resources on the most informative variables. Ensemble averaging across multiple trees reduces overfitting and provides stable predictions across different training samples. Individual trees maintain interpretable decision paths that show how Q-values are estimated for specific state-action pairs.</p>
</div>
<div id="computational-considerations" class="section level3 hasAnchor" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> Computational Considerations<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#computational-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The batch retraining approach creates distinct computational trade-offs that affect implementation decisions. Training frequency must balance responsiveness against computational cost, as more frequent updates improve adaptation but require additional processing time. Trees need sufficient data to learn meaningful patterns, which can slow initial learning compared to methods that update continuously. Memory requirements increase over time as training examples accumulate, requiring careful management of historical data.</p>
</div>
<div id="feature-importance-insights" class="section level3 hasAnchor" number="7.3.3">
<h3><span class="header-section-number">7.3.3</span> Feature Importance Insights<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-importance-insights" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random Forest methods naturally generate feature importance measures that reveal which states and actions most influence Q-value predictions. This interpretability provides diagnostic capabilities for understanding learning issues and analyzing policy decisions. The feature ranking can guide state representation choices and help identify redundant or irrelevant variables in the problem formulation.</p>
</div>
<div id="practical-implications-1" class="section level3 hasAnchor" number="7.3.4">
<h3><span class="header-section-number">7.3.4</span> Practical Implications<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#practical-implications-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random Forest function approximation occupies a position between simple linear models and neural networks in terms of complexity and capability. The method handles larger state spaces more effectively than tabular approaches while remaining computationally tractable. It captures non-linear patterns without requiring extensive feature engineering or domain expertise. The approach shows less sensitivity to hyperparameter choices compared to neural networks while maintaining stability across different problem instances. The inherent interpretability provides insights into the decision-making process that can be valuable for debugging and analysis.</p>
</div>
</div>
<div id="comparison-with-linear-approximation" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Comparison with Linear Approximation<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-linear-approximation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random Forest methods demonstrate several advantages and trade-offs when compared to linear function approximation. The tree-based approach excels at pattern recognition, learning state-action relationships that linear models cannot capture due to their representational limitations. However, initial learning proceeds more slowly as trees require sufficient data to construct meaningful decision boundaries. Computational costs are higher due to periodic retraining requirements, contrasting with the continuous gradient updates used in linear methods. Generalization performance tends to be superior, as ensemble averaging provides natural regularization that reduces overfitting tendencies.</p>
</div>
<div id="conclusion-4" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Conclusion<a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random Forest function approximation extends linear methods by offering enhanced modeling flexibility while preserving interpretability characteristics. The approach performs particularly well in environments with non-linear state-action relationships and provides regularization through ensemble averaging.</p>
<p>Several key observations emerge from this analysis. Non-linear function approximation can capture patterns that linear models miss, enabling better policy learning in complex environments. Batch learning approaches require careful consideration of training frequency and sample requirements to balance performance with computational efficiency. Feature importance analysis provides insights into learned policies that can guide problem formulation and debugging efforts. Tree-based methods offer an interpretable alternative to neural network approaches while maintaining theoretical foundations.</p>
<p>This exploration demonstrates how ensemble methods can enhance reinforcement learning without abandoning the established principles of Q-Learning. Future work could investigate online tree learning algorithms that avoid batch retraining requirements, adaptive schedules that optimize training frequency based on performance metrics, or hybrid approaches that combine strengths from different function approximation methods.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="function-approximation-q-learning-with-linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/edit/main/07-Q_FA_RF.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/blob/main/07-Q_FA_RF.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
