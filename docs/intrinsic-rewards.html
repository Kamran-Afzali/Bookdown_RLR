<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 15 Intrinsic Rewards | Reinforcement Learning in R</title>
  <meta name="description" content="Chapter 15 Intrinsic Rewards | Reinforcement Learning in R" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 15 Intrinsic Rewards | Reinforcement Learning in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 15 Intrinsic Rewards | Reinforcement Learning in R" />
  
  
  

<meta name="author" content="Kamran Afzalui" />


<meta name="date" content="2025-10-27" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="eligibility-traces.html"/>
<link rel="next" href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Reinforcement Learning in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Understanding Reinforcement Learning: From Bandits to Policy Optimization</strong></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-multi-armed-bandit-the-simplest-case"><i class="fa fa-check"></i><b>1.2</b> The Multi-Armed Bandit: The Simplest Case</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#transition-to-markov-decision-processes"><i class="fa fa-check"></i><b>1.3</b> Transition to Markov Decision Processes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#comparing-reinforcement-learning-methods"><i class="fa fa-check"></i><b>1.4</b> Comparing Reinforcement Learning Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#dynamic-programming-model-based-learning"><i class="fa fa-check"></i><b>1.4.1</b> Dynamic Programming: Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#model-free-approaches-monte-carlo-and-td-learning"><i class="fa fa-check"></i><b>1.4.2</b> Model-Free Approaches: Monte Carlo and TD Learning</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#dyna-bridging-model-free-and-model-based-learning"><i class="fa fa-check"></i><b>1.4.3</b> Dyna: Bridging Model-Free and Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#q-learning-and-function-approximation"><i class="fa fa-check"></i><b>1.4.4</b> Q-Learning and Function Approximation</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#policy-gradient-and-actor-critic-methods"><i class="fa fa-check"></i><b>1.4.5</b> Policy Gradient and Actor-Critic Methods</a></li>
<li class="chapter" data-level="1.4.6" data-path="index.html"><a href="index.html#advanced-policy-optimization-techniques"><i class="fa fa-check"></i><b>1.4.6</b> Advanced Policy Optimization Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#further-directions"><i class="fa fa-check"></i><b>1.5</b> Further Directions</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html"><i class="fa fa-check"></i><b>2</b> The Multi-Armed Bandit Problem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#mathematical-formalism"><i class="fa fa-check"></i><b>2.2</b> Mathematical Formalism</a></li>
<li class="chapter" data-level="2.3" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm"><i class="fa fa-check"></i><b>2.3</b> Frequentist Approach: UCB1 Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1"><i class="fa fa-check"></i><b>2.3.1</b> R Code for UCB1</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling"><i class="fa fa-check"></i><b>2.4</b> Bayesian Approach: Thompson Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling"><i class="fa fa-check"></i><b>2.4.1</b> R Code for Thompson Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy"><i class="fa fa-check"></i><b>2.5</b> Epsilon-Greedy Strategy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy"><i class="fa fa-check"></i><b>2.5.1</b> R Code for Epsilon-Greedy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#summary-table"><i class="fa fa-check"></i><b>2.6</b> Summary Table</a></li>
<li class="chapter" data-level="2.7" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html"><i class="fa fa-check"></i><b>3</b> Markov Decision Processes and Dynamic Programming</a>
<ul>
<li class="chapter" data-level="3.1" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#constructing-the-mdp-in-r"><i class="fa fa-check"></i><b>3.2</b> Constructing the MDP in R</a></li>
<li class="chapter" data-level="3.3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#value-iteration-algorithm"><i class="fa fa-check"></i><b>3.3</b> Value Iteration Algorithm</a></li>
<li class="chapter" data-level="3.4" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#evaluation-and-interpretation"><i class="fa fa-check"></i><b>3.4</b> Evaluation and Interpretation</a></li>
<li class="chapter" data-level="3.5" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties-of-value-iteration"><i class="fa fa-check"></i><b>3.5</b> Theoretical Properties of Value Iteration</a></li>
<li class="chapter" data-level="3.6" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-1"><i class="fa fa-check"></i><b>3.6</b> Summary Table</a></li>
<li class="chapter" data-level="3.7" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><i class="fa fa-check"></i><b>4</b> Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#theoretical-background"><i class="fa fa-check"></i><b>4.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#temporal-difference-learning-q-learning"><i class="fa fa-check"></i><b>4.2.1</b> Temporal Difference Learning (Q-Learning)</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#monte-carlo-methods"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.2.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-1-defining-the-environment-in-r"><i class="fa fa-check"></i><b>4.2.3</b> Step 1: Defining the Environment in R</a></li>
<li class="chapter" data-level="4.2.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-2-q-learning-implementation-in-r"><i class="fa fa-check"></i><b>4.2.4</b> Step 2: Q-Learning Implementation in R</a></li>
<li class="chapter" data-level="4.2.5" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-3-monte-carlo-every-visit-implementation"><i class="fa fa-check"></i><b>4.2.5</b> Step 3: Monte Carlo Every-Visit Implementation</a></li>
<li class="chapter" data-level="4.2.6" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-4-simulating-outcome-devaluation"><i class="fa fa-check"></i><b>4.2.6</b> Step 4: Simulating Outcome Devaluation</a></li>
<li class="chapter" data-level="4.2.7" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-5-comparing-policies-before-and-after-devaluation"><i class="fa fa-check"></i><b>4.2.7</b> Step 5: Comparing Policies Before and After Devaluation</a></li>
<li class="chapter" data-level="4.2.8" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-6-visualizing-the-policies"><i class="fa fa-check"></i><b>4.2.8</b> Step 6: Visualizing the Policies</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#interpretation-and-discussion"><i class="fa fa-check"></i><b>4.3</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="4.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#conclusion-2"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><i class="fa fa-check"></i><b>5</b> On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-on-policy"><i class="fa fa-check"></i><b>5.2</b> SARSA (On-Policy)</a></li>
<li class="chapter" data-level="5.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-off-policy"><i class="fa fa-check"></i><b>5.3</b> Q-Learning (Off-Policy)</a></li>
<li class="chapter" data-level="5.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-with-importance-sampling"><i class="fa fa-check"></i><b>5.4</b> Off-Policy Monte Carlo with Importance Sampling</a></li>
<li class="chapter" data-level="5.5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#key-differences"><i class="fa fa-check"></i><b>5.5</b> Key Differences</a></li>
<li class="chapter" data-level="5.6" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#interpretation-and-discussion-1"><i class="fa fa-check"></i><b>5.6</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#conclusion-3"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
<li class="chapter" data-level="5.8" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#comparison-table"><i class="fa fa-check"></i><b>5.8</b> Comparison Table</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html"><i class="fa fa-check"></i><b>6</b> Function Approximation Q-Learning with Linear Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#q-learning-with-function-approximation"><i class="fa fa-check"></i><b>6.1.1</b> Q-Learning with Function Approximation</a></li>
<li class="chapter" data-level="6.1.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#comparison-with-tabular-q-learning"><i class="fa fa-check"></i><b>6.1.2</b> Comparison with Tabular Q-Learning</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#r-implementation"><i class="fa fa-check"></i><b>6.2</b> R Implementation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><i class="fa fa-check"></i><b>7</b> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation"><i class="fa fa-check"></i><b>7.1.1</b> Q-Learning with Random Forest Approximation</a></li>
<li class="chapter" data-level="7.1.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods"><i class="fa fa-check"></i><b>7.1.2</b> Comparison with Previous Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1"><i class="fa fa-check"></i><b>7.2</b> R Implementation</a></li>
<li class="chapter" data-level="7.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights"><i class="fa fa-check"></i><b>7.3</b> Analysis and Insights</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#policy-learning-characteristics"><i class="fa fa-check"></i><b>7.3.1</b> Policy Learning Characteristics</a></li>
<li class="chapter" data-level="7.3.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#computational-considerations"><i class="fa fa-check"></i><b>7.3.2</b> Computational Considerations</a></li>
<li class="chapter" data-level="7.3.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-importance-insights"><i class="fa fa-check"></i><b>7.3.3</b> Feature Importance Insights</a></li>
<li class="chapter" data-level="7.3.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#practical-implications-1"><i class="fa fa-check"></i><b>7.3.4</b> Practical Implications</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-linear-approximation"><i class="fa fa-check"></i><b>7.4</b> Comparison with Linear Approximation</a></li>
<li class="chapter" data-level="7.5" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><i class="fa fa-check"></i><b>8</b> Deep Function Approximation: Q-Learning with Neural Networks in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation"><i class="fa fa-check"></i><b>8.2</b> Theoretical Foundation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity"><i class="fa fa-check"></i><b>8.2.1</b> Universal Approximation and Expressivity</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning"><i class="fa fa-check"></i><b>8.2.2</b> Gradient-Based Learning</a></li>
<li class="chapter" data-level="8.2.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches"><i class="fa fa-check"></i><b>8.2.3</b> Comparison with Previous Approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2"><i class="fa fa-check"></i><b>8.3</b> R Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation"><i class="fa fa-check"></i><b>8.4</b> Analysis and Interpretation</a></li>
<li class="chapter" data-level="8.5" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions"><i class="fa fa-check"></i><b>8.5</b> Future Directions</a></li>
<li class="chapter" data-level="8.6" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5"><i class="fa fa-check"></i><b>8.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html"><i class="fa fa-check"></i><b>9</b> Dyna and DynaQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#theoretical-framework"><i class="fa fa-check"></i><b>9.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#the-dyna-architecture"><i class="fa fa-check"></i><b>9.2.1</b> The Dyna Architecture</a></li>
<li class="chapter" data-level="9.2.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#model-representation"><i class="fa fa-check"></i><b>9.2.2</b> Model Representation</a></li>
<li class="chapter" data-level="9.2.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#convergence-properties"><i class="fa fa-check"></i><b>9.2.3</b> Convergence Properties</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-in-r"><i class="fa fa-check"></i><b>9.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#environment-setup"><i class="fa fa-check"></i><b>9.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="9.3.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#dyna-q-implementation"><i class="fa fa-check"></i><b>9.3.2</b> Dyna-Q Implementation</a></li>
<li class="chapter" data-level="9.3.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#standard-q-learning-for-comparison"><i class="fa fa-check"></i><b>9.3.3</b> Standard Q-Learning for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#experimental-analysis"><i class="fa fa-check"></i><b>9.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#learning-efficiency-comparison"><i class="fa fa-check"></i><b>9.4.1</b> Learning Efficiency Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
<li class="chapter" data-level="9.6" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-considerations-and-conclusion"><i class="fa fa-check"></i><b>9.6</b> Implementation Considerations and Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><i class="fa fa-check"></i><b>10</b> Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#theoretical-framework-1"><i class="fa fa-check"></i><b>10.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#the-exploration-bonus-mechanism"><i class="fa fa-check"></i><b>10.2.1</b> The Exploration Bonus Mechanism</a></li>
<li class="chapter" data-level="10.2.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#complete-dyna-q-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> Complete Dyna-Q+ Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#convergence-and-stability"><i class="fa fa-check"></i><b>10.2.3</b> Convergence and Stability</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#implementation-in-r-1"><i class="fa fa-check"></i><b>10.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#environment-setup-1"><i class="fa fa-check"></i><b>10.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="10.3.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-implementation-1"><i class="fa fa-check"></i><b>10.3.2</b> Dyna-Q+ Implementation</a></li>
<li class="chapter" data-level="10.3.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#standard-dyna-for-comparison"><i class="fa fa-check"></i><b>10.3.3</b> Standard Dyna for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#experimental-analysis-1"><i class="fa fa-check"></i><b>10.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#adaptation-to-environmental-changes"><i class="fa fa-check"></i><b>10.4.1</b> Adaptation to Environmental Changes</a></li>
<li class="chapter" data-level="10.4.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#parameter-sensitivity-analysis"><i class="fa fa-check"></i><b>10.4.2</b> Parameter Sensitivity Analysis</a></li>
<li class="chapter" data-level="10.4.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#exploration-pattern-analysis"><i class="fa fa-check"></i><b>10.4.3</b> Exploration Pattern Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#discussion-and-implementation-considerations"><i class="fa fa-check"></i><b>10.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="10.6" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#conclusion-6"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html"><i class="fa fa-check"></i><b>11</b> Function Approximation And Feature Engineering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#feature-engineering-and-state-representation"><i class="fa fa-check"></i><b>11.1</b> Feature Engineering and State Representation</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-discrimination-vs.-generalization-tradeoff"><i class="fa fa-check"></i><b>11.1.1</b> The Discrimination vs. Generalization Tradeoff</a></li>
<li class="chapter" data-level="11.1.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#principles-of-effective-feature-design"><i class="fa fa-check"></i><b>11.1.2</b> Principles of Effective Feature Design</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#mathematical-foundations-of-linear-function-approximation"><i class="fa fa-check"></i><b>11.2</b> Mathematical Foundations of Linear Function Approximation</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#linear-value-functions-and-their-properties"><i class="fa fa-check"></i><b>11.2.1</b> Linear Value Functions and Their Properties</a></li>
<li class="chapter" data-level="11.2.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#temporal-difference-learning-with-linear-approximation"><i class="fa fa-check"></i><b>11.2.2</b> Temporal Difference Learning with Linear Approximation</a></li>
<li class="chapter" data-level="11.2.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-deadly-triad-and-stability-concerns"><i class="fa fa-check"></i><b>11.2.3</b> The Deadly Triad and Stability Concerns</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#classical-basis-function-methods"><i class="fa fa-check"></i><b>11.3</b> Classical Basis Function Methods</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#coarse-coding-overlapping-receptive-fields"><i class="fa fa-check"></i><b>11.3.1</b> Coarse Coding: Overlapping Receptive Fields</a></li>
<li class="chapter" data-level="11.3.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#tile-coding-structured-overlapping-grids"><i class="fa fa-check"></i><b>11.3.2</b> Tile Coding: Structured Overlapping Grids</a></li>
<li class="chapter" data-level="11.3.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#radial-basis-functions-smooth-continuous-features"><i class="fa fa-check"></i><b>11.3.3</b> Radial Basis Functions: Smooth Continuous Features</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#comparative-analysis-and-practical-considerations"><i class="fa fa-check"></i><b>11.4</b> Comparative Analysis and Practical Considerations</a></li>
<li class="chapter" data-level="11.5" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#bridging-classical-and-modern-approaches"><i class="fa fa-check"></i><b>11.5</b> Bridging Classical and Modern Approaches</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html"><i class="fa fa-check"></i><b>12</b> Learning Policies versus Learning Values:</a>
<ul>
<li class="chapter" data-level="12.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#the-two-paradigms-of-reinforcement-learning"><i class="fa fa-check"></i><b>12.1</b> The Two Paradigms of Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#value-based-methods-learning-worth-before-action"><i class="fa fa-check"></i><b>12.1.1</b> Value-Based Methods: Learning Worth Before Action</a></li>
<li class="chapter" data-level="12.1.2" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#policy-based-methods-direct-optimization-of-behavior"><i class="fa fa-check"></i><b>12.1.2</b> Policy-Based Methods: Direct Optimization of Behavior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><i class="fa fa-check"></i><b>13</b> Average Reward in Reinforcement Learning: A Comprehensive Guide</a>
<ul>
<li class="chapter" data-level="13.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-limitations-of-discounted-reward-formulations"><i class="fa fa-check"></i><b>13.1</b> The Limitations of Discounted Reward Formulations</a></li>
<li class="chapter" data-level="13.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-average-reward-alternative-theoretical-foundations"><i class="fa fa-check"></i><b>13.2</b> The Average Reward Alternative: Theoretical Foundations</a></li>
<li class="chapter" data-level="13.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#value-functions-in-the-average-reward-setting"><i class="fa fa-check"></i><b>13.3</b> Value Functions in the Average Reward Setting</a></li>
<li class="chapter" data-level="13.4" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-theory-for-average-reward"><i class="fa fa-check"></i><b>13.4</b> Optimality Theory for Average Reward</a></li>
<li class="chapter" data-level="13.5" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#learning-algorithms-for-average-reward"><i class="fa fa-check"></i><b>13.5</b> Learning Algorithms for Average Reward</a></li>
<li class="chapter" data-level="13.6" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#practical-implementation-server-load-balancing"><i class="fa fa-check"></i><b>13.6</b> Practical Implementation: Server Load Balancing</a></li>
<li class="chapter" data-level="13.7" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#implementation-considerations"><i class="fa fa-check"></i><b>13.7</b> Implementation Considerations</a></li>
<li class="chapter" data-level="13.8" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#when-to-choose-average-reward-over-discounting"><i class="fa fa-check"></i><b>13.8</b> When to Choose Average Reward Over Discounting</a></li>
<li class="chapter" data-level="13.9" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#appendix-a-mathematical-proofs-and-derivations"><i class="fa fa-check"></i><b>13.9</b> Appendix A: Mathematical Proofs and Derivations</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#convergence-of-average-reward-td-learning"><i class="fa fa-check"></i><b>13.9.1</b> Convergence of Average Reward TD Learning</a></li>
<li class="chapter" data-level="13.9.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#policy-gradient-theorem-for-average-reward"><i class="fa fa-check"></i><b>13.9.2</b> Policy Gradient Theorem for Average Reward</a></li>
<li class="chapter" data-level="13.9.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-equations-derivation"><i class="fa fa-check"></i><b>13.9.3</b> Optimality Equations Derivation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="eligibility-traces.html"><a href="eligibility-traces.html"><i class="fa fa-check"></i><b>14</b> Eligibility Traces</a>
<ul>
<li class="chapter" data-level="14.1" data-path="eligibility-traces.html"><a href="eligibility-traces.html#from-one-step-to-multi-step-learning"><i class="fa fa-check"></i><b>14.1</b> From One-Step to Multi-Step Learning</a></li>
<li class="chapter" data-level="14.2" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-mechanics-of-eligibility-traces-a-backward-view"><i class="fa fa-check"></i><b>14.2</b> The Mechanics of Eligibility Traces: A Backward View</a></li>
<li class="chapter" data-level="14.3" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-tdlambda-algorithm-for-prediction"><i class="fa fa-check"></i><b>14.3</b> The TD(<span class="math inline">\(\\lambda\)</span>) Algorithm for Prediction</a></li>
<li class="chapter" data-level="14.4" data-path="eligibility-traces.html"><a href="eligibility-traces.html#control-with-eligibility-traces-average-reward-sarsalambda"><i class="fa fa-check"></i><b>14.4</b> Control with Eligibility Traces: Average Reward Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.5" data-path="eligibility-traces.html"><a href="eligibility-traces.html#practical-implementation-server-load-balancing-with-sarsalambda"><i class="fa fa-check"></i><b>14.5</b> Practical Implementation: Server Load Balancing with Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.6" data-path="eligibility-traces.html"><a href="eligibility-traces.html#computational-and-performance-considerations"><i class="fa fa-check"></i><b>14.6</b> Computational and Performance Considerations</a></li>
<li class="chapter" data-level="14.7" data-path="eligibility-traces.html"><a href="eligibility-traces.html#conclusion-8"><i class="fa fa-check"></i><b>14.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html"><i class="fa fa-check"></i><b>15</b> Intrinsic Rewards</a>
<ul>
<li class="chapter" data-level="15.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#the-sparse-reward-problem-and-its-implications"><i class="fa fa-check"></i><b>15.1</b> The Sparse Reward Problem and Its Implications</a></li>
<li class="chapter" data-level="15.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#mathematical-foundations-of-intrinsic-motivation"><i class="fa fa-check"></i><b>15.2</b> Mathematical Foundations of Intrinsic Motivation</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#curiosity-driven-learning"><i class="fa fa-check"></i><b>15.2.1</b> Curiosity-Driven Learning</a></li>
<li class="chapter" data-level="15.2.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#count-based-exploration"><i class="fa fa-check"></i><b>15.2.2</b> Count-Based Exploration</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#practical-implementation-curiosity-driven-grid-world-navigation"><i class="fa fa-check"></i><b>15.3</b> Practical Implementation: Curiosity-Driven Grid World Navigation</a></li>
<li class="chapter" data-level="15.4" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#advanced-intrinsic-motivation-mechanisms"><i class="fa fa-check"></i><b>15.4</b> Advanced Intrinsic Motivation Mechanisms</a></li>
<li class="chapter" data-level="15.5" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#implementation-considerations-and-practical-guidelines"><i class="fa fa-check"></i><b>15.5</b> Implementation Considerations and Practical Guidelines</a></li>
<li class="chapter" data-level="15.6" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#theoretical-analysis-and-convergence-properties"><i class="fa fa-check"></i><b>15.6</b> Theoretical Analysis and Convergence Properties</a></li>
<li class="chapter" data-level="15.7" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#applications-and-empirical-results"><i class="fa fa-check"></i><b>15.7</b> Applications and Empirical Results</a></li>
<li class="chapter" data-level="15.8" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#limitations-and-future-directions"><i class="fa fa-check"></i><b>15.8</b> Limitations and Future Directions</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>16</b> Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="16.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#introduction-9"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#theoretical-framework-2"><i class="fa fa-check"></i><b>16.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#the-policy-gradient-theorem-1"><i class="fa fa-check"></i><b>16.2.1</b> The Policy Gradient Theorem</a></li>
<li class="chapter" data-level="16.2.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-algorithm"><i class="fa fa-check"></i><b>16.2.2</b> REINFORCE Algorithm</a></li>
<li class="chapter" data-level="16.2.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#baseline-subtraction-and-variance-reduction"><i class="fa fa-check"></i><b>16.2.3</b> Baseline Subtraction and Variance Reduction</a></li>
<li class="chapter" data-level="16.2.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-parameterization"><i class="fa fa-check"></i><b>16.2.4</b> Softmax Policy Parameterization</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#implementation-in-r-2"><i class="fa fa-check"></i><b>16.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#environment-and-feature-representation"><i class="fa fa-check"></i><b>16.3.1</b> Environment and Feature Representation</a></li>
<li class="chapter" data-level="16.3.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-implementation"><i class="fa fa-check"></i><b>16.3.2</b> Softmax Policy Implementation</a></li>
<li class="chapter" data-level="16.3.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-implementation"><i class="fa fa-check"></i><b>16.3.3</b> REINFORCE Implementation</a></li>
<li class="chapter" data-level="16.3.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#actor-critic-implementation"><i class="fa fa-check"></i><b>16.3.4</b> Actor-Critic Implementation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#experimental-analysis-2"><i class="fa fa-check"></i><b>16.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#comparison-of-policy-gradient-variants"><i class="fa fa-check"></i><b>16.4.1</b> Comparison of Policy Gradient Variants</a></li>
<li class="chapter" data-level="16.4.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#learning-rate-sensitivity-analysis"><i class="fa fa-check"></i><b>16.4.2</b> Learning Rate Sensitivity Analysis</a></li>
<li class="chapter" data-level="16.4.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#variance-analysis-of-gradient-estimates"><i class="fa fa-check"></i><b>16.4.3</b> Variance Analysis of Gradient Estimates</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#discussion-and-implementation-considerations-1"><i class="fa fa-check"></i><b>16.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="16.6" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#conclusion-9"><i class="fa fa-check"></i><b>16.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>17</b> Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="17.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#theoretical-framework-3"><i class="fa fa-check"></i><b>17.1</b> Theoretical Framework</a></li>
<li class="chapter" data-level="17.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#implementation-and-comparative-analysis"><i class="fa fa-check"></i><b>17.2</b> Implementation and Comparative Analysis</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#variance-analysis-and-learning-dynamics"><i class="fa fa-check"></i><b>17.2.1</b> Variance Analysis and Learning Dynamics</a></li>
<li class="chapter" data-level="17.2.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#algorithmic-variants-and-extensions"><i class="fa fa-check"></i><b>17.2.2</b> Algorithmic Variants and Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#computational-and-convergence-considerations"><i class="fa fa-check"></i><b>17.3</b> Computational and Convergence Considerations</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#comparative-performance-analysis"><i class="fa fa-check"></i><b>17.3.1</b> Comparative Performance Analysis</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#conclusion-10"><i class="fa fa-check"></i><b>17.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>18</b> Appendix</a>
<ul>
<li class="chapter" data-level="18.1" data-path="appendix.html"><a href="appendix.html#comprehensive-reinforcement-learning-concepts-guide"><i class="fa fa-check"></i><b>18.1</b> Comprehensive Reinforcement Learning Concepts Guide</a></li>
<li class="chapter" data-level="18.2" data-path="appendix.html"><a href="appendix.html#learning-mechanisms"><i class="fa fa-check"></i><b>18.2</b> Learning Mechanisms</a></li>
<li class="chapter" data-level="18.3" data-path="appendix.html"><a href="appendix.html#environment-properties"><i class="fa fa-check"></i><b>18.3</b> Environment Properties</a></li>
<li class="chapter" data-level="18.4" data-path="appendix.html"><a href="appendix.html#learning-paradigms"><i class="fa fa-check"></i><b>18.4</b> Learning Paradigms</a></li>
<li class="chapter" data-level="18.5" data-path="appendix.html"><a href="appendix.html#exploration-strategies"><i class="fa fa-check"></i><b>18.5</b> Exploration Strategies</a></li>
<li class="chapter" data-level="18.6" data-path="appendix.html"><a href="appendix.html#key-algorithms-methods"><i class="fa fa-check"></i><b>18.6</b> Key Algorithms &amp; Methods</a></li>
<li class="chapter" data-level="18.7" data-path="appendix.html"><a href="appendix.html#advanced-concepts"><i class="fa fa-check"></i><b>18.7</b> Advanced Concepts</a></li>
<li class="chapter" data-level="18.8" data-path="appendix.html"><a href="appendix.html#fundamental-equations"><i class="fa fa-check"></i><b>18.8</b> Fundamental Equations</a>
<ul>
<li class="chapter" data-level="18.8.1" data-path="appendix.html"><a href="appendix.html#bellman-equations"><i class="fa fa-check"></i><b>18.8.1</b> <strong>Bellman Equations</strong></a></li>
<li class="chapter" data-level="18.8.2" data-path="appendix.html"><a href="appendix.html#policy-gradient-theorem"><i class="fa fa-check"></i><b>18.8.2</b> <strong>Policy Gradient Theorem</strong></a></li>
<li class="chapter" data-level="18.8.3" data-path="appendix.html"><a href="appendix.html#temporal-difference-error"><i class="fa fa-check"></i><b>18.8.3</b> <strong>Temporal Difference Error</strong></a></li>
</ul></li>
<li class="chapter" data-level="18.9" data-path="appendix.html"><a href="appendix.html#common-challenges-solutions"><i class="fa fa-check"></i><b>18.9</b> Common Challenges &amp; Solutions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="intrinsic-rewards" class="section level1 hasAnchor" number="15">
<h1><span class="header-section-number">Chapter 15</span> Intrinsic Rewards<a href="intrinsic-rewards.html#intrinsic-rewards" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The standard reinforcement learning paradigm assumes agents learn solely from extrinsic rewards provided by the environment. While this approach has achieved remarkable successes in game-playing and robotics, it fundamentally relies on carefully engineered reward functions that capture all aspects of desired behavior. For many real-world applications, designing such reward functions proves either impractical or impossible. Sparse reward environments, where meaningful feedback occurs infrequently, present particular challenges that have motivated the development of intrinsic reward mechanisms.</p>
<p>Intrinsic motivation addresses these limitations by endowing agents with internal reward signals that supplement or replace environmental rewards. These mechanisms draw inspiration from biological systems, where curiosity, novelty-seeking, and information-gathering behaviors emerge without explicit external reinforcement. The mathematical formalization of these concepts has led to practical algorithms that demonstrate superior exploration capabilities and learning efficiency across diverse domains.</p>
<div id="the-sparse-reward-problem-and-its-implications" class="section level2 hasAnchor" number="15.1">
<h2><span class="header-section-number">15.1</span> The Sparse Reward Problem and Its Implications<a href="intrinsic-rewards.html#the-sparse-reward-problem-and-its-implications" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Traditional RL assumes the existence of informative reward signals that guide learning toward desired behaviors. The agent’s objective is to maximize expected cumulative reward:</p>
<p><span class="math display">\[J(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^T \gamma^t R_t \right]\]</span></p>
<p>where <span class="math inline">\(\tau\)</span> represents trajectories sampled under policy <span class="math inline">\(\pi\)</span>. This formulation works well when rewards are dense and informative, providing frequent feedback about action quality.</p>
<p>However, many environments provide only sparse, delayed, or uninformative rewards. Consider a robot learning to navigate to a goal location in a large maze. The environment provides reward only upon reaching the target, offering no guidance during exploration. Random exploration becomes inefficient as state space size grows, leading to sample complexity that scales exponentially with problem dimension.</p>
<p>The fundamental issue lies in the exploration-exploitation dilemma. Without intermediate rewards, agents have no basis for preferring one action over another until accidentally discovering successful strategies. This creates a bootstrapping problem where learning cannot begin until rare rewarding events occur through random chance.</p>
<p>Intrinsic motivation mechanisms address this challenge by providing internal reward signals that encourage systematic exploration, information gathering, and skill development. These signals augment the learning process by rewarding potentially useful behaviors even in the absence of extrinsic feedback.</p>
</div>
<div id="mathematical-foundations-of-intrinsic-motivation" class="section level2 hasAnchor" number="15.2">
<h2><span class="header-section-number">15.2</span> Mathematical Foundations of Intrinsic Motivation<a href="intrinsic-rewards.html#mathematical-foundations-of-intrinsic-motivation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Intrinsic reward mechanisms can be formalized as functions that map agent experiences to internal reward signals. The general framework augments the standard RL objective with intrinsic terms:</p>
<p><span class="math display">\[J_{\text{total}}(\pi) = J_{\text{extrinsic}}(\pi) + \lambda J_{\text{intrinsic}}(\pi)\]</span></p>
<p>where <span class="math inline">\(\lambda\)</span> controls the relative importance of intrinsic versus extrinsic motivation. The intrinsic component takes the form:</p>
<p><span class="math display">\[J_{\text{intrinsic}}(\pi) = \mathbb{E}_{\tau \sim \pi} \left[ \sum_{t=0}^T \gamma^t R_t^{\text{int}} \right]\]</span></p>
<p>The key challenge lies in designing intrinsic reward functions <span class="math inline">\(R_t^{\text{int}}\)</span> that promote beneficial exploration without overwhelming or conflicting with task objectives.</p>
<div id="curiosity-driven-learning" class="section level3 hasAnchor" number="15.2.1">
<h3><span class="header-section-number">15.2.1</span> Curiosity-Driven Learning<a href="intrinsic-rewards.html#curiosity-driven-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One prominent approach bases intrinsic rewards on prediction errors. The intuition is that agents should be curious about aspects of the environment they cannot predict well, as these regions likely contain novel or important information.</p>
<p>The <strong>prediction error curiosity</strong> model maintains a forward dynamics model <span class="math inline">\(\hat{f}\)</span> that attempts to predict next states given current states and actions:</p>
<p><span class="math display">\[\hat{s}_{t+1} = \hat{f}(s_t, a_t)\]</span></p>
<p>The intrinsic reward is proportional to the prediction error:</p>
<p><span class="math display">\[R_t^{\text{int}} = \frac{1}{2} \|\hat{f}(s_t, a_t) - s_{t+1}\|^2\]</span></p>
<p>This approach encourages agents to explore regions where their world model is inaccurate, gradually improving understanding of environment dynamics.</p>
<p>However, raw prediction error can be misleading in stochastic environments or those with irrelevant but unpredictable elements. A more sophisticated approach uses the <strong>Intrinsic Curiosity Module (ICM)</strong> framework, which combines forward and inverse dynamics models.</p>
<p>The ICM learns a feature representation <span class="math inline">\(\phi(s)\)</span> that captures task-relevant aspects of states while filtering out irrelevant details. The inverse dynamics model predicts actions from state transitions:</p>
<p><span class="math display">\[\hat{a}_t = g(\phi(s_t), \phi(s_{t+1}))\]</span></p>
<p>The forward model predicts feature representations rather than raw states:</p>
<p><span class="math display">\[\hat{\phi}(s_{t+1}) = f(\phi(s_t), a_t)\]</span></p>
<p>The intrinsic reward combines prediction errors from both models:</p>
<p><span class="math display">\[R_t^{\text{int}} = \frac{\eta}{2} \|\hat{\phi}(s_{t+1}) - \phi(s_{t+1})\|^2\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is a scaling factor. This formulation focuses curiosity on aspects of the environment that are both unpredictable and controllable by the agent’s actions.</p>
</div>
<div id="count-based-exploration" class="section level3 hasAnchor" number="15.2.2">
<h3><span class="header-section-number">15.2.2</span> Count-Based Exploration<a href="intrinsic-rewards.html#count-based-exploration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An alternative approach grounds intrinsic motivation in visitation statistics. States that have been visited infrequently receive higher intrinsic rewards, encouraging systematic exploration of the state space.</p>
<p>The classical approach uses pseudo-counts based on density models. For a generative model <span class="math inline">\(\rho\)</span> that assigns probabilities to states, the pseudo-count for state <span class="math inline">\(s\)</span> after <span class="math inline">\(n\)</span> observations is:</p>
<p><span class="math display">\[N(s) = \frac{\rho(s)}{1 - \rho(s)} \cdot n\]</span></p>
<p>The intrinsic reward follows the form:</p>
<p><span class="math display">\[R_t^{\text{int}} = \frac{\beta}{\sqrt{N(s_t)}}\]</span></p>
<p>where <span class="math inline">\(\beta\)</span> controls the exploration bonus magnitude. This creates larger rewards for rarely visited states and smaller rewards for familiar regions.</p>
<p>For high-dimensional state spaces, exact counting becomes impractical. The <strong>Random Network Distillation (RND)</strong> approach addresses this by training a neural network to predict the outputs of a fixed random network. The prediction error serves as a novelty measure:</p>
<p><span class="math display">\[R_t^{\text{int}} = \|f_{\theta}(s_t) - \hat{f}_{\phi}(s_t)\|^2\]</span></p>
<p>where <span class="math inline">\(f_{\theta}\)</span> is a randomly initialized fixed network and <span class="math inline">\(\hat{f}_{\phi}\)</span> is a trainable predictor. States that are visited frequently will have smaller prediction errors as the predictor network learns to match the random network’s outputs.</p>
</div>
</div>
<div id="practical-implementation-curiosity-driven-grid-world-navigation" class="section level2 hasAnchor" number="15.3">
<h2><span class="header-section-number">15.3</span> Practical Implementation: Curiosity-Driven Grid World Navigation<a href="intrinsic-rewards.html#practical-implementation-curiosity-driven-grid-world-navigation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a grid world environment where an agent must learn to navigate to a goal location. The environment provides rewards only upon reaching the goal, making exploration crucial for learning.</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb158-1"><a href="intrinsic-rewards.html#cb158-1" tabindex="-1"></a><span class="co"># Curiosity-Driven Grid World Navigation</span></span>
<span id="cb158-2"><a href="intrinsic-rewards.html#cb158-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb158-3"><a href="intrinsic-rewards.html#cb158-3" tabindex="-1"></a><span class="fu">library</span>(reshape2)</span>
<span id="cb158-4"><a href="intrinsic-rewards.html#cb158-4" tabindex="-1"></a></span>
<span id="cb158-5"><a href="intrinsic-rewards.html#cb158-5" tabindex="-1"></a><span class="co"># Environment setup</span></span>
<span id="cb158-6"><a href="intrinsic-rewards.html#cb158-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb158-7"><a href="intrinsic-rewards.html#cb158-7" tabindex="-1"></a>grid_size <span class="ot">&lt;-</span> <span class="dv">8</span></span>
<span id="cb158-8"><a href="intrinsic-rewards.html#cb158-8" tabindex="-1"></a>goal_state <span class="ot">&lt;-</span> <span class="fu">c</span>(grid_size, grid_size)</span>
<span id="cb158-9"><a href="intrinsic-rewards.html#cb158-9" tabindex="-1"></a>n_episodes <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb158-10"><a href="intrinsic-rewards.html#cb158-10" tabindex="-1"></a>max_steps <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb158-11"><a href="intrinsic-rewards.html#cb158-11" tabindex="-1"></a></span>
<span id="cb158-12"><a href="intrinsic-rewards.html#cb158-12" tabindex="-1"></a><span class="co"># State encoding and action space</span></span>
<span id="cb158-13"><a href="intrinsic-rewards.html#cb158-13" tabindex="-1"></a>encode_state <span class="ot">&lt;-</span> <span class="cf">function</span>(row, col) {</span>
<span id="cb158-14"><a href="intrinsic-rewards.html#cb158-14" tabindex="-1"></a>  (row <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> grid_size <span class="sc">+</span> col</span>
<span id="cb158-15"><a href="intrinsic-rewards.html#cb158-15" tabindex="-1"></a>}</span>
<span id="cb158-16"><a href="intrinsic-rewards.html#cb158-16" tabindex="-1"></a></span>
<span id="cb158-17"><a href="intrinsic-rewards.html#cb158-17" tabindex="-1"></a>decode_state <span class="ot">&lt;-</span> <span class="cf">function</span>(state_id) {</span>
<span id="cb158-18"><a href="intrinsic-rewards.html#cb158-18" tabindex="-1"></a>  row <span class="ot">&lt;-</span> <span class="fu">floor</span>((state_id <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> grid_size) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb158-19"><a href="intrinsic-rewards.html#cb158-19" tabindex="-1"></a>  col <span class="ot">&lt;-</span> (state_id <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">%%</span> grid_size <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb158-20"><a href="intrinsic-rewards.html#cb158-20" tabindex="-1"></a>  <span class="fu">c</span>(row, col)</span>
<span id="cb158-21"><a href="intrinsic-rewards.html#cb158-21" tabindex="-1"></a>}</span>
<span id="cb158-22"><a href="intrinsic-rewards.html#cb158-22" tabindex="-1"></a></span>
<span id="cb158-23"><a href="intrinsic-rewards.html#cb158-23" tabindex="-1"></a><span class="co"># Actions: 1=up, 2=right, 3=down, 4=left</span></span>
<span id="cb158-24"><a href="intrinsic-rewards.html#cb158-24" tabindex="-1"></a>get_next_state <span class="ot">&lt;-</span> <span class="cf">function</span>(state, action) {</span>
<span id="cb158-25"><a href="intrinsic-rewards.html#cb158-25" tabindex="-1"></a>  pos <span class="ot">&lt;-</span> <span class="fu">decode_state</span>(state)</span>
<span id="cb158-26"><a href="intrinsic-rewards.html#cb158-26" tabindex="-1"></a>  row <span class="ot">&lt;-</span> pos[<span class="dv">1</span>]</span>
<span id="cb158-27"><a href="intrinsic-rewards.html#cb158-27" tabindex="-1"></a>  col <span class="ot">&lt;-</span> pos[<span class="dv">2</span>]</span>
<span id="cb158-28"><a href="intrinsic-rewards.html#cb158-28" tabindex="-1"></a>  </span>
<span id="cb158-29"><a href="intrinsic-rewards.html#cb158-29" tabindex="-1"></a>  <span class="cf">if</span> (action <span class="sc">==</span> <span class="dv">1</span>) row <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="dv">1</span>, row <span class="sc">-</span> <span class="dv">1</span>)  <span class="co"># up</span></span>
<span id="cb158-30"><a href="intrinsic-rewards.html#cb158-30" tabindex="-1"></a>  <span class="cf">else</span> <span class="cf">if</span> (action <span class="sc">==</span> <span class="dv">2</span>) col <span class="ot">&lt;-</span> <span class="fu">min</span>(grid_size, col <span class="sc">+</span> <span class="dv">1</span>)  <span class="co"># right</span></span>
<span id="cb158-31"><a href="intrinsic-rewards.html#cb158-31" tabindex="-1"></a>  <span class="cf">else</span> <span class="cf">if</span> (action <span class="sc">==</span> <span class="dv">3</span>) row <span class="ot">&lt;-</span> <span class="fu">min</span>(grid_size, row <span class="sc">+</span> <span class="dv">1</span>)  <span class="co"># down</span></span>
<span id="cb158-32"><a href="intrinsic-rewards.html#cb158-32" tabindex="-1"></a>  <span class="cf">else</span> <span class="cf">if</span> (action <span class="sc">==</span> <span class="dv">4</span>) col <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="dv">1</span>, col <span class="sc">-</span> <span class="dv">1</span>)  <span class="co"># left</span></span>
<span id="cb158-33"><a href="intrinsic-rewards.html#cb158-33" tabindex="-1"></a>  </span>
<span id="cb158-34"><a href="intrinsic-rewards.html#cb158-34" tabindex="-1"></a>  <span class="fu">encode_state</span>(row, col)</span>
<span id="cb158-35"><a href="intrinsic-rewards.html#cb158-35" tabindex="-1"></a>}</span>
<span id="cb158-36"><a href="intrinsic-rewards.html#cb158-36" tabindex="-1"></a></span>
<span id="cb158-37"><a href="intrinsic-rewards.html#cb158-37" tabindex="-1"></a><span class="co"># Extrinsic reward function</span></span>
<span id="cb158-38"><a href="intrinsic-rewards.html#cb158-38" tabindex="-1"></a>get_extrinsic_reward <span class="ot">&lt;-</span> <span class="cf">function</span>(state) {</span>
<span id="cb158-39"><a href="intrinsic-rewards.html#cb158-39" tabindex="-1"></a>  pos <span class="ot">&lt;-</span> <span class="fu">decode_state</span>(state)</span>
<span id="cb158-40"><a href="intrinsic-rewards.html#cb158-40" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">all</span>(pos <span class="sc">==</span> goal_state)) <span class="fu">return</span>(<span class="dv">10</span>)</span>
<span id="cb158-41"><a href="intrinsic-rewards.html#cb158-41" tabindex="-1"></a>  <span class="fu">return</span>(<span class="dv">0</span>)</span>
<span id="cb158-42"><a href="intrinsic-rewards.html#cb158-42" tabindex="-1"></a>}</span>
<span id="cb158-43"><a href="intrinsic-rewards.html#cb158-43" tabindex="-1"></a></span>
<span id="cb158-44"><a href="intrinsic-rewards.html#cb158-44" tabindex="-1"></a><span class="co"># Intrinsic Curiosity Module implementation</span></span>
<span id="cb158-45"><a href="intrinsic-rewards.html#cb158-45" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> grid_size<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb158-46"><a href="intrinsic-rewards.html#cb158-46" tabindex="-1"></a>n_actions <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb158-47"><a href="intrinsic-rewards.html#cb158-47" tabindex="-1"></a>feature_dim <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb158-48"><a href="intrinsic-rewards.html#cb158-48" tabindex="-1"></a></span>
<span id="cb158-49"><a href="intrinsic-rewards.html#cb158-49" tabindex="-1"></a><span class="co"># Initialize networks (simplified linear models for demonstration)</span></span>
<span id="cb158-50"><a href="intrinsic-rewards.html#cb158-50" tabindex="-1"></a><span class="co"># Forward model: predicts next state features from current features and action</span></span>
<span id="cb158-51"><a href="intrinsic-rewards.html#cb158-51" tabindex="-1"></a>forward_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">runif</span>(feature_dim <span class="sc">*</span> (feature_dim <span class="sc">+</span> n_actions), <span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>),</span>
<span id="cb158-52"><a href="intrinsic-rewards.html#cb158-52" tabindex="-1"></a>                      <span class="at">dim =</span> <span class="fu">c</span>(feature_dim, feature_dim <span class="sc">+</span> n_actions))</span>
<span id="cb158-53"><a href="intrinsic-rewards.html#cb158-53" tabindex="-1"></a></span>
<span id="cb158-54"><a href="intrinsic-rewards.html#cb158-54" tabindex="-1"></a><span class="co"># Inverse model: predicts action from state transition</span></span>
<span id="cb158-55"><a href="intrinsic-rewards.html#cb158-55" tabindex="-1"></a>inverse_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">runif</span>(n_actions <span class="sc">*</span> (<span class="dv">2</span> <span class="sc">*</span> feature_dim), <span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>),</span>
<span id="cb158-56"><a href="intrinsic-rewards.html#cb158-56" tabindex="-1"></a>                      <span class="at">dim =</span> <span class="fu">c</span>(n_actions, <span class="dv">2</span> <span class="sc">*</span> feature_dim))</span>
<span id="cb158-57"><a href="intrinsic-rewards.html#cb158-57" tabindex="-1"></a></span>
<span id="cb158-58"><a href="intrinsic-rewards.html#cb158-58" tabindex="-1"></a><span class="co"># Feature encoder: maps states to feature representations</span></span>
<span id="cb158-59"><a href="intrinsic-rewards.html#cb158-59" tabindex="-1"></a>feature_encoder <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="fu">runif</span>(feature_dim <span class="sc">*</span> <span class="dv">2</span>, <span class="sc">-</span><span class="fl">0.1</span>, <span class="fl">0.1</span>),</span>
<span id="cb158-60"><a href="intrinsic-rewards.html#cb158-60" tabindex="-1"></a>                        <span class="at">dim =</span> <span class="fu">c</span>(feature_dim, <span class="dv">2</span>))</span>
<span id="cb158-61"><a href="intrinsic-rewards.html#cb158-61" tabindex="-1"></a></span>
<span id="cb158-62"><a href="intrinsic-rewards.html#cb158-62" tabindex="-1"></a><span class="co"># Helper functions for neural network operations</span></span>
<span id="cb158-63"><a href="intrinsic-rewards.html#cb158-63" tabindex="-1"></a>get_features <span class="ot">&lt;-</span> <span class="cf">function</span>(state) {</span>
<span id="cb158-64"><a href="intrinsic-rewards.html#cb158-64" tabindex="-1"></a>  pos <span class="ot">&lt;-</span> <span class="fu">decode_state</span>(state)</span>
<span id="cb158-65"><a href="intrinsic-rewards.html#cb158-65" tabindex="-1"></a>  normalized_pos <span class="ot">&lt;-</span> (pos <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> (grid_size <span class="sc">-</span> <span class="dv">1</span>)  <span class="co"># normalize to [0,1]</span></span>
<span id="cb158-66"><a href="intrinsic-rewards.html#cb158-66" tabindex="-1"></a>  features <span class="ot">&lt;-</span> feature_encoder <span class="sc">%*%</span> normalized_pos</span>
<span id="cb158-67"><a href="intrinsic-rewards.html#cb158-67" tabindex="-1"></a>  <span class="fu">tanh</span>(features)  <span class="co"># activation function</span></span>
<span id="cb158-68"><a href="intrinsic-rewards.html#cb158-68" tabindex="-1"></a>}</span>
<span id="cb158-69"><a href="intrinsic-rewards.html#cb158-69" tabindex="-1"></a></span>
<span id="cb158-70"><a href="intrinsic-rewards.html#cb158-70" tabindex="-1"></a>action_to_onehot <span class="ot">&lt;-</span> <span class="cf">function</span>(action) {</span>
<span id="cb158-71"><a href="intrinsic-rewards.html#cb158-71" tabindex="-1"></a>  onehot <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_actions)</span>
<span id="cb158-72"><a href="intrinsic-rewards.html#cb158-72" tabindex="-1"></a>  onehot[action] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb158-73"><a href="intrinsic-rewards.html#cb158-73" tabindex="-1"></a>  onehot</span>
<span id="cb158-74"><a href="intrinsic-rewards.html#cb158-74" tabindex="-1"></a>}</span>
<span id="cb158-75"><a href="intrinsic-rewards.html#cb158-75" tabindex="-1"></a></span>
<span id="cb158-76"><a href="intrinsic-rewards.html#cb158-76" tabindex="-1"></a><span class="co"># Compute intrinsic reward using ICM</span></span>
<span id="cb158-77"><a href="intrinsic-rewards.html#cb158-77" tabindex="-1"></a>get_intrinsic_reward <span class="ot">&lt;-</span> <span class="cf">function</span>(state, action, next_state, <span class="at">alpha =</span> <span class="fl">0.01</span>) {</span>
<span id="cb158-78"><a href="intrinsic-rewards.html#cb158-78" tabindex="-1"></a>  features_t <span class="ot">&lt;-</span> <span class="fu">get_features</span>(state)</span>
<span id="cb158-79"><a href="intrinsic-rewards.html#cb158-79" tabindex="-1"></a>  features_t1 <span class="ot">&lt;-</span> <span class="fu">get_features</span>(next_state)</span>
<span id="cb158-80"><a href="intrinsic-rewards.html#cb158-80" tabindex="-1"></a>  action_onehot <span class="ot">&lt;-</span> <span class="fu">action_to_onehot</span>(action)</span>
<span id="cb158-81"><a href="intrinsic-rewards.html#cb158-81" tabindex="-1"></a>  </span>
<span id="cb158-82"><a href="intrinsic-rewards.html#cb158-82" tabindex="-1"></a>  <span class="co"># Forward model prediction</span></span>
<span id="cb158-83"><a href="intrinsic-rewards.html#cb158-83" tabindex="-1"></a>  forward_input <span class="ot">&lt;-</span> <span class="fu">c</span>(features_t, action_onehot)</span>
<span id="cb158-84"><a href="intrinsic-rewards.html#cb158-84" tabindex="-1"></a>  predicted_features <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(forward_model <span class="sc">%*%</span> forward_input)</span>
<span id="cb158-85"><a href="intrinsic-rewards.html#cb158-85" tabindex="-1"></a>  </span>
<span id="cb158-86"><a href="intrinsic-rewards.html#cb158-86" tabindex="-1"></a>  <span class="co"># Prediction error as intrinsic reward</span></span>
<span id="cb158-87"><a href="intrinsic-rewards.html#cb158-87" tabindex="-1"></a>  prediction_error <span class="ot">&lt;-</span> <span class="fu">sum</span>((predicted_features <span class="sc">-</span> features_t1)<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb158-88"><a href="intrinsic-rewards.html#cb158-88" tabindex="-1"></a>  intrinsic_reward <span class="ot">&lt;-</span> alpha <span class="sc">*</span> prediction_error</span>
<span id="cb158-89"><a href="intrinsic-rewards.html#cb158-89" tabindex="-1"></a>  </span>
<span id="cb158-90"><a href="intrinsic-rewards.html#cb158-90" tabindex="-1"></a>  <span class="co"># Update forward model (simple gradient descent)</span></span>
<span id="cb158-91"><a href="intrinsic-rewards.html#cb158-91" tabindex="-1"></a>  <span class="co"># Gradient of loss w.r.t. weights: error * input</span></span>
<span id="cb158-92"><a href="intrinsic-rewards.html#cb158-92" tabindex="-1"></a>  error_gradient <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> (predicted_features <span class="sc">-</span> features_t1)</span>
<span id="cb158-93"><a href="intrinsic-rewards.html#cb158-93" tabindex="-1"></a>  </span>
<span id="cb158-94"><a href="intrinsic-rewards.html#cb158-94" tabindex="-1"></a>  <span class="co"># Update each row of the forward model</span></span>
<span id="cb158-95"><a href="intrinsic-rewards.html#cb158-95" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>feature_dim) {</span>
<span id="cb158-96"><a href="intrinsic-rewards.html#cb158-96" tabindex="-1"></a>    forward_model[i, ] <span class="ot">&lt;&lt;-</span> forward_model[i, ] <span class="sc">-</span> <span class="fl">0.001</span> <span class="sc">*</span> error_gradient[i] <span class="sc">*</span> forward_input</span>
<span id="cb158-97"><a href="intrinsic-rewards.html#cb158-97" tabindex="-1"></a>  }</span>
<span id="cb158-98"><a href="intrinsic-rewards.html#cb158-98" tabindex="-1"></a>  </span>
<span id="cb158-99"><a href="intrinsic-rewards.html#cb158-99" tabindex="-1"></a>  intrinsic_reward</span>
<span id="cb158-100"><a href="intrinsic-rewards.html#cb158-100" tabindex="-1"></a>}</span>
<span id="cb158-101"><a href="intrinsic-rewards.html#cb158-101" tabindex="-1"></a></span>
<span id="cb158-102"><a href="intrinsic-rewards.html#cb158-102" tabindex="-1"></a><span class="co"># Q-learning with intrinsic motivation</span></span>
<span id="cb158-103"><a href="intrinsic-rewards.html#cb158-103" tabindex="-1"></a>Q <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions))</span>
<span id="cb158-104"><a href="intrinsic-rewards.html#cb158-104" tabindex="-1"></a>visit_counts <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states))</span>
<span id="cb158-105"><a href="intrinsic-rewards.html#cb158-105" tabindex="-1"></a>learning_rate <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb158-106"><a href="intrinsic-rewards.html#cb158-106" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.99</span></span>
<span id="cb158-107"><a href="intrinsic-rewards.html#cb158-107" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fl">0.3</span></span>
<span id="cb158-108"><a href="intrinsic-rewards.html#cb158-108" tabindex="-1"></a>intrinsic_weight <span class="ot">&lt;-</span> <span class="fl">0.5</span></span>
<span id="cb158-109"><a href="intrinsic-rewards.html#cb158-109" tabindex="-1"></a></span>
<span id="cb158-110"><a href="intrinsic-rewards.html#cb158-110" tabindex="-1"></a><span class="co"># Training metrics</span></span>
<span id="cb158-111"><a href="intrinsic-rewards.html#cb158-111" tabindex="-1"></a>episode_rewards <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_episodes)</span>
<span id="cb158-112"><a href="intrinsic-rewards.html#cb158-112" tabindex="-1"></a>episode_lengths <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_episodes)</span>
<span id="cb158-113"><a href="intrinsic-rewards.html#cb158-113" tabindex="-1"></a>intrinsic_rewards_history <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb158-114"><a href="intrinsic-rewards.html#cb158-114" tabindex="-1"></a></span>
<span id="cb158-115"><a href="intrinsic-rewards.html#cb158-115" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Training curiosity-driven agent...</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Training curiosity-driven agent...</code></pre>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="intrinsic-rewards.html#cb160-1" tabindex="-1"></a><span class="cf">for</span> (episode <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_episodes) {</span>
<span id="cb160-2"><a href="intrinsic-rewards.html#cb160-2" tabindex="-1"></a>  state <span class="ot">&lt;-</span> <span class="fu">encode_state</span>(<span class="dv">1</span>, <span class="dv">1</span>)  <span class="co"># start at top-left</span></span>
<span id="cb160-3"><a href="intrinsic-rewards.html#cb160-3" tabindex="-1"></a>  episode_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb160-4"><a href="intrinsic-rewards.html#cb160-4" tabindex="-1"></a>  episode_intrinsic <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb160-5"><a href="intrinsic-rewards.html#cb160-5" tabindex="-1"></a>  </span>
<span id="cb160-6"><a href="intrinsic-rewards.html#cb160-6" tabindex="-1"></a>  <span class="cf">for</span> (step <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>max_steps) {</span>
<span id="cb160-7"><a href="intrinsic-rewards.html#cb160-7" tabindex="-1"></a>    visit_counts[state] <span class="ot">&lt;-</span> visit_counts[state] <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb160-8"><a href="intrinsic-rewards.html#cb160-8" tabindex="-1"></a>    </span>
<span id="cb160-9"><a href="intrinsic-rewards.html#cb160-9" tabindex="-1"></a>    <span class="co"># Epsilon-greedy action selection</span></span>
<span id="cb160-10"><a href="intrinsic-rewards.html#cb160-10" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb160-11"><a href="intrinsic-rewards.html#cb160-11" tabindex="-1"></a>      action <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb160-12"><a href="intrinsic-rewards.html#cb160-12" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb160-13"><a href="intrinsic-rewards.html#cb160-13" tabindex="-1"></a>      action <span class="ot">&lt;-</span> <span class="fu">which.max</span>(Q[state, ])</span>
<span id="cb160-14"><a href="intrinsic-rewards.html#cb160-14" tabindex="-1"></a>    }</span>
<span id="cb160-15"><a href="intrinsic-rewards.html#cb160-15" tabindex="-1"></a>    </span>
<span id="cb160-16"><a href="intrinsic-rewards.html#cb160-16" tabindex="-1"></a>    <span class="co"># Take action</span></span>
<span id="cb160-17"><a href="intrinsic-rewards.html#cb160-17" tabindex="-1"></a>    next_state <span class="ot">&lt;-</span> <span class="fu">get_next_state</span>(state, action)</span>
<span id="cb160-18"><a href="intrinsic-rewards.html#cb160-18" tabindex="-1"></a>    </span>
<span id="cb160-19"><a href="intrinsic-rewards.html#cb160-19" tabindex="-1"></a>    <span class="co"># Compute rewards</span></span>
<span id="cb160-20"><a href="intrinsic-rewards.html#cb160-20" tabindex="-1"></a>    extrinsic_reward <span class="ot">&lt;-</span> <span class="fu">get_extrinsic_reward</span>(next_state)</span>
<span id="cb160-21"><a href="intrinsic-rewards.html#cb160-21" tabindex="-1"></a>    intrinsic_reward <span class="ot">&lt;-</span> <span class="fu">get_intrinsic_reward</span>(state, action, next_state)</span>
<span id="cb160-22"><a href="intrinsic-rewards.html#cb160-22" tabindex="-1"></a>    total_reward <span class="ot">&lt;-</span> extrinsic_reward <span class="sc">+</span> intrinsic_weight <span class="sc">*</span> intrinsic_reward</span>
<span id="cb160-23"><a href="intrinsic-rewards.html#cb160-23" tabindex="-1"></a>    </span>
<span id="cb160-24"><a href="intrinsic-rewards.html#cb160-24" tabindex="-1"></a>    episode_intrinsic <span class="ot">&lt;-</span> <span class="fu">c</span>(episode_intrinsic, intrinsic_reward)</span>
<span id="cb160-25"><a href="intrinsic-rewards.html#cb160-25" tabindex="-1"></a>    episode_reward <span class="ot">&lt;-</span> episode_reward <span class="sc">+</span> extrinsic_reward</span>
<span id="cb160-26"><a href="intrinsic-rewards.html#cb160-26" tabindex="-1"></a>    </span>
<span id="cb160-27"><a href="intrinsic-rewards.html#cb160-27" tabindex="-1"></a>    <span class="co"># Q-learning update</span></span>
<span id="cb160-28"><a href="intrinsic-rewards.html#cb160-28" tabindex="-1"></a>    td_target <span class="ot">&lt;-</span> total_reward <span class="sc">+</span> gamma <span class="sc">*</span> <span class="fu">max</span>(Q[next_state, ])</span>
<span id="cb160-29"><a href="intrinsic-rewards.html#cb160-29" tabindex="-1"></a>    td_error <span class="ot">&lt;-</span> td_target <span class="sc">-</span> Q[state, action]</span>
<span id="cb160-30"><a href="intrinsic-rewards.html#cb160-30" tabindex="-1"></a>    Q[state, action] <span class="ot">&lt;-</span> Q[state, action] <span class="sc">+</span> learning_rate <span class="sc">*</span> td_error</span>
<span id="cb160-31"><a href="intrinsic-rewards.html#cb160-31" tabindex="-1"></a>    </span>
<span id="cb160-32"><a href="intrinsic-rewards.html#cb160-32" tabindex="-1"></a>    <span class="co"># Check for goal</span></span>
<span id="cb160-33"><a href="intrinsic-rewards.html#cb160-33" tabindex="-1"></a>    <span class="cf">if</span> (extrinsic_reward <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb160-34"><a href="intrinsic-rewards.html#cb160-34" tabindex="-1"></a>      episode_lengths[episode] <span class="ot">&lt;-</span> step</span>
<span id="cb160-35"><a href="intrinsic-rewards.html#cb160-35" tabindex="-1"></a>      <span class="cf">break</span></span>
<span id="cb160-36"><a href="intrinsic-rewards.html#cb160-36" tabindex="-1"></a>    }</span>
<span id="cb160-37"><a href="intrinsic-rewards.html#cb160-37" tabindex="-1"></a>    </span>
<span id="cb160-38"><a href="intrinsic-rewards.html#cb160-38" tabindex="-1"></a>    state <span class="ot">&lt;-</span> next_state</span>
<span id="cb160-39"><a href="intrinsic-rewards.html#cb160-39" tabindex="-1"></a>  }</span>
<span id="cb160-40"><a href="intrinsic-rewards.html#cb160-40" tabindex="-1"></a>  </span>
<span id="cb160-41"><a href="intrinsic-rewards.html#cb160-41" tabindex="-1"></a>  <span class="cf">if</span> (episode_reward <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb160-42"><a href="intrinsic-rewards.html#cb160-42" tabindex="-1"></a>    episode_lengths[episode] <span class="ot">&lt;-</span> max_steps</span>
<span id="cb160-43"><a href="intrinsic-rewards.html#cb160-43" tabindex="-1"></a>  }</span>
<span id="cb160-44"><a href="intrinsic-rewards.html#cb160-44" tabindex="-1"></a>  </span>
<span id="cb160-45"><a href="intrinsic-rewards.html#cb160-45" tabindex="-1"></a>  episode_rewards[episode] <span class="ot">&lt;-</span> episode_reward</span>
<span id="cb160-46"><a href="intrinsic-rewards.html#cb160-46" tabindex="-1"></a>  intrinsic_rewards_history[[episode]] <span class="ot">&lt;-</span> episode_intrinsic</span>
<span id="cb160-47"><a href="intrinsic-rewards.html#cb160-47" tabindex="-1"></a>  </span>
<span id="cb160-48"><a href="intrinsic-rewards.html#cb160-48" tabindex="-1"></a>  <span class="co"># Decay exploration</span></span>
<span id="cb160-49"><a href="intrinsic-rewards.html#cb160-49" tabindex="-1"></a>  <span class="cf">if</span> (episode <span class="sc">%%</span> <span class="dv">50</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb160-50"><a href="intrinsic-rewards.html#cb160-50" tabindex="-1"></a>    epsilon <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fl">0.05</span>, epsilon <span class="sc">*</span> <span class="fl">0.95</span>)</span>
<span id="cb160-51"><a href="intrinsic-rewards.html#cb160-51" tabindex="-1"></a>    avg_length <span class="ot">&lt;-</span> <span class="fu">mean</span>(episode_lengths[<span class="fu">max</span>(<span class="dv">1</span>, episode<span class="dv">-49</span>)<span class="sc">:</span>episode])</span>
<span id="cb160-52"><a href="intrinsic-rewards.html#cb160-52" tabindex="-1"></a>    success_rate <span class="ot">&lt;-</span> <span class="fu">mean</span>(episode_rewards[<span class="fu">max</span>(<span class="dv">1</span>, episode<span class="dv">-49</span>)<span class="sc">:</span>episode] <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb160-53"><a href="intrinsic-rewards.html#cb160-53" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Episode %d: Success rate %.2f, Avg length %.1f</span><span class="sc">\n</span><span class="st">&quot;</span>, </span>
<span id="cb160-54"><a href="intrinsic-rewards.html#cb160-54" tabindex="-1"></a>                episode, success_rate, avg_length))</span>
<span id="cb160-55"><a href="intrinsic-rewards.html#cb160-55" tabindex="-1"></a>  }</span>
<span id="cb160-56"><a href="intrinsic-rewards.html#cb160-56" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Episode 50: Success rate 0.00, Avg length 100.0
## Episode 100: Success rate 0.00, Avg length 100.0
## Episode 150: Success rate 0.00, Avg length 100.0
## Episode 200: Success rate 0.00, Avg length 100.0
## Episode 250: Success rate 0.00, Avg length 100.0
## Episode 300: Success rate 0.00, Avg length 100.0
## Episode 350: Success rate 0.00, Avg length 100.0
## Episode 400: Success rate 0.00, Avg length 100.0
## Episode 450: Success rate 0.00, Avg length 100.0
## Episode 500: Success rate 0.00, Avg length 100.0</code></pre>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="intrinsic-rewards.html#cb162-1" tabindex="-1"></a><span class="co"># Analyze learning progress</span></span>
<span id="cb162-2"><a href="intrinsic-rewards.html#cb162-2" tabindex="-1"></a>success_episodes <span class="ot">&lt;-</span> <span class="fu">which</span>(episode_rewards <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb162-3"><a href="intrinsic-rewards.html#cb162-3" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">length</span>(success_episodes) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb162-4"><a href="intrinsic-rewards.html#cb162-4" tabindex="-1"></a>  first_success <span class="ot">&lt;-</span> <span class="fu">min</span>(success_episodes)</span>
<span id="cb162-5"><a href="intrinsic-rewards.html#cb162-5" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;First successful episode: %d</span><span class="sc">\n</span><span class="st">&quot;</span>, first_success))</span>
<span id="cb162-6"><a href="intrinsic-rewards.html#cb162-6" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Final success rate (last 100 episodes): %.2f</span><span class="sc">\n</span><span class="st">&quot;</span>, </span>
<span id="cb162-7"><a href="intrinsic-rewards.html#cb162-7" tabindex="-1"></a>              <span class="fu">mean</span>(episode_rewards[(n_episodes<span class="dv">-99</span>)<span class="sc">:</span>n_episodes] <span class="sc">&gt;</span> <span class="dv">0</span>)))</span>
<span id="cb162-8"><a href="intrinsic-rewards.html#cb162-8" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb162-9"><a href="intrinsic-rewards.html#cb162-9" tabindex="-1"></a>  first_success <span class="ot">&lt;-</span> n_episodes</span>
<span id="cb162-10"><a href="intrinsic-rewards.html#cb162-10" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;No successful episodes found</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb162-11"><a href="intrinsic-rewards.html#cb162-11" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## No successful episodes found</code></pre>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="intrinsic-rewards.html#cb164-1" tabindex="-1"></a><span class="co"># Visualize learned policy</span></span>
<span id="cb164-2"><a href="intrinsic-rewards.html#cb164-2" tabindex="-1"></a>learned_policy <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(grid_size, grid_size))</span>
<span id="cb164-3"><a href="intrinsic-rewards.html#cb164-3" tabindex="-1"></a><span class="cf">for</span> (row <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>grid_size) {</span>
<span id="cb164-4"><a href="intrinsic-rewards.html#cb164-4" tabindex="-1"></a>  <span class="cf">for</span> (col <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>grid_size) {</span>
<span id="cb164-5"><a href="intrinsic-rewards.html#cb164-5" tabindex="-1"></a>    state <span class="ot">&lt;-</span> <span class="fu">encode_state</span>(row, col)</span>
<span id="cb164-6"><a href="intrinsic-rewards.html#cb164-6" tabindex="-1"></a>    learned_policy[row, col] <span class="ot">&lt;-</span> <span class="fu">which.max</span>(Q[state, ])</span>
<span id="cb164-7"><a href="intrinsic-rewards.html#cb164-7" tabindex="-1"></a>  }</span>
<span id="cb164-8"><a href="intrinsic-rewards.html#cb164-8" tabindex="-1"></a>}</span>
<span id="cb164-9"><a href="intrinsic-rewards.html#cb164-9" tabindex="-1"></a></span>
<span id="cb164-10"><a href="intrinsic-rewards.html#cb164-10" tabindex="-1"></a><span class="co"># Create visualization data frame</span></span>
<span id="cb164-11"><a href="intrinsic-rewards.html#cb164-11" tabindex="-1"></a>policy_df <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">row =</span> <span class="dv">1</span><span class="sc">:</span>grid_size, <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span>grid_size)</span>
<span id="cb164-12"><a href="intrinsic-rewards.html#cb164-12" tabindex="-1"></a>policy_df<span class="sc">$</span>action <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(learned_policy)</span>
<span id="cb164-13"><a href="intrinsic-rewards.html#cb164-13" tabindex="-1"></a>policy_df<span class="sc">$</span>action_char <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;↑&quot;</span>, <span class="st">&quot;→&quot;</span>, <span class="st">&quot;↓&quot;</span>, <span class="st">&quot;←&quot;</span>)[policy_df<span class="sc">$</span>action]</span>
<span id="cb164-14"><a href="intrinsic-rewards.html#cb164-14" tabindex="-1"></a></span>
<span id="cb164-15"><a href="intrinsic-rewards.html#cb164-15" tabindex="-1"></a><span class="co"># Create heatmap of visit counts</span></span>
<span id="cb164-16"><a href="intrinsic-rewards.html#cb164-16" tabindex="-1"></a>visit_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(visit_counts, <span class="at">nrow =</span> grid_size, <span class="at">ncol =</span> grid_size, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb164-17"><a href="intrinsic-rewards.html#cb164-17" tabindex="-1"></a>visit_df <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">row =</span> <span class="dv">1</span><span class="sc">:</span>grid_size, <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span>grid_size)</span>
<span id="cb164-18"><a href="intrinsic-rewards.html#cb164-18" tabindex="-1"></a>visit_df<span class="sc">$</span>visits <span class="ot">&lt;-</span> <span class="fu">as.vector</span>(visit_matrix)</span>
<span id="cb164-19"><a href="intrinsic-rewards.html#cb164-19" tabindex="-1"></a></span>
<span id="cb164-20"><a href="intrinsic-rewards.html#cb164-20" tabindex="-1"></a><span class="co"># Plot learning curves</span></span>
<span id="cb164-21"><a href="intrinsic-rewards.html#cb164-21" tabindex="-1"></a>learning_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb164-22"><a href="intrinsic-rewards.html#cb164-22" tabindex="-1"></a>  <span class="at">episode =</span> <span class="dv">1</span><span class="sc">:</span>n_episodes,</span>
<span id="cb164-23"><a href="intrinsic-rewards.html#cb164-23" tabindex="-1"></a>  <span class="at">success =</span> <span class="fu">as.numeric</span>(episode_rewards <span class="sc">&gt;</span> <span class="dv">0</span>),</span>
<span id="cb164-24"><a href="intrinsic-rewards.html#cb164-24" tabindex="-1"></a>  <span class="at">length =</span> episode_lengths</span>
<span id="cb164-25"><a href="intrinsic-rewards.html#cb164-25" tabindex="-1"></a>)</span>
<span id="cb164-26"><a href="intrinsic-rewards.html#cb164-26" tabindex="-1"></a></span>
<span id="cb164-27"><a href="intrinsic-rewards.html#cb164-27" tabindex="-1"></a><span class="co"># Moving average for smoothing (handle edge cases)</span></span>
<span id="cb164-28"><a href="intrinsic-rewards.html#cb164-28" tabindex="-1"></a>window_size <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb164-29"><a href="intrinsic-rewards.html#cb164-29" tabindex="-1"></a><span class="cf">if</span> (n_episodes <span class="sc">&gt;=</span> window_size) {</span>
<span id="cb164-30"><a href="intrinsic-rewards.html#cb164-30" tabindex="-1"></a>  success_smooth <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="cn">NA</span>, n_episodes)</span>
<span id="cb164-31"><a href="intrinsic-rewards.html#cb164-31" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> window_size<span class="sc">:</span>n_episodes) {</span>
<span id="cb164-32"><a href="intrinsic-rewards.html#cb164-32" tabindex="-1"></a>    success_smooth[i] <span class="ot">&lt;-</span> <span class="fu">mean</span>(learning_data<span class="sc">$</span>success[(i<span class="sc">-</span>window_size<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>i])</span>
<span id="cb164-33"><a href="intrinsic-rewards.html#cb164-33" tabindex="-1"></a>  }</span>
<span id="cb164-34"><a href="intrinsic-rewards.html#cb164-34" tabindex="-1"></a>  learning_data<span class="sc">$</span>success_ma <span class="ot">&lt;-</span> success_smooth</span>
<span id="cb164-35"><a href="intrinsic-rewards.html#cb164-35" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb164-36"><a href="intrinsic-rewards.html#cb164-36" tabindex="-1"></a>  learning_data<span class="sc">$</span>success_ma <span class="ot">&lt;-</span> <span class="fu">cumsum</span>(learning_data<span class="sc">$</span>success) <span class="sc">/</span> <span class="fu">seq_along</span>(learning_data<span class="sc">$</span>success)</span>
<span id="cb164-37"><a href="intrinsic-rewards.html#cb164-37" tabindex="-1"></a>}</span>
<span id="cb164-38"><a href="intrinsic-rewards.html#cb164-38" tabindex="-1"></a></span>
<span id="cb164-39"><a href="intrinsic-rewards.html#cb164-39" tabindex="-1"></a><span class="co"># Create learning curve plot</span></span>
<span id="cb164-40"><a href="intrinsic-rewards.html#cb164-40" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(learning_data, <span class="fu">aes</span>(<span class="at">x =</span> episode)) <span class="sc">+</span></span>
<span id="cb164-41"><a href="intrinsic-rewards.html#cb164-41" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> success_ma), <span class="at">color =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb164-42"><a href="intrinsic-rewards.html#cb164-42" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="fu">aes</span>(<span class="at">y =</span> success), <span class="at">method =</span> <span class="st">&quot;loess&quot;</span>, <span class="at">span =</span> <span class="fl">0.3</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">color =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb164-43"><a href="intrinsic-rewards.html#cb164-43" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Learning Progress&quot;</span>,</span>
<span id="cb164-44"><a href="intrinsic-rewards.html#cb164-44" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">&quot;Episode&quot;</span>,</span>
<span id="cb164-45"><a href="intrinsic-rewards.html#cb164-45" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Success Rate&quot;</span>,</span>
<span id="cb164-46"><a href="intrinsic-rewards.html#cb164-46" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Blue: Moving average, Red: Smoothed trend&quot;</span>) <span class="sc">+</span></span>
<span id="cb164-47"><a href="intrinsic-rewards.html#cb164-47" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<pre><code>## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
## ℹ Please use `linewidth` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></pre>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="intrinsic-rewards.html#cb166-1" tabindex="-1"></a><span class="co"># Create policy visualization</span></span>
<span id="cb166-2"><a href="intrinsic-rewards.html#cb166-2" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(policy_df, <span class="fu">aes</span>(<span class="at">x =</span> col, <span class="at">y =</span> grid_size <span class="sc">+</span> <span class="dv">1</span> <span class="sc">-</span> row)) <span class="sc">+</span></span>
<span id="cb166-3"><a href="intrinsic-rewards.html#cb166-3" tabindex="-1"></a>  <span class="fu">geom_tile</span>(<span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">fill =</span> <span class="st">&quot;white&quot;</span>) <span class="sc">+</span></span>
<span id="cb166-4"><a href="intrinsic-rewards.html#cb166-4" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> action_char), <span class="at">size =</span> <span class="dv">6</span>) <span class="sc">+</span></span>
<span id="cb166-5"><a href="intrinsic-rewards.html#cb166-5" tabindex="-1"></a>  <span class="fu">geom_tile</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">col =</span> grid_size, <span class="at">row =</span> <span class="dv">1</span>), </span>
<span id="cb166-6"><a href="intrinsic-rewards.html#cb166-6" tabindex="-1"></a>            <span class="fu">aes</span>(<span class="at">x =</span> col, <span class="at">y =</span> row), <span class="at">fill =</span> <span class="st">&quot;gold&quot;</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb166-7"><a href="intrinsic-rewards.html#cb166-7" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Learned Policy&quot;</span>, </span>
<span id="cb166-8"><a href="intrinsic-rewards.html#cb166-8" tabindex="-1"></a>       <span class="at">subtitle =</span> <span class="st">&quot;Arrows show best action, Gold = Goal&quot;</span>) <span class="sc">+</span></span>
<span id="cb166-9"><a href="intrinsic-rewards.html#cb166-9" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>grid_size) <span class="sc">+</span></span>
<span id="cb166-10"><a href="intrinsic-rewards.html#cb166-10" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>grid_size) <span class="sc">+</span></span>
<span id="cb166-11"><a href="intrinsic-rewards.html#cb166-11" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb166-12"><a href="intrinsic-rewards.html#cb166-12" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">aspect.ratio =</span> <span class="dv">1</span>)</span>
<span id="cb166-13"><a href="intrinsic-rewards.html#cb166-13" tabindex="-1"></a></span>
<span id="cb166-14"><a href="intrinsic-rewards.html#cb166-14" tabindex="-1"></a><span class="co"># Create visit count heatmap</span></span>
<span id="cb166-15"><a href="intrinsic-rewards.html#cb166-15" tabindex="-1"></a>p3 <span class="ot">&lt;-</span> <span class="fu">ggplot</span>(visit_df, <span class="fu">aes</span>(<span class="at">x =</span> col, <span class="at">y =</span> grid_size <span class="sc">+</span> <span class="dv">1</span> <span class="sc">-</span> row, <span class="at">fill =</span> visits)) <span class="sc">+</span></span>
<span id="cb166-16"><a href="intrinsic-rewards.html#cb166-16" tabindex="-1"></a>  <span class="fu">geom_tile</span>() <span class="sc">+</span></span>
<span id="cb166-17"><a href="intrinsic-rewards.html#cb166-17" tabindex="-1"></a>  <span class="fu">scale_fill_gradient</span>(<span class="at">low =</span> <span class="st">&quot;white&quot;</span>, <span class="at">high =</span> <span class="st">&quot;darkblue&quot;</span>) <span class="sc">+</span></span>
<span id="cb166-18"><a href="intrinsic-rewards.html#cb166-18" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;State Visitation Frequency&quot;</span>,</span>
<span id="cb166-19"><a href="intrinsic-rewards.html#cb166-19" tabindex="-1"></a>       <span class="at">fill =</span> <span class="st">&quot;Visit Count&quot;</span>) <span class="sc">+</span></span>
<span id="cb166-20"><a href="intrinsic-rewards.html#cb166-20" tabindex="-1"></a>  <span class="fu">scale_x_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>grid_size) <span class="sc">+</span></span>
<span id="cb166-21"><a href="intrinsic-rewards.html#cb166-21" tabindex="-1"></a>  <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> <span class="dv">1</span><span class="sc">:</span>grid_size) <span class="sc">+</span></span>
<span id="cb166-22"><a href="intrinsic-rewards.html#cb166-22" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb166-23"><a href="intrinsic-rewards.html#cb166-23" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">aspect.ratio =</span> <span class="dv">1</span>)</span>
<span id="cb166-24"><a href="intrinsic-rewards.html#cb166-24" tabindex="-1"></a></span>
<span id="cb166-25"><a href="intrinsic-rewards.html#cb166-25" tabindex="-1"></a><span class="fu">print</span>(p1)</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<pre><code>## Warning: Removed 49 rows containing missing values or values outside the scale range (`geom_line()`).</code></pre>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="intrinsic-rewards.html#cb169-1" tabindex="-1"></a><span class="fu">print</span>(p2)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-29-2.png" width="672" /></p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb170-1"><a href="intrinsic-rewards.html#cb170-1" tabindex="-1"></a><span class="fu">print</span>(p3)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-29-3.png" width="672" /></p>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="intrinsic-rewards.html#cb171-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Learned policy (arrows show best action):</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Learned policy (arrows show best action):</code></pre>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="intrinsic-rewards.html#cb173-1" tabindex="-1"></a><span class="cf">for</span> (row <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>grid_size) {</span>
<span id="cb173-2"><a href="intrinsic-rewards.html#cb173-2" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Row %d: %s</span><span class="sc">\n</span><span class="st">&quot;</span>, row, </span>
<span id="cb173-3"><a href="intrinsic-rewards.html#cb173-3" tabindex="-1"></a>              <span class="fu">paste</span>(<span class="fu">c</span>(<span class="st">&quot;↑&quot;</span>, <span class="st">&quot;→&quot;</span>, <span class="st">&quot;↓&quot;</span>, <span class="st">&quot;←&quot;</span>)[learned_policy[row, ]], <span class="at">collapse =</span> <span class="st">&quot; &quot;</span>)))</span>
<span id="cb173-4"><a href="intrinsic-rewards.html#cb173-4" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Row 1: → → → → → → → ←
## Row 2: ↑ ← ↑ ↑ ↑ ↑ ↑ ←
## Row 3: ↑ ↑ ↑ ↑ ↑ ← → ←
## Row 4: ↑ ↑ ↑ ↑ ← ← ← ↑
## Row 5: ↑ ↑ ↑ ↑ ↑ ↑ ← ↑
## Row 6: ↑ ↑ → ↑ ↑ ↑ ↑ ↑
## Row 7: ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑
## Row 8: ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑</code></pre>
<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb175-1"><a href="intrinsic-rewards.html#cb175-1" tabindex="-1"></a><span class="co"># Compare with baseline (random exploration)</span></span>
<span id="cb175-2"><a href="intrinsic-rewards.html#cb175-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Comparing with random baseline...</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Comparing with random baseline...</code></pre>
<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb177-1"><a href="intrinsic-rewards.html#cb177-1" tabindex="-1"></a>Q_random <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions))</span>
<span id="cb177-2"><a href="intrinsic-rewards.html#cb177-2" tabindex="-1"></a>random_success <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_episodes)</span>
<span id="cb177-3"><a href="intrinsic-rewards.html#cb177-3" tabindex="-1"></a></span>
<span id="cb177-4"><a href="intrinsic-rewards.html#cb177-4" tabindex="-1"></a><span class="cf">for</span> (episode <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_episodes) {</span>
<span id="cb177-5"><a href="intrinsic-rewards.html#cb177-5" tabindex="-1"></a>  state <span class="ot">&lt;-</span> <span class="fu">encode_state</span>(<span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb177-6"><a href="intrinsic-rewards.html#cb177-6" tabindex="-1"></a>  </span>
<span id="cb177-7"><a href="intrinsic-rewards.html#cb177-7" tabindex="-1"></a>  <span class="cf">for</span> (step <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>max_steps) {</span>
<span id="cb177-8"><a href="intrinsic-rewards.html#cb177-8" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> <span class="fl">0.3</span>) {</span>
<span id="cb177-9"><a href="intrinsic-rewards.html#cb177-9" tabindex="-1"></a>      action <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb177-10"><a href="intrinsic-rewards.html#cb177-10" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb177-11"><a href="intrinsic-rewards.html#cb177-11" tabindex="-1"></a>      action <span class="ot">&lt;-</span> <span class="fu">which.max</span>(Q_random[state, ])</span>
<span id="cb177-12"><a href="intrinsic-rewards.html#cb177-12" tabindex="-1"></a>    }</span>
<span id="cb177-13"><a href="intrinsic-rewards.html#cb177-13" tabindex="-1"></a>    </span>
<span id="cb177-14"><a href="intrinsic-rewards.html#cb177-14" tabindex="-1"></a>    next_state <span class="ot">&lt;-</span> <span class="fu">get_next_state</span>(state, action)</span>
<span id="cb177-15"><a href="intrinsic-rewards.html#cb177-15" tabindex="-1"></a>    extrinsic_reward <span class="ot">&lt;-</span> <span class="fu">get_extrinsic_reward</span>(next_state)</span>
<span id="cb177-16"><a href="intrinsic-rewards.html#cb177-16" tabindex="-1"></a>    </span>
<span id="cb177-17"><a href="intrinsic-rewards.html#cb177-17" tabindex="-1"></a>    <span class="co"># Standard Q-learning (no intrinsic reward)</span></span>
<span id="cb177-18"><a href="intrinsic-rewards.html#cb177-18" tabindex="-1"></a>    td_target <span class="ot">&lt;-</span> extrinsic_reward <span class="sc">+</span> gamma <span class="sc">*</span> <span class="fu">max</span>(Q_random[next_state, ])</span>
<span id="cb177-19"><a href="intrinsic-rewards.html#cb177-19" tabindex="-1"></a>    td_error <span class="ot">&lt;-</span> td_target <span class="sc">-</span> Q_random[state, action]</span>
<span id="cb177-20"><a href="intrinsic-rewards.html#cb177-20" tabindex="-1"></a>    Q_random[state, action] <span class="ot">&lt;-</span> Q_random[state, action] <span class="sc">+</span> learning_rate <span class="sc">*</span> td_error</span>
<span id="cb177-21"><a href="intrinsic-rewards.html#cb177-21" tabindex="-1"></a>    </span>
<span id="cb177-22"><a href="intrinsic-rewards.html#cb177-22" tabindex="-1"></a>    <span class="cf">if</span> (extrinsic_reward <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb177-23"><a href="intrinsic-rewards.html#cb177-23" tabindex="-1"></a>      random_success[episode] <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb177-24"><a href="intrinsic-rewards.html#cb177-24" tabindex="-1"></a>      <span class="cf">break</span></span>
<span id="cb177-25"><a href="intrinsic-rewards.html#cb177-25" tabindex="-1"></a>    }</span>
<span id="cb177-26"><a href="intrinsic-rewards.html#cb177-26" tabindex="-1"></a>    </span>
<span id="cb177-27"><a href="intrinsic-rewards.html#cb177-27" tabindex="-1"></a>    state <span class="ot">&lt;-</span> next_state</span>
<span id="cb177-28"><a href="intrinsic-rewards.html#cb177-28" tabindex="-1"></a>  }</span>
<span id="cb177-29"><a href="intrinsic-rewards.html#cb177-29" tabindex="-1"></a>}</span>
<span id="cb177-30"><a href="intrinsic-rewards.html#cb177-30" tabindex="-1"></a></span>
<span id="cb177-31"><a href="intrinsic-rewards.html#cb177-31" tabindex="-1"></a>random_first_success <span class="ot">&lt;-</span> <span class="fu">which</span>(random_success <span class="sc">&gt;</span> <span class="dv">0</span>)[<span class="dv">1</span>]</span>
<span id="cb177-32"><a href="intrinsic-rewards.html#cb177-32" tabindex="-1"></a><span class="cf">if</span> (<span class="fu">is.na</span>(random_first_success)) random_first_success <span class="ot">&lt;-</span> n_episodes</span>
<span id="cb177-33"><a href="intrinsic-rewards.html#cb177-33" tabindex="-1"></a></span>
<span id="cb177-34"><a href="intrinsic-rewards.html#cb177-34" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Curiosity agent first success: episode %d</span><span class="sc">\n</span><span class="st">&quot;</span>, first_success))</span></code></pre></div>
<pre><code>## Curiosity agent first success: episode 500</code></pre>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb179-1"><a href="intrinsic-rewards.html#cb179-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Random agent first success: episode %d</span><span class="sc">\n</span><span class="st">&quot;</span>, random_first_success))</span></code></pre></div>
<pre><code>## Random agent first success: episode 500</code></pre>
<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb181-1"><a href="intrinsic-rewards.html#cb181-1" tabindex="-1"></a><span class="cf">if</span> (first_success <span class="sc">&lt;</span> n_episodes <span class="sc">&amp;&amp;</span> random_first_success <span class="sc">&lt;</span> n_episodes) {</span>
<span id="cb181-2"><a href="intrinsic-rewards.html#cb181-2" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Improvement: %.1fx faster learning</span><span class="sc">\n</span><span class="st">&quot;</span>, </span>
<span id="cb181-3"><a href="intrinsic-rewards.html#cb181-3" tabindex="-1"></a>              random_first_success <span class="sc">/</span> first_success))</span>
<span id="cb181-4"><a href="intrinsic-rewards.html#cb181-4" tabindex="-1"></a>} <span class="cf">else</span> <span class="cf">if</span> (first_success <span class="sc">&lt;</span> random_first_success) {</span>
<span id="cb181-5"><a href="intrinsic-rewards.html#cb181-5" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Curiosity agent learned faster</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb181-6"><a href="intrinsic-rewards.html#cb181-6" tabindex="-1"></a>} <span class="cf">else</span> {</span>
<span id="cb181-7"><a href="intrinsic-rewards.html#cb181-7" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="st">&quot;Both agents had similar performance</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb181-8"><a href="intrinsic-rewards.html#cb181-8" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Both agents had similar performance</code></pre>
<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb183-1"><a href="intrinsic-rewards.html#cb183-1" tabindex="-1"></a><span class="co"># Final performance comparison</span></span>
<span id="cb183-2"><a href="intrinsic-rewards.html#cb183-2" tabindex="-1"></a>curiosity_final_success <span class="ot">&lt;-</span> <span class="fu">mean</span>(episode_rewards[(n_episodes<span class="dv">-99</span>)<span class="sc">:</span>n_episodes] <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb183-3"><a href="intrinsic-rewards.html#cb183-3" tabindex="-1"></a>random_final_success <span class="ot">&lt;-</span> <span class="fu">mean</span>(random_success[(n_episodes<span class="dv">-99</span>)<span class="sc">:</span>n_episodes])</span>
<span id="cb183-4"><a href="intrinsic-rewards.html#cb183-4" tabindex="-1"></a></span>
<span id="cb183-5"><a href="intrinsic-rewards.html#cb183-5" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Final 100 episodes success rate:</span><span class="sc">\n</span><span class="st">&quot;</span>))</span></code></pre></div>
<pre><code>## Final 100 episodes success rate:</code></pre>
<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb185-1"><a href="intrinsic-rewards.html#cb185-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Curiosity agent: %.2f</span><span class="sc">\n</span><span class="st">&quot;</span>, curiosity_final_success))</span></code></pre></div>
<pre><code>## Curiosity agent: 0.00</code></pre>
<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb187-1"><a href="intrinsic-rewards.html#cb187-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Random agent: %.2f</span><span class="sc">\n</span><span class="st">&quot;</span>, random_final_success))</span></code></pre></div>
<pre><code>## Random agent: 0.00</code></pre>
</div>
<div id="advanced-intrinsic-motivation-mechanisms" class="section level2 hasAnchor" number="15.4">
<h2><span class="header-section-number">15.4</span> Advanced Intrinsic Motivation Mechanisms<a href="intrinsic-rewards.html#advanced-intrinsic-motivation-mechanisms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Beyond basic curiosity and count-based approaches, several sophisticated mechanisms have emerged to address specific challenges in intrinsic motivation.</p>
<p>The <strong>Go-Explore</strong> algorithm addresses the problem of detachment, where agents may discover promising regions but fail to return to them reliably. The method maintains an archive of interesting states and uses goal-conditioned policies to return to promising regions for further exploration.</p>
<p><strong>Empowerment</strong> provides another perspective on intrinsic motivation, formalizing the intuition that agents should seek states where they have maximal influence over future outcomes. Mathematically, empowerment is defined as the mutual information between actions and future states:</p>
<p><span class="math display">\[\text{Empowerment}(s) = I(A_{t:t+n}; S_{t+n+1} | S_t = s)\]</span></p>
<p>This quantity measures how much an agent’s action choices affect future state distributions, providing intrinsic motivation toward states with high controllability.</p>
<p><strong>Information Gain</strong> approaches reward agents for reducing uncertainty about environment parameters or dynamics. If the agent maintains a belief distribution <span class="math inline">\(p(\theta)\)</span> over environment parameters, the intrinsic reward for a transition <span class="math inline">\((s, a, s&#39;)\)</span> becomes:</p>
<p><span class="math display">\[R_t^{\text{int}} = H[p(\theta | h_t)] - H[p(\theta | h_t, s_t, a_t, s_{t+1})]\]</span></p>
<p>where <span class="math inline">\(H[\cdot]\)</span> denotes entropy and <span class="math inline">\(h_t\)</span> represents the agent’s history up to time <span class="math inline">\(t\)</span>.</p>
</div>
<div id="implementation-considerations-and-practical-guidelines" class="section level2 hasAnchor" number="15.5">
<h2><span class="header-section-number">15.5</span> Implementation Considerations and Practical Guidelines<a href="intrinsic-rewards.html#implementation-considerations-and-practical-guidelines" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Successful deployment of intrinsic motivation requires careful attention to several design choices. The balance between intrinsic and extrinsic motivation proves crucial, as excessive intrinsic rewards can prevent agents from focusing on task objectives. The weighting parameter <span class="math inline">\(\lambda\)</span> typically requires tuning for each domain, with common values ranging from 0.1 to 1.0.</p>
<p>The choice of intrinsic mechanism depends on problem characteristics. Curiosity-driven approaches work well in deterministic environments with learnable dynamics, while count-based methods suit stochastic environments where prediction-based curiosity might be misled by irreducant randomness. Information gain approaches excel when the environment has hidden parameters that can be discovered through exploration.</p>
<p>Computational overhead represents another important consideration. Some intrinsic motivation mechanisms require training additional neural networks or maintaining complex data structures, potentially doubling computational requirements. Efficient implementations often approximate theoretical ideals to maintain practical feasibility.</p>
</div>
<div id="theoretical-analysis-and-convergence-properties" class="section level2 hasAnchor" number="15.6">
<h2><span class="header-section-number">15.6</span> Theoretical Analysis and Convergence Properties<a href="intrinsic-rewards.html#theoretical-analysis-and-convergence-properties" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The theoretical foundations of intrinsic motivation remain an active area of research. Unlike standard RL, where convergence guarantees are well-established, intrinsic motivation introduces additional complexity through time-varying reward functions that depend on the agent’s learning progress.</p>
<p>For count-based approaches, theoretical analysis shows that under certain regularity conditions, the induced exploration behavior leads to polynomial sample complexity for discovering rewarding states. The key insight is that intrinsic rewards create a curriculum that guides agents toward systematic exploration rather than random wandering.</p>
<p>Curiosity-driven approaches present greater theoretical challenges due to their adaptive nature. The forward model’s learning progress affects the intrinsic reward signal, creating a non-stationary optimization landscape. Recent work has established convergence guarantees under assumptions of bounded prediction error and sufficient exploration, but practical algorithms often deviate from theoretical requirements.</p>
</div>
<div id="applications-and-empirical-results" class="section level2 hasAnchor" number="15.7">
<h2><span class="header-section-number">15.7</span> Applications and Empirical Results<a href="intrinsic-rewards.html#applications-and-empirical-results" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Intrinsic motivation has demonstrated significant improvements across diverse domains. In robotics, curiosity-driven learning enables robots to acquire motion skills without hand-crafted rewards, learning to manipulate objects through self-supervised exploration. The key advantage lies in avoiding the reward engineering bottleneck while still acquiring useful behaviors.</p>
<p>Game-playing environments have provided compelling demonstrations of intrinsic motivation’s effectiveness. In exploration-heavy games like Montezuma’s Revenge, curiosity-driven agents achieve superhuman performance while baseline methods fail to make meaningful progress. The sparse reward structure of these games closely mirrors real-world challenges where feedback is delayed or infrequent.</p>
<p>Scientific discovery represents an emerging application area where intrinsic motivation shows particular promise. Agents can learn to conduct experiments and form hypotheses about unknown systems purely through curiosity, potentially accelerating research in domains where human intuition provides limited guidance.</p>
</div>
<div id="limitations-and-future-directions" class="section level2 hasAnchor" number="15.8">
<h2><span class="header-section-number">15.8</span> Limitations and Future Directions<a href="intrinsic-rewards.html#limitations-and-future-directions" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Despite significant advances, intrinsic motivation faces several fundamental limitations. The reward hacking problem occurs when agents discover ways to maximize intrinsic rewards without learning useful behaviors, such as finding screen regions with unpredictable visual noise in video games.</p>
<p>The noisy TV problem illustrates how stochastic elements can mislead curiosity-based agents. If the environment contains sources of unpredictable but irrelevant variation, agents may focus excessive attention on these regions rather than exploring meaningful state space regions.</p>
<p>Scale and transfer represent ongoing challenges. Most intrinsic motivation work has focused on relatively simple environments, and scaling to complex real-world domains remains difficult. Additionally, skills acquired through intrinsic motivation don’t always transfer effectively to new tasks or domains.</p>
<p>Future research directions include developing more robust intrinsic reward functions that resist exploitation, creating hierarchical approaches that operate across multiple temporal scales, and establishing better theoretical foundations for understanding when and why intrinsic motivation succeeds or fails.</p>
<p>The integration of intrinsic motivation with other RL advances, such as meta-learning and multi-task learning, offers promising avenues for creating more capable and autonomous agents. As environments become more complex and reward engineering becomes more challenging, intrinsic motivation mechanisms will likely play increasingly important roles in developing intelligent systems that can learn and adapt independently.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="eligibility-traces.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/edit/main/15-Intrinsic_Reward.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/blob/main/15-Intrinsic_Reward.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
