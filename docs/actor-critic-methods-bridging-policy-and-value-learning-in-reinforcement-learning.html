<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 17 Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning | Reinforcement Learning in R</title>
  <meta name="description" content="Chapter 17 Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning | Reinforcement Learning in R" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 17 Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning | Reinforcement Learning in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 17 Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning | Reinforcement Learning in R" />
  
  
  

<meta name="author" content="Kamran Afzalui" />


<meta name="date" content="2025-11-01" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"/>
<link rel="next" href="appendix.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Reinforcement Learning in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Understanding Reinforcement Learning: From Bandits to Policy Optimization</strong></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-multi-armed-bandit-the-simplest-case"><i class="fa fa-check"></i><b>1.2</b> The Multi-Armed Bandit: The Simplest Case</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#transition-to-markov-decision-processes"><i class="fa fa-check"></i><b>1.3</b> Transition to Markov Decision Processes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#comparing-reinforcement-learning-methods"><i class="fa fa-check"></i><b>1.4</b> Comparing Reinforcement Learning Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#dynamic-programming-model-based-learning"><i class="fa fa-check"></i><b>1.4.1</b> Dynamic Programming: Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#model-free-approaches-monte-carlo-and-td-learning"><i class="fa fa-check"></i><b>1.4.2</b> Model-Free Approaches: Monte Carlo and TD Learning</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#dyna-bridging-model-free-and-model-based-learning"><i class="fa fa-check"></i><b>1.4.3</b> Dyna: Bridging Model-Free and Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#q-learning-and-function-approximation"><i class="fa fa-check"></i><b>1.4.4</b> Q-Learning and Function Approximation</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#policy-gradient-and-actor-critic-methods"><i class="fa fa-check"></i><b>1.4.5</b> Policy Gradient and Actor-Critic Methods</a></li>
<li class="chapter" data-level="1.4.6" data-path="index.html"><a href="index.html#advanced-policy-optimization-techniques"><i class="fa fa-check"></i><b>1.4.6</b> Advanced Policy Optimization Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#further-directions"><i class="fa fa-check"></i><b>1.5</b> Further Directions</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html"><i class="fa fa-check"></i><b>2</b> The Multi-Armed Bandit Problem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#mathematical-formalism"><i class="fa fa-check"></i><b>2.2</b> Mathematical Formalism</a></li>
<li class="chapter" data-level="2.3" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm"><i class="fa fa-check"></i><b>2.3</b> Frequentist Approach: UCB1 Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1"><i class="fa fa-check"></i><b>2.3.1</b> R Code for UCB1</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling"><i class="fa fa-check"></i><b>2.4</b> Bayesian Approach: Thompson Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling"><i class="fa fa-check"></i><b>2.4.1</b> R Code for Thompson Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy"><i class="fa fa-check"></i><b>2.5</b> Epsilon-Greedy Strategy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy"><i class="fa fa-check"></i><b>2.5.1</b> R Code for Epsilon-Greedy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#summary-table"><i class="fa fa-check"></i><b>2.6</b> Summary Table</a></li>
<li class="chapter" data-level="2.7" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html"><i class="fa fa-check"></i><b>3</b> Markov Decision Processes and Dynamic Programming</a>
<ul>
<li class="chapter" data-level="3.1" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#constructing-the-mdp-in-r"><i class="fa fa-check"></i><b>3.2</b> Constructing the MDP in R</a></li>
<li class="chapter" data-level="3.3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#value-iteration-algorithm"><i class="fa fa-check"></i><b>3.3</b> Value Iteration Algorithm</a></li>
<li class="chapter" data-level="3.4" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#evaluation-and-interpretation"><i class="fa fa-check"></i><b>3.4</b> Evaluation and Interpretation</a></li>
<li class="chapter" data-level="3.5" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties-of-value-iteration"><i class="fa fa-check"></i><b>3.5</b> Theoretical Properties of Value Iteration</a></li>
<li class="chapter" data-level="3.6" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-1"><i class="fa fa-check"></i><b>3.6</b> Summary Table</a></li>
<li class="chapter" data-level="3.7" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><i class="fa fa-check"></i><b>4</b> Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#theoretical-background"><i class="fa fa-check"></i><b>4.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#temporal-difference-learning-q-learning"><i class="fa fa-check"></i><b>4.2.1</b> Temporal Difference Learning (Q-Learning)</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#monte-carlo-methods"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.2.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-1-defining-the-environment-in-r"><i class="fa fa-check"></i><b>4.2.3</b> Step 1: Defining the Environment in R</a></li>
<li class="chapter" data-level="4.2.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-2-q-learning-implementation-in-r"><i class="fa fa-check"></i><b>4.2.4</b> Step 2: Q-Learning Implementation in R</a></li>
<li class="chapter" data-level="4.2.5" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-3-monte-carlo-every-visit-implementation"><i class="fa fa-check"></i><b>4.2.5</b> Step 3: Monte Carlo Every-Visit Implementation</a></li>
<li class="chapter" data-level="4.2.6" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-4-simulating-outcome-devaluation"><i class="fa fa-check"></i><b>4.2.6</b> Step 4: Simulating Outcome Devaluation</a></li>
<li class="chapter" data-level="4.2.7" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-5-comparing-policies-before-and-after-devaluation"><i class="fa fa-check"></i><b>4.2.7</b> Step 5: Comparing Policies Before and After Devaluation</a></li>
<li class="chapter" data-level="4.2.8" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-6-visualizing-the-policies"><i class="fa fa-check"></i><b>4.2.8</b> Step 6: Visualizing the Policies</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#interpretation-and-discussion"><i class="fa fa-check"></i><b>4.3</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="4.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#conclusion-2"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><i class="fa fa-check"></i><b>5</b> On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-on-policy"><i class="fa fa-check"></i><b>5.2</b> SARSA (On-Policy)</a></li>
<li class="chapter" data-level="5.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-off-policy"><i class="fa fa-check"></i><b>5.3</b> Q-Learning (Off-Policy)</a></li>
<li class="chapter" data-level="5.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-with-importance-sampling"><i class="fa fa-check"></i><b>5.4</b> Off-Policy Monte Carlo with Importance Sampling</a></li>
<li class="chapter" data-level="5.5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#key-differences"><i class="fa fa-check"></i><b>5.5</b> Key Differences</a></li>
<li class="chapter" data-level="5.6" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#interpretation-and-discussion-1"><i class="fa fa-check"></i><b>5.6</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#conclusion-3"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
<li class="chapter" data-level="5.8" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#comparison-table"><i class="fa fa-check"></i><b>5.8</b> Comparison Table</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html"><i class="fa fa-check"></i><b>6</b> Function Approximation Q-Learning with Linear Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#q-learning-with-function-approximation"><i class="fa fa-check"></i><b>6.1.1</b> Q-Learning with Function Approximation</a></li>
<li class="chapter" data-level="6.1.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#comparison-with-tabular-q-learning"><i class="fa fa-check"></i><b>6.1.2</b> Comparison with Tabular Q-Learning</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#r-implementation"><i class="fa fa-check"></i><b>6.2</b> R Implementation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><i class="fa fa-check"></i><b>7</b> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation"><i class="fa fa-check"></i><b>7.1.1</b> Q-Learning with Random Forest Approximation</a></li>
<li class="chapter" data-level="7.1.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods"><i class="fa fa-check"></i><b>7.1.2</b> Comparison with Previous Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1"><i class="fa fa-check"></i><b>7.2</b> R Implementation</a></li>
<li class="chapter" data-level="7.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights"><i class="fa fa-check"></i><b>7.3</b> Analysis and Insights</a></li>
<li class="chapter" data-level="7.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4"><i class="fa fa-check"></i><b>7.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><i class="fa fa-check"></i><b>8</b> Deep Function Approximation: Q-Learning with Neural Networks in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation"><i class="fa fa-check"></i><b>8.2</b> Theoretical Foundation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity"><i class="fa fa-check"></i><b>8.2.1</b> Universal Approximation and Expressivity</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning"><i class="fa fa-check"></i><b>8.2.2</b> Gradient-Based Learning</a></li>
<li class="chapter" data-level="8.2.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches"><i class="fa fa-check"></i><b>8.2.3</b> Comparison with Previous Approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2"><i class="fa fa-check"></i><b>8.3</b> R Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation"><i class="fa fa-check"></i><b>8.4</b> Analysis and Interpretation</a></li>
<li class="chapter" data-level="8.5" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions"><i class="fa fa-check"></i><b>8.5</b> Future Directions</a></li>
<li class="chapter" data-level="8.6" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5"><i class="fa fa-check"></i><b>8.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html"><i class="fa fa-check"></i><b>9</b> Dyna and DynaQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#theoretical-framework"><i class="fa fa-check"></i><b>9.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#the-dyna-architecture"><i class="fa fa-check"></i><b>9.2.1</b> The Dyna Architecture</a></li>
<li class="chapter" data-level="9.2.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#model-representation"><i class="fa fa-check"></i><b>9.2.2</b> Model Representation</a></li>
<li class="chapter" data-level="9.2.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#convergence-properties"><i class="fa fa-check"></i><b>9.2.3</b> Convergence Properties</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-in-r"><i class="fa fa-check"></i><b>9.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#environment-setup"><i class="fa fa-check"></i><b>9.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="9.3.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#dyna-q-implementation"><i class="fa fa-check"></i><b>9.3.2</b> Dyna-Q Implementation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#experimental-analysis"><i class="fa fa-check"></i><b>9.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#learning-efficiency-comparison"><i class="fa fa-check"></i><b>9.4.1</b> Learning Efficiency Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
<li class="chapter" data-level="9.6" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-considerations-and-conclusion"><i class="fa fa-check"></i><b>9.6</b> Implementation Considerations and Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><i class="fa fa-check"></i><b>10</b> Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#theoretical-framework-1"><i class="fa fa-check"></i><b>10.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#the-exploration-bonus-mechanism"><i class="fa fa-check"></i><b>10.2.1</b> The Exploration Bonus Mechanism</a></li>
<li class="chapter" data-level="10.2.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#complete-dyna-q-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> Complete Dyna-Q+ Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#convergence-and-stability"><i class="fa fa-check"></i><b>10.2.3</b> Convergence and Stability</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#implementation-in-r-1"><i class="fa fa-check"></i><b>10.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#environment-setup-1"><i class="fa fa-check"></i><b>10.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="10.3.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-implementation-1"><i class="fa fa-check"></i><b>10.3.2</b> Dyna-Q+ Implementation</a></li>
<li class="chapter" data-level="10.3.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#standard-dyna-for-comparison"><i class="fa fa-check"></i><b>10.3.3</b> Standard Dyna for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#experimental-analysis-1"><i class="fa fa-check"></i><b>10.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#adaptation-to-environmental-changes"><i class="fa fa-check"></i><b>10.4.1</b> Adaptation to Environmental Changes</a></li>
<li class="chapter" data-level="10.4.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#parameter-sensitivity-analysis"><i class="fa fa-check"></i><b>10.4.2</b> Parameter Sensitivity Analysis</a></li>
<li class="chapter" data-level="10.4.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#exploration-pattern-analysis"><i class="fa fa-check"></i><b>10.4.3</b> Exploration Pattern Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#discussion-and-implementation-considerations"><i class="fa fa-check"></i><b>10.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="10.6" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#conclusion-6"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html"><i class="fa fa-check"></i><b>11</b> Function Approximation And Feature Engineering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#feature-engineering-and-state-representation"><i class="fa fa-check"></i><b>11.1</b> Feature Engineering and State Representation</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-discrimination-vs.-generalization-tradeoff"><i class="fa fa-check"></i><b>11.1.1</b> The Discrimination vs.Â Generalization Tradeoff</a></li>
<li class="chapter" data-level="11.1.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#principles-of-effective-feature-design"><i class="fa fa-check"></i><b>11.1.2</b> Principles of Effective Feature Design</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#mathematical-foundations-of-linear-function-approximation"><i class="fa fa-check"></i><b>11.2</b> Mathematical Foundations of Linear Function Approximation</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#linear-value-functions-and-their-properties"><i class="fa fa-check"></i><b>11.2.1</b> Linear Value Functions and Their Properties</a></li>
<li class="chapter" data-level="11.2.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#temporal-difference-learning-with-linear-approximation"><i class="fa fa-check"></i><b>11.2.2</b> Temporal Difference Learning with Linear Approximation</a></li>
<li class="chapter" data-level="11.2.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-deadly-triad-and-stability-concerns"><i class="fa fa-check"></i><b>11.2.3</b> The Deadly Triad and Stability Concerns</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#classical-basis-function-methods"><i class="fa fa-check"></i><b>11.3</b> Classical Basis Function Methods</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#coarse-coding-overlapping-receptive-fields"><i class="fa fa-check"></i><b>11.3.1</b> Coarse Coding: Overlapping Receptive Fields</a></li>
<li class="chapter" data-level="11.3.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#tile-coding-structured-overlapping-grids"><i class="fa fa-check"></i><b>11.3.2</b> Tile Coding: Structured Overlapping Grids</a></li>
<li class="chapter" data-level="11.3.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#radial-basis-functions-smooth-continuous-features"><i class="fa fa-check"></i><b>11.3.3</b> Radial Basis Functions: Smooth Continuous Features</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#comparative-analysis-and-practical-considerations"><i class="fa fa-check"></i><b>11.4</b> Comparative Analysis and Practical Considerations</a></li>
<li class="chapter" data-level="11.5" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#bridging-classical-and-modern-approaches"><i class="fa fa-check"></i><b>11.5</b> Bridging Classical and Modern Approaches</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html"><i class="fa fa-check"></i><b>12</b> Learning Policies versus Learning Values:</a>
<ul>
<li class="chapter" data-level="12.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#the-two-paradigms-of-reinforcement-learning"><i class="fa fa-check"></i><b>12.1</b> The Two Paradigms of Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#value-based-methods-learning-worth-before-action"><i class="fa fa-check"></i><b>12.1.1</b> Value-Based Methods: Learning Worth Before Action</a></li>
<li class="chapter" data-level="12.1.2" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#policy-based-methods-direct-optimization-of-behavior"><i class="fa fa-check"></i><b>12.1.2</b> Policy-Based Methods: Direct Optimization of Behavior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><i class="fa fa-check"></i><b>13</b> Average Reward in Reinforcement Learning: A Comprehensive Guide</a>
<ul>
<li class="chapter" data-level="13.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-limitations-of-discounted-reward-formulations"><i class="fa fa-check"></i><b>13.1</b> The Limitations of Discounted Reward Formulations</a></li>
<li class="chapter" data-level="13.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-average-reward-alternative-theoretical-foundations"><i class="fa fa-check"></i><b>13.2</b> The Average Reward Alternative: Theoretical Foundations</a></li>
<li class="chapter" data-level="13.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#value-functions-in-the-average-reward-setting"><i class="fa fa-check"></i><b>13.3</b> Value Functions in the Average Reward Setting</a></li>
<li class="chapter" data-level="13.4" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-theory-for-average-reward"><i class="fa fa-check"></i><b>13.4</b> Optimality Theory for Average Reward</a></li>
<li class="chapter" data-level="13.5" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#learning-algorithms-for-average-reward"><i class="fa fa-check"></i><b>13.5</b> Learning Algorithms for Average Reward</a></li>
<li class="chapter" data-level="13.6" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#practical-implementation-server-load-balancing"><i class="fa fa-check"></i><b>13.6</b> Practical Implementation: Server Load Balancing</a></li>
<li class="chapter" data-level="13.7" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#implementation-considerations"><i class="fa fa-check"></i><b>13.7</b> Implementation Considerations</a></li>
<li class="chapter" data-level="13.8" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#when-to-choose-average-reward-over-discounting"><i class="fa fa-check"></i><b>13.8</b> When to Choose Average Reward Over Discounting</a></li>
<li class="chapter" data-level="13.9" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#appendix-a-mathematical-proofs-and-derivations"><i class="fa fa-check"></i><b>13.9</b> Appendix A: Mathematical Proofs and Derivations</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#convergence-of-average-reward-td-learning"><i class="fa fa-check"></i><b>13.9.1</b> Convergence of Average Reward TD Learning</a></li>
<li class="chapter" data-level="13.9.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#policy-gradient-theorem-for-average-reward"><i class="fa fa-check"></i><b>13.9.2</b> Policy Gradient Theorem for Average Reward</a></li>
<li class="chapter" data-level="13.9.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-equations-derivation"><i class="fa fa-check"></i><b>13.9.3</b> Optimality Equations Derivation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="eligibility-traces.html"><a href="eligibility-traces.html"><i class="fa fa-check"></i><b>14</b> Eligibility Traces</a>
<ul>
<li class="chapter" data-level="14.1" data-path="eligibility-traces.html"><a href="eligibility-traces.html#from-one-step-to-multi-step-learning"><i class="fa fa-check"></i><b>14.1</b> From One-Step to Multi-Step Learning</a></li>
<li class="chapter" data-level="14.2" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-mechanics-of-eligibility-traces-a-backward-view"><i class="fa fa-check"></i><b>14.2</b> The Mechanics of Eligibility Traces: A Backward View</a></li>
<li class="chapter" data-level="14.3" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-tdlambda-algorithm-for-prediction"><i class="fa fa-check"></i><b>14.3</b> The TD(<span class="math inline">\(\\lambda\)</span>) Algorithm for Prediction</a></li>
<li class="chapter" data-level="14.4" data-path="eligibility-traces.html"><a href="eligibility-traces.html#control-with-eligibility-traces-average-reward-sarsalambda"><i class="fa fa-check"></i><b>14.4</b> Control with Eligibility Traces: Average Reward Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.5" data-path="eligibility-traces.html"><a href="eligibility-traces.html#practical-implementation-server-load-balancing-with-sarsalambda"><i class="fa fa-check"></i><b>14.5</b> Practical Implementation: Server Load Balancing with Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.6" data-path="eligibility-traces.html"><a href="eligibility-traces.html#computational-and-performance-considerations"><i class="fa fa-check"></i><b>14.6</b> Computational and Performance Considerations</a></li>
<li class="chapter" data-level="14.7" data-path="eligibility-traces.html"><a href="eligibility-traces.html#conclusion-8"><i class="fa fa-check"></i><b>14.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html"><i class="fa fa-check"></i><b>15</b> Intrinsic Rewards</a>
<ul>
<li class="chapter" data-level="15.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#the-sparse-reward-problem-and-its-implications"><i class="fa fa-check"></i><b>15.1</b> The Sparse Reward Problem and Its Implications</a></li>
<li class="chapter" data-level="15.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#mathematical-foundations-of-intrinsic-motivation"><i class="fa fa-check"></i><b>15.2</b> Mathematical Foundations of Intrinsic Motivation</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#curiosity-driven-learning"><i class="fa fa-check"></i><b>15.2.1</b> Curiosity-Driven Learning</a></li>
<li class="chapter" data-level="15.2.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#count-based-exploration"><i class="fa fa-check"></i><b>15.2.2</b> Count-Based Exploration</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#practical-implementation-curiosity-driven-grid-world-navigation"><i class="fa fa-check"></i><b>15.3</b> Practical Implementation: Curiosity-Driven Grid World Navigation</a></li>
<li class="chapter" data-level="15.4" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#advanced-intrinsic-motivation-mechanisms"><i class="fa fa-check"></i><b>15.4</b> Advanced Intrinsic Motivation Mechanisms</a></li>
<li class="chapter" data-level="15.5" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#implementation-considerations-and-practical-guidelines"><i class="fa fa-check"></i><b>15.5</b> Implementation Considerations and Practical Guidelines</a></li>
<li class="chapter" data-level="15.6" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#theoretical-analysis-and-convergence-properties"><i class="fa fa-check"></i><b>15.6</b> Theoretical Analysis and Convergence Properties</a></li>
<li class="chapter" data-level="15.7" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#applications-and-empirical-results"><i class="fa fa-check"></i><b>15.7</b> Applications and Empirical Results</a></li>
<li class="chapter" data-level="15.8" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#limitations-and-future-directions"><i class="fa fa-check"></i><b>15.8</b> Limitations and Future Directions</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>16</b> Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="16.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#introduction-9"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#theoretical-framework-2"><i class="fa fa-check"></i><b>16.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#the-policy-gradient-theorem-1"><i class="fa fa-check"></i><b>16.2.1</b> The Policy Gradient Theorem</a></li>
<li class="chapter" data-level="16.2.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-algorithm"><i class="fa fa-check"></i><b>16.2.2</b> REINFORCE Algorithm</a></li>
<li class="chapter" data-level="16.2.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#baseline-subtraction-and-variance-reduction"><i class="fa fa-check"></i><b>16.2.3</b> Baseline Subtraction and Variance Reduction</a></li>
<li class="chapter" data-level="16.2.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-parameterization"><i class="fa fa-check"></i><b>16.2.4</b> Softmax Policy Parameterization</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#implementation-in-r-2"><i class="fa fa-check"></i><b>16.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#environment-and-feature-representation"><i class="fa fa-check"></i><b>16.3.1</b> Environment and Feature Representation</a></li>
<li class="chapter" data-level="16.3.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-implementation"><i class="fa fa-check"></i><b>16.3.2</b> Softmax Policy Implementation</a></li>
<li class="chapter" data-level="16.3.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-implementation"><i class="fa fa-check"></i><b>16.3.3</b> REINFORCE Implementation</a></li>
<li class="chapter" data-level="16.3.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#actor-critic-implementation"><i class="fa fa-check"></i><b>16.3.4</b> Actor-Critic Implementation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#experimental-analysis-2"><i class="fa fa-check"></i><b>16.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#comparison-of-policy-gradient-variants"><i class="fa fa-check"></i><b>16.4.1</b> Comparison of Policy Gradient Variants</a></li>
<li class="chapter" data-level="16.4.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#learning-rate-sensitivity-analysis"><i class="fa fa-check"></i><b>16.4.2</b> Learning Rate Sensitivity Analysis</a></li>
<li class="chapter" data-level="16.4.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#variance-analysis-of-gradient-estimates"><i class="fa fa-check"></i><b>16.4.3</b> Variance Analysis of Gradient Estimates</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#discussion-and-implementation-considerations-1"><i class="fa fa-check"></i><b>16.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="16.6" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#conclusion-9"><i class="fa fa-check"></i><b>16.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>17</b> Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="17.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#theoretical-framework-3"><i class="fa fa-check"></i><b>17.1</b> Theoretical Framework</a></li>
<li class="chapter" data-level="17.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#implementation-and-comparative-analysis"><i class="fa fa-check"></i><b>17.2</b> Implementation and Comparative Analysis</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#variance-analysis-and-learning-dynamics"><i class="fa fa-check"></i><b>17.2.1</b> Variance Analysis and Learning Dynamics</a></li>
<li class="chapter" data-level="17.2.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#algorithmic-variants-and-extensions"><i class="fa fa-check"></i><b>17.2.2</b> Algorithmic Variants and Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#computational-and-convergence-considerations"><i class="fa fa-check"></i><b>17.3</b> Computational and Convergence Considerations</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#comparative-performance-analysis"><i class="fa fa-check"></i><b>17.3.1</b> Comparative Performance Analysis</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#conclusion-10"><i class="fa fa-check"></i><b>17.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>18</b> Appendix</a>
<ul>
<li class="chapter" data-level="18.1" data-path="appendix.html"><a href="appendix.html#comprehensive-reinforcement-learning-concepts-guide"><i class="fa fa-check"></i><b>18.1</b> Comprehensive Reinforcement Learning Concepts Guide</a></li>
<li class="chapter" data-level="18.2" data-path="appendix.html"><a href="appendix.html#learning-mechanisms"><i class="fa fa-check"></i><b>18.2</b> Learning Mechanisms</a></li>
<li class="chapter" data-level="18.3" data-path="appendix.html"><a href="appendix.html#environment-properties"><i class="fa fa-check"></i><b>18.3</b> Environment Properties</a></li>
<li class="chapter" data-level="18.4" data-path="appendix.html"><a href="appendix.html#learning-paradigms"><i class="fa fa-check"></i><b>18.4</b> Learning Paradigms</a></li>
<li class="chapter" data-level="18.5" data-path="appendix.html"><a href="appendix.html#exploration-strategies"><i class="fa fa-check"></i><b>18.5</b> Exploration Strategies</a></li>
<li class="chapter" data-level="18.6" data-path="appendix.html"><a href="appendix.html#key-algorithms-methods"><i class="fa fa-check"></i><b>18.6</b> Key Algorithms &amp; Methods</a></li>
<li class="chapter" data-level="18.7" data-path="appendix.html"><a href="appendix.html#advanced-concepts"><i class="fa fa-check"></i><b>18.7</b> Advanced Concepts</a></li>
<li class="chapter" data-level="18.8" data-path="appendix.html"><a href="appendix.html#fundamental-equations"><i class="fa fa-check"></i><b>18.8</b> Fundamental Equations</a>
<ul>
<li class="chapter" data-level="18.8.1" data-path="appendix.html"><a href="appendix.html#bellman-equations"><i class="fa fa-check"></i><b>18.8.1</b> <strong>Bellman Equations</strong></a></li>
<li class="chapter" data-level="18.8.2" data-path="appendix.html"><a href="appendix.html#policy-gradient-theorem"><i class="fa fa-check"></i><b>18.8.2</b> <strong>Policy Gradient Theorem</strong></a></li>
<li class="chapter" data-level="18.8.3" data-path="appendix.html"><a href="appendix.html#temporal-difference-error"><i class="fa fa-check"></i><b>18.8.3</b> <strong>Temporal Difference Error</strong></a></li>
</ul></li>
<li class="chapter" data-level="18.9" data-path="appendix.html"><a href="appendix.html#common-challenges-solutions"><i class="fa fa-check"></i><b>18.9</b> Common Challenges &amp; Solutions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning" class="section level1 hasAnchor" number="17">
<h1><span class="header-section-number">Chapter 17</span> Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning<a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The trajectory from pure policy gradient methods to Actor-Critic algorithms represents one of the most elegant theoretical developments in reinforcement learning. While REINFORCE provides a direct path to policy optimization, its high variance and sample inefficiency reveal fundamental limitations that Actor-Critic methods address through a sophisticated marriage of policy and value learning. This synthesis creates algorithms that retain the policy optimization benefits of gradient methods while leveraging the sample efficiency advantages of temporal difference learning.</p>
<p>The conceptual leap from REINFORCE to Actor-Critic emerges from a deeper understanding of variance sources in policy gradients. When REINFORCE uses entire episode returns to weight policy updates, it conflates the value of individual actions with the accumulated randomness of complete trajectories. Actor-Critic methods resolve this by decomposing the learning problem into two interacting components: an actor that learns the policy and a critic that estimates state or action values to provide more immediate feedback. This decomposition transforms the high-variance Monte Carlo returns of REINFORCE into lower-variance temporal difference estimates, dramatically improving learning stability and speed.</p>
<p>The mathematical foundation underlying this transformation rests on the policy gradient theorem, but with a crucial insight about baseline subtraction. Recall that we can subtract any state-dependent baseline from policy gradient returns without introducing bias. The optimal baseline, in terms of variance minimization, turns out to be the state value function itself. This observation naturally leads to using advantage functions <span class="math inline">\(A(s,a) = Q(s,a) - V(s)\)</span> as the weighting terms in policy updates. Since we typically donât have access to the true advantage function, Actor-Critic methods learn approximations to both the policy and the value function simultaneously, creating a feedback loop where each component improves the other.</p>
<p>The elegance of this approach extends beyond variance reduction. While REINFORCE must wait until episode completion to update its policy, Actor-Critic methods can learn from individual transitions, enabling continuous adaptation throughout each episode. This online learning capability proves particularly valuable in continuing tasks or environments with very long episodes, where the delay inherent in Monte Carlo methods becomes prohibitive. The critic provides immediate feedback about action quality, allowing the actor to adjust its behavior based on short-term consequences rather than distant, noisy episode outcomes.</p>
<div id="theoretical-framework-3" class="section level2 hasAnchor" number="17.1">
<h2><span class="header-section-number">17.1</span> Theoretical Framework<a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#theoretical-framework-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The Actor-Critic architecture decomposes the policy optimization problem into two parallel learning tasks. The actor maintains a parameterized policy <span class="math inline">\(\pi(a|s, \theta)\)</span> and updates its parameters <span class="math inline">\(\theta\)</span> to maximize expected rewards. The critic maintains a value function approximation, typically the state value function <span class="math inline">\(V(s, w)\)</span> parameterized by weights <span class="math inline">\(w\)</span>, though some variants use action-value functions or advantage functions directly.</p>
<p>The criticâs role extends beyond simple value estimation. By learning to predict returns from each state, it provides the actor with a baseline for evaluating action quality. When the critic estimates that a state has high value, actions leading to rewards in that state receive less credit than they would under the raw return. Conversely, actions that generate rewards from states the critic considers poor receive amplified credit. This relative weighting scheme helps the actor focus on genuinely surprising outcomes rather than expected rewards.</p>
<p>The temporal difference error forms the bridge between actor and critic updates. For each transition <span class="math inline">\((s_t, a_t, r_{t+1}, s_{t+1})\)</span>, we compute the TD error:</p>
<p><span class="math display">\[\delta_t = r_{t+1} + \gamma V(s_{t+1}, w) - V(s_t, w)\]</span></p>
<p>This error serves dual purposes: it provides the critic with a learning signal for updating its value estimates, and it gives the actor an estimate of the advantage of action <span class="math inline">\(a_t\)</span> in state <span class="math inline">\(s_t\)</span>. The TD error represents how much better or worse the immediate outcome was compared to the criticâs expectation, making it a natural measure of action quality.</p>
<p>The actor update follows the standard policy gradient form but uses the TD error as the advantage estimate:</p>
<p><span class="math display">\[\theta \leftarrow \theta + \alpha_\theta \nabla_\theta \log \pi(a_t|s_t, \theta) \cdot \delta_t\]</span></p>
<p>The critic update uses the same TD error to improve its value predictions:</p>
<p><span class="math display">\[w \leftarrow w + \alpha_w \delta_t \nabla_w V(s_t, w)\]</span></p>
<p>The learning rates <span class="math inline">\(\alpha_\theta\)</span> and <span class="math inline">\(\alpha_w\)</span> are typically set independently, allowing fine-tuned control over the relative adaptation speeds of the two components. This separation proves crucial because the actor and critic operate on different timescales and have different convergence properties.</p>
<p>The theoretical guarantees for Actor-Critic methods require careful analysis because the algorithm involves two coupled learning processes. The criticâs value function estimates change as the policy evolves, while the policy updates depend on the criticâs current estimates. This interdependence creates a non-stationary learning problem where standard convergence proofs donât immediately apply. However, under appropriate conditions on learning rates and function approximation, convergence results exist that guarantee both components will improve over time.</p>
</div>
<div id="implementation-and-comparative-analysis" class="section level2 hasAnchor" number="17.2">
<h2><span class="header-section-number">17.2</span> Implementation and Comparative Analysis<a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#implementation-and-comparative-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Building upon our previous policy gradient implementations, we can construct a comprehensive Actor-Critic framework that demonstrates both the theoretical elegance and practical advantages of this approach. The implementation reveals subtle aspects of the algorithm that theory alone cannot capture, particularly regarding the interaction between actor and critic learning dynamics.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-1" tabindex="-1"></a><span class="co"># Enhanced Actor-Critic implementation with detailed analysis</span></span>
<span id="cb195-2"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-2" tabindex="-1"></a>actor_critic_enhanced <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha_actor =</span> <span class="fl">0.01</span>, <span class="at">alpha_critic =</span> <span class="fl">0.1</span>,</span>
<span id="cb195-3"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-3" tabindex="-1"></a>                                <span class="at">gamma =</span> <span class="fl">0.95</span>, <span class="at">lambda =</span> <span class="fl">0.9</span>) {</span>
<span id="cb195-4"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-4" tabindex="-1"></a>  </span>
<span id="cb195-5"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-5" tabindex="-1"></a>  <span class="co"># Initialize policy parameters (actor)</span></span>
<span id="cb195-6"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-6" tabindex="-1"></a>  n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb195-7"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-7" tabindex="-1"></a>  n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb195-8"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-8" tabindex="-1"></a>  n_params <span class="ot">&lt;-</span> n_states <span class="sc">*</span> n_actions</span>
<span id="cb195-9"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-9" tabindex="-1"></a>  theta <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_params, <span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb195-10"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-10" tabindex="-1"></a>  </span>
<span id="cb195-11"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-11" tabindex="-1"></a>  <span class="co"># Initialize value function parameters (critic) </span></span>
<span id="cb195-12"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-12" tabindex="-1"></a>  v_weights <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_states, <span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb195-13"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-13" tabindex="-1"></a>  </span>
<span id="cb195-14"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-14" tabindex="-1"></a>  <span class="co"># Eligibility traces for both actor and critic</span></span>
<span id="cb195-15"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-15" tabindex="-1"></a>  theta_eligibility <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_params)</span>
<span id="cb195-16"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-16" tabindex="-1"></a>  v_eligibility <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb195-17"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-17" tabindex="-1"></a>  </span>
<span id="cb195-18"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-18" tabindex="-1"></a>  <span class="co"># Tracking variables for analysis</span></span>
<span id="cb195-19"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-19" tabindex="-1"></a>  td_errors_history <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb195-20"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-20" tabindex="-1"></a>  value_estimates_history <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb195-21"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-21" tabindex="-1"></a>  policy_entropy_history <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb195-22"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-22" tabindex="-1"></a>  gradient_magnitudes <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb195-23"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-23" tabindex="-1"></a>  </span>
<span id="cb195-24"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-24" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb195-25"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-25" tabindex="-1"></a>    <span class="co"># Reset eligibility traces for new episode</span></span>
<span id="cb195-26"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-26" tabindex="-1"></a>    theta_eligibility <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_params)</span>
<span id="cb195-27"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-27" tabindex="-1"></a>    v_eligibility <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_states)</span>
<span id="cb195-28"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-28" tabindex="-1"></a>    </span>
<span id="cb195-29"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-29" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb195-30"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-30" tabindex="-1"></a>    step <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb195-31"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-31" tabindex="-1"></a>    episode_td_errors <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb195-32"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-32" tabindex="-1"></a>    episode_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>()</span>
<span id="cb195-33"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-33" tabindex="-1"></a>    </span>
<span id="cb195-34"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-34" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state <span class="sc">&amp;&amp;</span> step <span class="sc">&lt;</span> <span class="dv">100</span>) {</span>
<span id="cb195-35"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-35" tabindex="-1"></a>      step <span class="ot">&lt;-</span> step <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb195-36"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-36" tabindex="-1"></a>      </span>
<span id="cb195-37"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-37" tabindex="-1"></a>      <span class="co"># Store current value estimate</span></span>
<span id="cb195-38"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-38" tabindex="-1"></a>      v_current <span class="ot">&lt;-</span> <span class="fu">sum</span>(v_weights <span class="sc">*</span> <span class="fu">extract_features</span>(s))</span>
<span id="cb195-39"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-39" tabindex="-1"></a>      episode_values <span class="ot">&lt;-</span> <span class="fu">c</span>(episode_values, v_current)</span>
<span id="cb195-40"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-40" tabindex="-1"></a>      </span>
<span id="cb195-41"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-41" tabindex="-1"></a>      <span class="co"># Sample action from current policy</span></span>
<span id="cb195-42"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-42" tabindex="-1"></a>      action_probs <span class="ot">&lt;-</span> <span class="fu">softmax_policy</span>(s, theta)</span>
<span id="cb195-43"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-43" tabindex="-1"></a>      action <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>, <span class="at">prob =</span> action_probs)</span>
<span id="cb195-44"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-44" tabindex="-1"></a>      </span>
<span id="cb195-45"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-45" tabindex="-1"></a>      <span class="co"># Take action and observe outcome</span></span>
<span id="cb195-46"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-46" tabindex="-1"></a>      outcome <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, action)</span>
<span id="cb195-47"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-47" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> outcome<span class="sc">$</span>s_prime</span>
<span id="cb195-48"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-48" tabindex="-1"></a>      reward <span class="ot">&lt;-</span> outcome<span class="sc">$</span>reward</span>
<span id="cb195-49"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-49" tabindex="-1"></a>      </span>
<span id="cb195-50"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-50" tabindex="-1"></a>      <span class="co"># Compute TD error</span></span>
<span id="cb195-51"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-51" tabindex="-1"></a>      <span class="cf">if</span> (s_prime <span class="sc">==</span> terminal_state) {</span>
<span id="cb195-52"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-52" tabindex="-1"></a>        td_error <span class="ot">&lt;-</span> reward <span class="sc">-</span> v_current</span>
<span id="cb195-53"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-53" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb195-54"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-54" tabindex="-1"></a>        v_next <span class="ot">&lt;-</span> <span class="fu">sum</span>(v_weights <span class="sc">*</span> <span class="fu">extract_features</span>(s_prime))</span>
<span id="cb195-55"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-55" tabindex="-1"></a>        td_error <span class="ot">&lt;-</span> reward <span class="sc">+</span> gamma <span class="sc">*</span> v_next <span class="sc">-</span> v_current</span>
<span id="cb195-56"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-56" tabindex="-1"></a>      }</span>
<span id="cb195-57"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-57" tabindex="-1"></a>      </span>
<span id="cb195-58"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-58" tabindex="-1"></a>      episode_td_errors <span class="ot">&lt;-</span> <span class="fu">c</span>(episode_td_errors, td_error)</span>
<span id="cb195-59"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-59" tabindex="-1"></a>      </span>
<span id="cb195-60"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-60" tabindex="-1"></a>      <span class="co"># Update eligibility traces</span></span>
<span id="cb195-61"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-61" tabindex="-1"></a>      grad_log_pi <span class="ot">&lt;-</span> <span class="fu">grad_log_prob</span>(s, action, theta)</span>
<span id="cb195-62"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-62" tabindex="-1"></a>      theta_eligibility <span class="ot">&lt;-</span> gamma <span class="sc">*</span> lambda <span class="sc">*</span> theta_eligibility <span class="sc">+</span> grad_log_pi</span>
<span id="cb195-63"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-63" tabindex="-1"></a>      </span>
<span id="cb195-64"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-64" tabindex="-1"></a>      state_features <span class="ot">&lt;-</span> <span class="fu">extract_features</span>(s)</span>
<span id="cb195-65"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-65" tabindex="-1"></a>      v_eligibility <span class="ot">&lt;-</span> gamma <span class="sc">*</span> lambda <span class="sc">*</span> v_eligibility <span class="sc">+</span> state_features</span>
<span id="cb195-66"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-66" tabindex="-1"></a>      </span>
<span id="cb195-67"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-67" tabindex="-1"></a>      <span class="co"># Update actor and critic using eligibility traces</span></span>
<span id="cb195-68"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-68" tabindex="-1"></a>      theta <span class="ot">&lt;-</span> theta <span class="sc">+</span> alpha_actor <span class="sc">*</span> td_error <span class="sc">*</span> theta_eligibility</span>
<span id="cb195-69"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-69" tabindex="-1"></a>      v_weights <span class="ot">&lt;-</span> v_weights <span class="sc">+</span> alpha_critic <span class="sc">*</span> td_error <span class="sc">*</span> v_eligibility</span>
<span id="cb195-70"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-70" tabindex="-1"></a>      </span>
<span id="cb195-71"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-71" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb195-72"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-72" tabindex="-1"></a>    }</span>
<span id="cb195-73"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-73" tabindex="-1"></a>    </span>
<span id="cb195-74"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-74" tabindex="-1"></a>    <span class="co"># Calculate policy entropy for this episode</span></span>
<span id="cb195-75"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-75" tabindex="-1"></a>    entropy <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb195-76"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-76" tabindex="-1"></a>    <span class="cf">for</span> (state <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states<span class="dv">-1</span>)) {</span>
<span id="cb195-77"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-77" tabindex="-1"></a>      probs <span class="ot">&lt;-</span> <span class="fu">softmax_policy</span>(state, theta)</span>
<span id="cb195-78"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-78" tabindex="-1"></a>      entropy <span class="ot">&lt;-</span> entropy <span class="sc">-</span> <span class="fu">sum</span>(probs <span class="sc">*</span> <span class="fu">log</span>(probs <span class="sc">+</span> <span class="fl">1e-8</span>))</span>
<span id="cb195-79"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-79" tabindex="-1"></a>    }</span>
<span id="cb195-80"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-80" tabindex="-1"></a>    policy_entropy_history[ep] <span class="ot">&lt;-</span> entropy <span class="sc">/</span> (n_states <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb195-81"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-81" tabindex="-1"></a>    </span>
<span id="cb195-82"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-82" tabindex="-1"></a>    <span class="co"># Store episode statistics</span></span>
<span id="cb195-83"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-83" tabindex="-1"></a>    td_errors_history[[ep]] <span class="ot">&lt;-</span> episode_td_errors</span>
<span id="cb195-84"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-84" tabindex="-1"></a>    value_estimates_history[[ep]] <span class="ot">&lt;-</span> episode_values</span>
<span id="cb195-85"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-85" tabindex="-1"></a>    </span>
<span id="cb195-86"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-86" tabindex="-1"></a>    <span class="co"># Compute gradient magnitude for analysis</span></span>
<span id="cb195-87"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-87" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">length</span>(episode_td_errors) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb195-88"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-88" tabindex="-1"></a>      avg_td_error <span class="ot">&lt;-</span> <span class="fu">mean</span>(episode_td_errors)</span>
<span id="cb195-89"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-89" tabindex="-1"></a>      gradient_mag <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">sum</span>((alpha_actor <span class="sc">*</span> avg_td_error <span class="sc">*</span> theta_eligibility)<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb195-90"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-90" tabindex="-1"></a>      gradient_magnitudes[ep] <span class="ot">&lt;-</span> gradient_mag</span>
<span id="cb195-91"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-91" tabindex="-1"></a>    }</span>
<span id="cb195-92"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-92" tabindex="-1"></a>  }</span>
<span id="cb195-93"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-93" tabindex="-1"></a>  </span>
<span id="cb195-94"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-94" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb195-95"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-95" tabindex="-1"></a>    <span class="at">theta =</span> theta,</span>
<span id="cb195-96"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-96" tabindex="-1"></a>    <span class="at">v_weights =</span> v_weights,</span>
<span id="cb195-97"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-97" tabindex="-1"></a>    <span class="at">td_errors_history =</span> td_errors_history,</span>
<span id="cb195-98"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-98" tabindex="-1"></a>    <span class="at">value_estimates_history =</span> value_estimates_history,</span>
<span id="cb195-99"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-99" tabindex="-1"></a>    <span class="at">policy_entropy_history =</span> policy_entropy_history,</span>
<span id="cb195-100"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-100" tabindex="-1"></a>    <span class="at">gradient_magnitudes =</span> gradient_magnitudes</span>
<span id="cb195-101"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-101" tabindex="-1"></a>  ))</span>
<span id="cb195-102"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-102" tabindex="-1"></a>}</span>
<span id="cb195-103"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-103" tabindex="-1"></a></span>
<span id="cb195-104"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-104" tabindex="-1"></a><span class="co"># Comparative evaluation function</span></span>
<span id="cb195-105"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-105" tabindex="-1"></a>compare_methods <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">800</span>) {</span>
<span id="cb195-106"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-106" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb195-107"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-107" tabindex="-1"></a>  </span>
<span id="cb195-108"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-108" tabindex="-1"></a>  <span class="co"># Run REINFORCE with baseline</span></span>
<span id="cb195-109"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-109" tabindex="-1"></a>  reinforce_result <span class="ot">&lt;-</span> <span class="fu">reinforce</span>(<span class="at">episodes =</span> episodes, <span class="at">alpha =</span> <span class="fl">0.005</span>, <span class="at">baseline =</span> <span class="cn">TRUE</span>)</span>
<span id="cb195-110"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-110" tabindex="-1"></a>  </span>
<span id="cb195-111"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-111" tabindex="-1"></a>  <span class="co"># Run basic Actor-Critic</span></span>
<span id="cb195-112"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-112" tabindex="-1"></a>  ac_basic <span class="ot">&lt;-</span> <span class="fu">actor_critic</span>(<span class="at">episodes =</span> episodes, <span class="at">alpha_actor =</span> <span class="fl">0.005</span>, <span class="at">alpha_critic =</span> <span class="fl">0.02</span>)</span>
<span id="cb195-113"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-113" tabindex="-1"></a>  </span>
<span id="cb195-114"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-114" tabindex="-1"></a>  <span class="co"># Run enhanced Actor-Critic with eligibility traces</span></span>
<span id="cb195-115"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-115" tabindex="-1"></a>  ac_enhanced <span class="ot">&lt;-</span> <span class="fu">actor_critic_enhanced</span>(<span class="at">episodes =</span> episodes, <span class="at">alpha_actor =</span> <span class="fl">0.005</span>, </span>
<span id="cb195-116"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-116" tabindex="-1"></a>                                     <span class="at">alpha_critic =</span> <span class="fl">0.02</span>, <span class="at">lambda =</span> <span class="fl">0.9</span>)</span>
<span id="cb195-117"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-117" tabindex="-1"></a>  </span>
<span id="cb195-118"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-118" tabindex="-1"></a>  <span class="co"># Evaluate final policies</span></span>
<span id="cb195-119"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-119" tabindex="-1"></a>  evaluation_episodes <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb195-120"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-120" tabindex="-1"></a>  </span>
<span id="cb195-121"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-121" tabindex="-1"></a>  reinforce_performance <span class="ot">&lt;-</span> <span class="fu">evaluate_policy</span>(reinforce_result<span class="sc">$</span>theta, evaluation_episodes)</span>
<span id="cb195-122"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-122" tabindex="-1"></a>  ac_basic_performance <span class="ot">&lt;-</span> <span class="fu">evaluate_policy</span>(ac_basic<span class="sc">$</span>theta, evaluation_episodes)</span>
<span id="cb195-123"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-123" tabindex="-1"></a>  ac_enhanced_performance <span class="ot">&lt;-</span> <span class="fu">evaluate_policy</span>(ac_enhanced<span class="sc">$</span>theta, evaluation_episodes)</span>
<span id="cb195-124"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-124" tabindex="-1"></a>  </span>
<span id="cb195-125"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-125" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb195-126"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-126" tabindex="-1"></a>    <span class="at">reinforce =</span> <span class="fu">list</span>(<span class="at">result =</span> reinforce_result, <span class="at">performance =</span> reinforce_performance),</span>
<span id="cb195-127"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-127" tabindex="-1"></a>    <span class="at">ac_basic =</span> <span class="fu">list</span>(<span class="at">result =</span> ac_basic, <span class="at">performance =</span> ac_basic_performance),</span>
<span id="cb195-128"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-128" tabindex="-1"></a>    <span class="at">ac_enhanced =</span> <span class="fu">list</span>(<span class="at">result =</span> ac_enhanced, <span class="at">performance =</span> ac_enhanced_performance)</span>
<span id="cb195-129"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-129" tabindex="-1"></a>  ))</span>
<span id="cb195-130"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb195-130" tabindex="-1"></a>}</span></code></pre></div>
<p>The enhanced implementation incorporates eligibility traces, which extend the credit assignment mechanism of Actor-Critic methods. Rather than updating parameters based only on the current transition, eligibility traces create a decaying memory of recent state-action pairs, allowing TD errors to update multiple previous estimates simultaneously. This mechanism addresses the temporal credit assignment problem more effectively than basic one-step updates, particularly in environments where the consequences of actions unfold over multiple time steps.</p>
<p>The inclusion of policy entropy tracking provides insights into the exploration-exploitation dynamics of Actor-Critic learning. As the algorithm progresses, we expect the policy entropy to decrease as the policy becomes more deterministic around optimal actions. However, premature entropy collapse can indicate insufficient exploration, while persistently high entropy might suggest convergence problems. Monitoring this quantity helps diagnose learning difficulties and guide hyperparameter selection.</p>
<div id="variance-analysis-and-learning-dynamics" class="section level3 hasAnchor" number="17.2.1">
<h3><span class="header-section-number">17.2.1</span> Variance Analysis and Learning Dynamics<a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#variance-analysis-and-learning-dynamics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The fundamental advantage of Actor-Critic methods lies in their variance reduction compared to pure policy gradient approaches. To understand this improvement quantitatively, we need to examine how the different components contribute to gradient estimate variance and how the criticâs learning affects overall stability.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-1" tabindex="-1"></a><span class="co"># Detailed variance analysis function</span></span>
<span id="cb196-2"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-2" tabindex="-1"></a>analyze_learning_dynamics <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb196-3"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-3" tabindex="-1"></a>  comparison_results <span class="ot">&lt;-</span> <span class="fu">compare_methods</span>(<span class="at">episodes =</span> <span class="dv">600</span>)</span>
<span id="cb196-4"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-4" tabindex="-1"></a>  </span>
<span id="cb196-5"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-5" tabindex="-1"></a>  <span class="co"># Extract TD errors from Actor-Critic results</span></span>
<span id="cb196-6"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-6" tabindex="-1"></a>  ac_td_errors <span class="ot">&lt;-</span> comparison_results<span class="sc">$</span>ac_enhanced<span class="sc">$</span>result<span class="sc">$</span>td_errors_history</span>
<span id="cb196-7"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-7" tabindex="-1"></a>  </span>
<span id="cb196-8"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-8" tabindex="-1"></a>  <span class="co"># Compute variance statistics</span></span>
<span id="cb196-9"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-9" tabindex="-1"></a>  episode_td_variances <span class="ot">&lt;-</span> <span class="fu">sapply</span>(ac_td_errors, <span class="cf">function</span>(errors) {</span>
<span id="cb196-10"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-10" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">length</span>(errors) <span class="sc">&gt;</span> <span class="dv">1</span>) <span class="fu">var</span>(errors) <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb196-11"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-11" tabindex="-1"></a>  })</span>
<span id="cb196-12"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-12" tabindex="-1"></a>  </span>
<span id="cb196-13"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-13" tabindex="-1"></a>  episode_td_means <span class="ot">&lt;-</span> <span class="fu">sapply</span>(ac_td_errors, <span class="cf">function</span>(errors) {</span>
<span id="cb196-14"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-14" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">length</span>(errors) <span class="sc">&gt;</span> <span class="dv">0</span>) <span class="fu">mean</span>(errors) <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb196-15"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-15" tabindex="-1"></a>  })</span>
<span id="cb196-16"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-16" tabindex="-1"></a>  </span>
<span id="cb196-17"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-17" tabindex="-1"></a>  <span class="co"># Analyze value function learning</span></span>
<span id="cb196-18"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-18" tabindex="-1"></a>  value_histories <span class="ot">&lt;-</span> comparison_results<span class="sc">$</span>ac_enhanced<span class="sc">$</span>result<span class="sc">$</span>value_estimates_history</span>
<span id="cb196-19"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-19" tabindex="-1"></a>  </span>
<span id="cb196-20"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-20" tabindex="-1"></a>  <span class="co"># Compute how value estimates change over time for each state</span></span>
<span id="cb196-21"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-21" tabindex="-1"></a>  value_evolution <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> <span class="dv">600</span>, <span class="at">ncol =</span> <span class="dv">9</span>)  <span class="co"># Excluding terminal state</span></span>
<span id="cb196-22"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-22" tabindex="-1"></a>  </span>
<span id="cb196-23"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-23" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">min</span>(<span class="dv">600</span>, <span class="fu">length</span>(value_histories))) {</span>
<span id="cb196-24"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-24" tabindex="-1"></a>    <span class="cf">if</span> (<span class="fu">length</span>(value_histories[[ep]]) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb196-25"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-25" tabindex="-1"></a>      <span class="co"># Take first value estimate of episode (from starting state)</span></span>
<span id="cb196-26"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-26" tabindex="-1"></a>      value_evolution[ep, <span class="dv">1</span>] <span class="ot">&lt;-</span> value_histories[[ep]][<span class="dv">1</span>]</span>
<span id="cb196-27"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-27" tabindex="-1"></a>    }</span>
<span id="cb196-28"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-28" tabindex="-1"></a>  }</span>
<span id="cb196-29"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-29" tabindex="-1"></a>  </span>
<span id="cb196-30"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-30" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb196-31"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-31" tabindex="-1"></a>    <span class="at">td_variances =</span> episode_td_variances,</span>
<span id="cb196-32"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-32" tabindex="-1"></a>    <span class="at">td_means =</span> episode_td_means,</span>
<span id="cb196-33"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-33" tabindex="-1"></a>    <span class="at">value_evolution =</span> value_evolution,</span>
<span id="cb196-34"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-34" tabindex="-1"></a>    <span class="at">policy_entropy =</span> comparison_results<span class="sc">$</span>ac_enhanced<span class="sc">$</span>result<span class="sc">$</span>policy_entropy_history,</span>
<span id="cb196-35"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-35" tabindex="-1"></a>    <span class="at">gradient_magnitudes =</span> comparison_results<span class="sc">$</span>ac_enhanced<span class="sc">$</span>result<span class="sc">$</span>gradient_magnitudes</span>
<span id="cb196-36"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-36" tabindex="-1"></a>  ))</span>
<span id="cb196-37"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-37" tabindex="-1"></a>}</span>
<span id="cb196-38"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-38" tabindex="-1"></a></span>
<span id="cb196-39"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-39" tabindex="-1"></a><span class="co"># Visualization function for learning dynamics</span></span>
<span id="cb196-40"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-40" tabindex="-1"></a>plot_learning_dynamics <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb196-41"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-41" tabindex="-1"></a>  dynamics_data <span class="ot">&lt;-</span> <span class="fu">analyze_learning_dynamics</span>()</span>
<span id="cb196-42"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-42" tabindex="-1"></a>  </span>
<span id="cb196-43"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-43" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">2</span>))</span>
<span id="cb196-44"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-44" tabindex="-1"></a>  </span>
<span id="cb196-45"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-45" tabindex="-1"></a>  <span class="co"># Plot 1: TD error variance over episodes</span></span>
<span id="cb196-46"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-46" tabindex="-1"></a>  episodes <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(dynamics_data<span class="sc">$</span>td_variances)</span>
<span id="cb196-47"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-47" tabindex="-1"></a>  <span class="fu">plot</span>(episodes, dynamics_data<span class="sc">$</span>td_variances, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, </span>
<span id="cb196-48"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-48" tabindex="-1"></a>       <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb196-49"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-49" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Episode&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;TD Error Variance&quot;</span>,</span>
<span id="cb196-50"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-50" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;TD Error Variance Evolution&quot;</span>)</span>
<span id="cb196-51"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-51" tabindex="-1"></a>  </span>
<span id="cb196-52"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-52" tabindex="-1"></a>  <span class="co"># Add smoothed trend line</span></span>
<span id="cb196-53"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-53" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(dynamics_data<span class="sc">$</span>td_variances) <span class="sc">&gt;</span> <span class="dv">10</span>) {</span>
<span id="cb196-54"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-54" tabindex="-1"></a>    smoothed_var <span class="ot">&lt;-</span> zoo<span class="sc">::</span><span class="fu">rollmean</span>(dynamics_data<span class="sc">$</span>td_variances, <span class="at">k =</span> <span class="dv">20</span>, <span class="at">fill =</span> <span class="cn">NA</span>)</span>
<span id="cb196-55"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-55" tabindex="-1"></a>    <span class="fu">lines</span>(episodes, smoothed_var, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb196-56"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-56" tabindex="-1"></a>  }</span>
<span id="cb196-57"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-57" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb196-58"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-58" tabindex="-1"></a>  </span>
<span id="cb196-59"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-59" tabindex="-1"></a>  <span class="co"># Plot 2: Policy entropy evolution</span></span>
<span id="cb196-60"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-60" tabindex="-1"></a>  <span class="fu">plot</span>(episodes, dynamics_data<span class="sc">$</span>policy_entropy[<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(episodes)], </span>
<span id="cb196-61"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-61" tabindex="-1"></a>       <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;darkgreen&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb196-62"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-62" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Episode&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Policy Entropy&quot;</span>,</span>
<span id="cb196-63"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-63" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Policy Entropy Evolution&quot;</span>)</span>
<span id="cb196-64"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-64" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb196-65"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-65" tabindex="-1"></a>  </span>
<span id="cb196-66"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-66" tabindex="-1"></a>  <span class="co"># Plot 3: Value function learning for starting state</span></span>
<span id="cb196-67"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-67" tabindex="-1"></a>  <span class="fu">plot</span>(episodes, dynamics_data<span class="sc">$</span>value_evolution[<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(episodes), <span class="dv">1</span>], </span>
<span id="cb196-68"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-68" tabindex="-1"></a>       <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;purple&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb196-69"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-69" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Episode&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Value Estimate&quot;</span>,</span>
<span id="cb196-70"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-70" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Value Function Learning (Start State)&quot;</span>)</span>
<span id="cb196-71"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-71" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb196-72"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-72" tabindex="-1"></a>  </span>
<span id="cb196-73"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-73" tabindex="-1"></a>  <span class="co"># Plot 4: Gradient magnitude evolution</span></span>
<span id="cb196-74"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-74" tabindex="-1"></a>  <span class="fu">plot</span>(episodes, dynamics_data<span class="sc">$</span>gradient_magnitudes[<span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(episodes)], </span>
<span id="cb196-75"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-75" tabindex="-1"></a>       <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;orange&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb196-76"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-76" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Episode&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Gradient Magnitude&quot;</span>,</span>
<span id="cb196-77"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-77" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Policy Gradient Magnitudes&quot;</span>)</span>
<span id="cb196-78"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-78" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb196-79"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-79" tabindex="-1"></a>  </span>
<span id="cb196-80"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-80" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb196-81"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb196-81" tabindex="-1"></a>}</span></code></pre></div>
<p>The variance analysis reveals several key insights about Actor-Critic learning dynamics. Initially, TD errors exhibit high variance as both the policy and value function are poorly calibrated. As learning progresses, we typically observe a reduction in TD error variance, indicating that the critic is becoming better at predicting returns and providing more stable feedback to the actor. However, this trend isnât monotonicâperiods of increased variance often correspond to significant policy changes as the actor explores new behaviors.</p>
<p>The relationship between policy entropy and learning progress provides another window into algorithm behavior. Healthy Actor-Critic learning typically shows a gradual decline in entropy as the policy concentrates probability mass on better actions. Sudden entropy drops might indicate premature convergence to suboptimal policies, while persistently high entropy could suggest learning difficulties or inappropriate hyperparameters. The entropy evolution also reveals the exploration-exploitation balance inherent in the algorithmâs design.</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb197-1" tabindex="-1"></a><span class="co"># Execute the analysis</span></span>
<span id="cb197-2"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb197-2" tabindex="-1"></a><span class="fu">plot_learning_dynamics</span>()</span></code></pre></div>
</div>
<div id="algorithmic-variants-and-extensions" class="section level3 hasAnchor" number="17.2.2">
<h3><span class="header-section-number">17.2.2</span> Algorithmic Variants and Extensions<a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#algorithmic-variants-and-extensions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The basic Actor-Critic framework admits numerous variations, each addressing specific limitations or targeting particular problem domains. These variants illustrate the flexibility of the core concept while revealing the subtle design choices that affect algorithm behavior.</p>
<p>One significant variant involves the choice of critic architecture. While our implementation uses state value functions, action-value critics that learn <span class="math inline">\(Q(s,a,w)\)</span> provide richer information about individual action values. The advantage estimation becomes more direct since <span class="math inline">\(A(s,a) = Q(s,a) - \max_{a&#39;} Q(s,a&#39;)\)</span>, but the learning problem becomes more complex as the critic must learn values for all state-action pairs rather than just states.</p>
<p>Advantage Actor-Critic (A2C) methods represent another important direction, explicitly learning advantage functions rather than deriving them from value estimates. This approach can reduce bias in advantage estimates but requires careful handling of the advantage functionâs inherent constraintsâadvantages must sum to zero across actions for any given state.</p>
<p>The temporal scope of updates offers another dimension for algorithmic variation. Our implementation uses one-step TD errors, but n-step returns provide a natural interpolation between the low-variance, high-bias one-step estimates and the high-variance, low-bias Monte Carlo returns of REINFORCE. The parameter <span class="math inline">\(\lambda\)</span> in TD(<span class="math inline">\(\lambda\)</span>) methods controls this interpolation, with <span class="math inline">\(\lambda = 0\)</span> corresponding to pure TD learning and <span class="math inline">\(\lambda = 1\)</span> approximating Monte Carlo methods.</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-1" tabindex="-1"></a><span class="co"># N-step Actor-Critic implementation</span></span>
<span id="cb198-2"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-2" tabindex="-1"></a>n_step_actor_critic <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">500</span>, <span class="at">n_steps =</span> <span class="dv">5</span>, </span>
<span id="cb198-3"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-3" tabindex="-1"></a>                               <span class="at">alpha_actor =</span> <span class="fl">0.01</span>, <span class="at">alpha_critic =</span> <span class="fl">0.1</span>) {</span>
<span id="cb198-4"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-4" tabindex="-1"></a>  </span>
<span id="cb198-5"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-5" tabindex="-1"></a>  <span class="co"># Initialize parameters</span></span>
<span id="cb198-6"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-6" tabindex="-1"></a>  n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb198-7"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-7" tabindex="-1"></a>  n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb198-8"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-8" tabindex="-1"></a>  n_params <span class="ot">&lt;-</span> n_states <span class="sc">*</span> n_actions</span>
<span id="cb198-9"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-9" tabindex="-1"></a>  theta <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_params, <span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb198-10"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-10" tabindex="-1"></a>  v_weights <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_states, <span class="dv">0</span>, <span class="fl">0.1</span>)</span>
<span id="cb198-11"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-11" tabindex="-1"></a>  </span>
<span id="cb198-12"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-12" tabindex="-1"></a>  episode_returns <span class="ot">&lt;-</span> <span class="fu">numeric</span>(episodes)</span>
<span id="cb198-13"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-13" tabindex="-1"></a>  </span>
<span id="cb198-14"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-14" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb198-15"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-15" tabindex="-1"></a>    <span class="co"># Store trajectory for n-step updates</span></span>
<span id="cb198-16"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-16" tabindex="-1"></a>    trajectory <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb198-17"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-17" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb198-18"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-18" tabindex="-1"></a>    step <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb198-19"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-19" tabindex="-1"></a>    </span>
<span id="cb198-20"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-20" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state <span class="sc">&amp;&amp;</span> step <span class="sc">&lt;</span> <span class="dv">100</span>) {</span>
<span id="cb198-21"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-21" tabindex="-1"></a>      step <span class="ot">&lt;-</span> step <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb198-22"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-22" tabindex="-1"></a>      </span>
<span id="cb198-23"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-23" tabindex="-1"></a>      <span class="co"># Store current state and value</span></span>
<span id="cb198-24"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-24" tabindex="-1"></a>      current_value <span class="ot">&lt;-</span> <span class="fu">sum</span>(v_weights <span class="sc">*</span> <span class="fu">extract_features</span>(s))</span>
<span id="cb198-25"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-25" tabindex="-1"></a>      </span>
<span id="cb198-26"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-26" tabindex="-1"></a>      <span class="co"># Sample action</span></span>
<span id="cb198-27"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-27" tabindex="-1"></a>      action_probs <span class="ot">&lt;-</span> <span class="fu">softmax_policy</span>(s, theta)</span>
<span id="cb198-28"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-28" tabindex="-1"></a>      action <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>, <span class="at">prob =</span> action_probs)</span>
<span id="cb198-29"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-29" tabindex="-1"></a>      </span>
<span id="cb198-30"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-30" tabindex="-1"></a>      <span class="co"># Take action</span></span>
<span id="cb198-31"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-31" tabindex="-1"></a>      outcome <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, action)</span>
<span id="cb198-32"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-32" tabindex="-1"></a>      </span>
<span id="cb198-33"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-33" tabindex="-1"></a>      <span class="co"># Store experience</span></span>
<span id="cb198-34"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-34" tabindex="-1"></a>      trajectory[[step]] <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb198-35"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-35" tabindex="-1"></a>        <span class="at">state =</span> s,</span>
<span id="cb198-36"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-36" tabindex="-1"></a>        <span class="at">action =</span> action,</span>
<span id="cb198-37"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-37" tabindex="-1"></a>        <span class="at">reward =</span> outcome<span class="sc">$</span>reward,</span>
<span id="cb198-38"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-38" tabindex="-1"></a>        <span class="at">value =</span> current_value</span>
<span id="cb198-39"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-39" tabindex="-1"></a>      )</span>
<span id="cb198-40"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-40" tabindex="-1"></a>      </span>
<span id="cb198-41"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-41" tabindex="-1"></a>      s <span class="ot">&lt;-</span> outcome<span class="sc">$</span>s_prime</span>
<span id="cb198-42"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-42" tabindex="-1"></a>      </span>
<span id="cb198-43"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-43" tabindex="-1"></a>      <span class="co"># Perform n-step update if we have enough steps</span></span>
<span id="cb198-44"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-44" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">length</span>(trajectory) <span class="sc">&gt;=</span> n_steps) {</span>
<span id="cb198-45"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-45" tabindex="-1"></a>        update_idx <span class="ot">&lt;-</span> <span class="fu">length</span>(trajectory) <span class="sc">-</span> n_steps <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb198-46"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-46" tabindex="-1"></a>        </span>
<span id="cb198-47"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-47" tabindex="-1"></a>        <span class="co"># Compute n-step return</span></span>
<span id="cb198-48"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-48" tabindex="-1"></a>        n_step_return <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb198-49"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-49" tabindex="-1"></a>        <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">0</span><span class="sc">:</span>(n_steps<span class="dv">-1</span>)) {</span>
<span id="cb198-50"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-50" tabindex="-1"></a>          <span class="cf">if</span> (update_idx <span class="sc">+</span> i <span class="sc">&lt;=</span> <span class="fu">length</span>(trajectory)) {</span>
<span id="cb198-51"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-51" tabindex="-1"></a>            n_step_return <span class="ot">&lt;-</span> n_step_return <span class="sc">+</span> </span>
<span id="cb198-52"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-52" tabindex="-1"></a>              (gamma<span class="sc">^</span>i) <span class="sc">*</span> trajectory[[update_idx <span class="sc">+</span> i]]<span class="sc">$</span>reward</span>
<span id="cb198-53"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-53" tabindex="-1"></a>          }</span>
<span id="cb198-54"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-54" tabindex="-1"></a>        }</span>
<span id="cb198-55"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-55" tabindex="-1"></a>        </span>
<span id="cb198-56"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-56" tabindex="-1"></a>        <span class="co"># Add bootstrapped value if not terminal</span></span>
<span id="cb198-57"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-57" tabindex="-1"></a>        <span class="cf">if</span> (s <span class="sc">!=</span> terminal_state) {</span>
<span id="cb198-58"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-58" tabindex="-1"></a>          bootstrap_value <span class="ot">&lt;-</span> <span class="fu">sum</span>(v_weights <span class="sc">*</span> <span class="fu">extract_features</span>(s))</span>
<span id="cb198-59"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-59" tabindex="-1"></a>          n_step_return <span class="ot">&lt;-</span> n_step_return <span class="sc">+</span> (gamma<span class="sc">^</span>n_steps) <span class="sc">*</span> bootstrap_value</span>
<span id="cb198-60"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-60" tabindex="-1"></a>        }</span>
<span id="cb198-61"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-61" tabindex="-1"></a>        </span>
<span id="cb198-62"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-62" tabindex="-1"></a>        <span class="co"># Compute advantage</span></span>
<span id="cb198-63"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-63" tabindex="-1"></a>        advantage <span class="ot">&lt;-</span> n_step_return <span class="sc">-</span> trajectory[[update_idx]]<span class="sc">$</span>value</span>
<span id="cb198-64"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-64" tabindex="-1"></a>        </span>
<span id="cb198-65"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-65" tabindex="-1"></a>        <span class="co"># Update actor</span></span>
<span id="cb198-66"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-66" tabindex="-1"></a>        update_state <span class="ot">&lt;-</span> trajectory[[update_idx]]<span class="sc">$</span>state</span>
<span id="cb198-67"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-67" tabindex="-1"></a>        update_action <span class="ot">&lt;-</span> trajectory[[update_idx]]<span class="sc">$</span>action</span>
<span id="cb198-68"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-68" tabindex="-1"></a>        grad_log_pi <span class="ot">&lt;-</span> <span class="fu">grad_log_prob</span>(update_state, update_action, theta)</span>
<span id="cb198-69"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-69" tabindex="-1"></a>        theta <span class="ot">&lt;-</span> theta <span class="sc">+</span> alpha_actor <span class="sc">*</span> advantage <span class="sc">*</span> grad_log_pi</span>
<span id="cb198-70"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-70" tabindex="-1"></a>        </span>
<span id="cb198-71"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-71" tabindex="-1"></a>        <span class="co"># Update critic</span></span>
<span id="cb198-72"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-72" tabindex="-1"></a>        state_features <span class="ot">&lt;-</span> <span class="fu">extract_features</span>(update_state)</span>
<span id="cb198-73"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-73" tabindex="-1"></a>        v_weights <span class="ot">&lt;-</span> v_weights <span class="sc">+</span> alpha_critic <span class="sc">*</span> advantage <span class="sc">*</span> state_features</span>
<span id="cb198-74"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-74" tabindex="-1"></a>      }</span>
<span id="cb198-75"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-75" tabindex="-1"></a>    }</span>
<span id="cb198-76"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-76" tabindex="-1"></a>    </span>
<span id="cb198-77"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-77" tabindex="-1"></a>    <span class="co"># Final updates for remaining trajectory</span></span>
<span id="cb198-78"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-78" tabindex="-1"></a>    <span class="cf">while</span> (<span class="fu">length</span>(trajectory) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb198-79"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-79" tabindex="-1"></a>      update_idx <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb198-80"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-80" tabindex="-1"></a>      remaining_steps <span class="ot">&lt;-</span> <span class="fu">min</span>(n_steps, <span class="fu">length</span>(trajectory))</span>
<span id="cb198-81"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-81" tabindex="-1"></a>      </span>
<span id="cb198-82"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-82" tabindex="-1"></a>      <span class="co"># Compute return for remaining steps</span></span>
<span id="cb198-83"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-83" tabindex="-1"></a>      remaining_return <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb198-84"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-84" tabindex="-1"></a>      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">0</span><span class="sc">:</span>(remaining_steps<span class="dv">-1</span>)) {</span>
<span id="cb198-85"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-85" tabindex="-1"></a>        remaining_return <span class="ot">&lt;-</span> remaining_return <span class="sc">+</span> </span>
<span id="cb198-86"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-86" tabindex="-1"></a>          (gamma<span class="sc">^</span>i) <span class="sc">*</span> trajectory[[update_idx <span class="sc">+</span> i]]<span class="sc">$</span>reward</span>
<span id="cb198-87"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-87" tabindex="-1"></a>      }</span>
<span id="cb198-88"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-88" tabindex="-1"></a>      </span>
<span id="cb198-89"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-89" tabindex="-1"></a>      advantage <span class="ot">&lt;-</span> remaining_return <span class="sc">-</span> trajectory[[update_idx]]<span class="sc">$</span>value</span>
<span id="cb198-90"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-90" tabindex="-1"></a>      </span>
<span id="cb198-91"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-91" tabindex="-1"></a>      <span class="co"># Update parameters</span></span>
<span id="cb198-92"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-92" tabindex="-1"></a>      update_state <span class="ot">&lt;-</span> trajectory[[update_idx]]<span class="sc">$</span>state</span>
<span id="cb198-93"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-93" tabindex="-1"></a>      update_action <span class="ot">&lt;-</span> trajectory[[update_idx]]<span class="sc">$</span>action</span>
<span id="cb198-94"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-94" tabindex="-1"></a>      grad_log_pi <span class="ot">&lt;-</span> <span class="fu">grad_log_prob</span>(update_state, update_action, theta)</span>
<span id="cb198-95"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-95" tabindex="-1"></a>      theta <span class="ot">&lt;-</span> theta <span class="sc">+</span> alpha_actor <span class="sc">*</span> advantage <span class="sc">*</span> grad_log_pi</span>
<span id="cb198-96"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-96" tabindex="-1"></a>      </span>
<span id="cb198-97"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-97" tabindex="-1"></a>      state_features <span class="ot">&lt;-</span> <span class="fu">extract_features</span>(update_state)</span>
<span id="cb198-98"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-98" tabindex="-1"></a>      v_weights <span class="ot">&lt;-</span> v_weights <span class="sc">+</span> alpha_critic <span class="sc">*</span> advantage <span class="sc">*</span> state_features</span>
<span id="cb198-99"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-99" tabindex="-1"></a>      </span>
<span id="cb198-100"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-100" tabindex="-1"></a>      trajectory <span class="ot">&lt;-</span> trajectory[<span class="sc">-</span><span class="dv">1</span>]  <span class="co"># Remove processed step</span></span>
<span id="cb198-101"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-101" tabindex="-1"></a>    }</span>
<span id="cb198-102"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-102" tabindex="-1"></a>    </span>
<span id="cb198-103"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-103" tabindex="-1"></a>    <span class="co"># Evaluate episode performance periodically</span></span>
<span id="cb198-104"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-104" tabindex="-1"></a>    <span class="cf">if</span> (ep <span class="sc">%%</span> <span class="dv">50</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb198-105"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-105" tabindex="-1"></a>      episode_returns[ep] <span class="ot">&lt;-</span> <span class="fu">evaluate_policy</span>(theta, <span class="at">n_episodes =</span> <span class="dv">5</span>)</span>
<span id="cb198-106"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-106" tabindex="-1"></a>    }</span>
<span id="cb198-107"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-107" tabindex="-1"></a>  }</span>
<span id="cb198-108"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-108" tabindex="-1"></a>  </span>
<span id="cb198-109"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-109" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb198-110"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-110" tabindex="-1"></a>    <span class="at">theta =</span> theta,</span>
<span id="cb198-111"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-111" tabindex="-1"></a>    <span class="at">v_weights =</span> v_weights,</span>
<span id="cb198-112"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-112" tabindex="-1"></a>    <span class="at">episode_returns =</span> episode_returns</span>
<span id="cb198-113"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-113" tabindex="-1"></a>  ))</span>
<span id="cb198-114"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb198-114" tabindex="-1"></a>}</span></code></pre></div>
<p>The n-step variant demonstrates how Actor-Critic methods can be positioned along the bias-variance spectrum. Larger values of n reduce bias by incorporating more actual rewards before bootstrapping with value estimates, but they increase variance by accumulating more stochastic transitions. The optimal choice depends on the environmentâs characteristics and the quality of value function approximation.</p>
</div>
</div>
<div id="computational-and-convergence-considerations" class="section level2 hasAnchor" number="17.3">
<h2><span class="header-section-number">17.3</span> Computational and Convergence Considerations<a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#computational-and-convergence-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The practical success of Actor-Critic methods depends critically on the relative learning rates of actor and critic components. This relationship affects both convergence speed and final solution quality in ways that theory alone cannot fully predict. The critic must learn accurate value estimates to provide useful feedback to the actor, but the actorâs changing policy continuously shifts the target that the critic is trying to learn. This creates a non-stationary learning problem that requires careful balancing.</p>
<p>Empirically, critics typically require faster learning rates than actors because value function learning resembles supervised learning with clear targets, while policy learning must navigate a more complex optimization landscape. However, if the critic learns too quickly relative to the actor, it may overfit to the current policyâs behavior and provide misleading advantage estimates. Conversely, a critic that learns too slowly provides noisy, outdated feedback that can destabilize actor learning.</p>
<p>The choice of function approximation architecture significantly influences both componentsâ behavior. Linear function approximation provides theoretical guarantees but limits representational capacity, while neural network approximation enables complex policies and value functions but introduces optimization challenges. The interaction between actor and critic function approximation creates additional complexityâerrors in one component can compound in the other, leading to divergent behavior even when individual components would converge in isolation.</p>
<p>Memory and computational requirements offer another practical consideration. Actor-Critic methods require maintaining and updating two sets of parameters, roughly doubling the memory overhead compared to pure policy gradient approaches. However, this cost is often offset by improved sample efficiency, as Actor-Critic methods typically require fewer environment interactions to achieve good performance.</p>
<p>The online nature of Actor-Critic learning provides both advantages and challenges for practical implementation. The ability to learn from individual transitions enables continuous adaptation and makes these methods suitable for online learning scenarios. However, this same characteristic makes them sensitive to the sequence of experiences encountered during learning. Unlike batch methods that can smooth over bad experiences, Actor-Critic algorithms must cope with whatever sequence the current policy generates.</p>
<div id="comparative-performance-analysis" class="section level3 hasAnchor" number="17.3.1">
<h3><span class="header-section-number">17.3.1</span> Comparative Performance Analysis<a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#comparative-performance-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To fully appreciate the strengths and limitations of Actor-Critic methods, we need systematic comparison with alternative approaches across multiple performance dimensions. Raw sample efficiency tells only part of the storyâwe must also consider learning stability, final performance quality, and computational efficiency.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-1" tabindex="-1"></a><span class="co"># Comprehensive comparison function</span></span>
<span id="cb199-2"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-2" tabindex="-1"></a>comprehensive_comparison <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb199-3"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-3" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb199-4"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-4" tabindex="-1"></a>  episodes <span class="ot">&lt;-</span> <span class="dv">600</span></span>
<span id="cb199-5"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-5" tabindex="-1"></a>  </span>
<span id="cb199-6"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-6" tabindex="-1"></a>  <span class="co"># Methods to compare</span></span>
<span id="cb199-7"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-7" tabindex="-1"></a>  methods <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb199-8"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-8" tabindex="-1"></a>    <span class="at">reinforce_basic =</span> <span class="cf">function</span>() <span class="fu">reinforce</span>(episodes, <span class="at">alpha =</span> <span class="fl">0.003</span>, <span class="at">baseline =</span> <span class="cn">FALSE</span>),</span>
<span id="cb199-9"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-9" tabindex="-1"></a>    <span class="at">reinforce_baseline =</span> <span class="cf">function</span>() <span class="fu">reinforce</span>(episodes, <span class="at">alpha =</span> <span class="fl">0.003</span>, <span class="at">baseline =</span> <span class="cn">TRUE</span>),</span>
<span id="cb199-10"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-10" tabindex="-1"></a>    <span class="at">actor_critic_basic =</span> <span class="cf">function</span>() <span class="fu">actor_critic</span>(episodes, <span class="at">alpha_actor =</span> <span class="fl">0.003</span>, <span class="at">alpha_critic =</span> <span class="fl">0.015</span>),</span>
<span id="cb199-11"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-11" tabindex="-1"></a>    <span class="at">actor_critic_enhanced =</span> <span class="cf">function</span>() <span class="fu">actor_critic_enhanced</span>(episodes, <span class="at">alpha_actor =</span> <span class="fl">0.003</span>, </span>
<span id="cb199-12"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-12" tabindex="-1"></a>                                                           <span class="at">alpha_critic =</span> <span class="fl">0.015</span>, <span class="at">lambda =</span> <span class="fl">0.8</span>),</span>
<span id="cb199-13"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-13" tabindex="-1"></a>    <span class="at">n_step_ac =</span> <span class="cf">function</span>() <span class="fu">n_step_actor_critic</span>(episodes, <span class="at">n_steps =</span> <span class="dv">3</span>, </span>
<span id="cb199-14"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-14" tabindex="-1"></a>                                              <span class="at">alpha_actor =</span> <span class="fl">0.003</span>, <span class="at">alpha_critic =</span> <span class="fl">0.015</span>)</span>
<span id="cb199-15"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-15" tabindex="-1"></a>  )</span>
<span id="cb199-16"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-16" tabindex="-1"></a>  </span>
<span id="cb199-17"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-17" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb199-18"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-18" tabindex="-1"></a>  performance_metrics <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb199-19"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-19" tabindex="-1"></a>    <span class="at">method =</span> <span class="fu">names</span>(methods),</span>
<span id="cb199-20"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-20" tabindex="-1"></a>    <span class="at">final_performance =</span> <span class="fu">numeric</span>(<span class="fu">length</span>(methods)),</span>
<span id="cb199-21"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-21" tabindex="-1"></a>    <span class="at">convergence_speed =</span> <span class="fu">numeric</span>(<span class="fu">length</span>(methods)),</span>
<span id="cb199-22"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-22" tabindex="-1"></a>    <span class="at">learning_stability =</span> <span class="fu">numeric</span>(<span class="fu">length</span>(methods)),</span>
<span id="cb199-23"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-23" tabindex="-1"></a>    <span class="at">stringsAsFactors =</span> <span class="cn">FALSE</span></span>
<span id="cb199-24"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-24" tabindex="-1"></a>  )</span>
<span id="cb199-25"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-25" tabindex="-1"></a>  </span>
<span id="cb199-26"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-26" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(methods)) {</span>
<span id="cb199-27"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-27" tabindex="-1"></a>    method_name <span class="ot">&lt;-</span> <span class="fu">names</span>(methods)[i]</span>
<span id="cb199-28"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-28" tabindex="-1"></a>    <span class="fu">cat</span>(<span class="st">&quot;Running&quot;</span>, method_name, <span class="st">&quot;...</span><span class="sc">\n</span><span class="st">&quot;</span>)</span>
<span id="cb199-29"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-29" tabindex="-1"></a>    </span>
<span id="cb199-30"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-30" tabindex="-1"></a>    result <span class="ot">&lt;-</span> methods[[i]]()</span>
<span id="cb199-31"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-31" tabindex="-1"></a>    results[[method_name]] <span class="ot">&lt;-</span> result</span>
<span id="cb199-32"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-32" tabindex="-1"></a>    </span>
<span id="cb199-33"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-33" tabindex="-1"></a>    <span class="co"># Evaluate final performance</span></span>
<span id="cb199-34"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-34" tabindex="-1"></a>    final_perf <span class="ot">&lt;-</span> <span class="fu">evaluate_policy</span>(result<span class="sc">$</span>theta, <span class="at">n_episodes =</span> <span class="dv">20</span>)</span>
<span id="cb199-35"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-35" tabindex="-1"></a>    performance_metrics<span class="sc">$</span>final_performance[i] <span class="ot">&lt;-</span> final_perf</span>
<span id="cb199-36"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-36" tabindex="-1"></a>    </span>
<span id="cb199-37"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-37" tabindex="-1"></a>    <span class="co"># Estimate convergence speed (episodes to reach 80% of final performance)</span></span>
<span id="cb199-38"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-38" tabindex="-1"></a>    <span class="cf">if</span> (<span class="st">&quot;episode_returns&quot;</span> <span class="sc">%in%</span> <span class="fu">names</span>(result)) {</span>
<span id="cb199-39"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-39" tabindex="-1"></a>      returns <span class="ot">&lt;-</span> result<span class="sc">$</span>episode_returns[result<span class="sc">$</span>episode_returns <span class="sc">&gt;</span> <span class="dv">0</span>]</span>
<span id="cb199-40"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-40" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">length</span>(returns) <span class="sc">&gt;</span> <span class="dv">10</span>) {</span>
<span id="cb199-41"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-41" tabindex="-1"></a>        target_perf <span class="ot">&lt;-</span> <span class="fl">0.8</span> <span class="sc">*</span> final_perf</span>
<span id="cb199-42"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-42" tabindex="-1"></a>        convergence_ep <span class="ot">&lt;-</span> <span class="fu">which</span>(returns <span class="sc">&gt;=</span> target_perf)[<span class="dv">1</span>]</span>
<span id="cb199-43"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-43" tabindex="-1"></a>        performance_metrics<span class="sc">$</span>convergence_speed[i] <span class="ot">&lt;-</span> </span>
<span id="cb199-44"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-44" tabindex="-1"></a>          <span class="fu">ifelse</span>(<span class="fu">is.na</span>(convergence_ep), episodes, convergence_ep)</span>
<span id="cb199-45"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-45" tabindex="-1"></a>      }</span>
<span id="cb199-46"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-46" tabindex="-1"></a>    }</span>
<span id="cb199-47"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-47" tabindex="-1"></a>    </span>
<span id="cb199-48"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-48" tabindex="-1"></a>    <span class="co"># Measure learning stability (coefficient of variation in later episodes)</span></span>
<span id="cb199-49"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-49" tabindex="-1"></a>    <span class="cf">if</span> (<span class="st">&quot;episode_returns&quot;</span> <span class="sc">%in%</span> <span class="fu">names</span>(result)) {</span>
<span id="cb199-50"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-50" tabindex="-1"></a>      returns <span class="ot">&lt;-</span> result<span class="sc">$</span>episode_returns[result<span class="sc">$</span>episode_returns <span class="sc">&gt;</span> <span class="dv">0</span>]</span>
<span id="cb199-51"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-51" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">length</span>(returns) <span class="sc">&gt;</span> <span class="dv">50</span>) {</span>
<span id="cb199-52"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-52" tabindex="-1"></a>        later_returns <span class="ot">&lt;-</span> <span class="fu">tail</span>(returns, <span class="dv">50</span>)</span>
<span id="cb199-53"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-53" tabindex="-1"></a>        cv <span class="ot">&lt;-</span> <span class="fu">sd</span>(later_returns) <span class="sc">/</span> <span class="fu">mean</span>(later_returns)</span>
<span id="cb199-54"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-54" tabindex="-1"></a>        performance_metrics<span class="sc">$</span>learning_stability[i] <span class="ot">&lt;-</span> cv</span>
<span id="cb199-55"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-55" tabindex="-1"></a>      }</span>
<span id="cb199-56"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-56" tabindex="-1"></a>    }</span>
<span id="cb199-57"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-57" tabindex="-1"></a>  }</span>
<span id="cb199-58"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-58" tabindex="-1"></a>  </span>
<span id="cb199-59"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-59" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">list</span>(</span>
<span id="cb199-60"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-60" tabindex="-1"></a>    <span class="at">results =</span> results,</span>
<span id="cb199-61"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-61" tabindex="-1"></a>    <span class="at">metrics =</span> performance_metrics</span>
<span id="cb199-62"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-62" tabindex="-1"></a>  ))</span>
<span id="cb199-63"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-63" tabindex="-1"></a>}</span>
<span id="cb199-64"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-64" tabindex="-1"></a></span>
<span id="cb199-65"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-65" tabindex="-1"></a><span class="co"># Run comprehensive comparison</span></span>
<span id="cb199-66"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-66" tabindex="-1"></a>comparison_results <span class="ot">&lt;-</span> <span class="fu">comprehensive_comparison</span>()</span>
<span id="cb199-67"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#cb199-67" tabindex="-1"></a><span class="fu">print</span>(comparison_results<span class="sc">$</span>metrics)</span></code></pre></div>
<p>The comprehensive comparison reveals that Actor-Critic methods generally achieve better sample efficiency than pure policy gradient approaches, typically converging faster and with greater stability. The enhanced Actor-Critic with eligibility traces often shows the best overall performance, combining rapid initial learning with stable convergence. However, the relative performance depends significantly on hyperparameter tuning, and the added complexity of Actor-Critic methods can make them more sensitive to parameter choices.</p>
<p>The n-step variant demonstrates an interesting trade-off between the extremes of one-step TD and full Monte Carlo methods. With appropriate step sizes, it often achieves faster initial learning than basic Actor-Critic while maintaining better stability than REINFORCE. This flexibility makes n-step methods particularly attractive for practitioners who need to balance learning speed with stability requirements.</p>
<p>Learning stability metrics reveal another advantage of Actor-Critic methods. The continuous value function updates provide a stabilizing influence on policy learning, reducing the wild oscillations that can plague pure policy gradient methods. This stability proves especially valuable in environments with sparse rewards, where the immediate feedback provided by the critic helps maintain learning progress even when episodes rarely reach rewarding states.</p>
</div>
</div>
<div id="conclusion-10" class="section level2 hasAnchor" number="17.4">
<h2><span class="header-section-number">17.4</span> Conclusion<a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#conclusion-10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The successful implementation of Actor-Critic methods requires attention to numerous subtle details that can dramatically affect performance. The initialization of both actor and critic parameters influences early learning dynamics, with poor initialization potentially leading to unstable or slow convergence. The critic should typically be initialized to provide reasonable value estimates for early policy evaluation, while the actor initialization should promote adequate exploration during initial learning.</p>
<p>The coupling between actor and critic learning rates creates a multi-dimensional optimization problem for hyperparameter selection. Traditional grid search becomes prohibitively expensive, and the optimal ratio often depends on problem-specific characteristics. Adaptive learning rate methods can help, but they must account for the non-stationary nature of the optimization landscape as both components evolve.</p>
<p>Numerical stability considerations become more complex in Actor-Critic methods because errors can propagate between components. Gradient clipping proves essential for both actor and critic updates, but the appropriate clipping thresholds may differ between components. The criticâs value predictions should be monitored for explosive growth, which can destabilize the entire algorithm.</p>
<p>The choice of advantage estimation method significantly affects practical performance. While weâve focused on TD error-based advantages, other approaches like Generalized Advantage Estimation (GAE) provide additional control over the bias-variance trade-off. These methods use exponentially-weighted combinations of n-step advantages, offering fine-grained control over temporal credit assignment.</p>
<p>Actor-Critic methods represent a sophisticated synthesis of the two major paradigms in reinforcement learning: policy optimization and value function learning. By combining these approaches, they address key limitations of pure methods while introducing new complexities that require careful management. The theoretical elegance of using learned value functions as variance-reducing baselines translates into practical algorithms that often outperform their constituent components.</p>
<p>The journey from REINFORCE to Actor-Critic illustrates how algorithmic development in reinforcement learning often involves identifying and addressing specific failure modes through principled extensions. The high variance problem of policy gradients finds its solution not through abandoning the core approach but through augmenting it with complementary techniques. This pattern of incremental improvement built on solid theoretical foundations characterizes much of the progress in modern reinforcement learning.</p>
<p>The flexibility of the Actor-Critic framework enables numerous variants, each targeting specific aspects of the learning problem. From eligibility traces to n-step returns to different critic architectures, these variations demonstrate how a strong conceptual foundation can support diverse practical implementations. The ability to tune the bias-variance trade-off through algorithmic choices rather than just hyperparameters provides practitioners with powerful tools for adapting methods to specific problem domains.</p>
<p>The comparative analysis reveals that Actor-Critic methods generally provide superior sample efficiency and stability compared to pure policy gradient approaches, though at the cost of increased implementation complexity and hyperparameter sensitivity. This trade-off reflects a broader principle in machine learning: more sophisticated methods often achieve better performance but require more careful application.</p>
<p>Looking forward, the Actor-Critic framework continues to influence modern reinforcement learning research. Advanced methods like PPO, A3C, and SAC all build upon the core insight that simultaneous policy and value learning can be more effective than either approach alone. The principles underlying Actor-Critic methodsâvariance reduction through baseline subtraction, temporal difference learning for immediate feedback, and the separation of exploration from exploitationâremain relevant as the field tackles increasingly complex problems.</p>
<p>The implementation insights and practical considerations discussed here highlight the gap between theoretical understanding and successful application. While the mathematical foundations of Actor-Critic methods are well-established, achieving reliable performance requires careful attention to initialization, learning rates, numerical stability, and architectural choices. These practical aspects often determine success or failure in real applications, emphasizing the importance of implementation expertise alongside theoretical knowledge.</p>
<p>Actor-Critic methods ultimately demonstrate that the most effective algorithmic approaches often involve combining complementary techniques rather than perfecting individual components. The synergy between policy optimization and value function learning creates capabilities that neither approach achieves alone, providing a template for algorithm development that continues to yield insights in contemporary reinforcement learning research.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="appendix.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/edit/main/17-Actor_Critique.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/blob/main/17-Actor_Critique.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
