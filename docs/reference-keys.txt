understanding-reinforcement-learning-from-bandits-to-policy-optimization
introduction-to-reinforcement-learning
the-multi-armed-bandit-the-simplest-case
transition-to-markov-decision-processes
comparing-reinforcement-learning-methods
dynamic-programming-model-based-learning
model-free-approaches-monte-carlo-and-td-learning
q-learning-and-function-approximation
policy-gradient-and-actor-critic-methods
advanced-policy-optimization-techniques
conclusion-and-further-directions
references
the-multi-armed-bandit-problem
introduction
mathematical-formalism
frequentist-approach-ucb1-algorithm
r-code-for-ucb1
bayesian-approach-thompson-sampling
r-code-for-thompson-sampling
epsilon-greedy-strategy
r-code-for-epsilon-greedy
summary-table
conclusion
markov-decision-processes-and-dynamic-programming
introduction-1
constructing-the-mdp-in-r
value-iteration-algorithm
evaluation-and-interpretation
theoretical-properties-of-value-iteration
summary-table-1
conclusion-1
model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r
introduction-2
theoretical-background
temporal-difference-learning-q-learning
monte-carlo-methods
step-1-defining-the-environment-in-r
step-2-q-learning-implementation-in-r
step-3-monte-carlo-every-visit-implementation
step-4-simulating-outcome-devaluation
step-5-comparing-policies-before-and-after-devaluation
step-6-visualizing-the-policies
interpretation-and-discussion
conclusion-2
