<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Average Reward in Reinforcement Learning: A Comprehensive Guide | Reinforcement Learning in R</title>
  <meta name="description" content="Chapter 13 Average Reward in Reinforcement Learning: A Comprehensive Guide | Reinforcement Learning in R" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Average Reward in Reinforcement Learning: A Comprehensive Guide | Reinforcement Learning in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Average Reward in Reinforcement Learning: A Comprehensive Guide | Reinforcement Learning in R" />
  
  
  

<meta name="author" content="Kamran Afzalui" />


<meta name="date" content="2025-10-22" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="learning-policies-versus-learning-values.html"/>
<link rel="next" href="eligibility-traces.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Reinforcement Learning in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Understanding Reinforcement Learning: From Bandits to Policy Optimization</strong></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-multi-armed-bandit-the-simplest-case"><i class="fa fa-check"></i><b>1.2</b> The Multi-Armed Bandit: The Simplest Case</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#transition-to-markov-decision-processes"><i class="fa fa-check"></i><b>1.3</b> Transition to Markov Decision Processes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#comparing-reinforcement-learning-methods"><i class="fa fa-check"></i><b>1.4</b> Comparing Reinforcement Learning Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#dynamic-programming-model-based-learning"><i class="fa fa-check"></i><b>1.4.1</b> Dynamic Programming: Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#model-free-approaches-monte-carlo-and-td-learning"><i class="fa fa-check"></i><b>1.4.2</b> Model-Free Approaches: Monte Carlo and TD Learning</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#dyna-bridging-model-free-and-model-based-learning"><i class="fa fa-check"></i><b>1.4.3</b> Dyna: Bridging Model-Free and Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#q-learning-and-function-approximation"><i class="fa fa-check"></i><b>1.4.4</b> Q-Learning and Function Approximation</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#policy-gradient-and-actor-critic-methods"><i class="fa fa-check"></i><b>1.4.5</b> Policy Gradient and Actor-Critic Methods</a></li>
<li class="chapter" data-level="1.4.6" data-path="index.html"><a href="index.html#advanced-policy-optimization-techniques"><i class="fa fa-check"></i><b>1.4.6</b> Advanced Policy Optimization Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#further-directions"><i class="fa fa-check"></i><b>1.5</b> Further Directions</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html"><i class="fa fa-check"></i><b>2</b> The Multi-Armed Bandit Problem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#mathematical-formalism"><i class="fa fa-check"></i><b>2.2</b> Mathematical Formalism</a></li>
<li class="chapter" data-level="2.3" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm"><i class="fa fa-check"></i><b>2.3</b> Frequentist Approach: UCB1 Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1"><i class="fa fa-check"></i><b>2.3.1</b> R Code for UCB1</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling"><i class="fa fa-check"></i><b>2.4</b> Bayesian Approach: Thompson Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling"><i class="fa fa-check"></i><b>2.4.1</b> R Code for Thompson Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy"><i class="fa fa-check"></i><b>2.5</b> Epsilon-Greedy Strategy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy"><i class="fa fa-check"></i><b>2.5.1</b> R Code for Epsilon-Greedy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#summary-table"><i class="fa fa-check"></i><b>2.6</b> Summary Table</a></li>
<li class="chapter" data-level="2.7" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html"><i class="fa fa-check"></i><b>3</b> Markov Decision Processes and Dynamic Programming</a>
<ul>
<li class="chapter" data-level="3.1" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#constructing-the-mdp-in-r"><i class="fa fa-check"></i><b>3.2</b> Constructing the MDP in R</a></li>
<li class="chapter" data-level="3.3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#value-iteration-algorithm"><i class="fa fa-check"></i><b>3.3</b> Value Iteration Algorithm</a></li>
<li class="chapter" data-level="3.4" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#evaluation-and-interpretation"><i class="fa fa-check"></i><b>3.4</b> Evaluation and Interpretation</a></li>
<li class="chapter" data-level="3.5" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties-of-value-iteration"><i class="fa fa-check"></i><b>3.5</b> Theoretical Properties of Value Iteration</a></li>
<li class="chapter" data-level="3.6" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-1"><i class="fa fa-check"></i><b>3.6</b> Summary Table</a></li>
<li class="chapter" data-level="3.7" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><i class="fa fa-check"></i><b>4</b> Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#theoretical-background"><i class="fa fa-check"></i><b>4.2</b> Theoretical Background</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#temporal-difference-learning-q-learning"><i class="fa fa-check"></i><b>4.2.1</b> Temporal Difference Learning (Q-Learning)</a></li>
<li class="chapter" data-level="4.2.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#monte-carlo-methods"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Methods</a></li>
<li class="chapter" data-level="4.2.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-1-defining-the-environment-in-r"><i class="fa fa-check"></i><b>4.2.3</b> Step 1: Defining the Environment in R</a></li>
<li class="chapter" data-level="4.2.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-2-q-learning-implementation-in-r"><i class="fa fa-check"></i><b>4.2.4</b> Step 2: Q-Learning Implementation in R</a></li>
<li class="chapter" data-level="4.2.5" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-3-monte-carlo-every-visit-implementation"><i class="fa fa-check"></i><b>4.2.5</b> Step 3: Monte Carlo Every-Visit Implementation</a></li>
<li class="chapter" data-level="4.2.6" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-4-simulating-outcome-devaluation"><i class="fa fa-check"></i><b>4.2.6</b> Step 4: Simulating Outcome Devaluation</a></li>
<li class="chapter" data-level="4.2.7" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-5-comparing-policies-before-and-after-devaluation"><i class="fa fa-check"></i><b>4.2.7</b> Step 5: Comparing Policies Before and After Devaluation</a></li>
<li class="chapter" data-level="4.2.8" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#step-6-visualizing-the-policies"><i class="fa fa-check"></i><b>4.2.8</b> Step 6: Visualizing the Policies</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#interpretation-and-discussion"><i class="fa fa-check"></i><b>4.3</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="4.4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#conclusion-2"><i class="fa fa-check"></i><b>4.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><i class="fa fa-check"></i><b>5</b> On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-on-policy"><i class="fa fa-check"></i><b>5.2</b> SARSA (On-Policy)</a></li>
<li class="chapter" data-level="5.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-off-policy"><i class="fa fa-check"></i><b>5.3</b> Q-Learning (Off-Policy)</a></li>
<li class="chapter" data-level="5.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-with-importance-sampling"><i class="fa fa-check"></i><b>5.4</b> Off-Policy Monte Carlo with Importance Sampling</a></li>
<li class="chapter" data-level="5.5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#key-differences"><i class="fa fa-check"></i><b>5.5</b> Key Differences</a></li>
<li class="chapter" data-level="5.6" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#interpretation-and-discussion-1"><i class="fa fa-check"></i><b>5.6</b> Interpretation and Discussion</a></li>
<li class="chapter" data-level="5.7" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#conclusion-3"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
<li class="chapter" data-level="5.8" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#comparison-table"><i class="fa fa-check"></i><b>5.8</b> Comparison Table</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html"><i class="fa fa-check"></i><b>6</b> Function Approximation Q-Learning with Linear Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#q-learning-with-function-approximation"><i class="fa fa-check"></i><b>6.1.1</b> Q-Learning with Function Approximation</a></li>
<li class="chapter" data-level="6.1.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#comparison-with-tabular-q-learning"><i class="fa fa-check"></i><b>6.1.2</b> Comparison with Tabular Q-Learning</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#r-implementation"><i class="fa fa-check"></i><b>6.2</b> R Implementation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><i class="fa fa-check"></i><b>7</b> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation"><i class="fa fa-check"></i><b>7.1.1</b> Q-Learning with Random Forest Approximation</a></li>
<li class="chapter" data-level="7.1.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods"><i class="fa fa-check"></i><b>7.1.2</b> Comparison with Previous Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1"><i class="fa fa-check"></i><b>7.2</b> R Implementation</a></li>
<li class="chapter" data-level="7.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights"><i class="fa fa-check"></i><b>7.3</b> Analysis and Insights</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#policy-learning-characteristics"><i class="fa fa-check"></i><b>7.3.1</b> Policy Learning Characteristics</a></li>
<li class="chapter" data-level="7.3.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#computational-considerations"><i class="fa fa-check"></i><b>7.3.2</b> Computational Considerations</a></li>
<li class="chapter" data-level="7.3.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#feature-importance-insights"><i class="fa fa-check"></i><b>7.3.3</b> Feature Importance Insights</a></li>
<li class="chapter" data-level="7.3.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#practical-implications-1"><i class="fa fa-check"></i><b>7.3.4</b> Practical Implications</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-linear-approximation"><i class="fa fa-check"></i><b>7.4</b> Comparison with Linear Approximation</a></li>
<li class="chapter" data-level="7.5" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4"><i class="fa fa-check"></i><b>7.5</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><i class="fa fa-check"></i><b>8</b> Deep Function Approximation: Q-Learning with Neural Networks in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation"><i class="fa fa-check"></i><b>8.2</b> Theoretical Foundation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity"><i class="fa fa-check"></i><b>8.2.1</b> Universal Approximation and Expressivity</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning"><i class="fa fa-check"></i><b>8.2.2</b> Gradient-Based Learning</a></li>
<li class="chapter" data-level="8.2.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches"><i class="fa fa-check"></i><b>8.2.3</b> Comparison with Previous Approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2"><i class="fa fa-check"></i><b>8.3</b> R Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation"><i class="fa fa-check"></i><b>8.4</b> Analysis and Interpretation</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#learning-dynamics"><i class="fa fa-check"></i><b>8.4.1</b> Learning Dynamics</a></li>
<li class="chapter" data-level="8.4.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#function-representation"><i class="fa fa-check"></i><b>8.4.2</b> Function Representation</a></li>
<li class="chapter" data-level="8.4.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#generalization-properties"><i class="fa fa-check"></i><b>8.4.3</b> Generalization Properties</a></li>
<li class="chapter" data-level="8.4.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#training-stability"><i class="fa fa-check"></i><b>8.4.4</b> Training Stability</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#practical-considerations"><i class="fa fa-check"></i><b>8.5</b> Practical Considerations</a>
<ul>
<li class="chapter" data-level="8.5.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#architecture-selection"><i class="fa fa-check"></i><b>8.5.1</b> Architecture Selection</a></li>
<li class="chapter" data-level="8.5.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#training-frequency"><i class="fa fa-check"></i><b>8.5.2</b> Training Frequency</a></li>
<li class="chapter" data-level="8.5.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#regularization"><i class="fa fa-check"></i><b>8.5.3</b> Regularization</a></li>
<li class="chapter" data-level="8.5.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#initialization-and-convergence"><i class="fa fa-check"></i><b>8.5.4</b> Initialization and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-across-function-approximation-methods"><i class="fa fa-check"></i><b>8.6</b> Comparison Across Function Approximation Methods</a></li>
<li class="chapter" data-level="8.7" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions"><i class="fa fa-check"></i><b>8.7</b> Future Directions</a></li>
<li class="chapter" data-level="8.8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5"><i class="fa fa-check"></i><b>8.8</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html"><i class="fa fa-check"></i><b>9</b> Dyna and DynaQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#theoretical-framework"><i class="fa fa-check"></i><b>9.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#the-dyna-architecture"><i class="fa fa-check"></i><b>9.2.1</b> The Dyna Architecture</a></li>
<li class="chapter" data-level="9.2.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#model-representation"><i class="fa fa-check"></i><b>9.2.2</b> Model Representation</a></li>
<li class="chapter" data-level="9.2.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#convergence-properties"><i class="fa fa-check"></i><b>9.2.3</b> Convergence Properties</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-in-r"><i class="fa fa-check"></i><b>9.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#environment-setup"><i class="fa fa-check"></i><b>9.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="9.3.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#dyna-q-implementation"><i class="fa fa-check"></i><b>9.3.2</b> Dyna-Q Implementation</a></li>
<li class="chapter" data-level="9.3.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#standard-q-learning-for-comparison"><i class="fa fa-check"></i><b>9.3.3</b> Standard Q-Learning for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#experimental-analysis"><i class="fa fa-check"></i><b>9.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#learning-efficiency-comparison"><i class="fa fa-check"></i><b>9.4.1</b> Learning Efficiency Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
<li class="chapter" data-level="9.6" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-considerations-and-conclusion"><i class="fa fa-check"></i><b>9.6</b> Implementation Considerations and Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><i class="fa fa-check"></i><b>10</b> Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#theoretical-framework-1"><i class="fa fa-check"></i><b>10.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#the-exploration-bonus-mechanism"><i class="fa fa-check"></i><b>10.2.1</b> The Exploration Bonus Mechanism</a></li>
<li class="chapter" data-level="10.2.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#complete-dyna-q-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> Complete Dyna-Q+ Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#convergence-and-stability"><i class="fa fa-check"></i><b>10.2.3</b> Convergence and Stability</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#implementation-in-r-1"><i class="fa fa-check"></i><b>10.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#environment-setup-1"><i class="fa fa-check"></i><b>10.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="10.3.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-implementation-1"><i class="fa fa-check"></i><b>10.3.2</b> Dyna-Q+ Implementation</a></li>
<li class="chapter" data-level="10.3.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#standard-dyna-for-comparison"><i class="fa fa-check"></i><b>10.3.3</b> Standard Dyna for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#experimental-analysis-1"><i class="fa fa-check"></i><b>10.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#adaptation-to-environmental-changes"><i class="fa fa-check"></i><b>10.4.1</b> Adaptation to Environmental Changes</a></li>
<li class="chapter" data-level="10.4.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#parameter-sensitivity-analysis"><i class="fa fa-check"></i><b>10.4.2</b> Parameter Sensitivity Analysis</a></li>
<li class="chapter" data-level="10.4.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#exploration-pattern-analysis"><i class="fa fa-check"></i><b>10.4.3</b> Exploration Pattern Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#discussion-and-implementation-considerations"><i class="fa fa-check"></i><b>10.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="10.6" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#conclusion-6"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html"><i class="fa fa-check"></i><b>11</b> Function Approximation And Feature Engineering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#feature-engineering-and-state-representation"><i class="fa fa-check"></i><b>11.1</b> Feature Engineering and State Representation</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-discrimination-vs.-generalization-tradeoff"><i class="fa fa-check"></i><b>11.1.1</b> The Discrimination vs. Generalization Tradeoff</a></li>
<li class="chapter" data-level="11.1.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#principles-of-effective-feature-design"><i class="fa fa-check"></i><b>11.1.2</b> Principles of Effective Feature Design</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#mathematical-foundations-of-linear-function-approximation"><i class="fa fa-check"></i><b>11.2</b> Mathematical Foundations of Linear Function Approximation</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#linear-value-functions-and-their-properties"><i class="fa fa-check"></i><b>11.2.1</b> Linear Value Functions and Their Properties</a></li>
<li class="chapter" data-level="11.2.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#temporal-difference-learning-with-linear-approximation"><i class="fa fa-check"></i><b>11.2.2</b> Temporal Difference Learning with Linear Approximation</a></li>
<li class="chapter" data-level="11.2.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-deadly-triad-and-stability-concerns"><i class="fa fa-check"></i><b>11.2.3</b> The Deadly Triad and Stability Concerns</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#classical-basis-function-methods"><i class="fa fa-check"></i><b>11.3</b> Classical Basis Function Methods</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#coarse-coding-overlapping-receptive-fields"><i class="fa fa-check"></i><b>11.3.1</b> Coarse Coding: Overlapping Receptive Fields</a></li>
<li class="chapter" data-level="11.3.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#tile-coding-structured-overlapping-grids"><i class="fa fa-check"></i><b>11.3.2</b> Tile Coding: Structured Overlapping Grids</a></li>
<li class="chapter" data-level="11.3.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#radial-basis-functions-smooth-continuous-features"><i class="fa fa-check"></i><b>11.3.3</b> Radial Basis Functions: Smooth Continuous Features</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#comparative-analysis-and-practical-considerations"><i class="fa fa-check"></i><b>11.4</b> Comparative Analysis and Practical Considerations</a></li>
<li class="chapter" data-level="11.5" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#bridging-classical-and-modern-approaches"><i class="fa fa-check"></i><b>11.5</b> Bridging Classical and Modern Approaches</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html"><i class="fa fa-check"></i><b>12</b> Learning Policies versus Learning Values:</a>
<ul>
<li class="chapter" data-level="12.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#the-two-paradigms-of-reinforcement-learning"><i class="fa fa-check"></i><b>12.1</b> The Two Paradigms of Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="12.1.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#value-based-methods-learning-worth-before-action"><i class="fa fa-check"></i><b>12.1.1</b> Value-Based Methods: Learning Worth Before Action</a></li>
<li class="chapter" data-level="12.1.2" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#policy-based-methods-direct-optimization-of-behavior"><i class="fa fa-check"></i><b>12.1.2</b> Policy-Based Methods: Direct Optimization of Behavior</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><i class="fa fa-check"></i><b>13</b> Average Reward in Reinforcement Learning: A Comprehensive Guide</a>
<ul>
<li class="chapter" data-level="13.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-limitations-of-discounted-reward-formulations"><i class="fa fa-check"></i><b>13.1</b> The Limitations of Discounted Reward Formulations</a></li>
<li class="chapter" data-level="13.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-average-reward-alternative-theoretical-foundations"><i class="fa fa-check"></i><b>13.2</b> The Average Reward Alternative: Theoretical Foundations</a></li>
<li class="chapter" data-level="13.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#value-functions-in-the-average-reward-setting"><i class="fa fa-check"></i><b>13.3</b> Value Functions in the Average Reward Setting</a></li>
<li class="chapter" data-level="13.4" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-theory-for-average-reward"><i class="fa fa-check"></i><b>13.4</b> Optimality Theory for Average Reward</a></li>
<li class="chapter" data-level="13.5" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#learning-algorithms-for-average-reward"><i class="fa fa-check"></i><b>13.5</b> Learning Algorithms for Average Reward</a></li>
<li class="chapter" data-level="13.6" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#practical-implementation-server-load-balancing"><i class="fa fa-check"></i><b>13.6</b> Practical Implementation: Server Load Balancing</a></li>
<li class="chapter" data-level="13.7" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#implementation-considerations"><i class="fa fa-check"></i><b>13.7</b> Implementation Considerations</a></li>
<li class="chapter" data-level="13.8" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#when-to-choose-average-reward-over-discounting"><i class="fa fa-check"></i><b>13.8</b> When to Choose Average Reward Over Discounting</a></li>
<li class="chapter" data-level="13.9" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#appendix-a-mathematical-proofs-and-derivations"><i class="fa fa-check"></i><b>13.9</b> Appendix A: Mathematical Proofs and Derivations</a>
<ul>
<li class="chapter" data-level="13.9.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#convergence-of-average-reward-td-learning"><i class="fa fa-check"></i><b>13.9.1</b> Convergence of Average Reward TD Learning</a></li>
<li class="chapter" data-level="13.9.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#policy-gradient-theorem-for-average-reward"><i class="fa fa-check"></i><b>13.9.2</b> Policy Gradient Theorem for Average Reward</a></li>
<li class="chapter" data-level="13.9.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-equations-derivation"><i class="fa fa-check"></i><b>13.9.3</b> Optimality Equations Derivation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="eligibility-traces.html"><a href="eligibility-traces.html"><i class="fa fa-check"></i><b>14</b> Eligibility Traces</a>
<ul>
<li class="chapter" data-level="14.1" data-path="eligibility-traces.html"><a href="eligibility-traces.html#from-one-step-to-multi-step-learning"><i class="fa fa-check"></i><b>14.1</b> From One-Step to Multi-Step Learning</a></li>
<li class="chapter" data-level="14.2" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-mechanics-of-eligibility-traces-a-backward-view"><i class="fa fa-check"></i><b>14.2</b> The Mechanics of Eligibility Traces: A Backward View</a></li>
<li class="chapter" data-level="14.3" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-tdlambda-algorithm-for-prediction"><i class="fa fa-check"></i><b>14.3</b> The TD(<span class="math inline">\(\\lambda\)</span>) Algorithm for Prediction</a></li>
<li class="chapter" data-level="14.4" data-path="eligibility-traces.html"><a href="eligibility-traces.html#control-with-eligibility-traces-average-reward-sarsalambda"><i class="fa fa-check"></i><b>14.4</b> Control with Eligibility Traces: Average Reward Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.5" data-path="eligibility-traces.html"><a href="eligibility-traces.html#practical-implementation-server-load-balancing-with-sarsalambda"><i class="fa fa-check"></i><b>14.5</b> Practical Implementation: Server Load Balancing with Sarsa(<span class="math inline">\(\\lambda\)</span>)</a></li>
<li class="chapter" data-level="14.6" data-path="eligibility-traces.html"><a href="eligibility-traces.html#computational-and-performance-considerations"><i class="fa fa-check"></i><b>14.6</b> Computational and Performance Considerations</a></li>
<li class="chapter" data-level="14.7" data-path="eligibility-traces.html"><a href="eligibility-traces.html#conclusion-8"><i class="fa fa-check"></i><b>14.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html"><i class="fa fa-check"></i><b>15</b> Intrinsic Rewards</a>
<ul>
<li class="chapter" data-level="15.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#the-sparse-reward-problem-and-its-implications"><i class="fa fa-check"></i><b>15.1</b> The Sparse Reward Problem and Its Implications</a></li>
<li class="chapter" data-level="15.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#mathematical-foundations-of-intrinsic-motivation"><i class="fa fa-check"></i><b>15.2</b> Mathematical Foundations of Intrinsic Motivation</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#curiosity-driven-learning"><i class="fa fa-check"></i><b>15.2.1</b> Curiosity-Driven Learning</a></li>
<li class="chapter" data-level="15.2.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#count-based-exploration"><i class="fa fa-check"></i><b>15.2.2</b> Count-Based Exploration</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#practical-implementation-curiosity-driven-grid-world-navigation"><i class="fa fa-check"></i><b>15.3</b> Practical Implementation: Curiosity-Driven Grid World Navigation</a></li>
<li class="chapter" data-level="15.4" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#advanced-intrinsic-motivation-mechanisms"><i class="fa fa-check"></i><b>15.4</b> Advanced Intrinsic Motivation Mechanisms</a></li>
<li class="chapter" data-level="15.5" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#implementation-considerations-and-practical-guidelines"><i class="fa fa-check"></i><b>15.5</b> Implementation Considerations and Practical Guidelines</a></li>
<li class="chapter" data-level="15.6" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#theoretical-analysis-and-convergence-properties"><i class="fa fa-check"></i><b>15.6</b> Theoretical Analysis and Convergence Properties</a></li>
<li class="chapter" data-level="15.7" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#applications-and-empirical-results"><i class="fa fa-check"></i><b>15.7</b> Applications and Empirical Results</a></li>
<li class="chapter" data-level="15.8" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#limitations-and-future-directions"><i class="fa fa-check"></i><b>15.8</b> Limitations and Future Directions</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>16</b> Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="16.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#introduction-9"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#theoretical-framework-2"><i class="fa fa-check"></i><b>16.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#the-policy-gradient-theorem-1"><i class="fa fa-check"></i><b>16.2.1</b> The Policy Gradient Theorem</a></li>
<li class="chapter" data-level="16.2.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-algorithm"><i class="fa fa-check"></i><b>16.2.2</b> REINFORCE Algorithm</a></li>
<li class="chapter" data-level="16.2.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#baseline-subtraction-and-variance-reduction"><i class="fa fa-check"></i><b>16.2.3</b> Baseline Subtraction and Variance Reduction</a></li>
<li class="chapter" data-level="16.2.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-parameterization"><i class="fa fa-check"></i><b>16.2.4</b> Softmax Policy Parameterization</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#implementation-in-r-2"><i class="fa fa-check"></i><b>16.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#environment-and-feature-representation"><i class="fa fa-check"></i><b>16.3.1</b> Environment and Feature Representation</a></li>
<li class="chapter" data-level="16.3.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-implementation"><i class="fa fa-check"></i><b>16.3.2</b> Softmax Policy Implementation</a></li>
<li class="chapter" data-level="16.3.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-implementation"><i class="fa fa-check"></i><b>16.3.3</b> REINFORCE Implementation</a></li>
<li class="chapter" data-level="16.3.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#actor-critic-implementation"><i class="fa fa-check"></i><b>16.3.4</b> Actor-Critic Implementation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#experimental-analysis-2"><i class="fa fa-check"></i><b>16.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#comparison-of-policy-gradient-variants"><i class="fa fa-check"></i><b>16.4.1</b> Comparison of Policy Gradient Variants</a></li>
<li class="chapter" data-level="16.4.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#learning-rate-sensitivity-analysis"><i class="fa fa-check"></i><b>16.4.2</b> Learning Rate Sensitivity Analysis</a></li>
<li class="chapter" data-level="16.4.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#variance-analysis-of-gradient-estimates"><i class="fa fa-check"></i><b>16.4.3</b> Variance Analysis of Gradient Estimates</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#discussion-and-implementation-considerations-1"><i class="fa fa-check"></i><b>16.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="16.6" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#conclusion-9"><i class="fa fa-check"></i><b>16.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>17</b> Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="17.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#theoretical-framework-3"><i class="fa fa-check"></i><b>17.1</b> Theoretical Framework</a></li>
<li class="chapter" data-level="17.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#implementation-and-comparative-analysis"><i class="fa fa-check"></i><b>17.2</b> Implementation and Comparative Analysis</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#variance-analysis-and-learning-dynamics"><i class="fa fa-check"></i><b>17.2.1</b> Variance Analysis and Learning Dynamics</a></li>
<li class="chapter" data-level="17.2.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#algorithmic-variants-and-extensions"><i class="fa fa-check"></i><b>17.2.2</b> Algorithmic Variants and Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#computational-and-convergence-considerations"><i class="fa fa-check"></i><b>17.3</b> Computational and Convergence Considerations</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#comparative-performance-analysis"><i class="fa fa-check"></i><b>17.3.1</b> Comparative Performance Analysis</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#conclusion-10"><i class="fa fa-check"></i><b>17.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>18</b> Appendix</a>
<ul>
<li class="chapter" data-level="18.1" data-path="appendix.html"><a href="appendix.html#comprehensive-reinforcement-learning-concepts-guide"><i class="fa fa-check"></i><b>18.1</b> Comprehensive Reinforcement Learning Concepts Guide</a></li>
<li class="chapter" data-level="18.2" data-path="appendix.html"><a href="appendix.html#learning-mechanisms"><i class="fa fa-check"></i><b>18.2</b> Learning Mechanisms</a></li>
<li class="chapter" data-level="18.3" data-path="appendix.html"><a href="appendix.html#environment-properties"><i class="fa fa-check"></i><b>18.3</b> Environment Properties</a></li>
<li class="chapter" data-level="18.4" data-path="appendix.html"><a href="appendix.html#learning-paradigms"><i class="fa fa-check"></i><b>18.4</b> Learning Paradigms</a></li>
<li class="chapter" data-level="18.5" data-path="appendix.html"><a href="appendix.html#exploration-strategies"><i class="fa fa-check"></i><b>18.5</b> Exploration Strategies</a></li>
<li class="chapter" data-level="18.6" data-path="appendix.html"><a href="appendix.html#key-algorithms-methods"><i class="fa fa-check"></i><b>18.6</b> Key Algorithms &amp; Methods</a></li>
<li class="chapter" data-level="18.7" data-path="appendix.html"><a href="appendix.html#advanced-concepts"><i class="fa fa-check"></i><b>18.7</b> Advanced Concepts</a></li>
<li class="chapter" data-level="18.8" data-path="appendix.html"><a href="appendix.html#fundamental-equations"><i class="fa fa-check"></i><b>18.8</b> Fundamental Equations</a>
<ul>
<li class="chapter" data-level="18.8.1" data-path="appendix.html"><a href="appendix.html#bellman-equations"><i class="fa fa-check"></i><b>18.8.1</b> <strong>Bellman Equations</strong></a></li>
<li class="chapter" data-level="18.8.2" data-path="appendix.html"><a href="appendix.html#policy-gradient-theorem"><i class="fa fa-check"></i><b>18.8.2</b> <strong>Policy Gradient Theorem</strong></a></li>
<li class="chapter" data-level="18.8.3" data-path="appendix.html"><a href="appendix.html#temporal-difference-error"><i class="fa fa-check"></i><b>18.8.3</b> <strong>Temporal Difference Error</strong></a></li>
</ul></li>
<li class="chapter" data-level="18.9" data-path="appendix.html"><a href="appendix.html#common-challenges-solutions"><i class="fa fa-check"></i><b>18.9</b> Common Challenges &amp; Solutions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="average-reward-in-reinforcement-learning-a-comprehensive-guide" class="section level1 hasAnchor" number="13">
<h1><span class="header-section-number">Chapter 13</span> Average Reward in Reinforcement Learning: A Comprehensive Guide<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#average-reward-in-reinforcement-learning-a-comprehensive-guide" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p>The overwhelming majority of reinforcement learning (RL) literature centers on maximizing discounted cumulative rewards. This approach, while mathematically elegant and computationally convenient, represents just one way to formalize sequential decision-making problems. For many real-world applications—particularly those involving continuous operation without natural episode boundaries—an alternative formulation based on average reward per time step offers both theoretical advantages and practical benefits that deserve serious consideration.</p>
<div id="the-limitations-of-discounted-reward-formulations" class="section level2 hasAnchor" number="13.1">
<h2><span class="header-section-number">13.1</span> The Limitations of Discounted Reward Formulations<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-limitations-of-discounted-reward-formulations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Traditional RL formulates the control problem around maximizing the expected discounted return:</p>
<p><span class="math display">\[G_t = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}\]</span></p>
<p>where <span class="math inline">\(0 \leq \gamma &lt; 1\)</span> is the discount factor. The policy evaluation objective becomes:</p>
<p><span class="math display">\[J(\pi) = \mathbb{E}_\pi [G_t] = \mathbb{E}_\pi \left[ \sum_{k=0}^\infty \gamma^k R_{t+k+1} \right]\]</span></p>
<p>This formulation dominates the field for compelling mathematical reasons: it ensures convergence of infinite sums, provides contraction properties for Bellman operators, and offers computational tractability. However, these advantages come with conceptual costs that become apparent when we examine real-world applications.</p>
<p>Consider a server load balancing system that must operate continuously for months or years. In such systems, the choice of discount factor becomes arbitrary and potentially misleading. A discount factor of 0.9 implies that a reward received 22 steps in the future is worth only 10% of an immediate reward—a perspective that may not align with the true operational objectives. Similarly, in medical treatment scheduling, financial portfolio management, or industrial control systems, the fundamental goal is often to maximize long-term average performance rather than to optimize a present-value calculation with an artificially imposed time preference.</p>
<p>The prevalence of discounted formulations partly stems from RL’s historical focus on episodic tasks with clear terminal states. Games, navigation problems, and many benchmark environments naturally decompose into episodes with definite endings. However, this episodic perspective may be the exception rather than the rule in real-world applications. Manufacturing systems, recommendation engines, traffic control networks, and autonomous vehicles operate in continuing environments where the notion of “episodes” is artificial.</p>
</div>
<div id="the-average-reward-alternative-theoretical-foundations" class="section level2 hasAnchor" number="13.2">
<h2><span class="header-section-number">13.2</span> The Average Reward Alternative: Theoretical Foundations<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-average-reward-alternative-theoretical-foundations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The average reward formulation abandons discounting entirely, focusing instead on the long-run average payoff:</p>
<p><span class="math display">\[\rho(\pi) = \lim_{T \to \infty} \frac{1}{T} \; \mathbb{E}_\pi \left[ \sum_{t=1}^T R_t \right]\]</span></p>
<p>This quantity represents the steady-state reward rate under policy <span class="math inline">\(\pi\)</span>—the expected reward per time step in the long run. The control problem becomes:</p>
<p><span class="math display">\[\pi^* = \arg\max_\pi \rho(\pi)\]</span></p>
<p>Unlike discounted formulations, this objective requires no arbitrary parameters and treats all time steps equally. The interpretation is straightforward: find the policy that maximizes long-term throughput.</p>
<p>The average reward formulation relies on ergodic theory for its theoretical foundation. Under standard assumptions—primarily that the Markov chains induced by policies are finite, irreducible, and aperiodic—several important properties hold. The well-definedness property ensures the limit defining <span class="math inline">\(\rho(\pi)\)</span> exists and is finite. Initial state independence means the average reward is independent of the starting state. Finally, uniqueness of stationary distribution guarantees each policy induces a unique long-run state distribution.</p>
<p>These conditions are satisfied by most practical MDPs, making the average reward criterion broadly applicable. However, care must be taken with absorbing states or poorly connected state spaces, where ergodicity assumptions may fail.</p>
<p>A key insight is that average reward directly relates to the stationary distribution induced by a policy. If <span class="math inline">\(d^\pi(s)\)</span> denotes the long-run probability of being in state <span class="math inline">\(s\)</span> under policy <span class="math inline">\(\pi\)</span>, then:</p>
<p><span class="math display">\[\rho(\pi) = \sum_s d^\pi(s) \sum_a \pi(a|s) \sum_{s&#39;} P(s&#39;|s,a) R(s,a,s&#39;)\]</span></p>
<p>This expression reveals that optimizing average reward requires finding policies that induce favorable stationary distributions—those that concentrate probability mass on high-reward regions of the state space.</p>
</div>
<div id="value-functions-in-the-average-reward-setting" class="section level2 hasAnchor" number="13.3">
<h2><span class="header-section-number">13.3</span> Value Functions in the Average Reward Setting<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#value-functions-in-the-average-reward-setting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Without discounting, the traditional notion of value function becomes problematic. The infinite sum <span class="math inline">\(\sum_{k=0}^\infty R_{t+k+1}\)</span> generally diverges, making it impossible to define meaningful state values in the conventional sense.</p>
<p>The solution is to work with relative values rather than absolute ones. The <strong>differential value function</strong> captures the relative advantage of starting in each state:</p>
<p><span class="math display">\[h_\pi(s) = \mathbb{E}_\pi \left[ \sum_{t=0}^\infty (R_{t+1} - \rho(\pi)) \;\big|\; S_0 = s \right]\]</span></p>
<p>This quantity measures how much better (or worse) it is to start in state <span class="math inline">\(s\)</span> compared to the long-run average. Crucially, this sum converges under ergodic assumptions because the centered rewards <span class="math inline">\((R_{t+1} - \rho(\pi))\)</span> have zero mean in the long run.</p>
<p>The differential value function satisfies a modified Bellman equation:</p>
<p><span class="math display">\[h_\pi(s) = \sum_a \pi(a|s) \sum_{s&#39;} P(s&#39;|s,a) \left[ R(s,a,s&#39;) - \rho(\pi) + h_\pi(s&#39;) \right]\]</span></p>
<p>This equation is structurally similar to the discounted case but with two key differences: the immediate reward is adjusted by subtracting the average reward, and no discount factor appears before the future differential value.</p>
<p>We can similarly define differential action-value functions:</p>
<p><span class="math display">\[q_\pi(s,a) = \sum_{s&#39;} P(s&#39;|s,a) \left[ R(s,a,s&#39;) - \rho(\pi) + h_\pi(s&#39;) \right]\]</span></p>
<p>These satisfy the consistency relation: <span class="math inline">\(h_\pi(s) = \sum_a \pi(a|s) q_\pi(s,a)\)</span>.</p>
</div>
<div id="optimality-theory-for-average-reward" class="section level2 hasAnchor" number="13.4">
<h2><span class="header-section-number">13.4</span> Optimality Theory for Average Reward<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-theory-for-average-reward" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Optimal policies satisfy the <strong>average reward optimality equations</strong>:</p>
<p><span class="math display">\[\rho^* + h^*(s) = \max_a \sum_{s&#39;} P(s&#39;|s,a) \left[ R(s,a,s&#39;) + h^*(s&#39;) \right]\]</span></p>
<p>Here, <span class="math inline">\(\rho^*\)</span> is the optimal average reward (achieved by any optimal policy), and <span class="math inline">\(h^*\)</span> is the optimal differential value function. Unlike the discounted case, all optimal policies achieve the same average reward, though they may differ in their differential values.</p>
<p>The policy improvement theorem extends naturally to the average reward setting. Given the differential action-values under the current policy, improvement is achieved by acting greedily:</p>
<p><span class="math display">\[\pi&#39;(s) = \arg\max_a q_\pi(s,a)\]</span></p>
<p>This greedy policy is guaranteed to achieve average reward <span class="math inline">\(\rho(\pi&#39;) \geq \rho(\pi)\)</span>, with equality only when <span class="math inline">\(\pi\)</span> is already optimal.</p>
</div>
<div id="learning-algorithms-for-average-reward" class="section level2 hasAnchor" number="13.5">
<h2><span class="header-section-number">13.5</span> Learning Algorithms for Average Reward<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#learning-algorithms-for-average-reward" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The most direct extension of TD learning to average reward settings involves updating both the differential value function and an estimate of the average reward. The <strong>average reward TD(0)</strong> algorithm proceeds as follows:</p>
<ol style="list-style-type: decimal">
<li>Initialize: <span class="math inline">\(h(s) = 0\)</span> for all <span class="math inline">\(s\)</span>, <span class="math inline">\(\hat{\rho} = 0\)</span></li>
<li>For each time step <span class="math inline">\(t\)</span>:
<ul>
<li>Observe <span class="math inline">\(S_t\)</span>, take action <span class="math inline">\(A_t\)</span>, observe <span class="math inline">\(R_{t+1}\)</span> and <span class="math inline">\(S_{t+1}\)</span></li>
<li>Compute TD error: <span class="math inline">\(\delta_t = R_{t+1} - \hat{\rho} + h(S_{t+1}) - h(S_t)\)</span></li>
<li>Update differential value: <span class="math inline">\(h(S_t) \leftarrow h(S_t) + \alpha \delta_t\)</span></li>
<li>Update average reward estimate: <span class="math inline">\(\hat{\rho} \leftarrow \hat{\rho} + \beta \delta_t\)</span></li>
</ul></li>
</ol>
<p>The step sizes <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> control the learning rates for differential values and average reward, respectively. Typical choices satisfy <span class="math inline">\(\beta \ll \alpha\)</span> to ensure the average reward estimate changes more slowly than differential values.</p>
<p>Convergence of average reward TD methods requires more delicate analysis than their discounted counterparts. The key challenge is that both <span class="math inline">\(h\)</span> and <span class="math inline">\(\rho\)</span> are being estimated simultaneously, creating a coupled system of stochastic approximations. Under standard assumptions (diminishing step sizes, sufficient exploration, ergodic Markov chains), convergence can be established using two-timescale analysis. The intuition is that <span class="math inline">\(\rho\)</span> is estimated on a slower timescale (smaller step size), allowing the differential values to track the instantaneous policy while the average reward estimate provides a stable reference point.</p>
<p>Q-learning extends to average reward settings through the <strong>average reward Q-learning</strong> algorithm:</p>
<ol style="list-style-type: decimal">
<li>Initialize: <span class="math inline">\(q(s,a) = 0\)</span> for all <span class="math inline">\((s,a)\)</span>, <span class="math inline">\(\hat{\rho} = 0\)</span></li>
<li>For each time step <span class="math inline">\(t\)</span>:
<ul>
<li>Observe <span class="math inline">\(S_t\)</span>, choose <span class="math inline">\(A_t\)</span> (e.g., <span class="math inline">\(\epsilon\)</span>-greedily)</li>
<li>Observe <span class="math inline">\(R_{t+1}\)</span> and <span class="math inline">\(S_{t+1}\)</span></li>
<li>Compute TD error: <span class="math inline">\(\delta_t = R_{t+1} - \hat{\rho} + \max_a q(S_{t+1}, a) - q(S_t, A_t)\)</span></li>
<li>Update: <span class="math inline">\(q(S_t, A_t) \leftarrow q(S_t, A_t) + \alpha \delta_t\)</span></li>
<li>Update: <span class="math inline">\(\hat{\rho} \leftarrow \hat{\rho} + \beta \delta_t\)</span></li>
</ul></li>
</ol>
<p>This algorithm learns differential action-values directly, avoiding the need to maintain a separate state value function.</p>
</div>
<div id="practical-implementation-server-load-balancing" class="section level2 hasAnchor" number="13.6">
<h2><span class="header-section-number">13.6</span> Practical Implementation: Server Load Balancing<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#practical-implementation-server-load-balancing" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a web server load balancing system with three servers. Requests arrive continuously and must be routed to minimize average response time. This is naturally a continuing task where episodes don’t exist, making it ideal for average reward formulation.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb98-1"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-1" tabindex="-1"></a><span class="co"># Server Load Balancing with Average Reward Q-Learning</span></span>
<span id="cb98-2"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb98-3"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-3" tabindex="-1"></a></span>
<span id="cb98-4"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-4" tabindex="-1"></a><span class="co"># Environment setup</span></span>
<span id="cb98-5"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb98-6"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-6" tabindex="-1"></a>n_servers <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb98-7"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-7" tabindex="-1"></a>max_load <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb98-8"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-8" tabindex="-1"></a>n_steps <span class="ot">&lt;-</span> <span class="dv">50000</span></span>
<span id="cb98-9"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-9" tabindex="-1"></a></span>
<span id="cb98-10"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-10" tabindex="-1"></a><span class="co"># State encoding and reward function</span></span>
<span id="cb98-11"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-11" tabindex="-1"></a>encode_state <span class="ot">&lt;-</span> <span class="cf">function</span>(loads) {</span>
<span id="cb98-12"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-12" tabindex="-1"></a>  <span class="fu">sum</span>(loads <span class="sc">*</span> (max_load <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">^</span>(<span class="dv">0</span><span class="sc">:</span>(<span class="fu">length</span>(loads)<span class="sc">-</span><span class="dv">1</span>))) <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb98-13"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-13" tabindex="-1"></a>}</span>
<span id="cb98-14"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-14" tabindex="-1"></a></span>
<span id="cb98-15"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-15" tabindex="-1"></a>decode_state <span class="ot">&lt;-</span> <span class="cf">function</span>(state_id) {</span>
<span id="cb98-16"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-16" tabindex="-1"></a>  state_id <span class="ot">&lt;-</span> state_id <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb98-17"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-17" tabindex="-1"></a>  loads <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_servers)</span>
<span id="cb98-18"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-18" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_servers) {</span>
<span id="cb98-19"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-19" tabindex="-1"></a>    loads[i] <span class="ot">&lt;-</span> state_id <span class="sc">%%</span> (max_load <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb98-20"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-20" tabindex="-1"></a>    state_id <span class="ot">&lt;-</span> state_id <span class="sc">%/%</span> (max_load <span class="sc">+</span> <span class="dv">1</span>)</span>
<span id="cb98-21"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-21" tabindex="-1"></a>  }</span>
<span id="cb98-22"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-22" tabindex="-1"></a>  loads</span>
<span id="cb98-23"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-23" tabindex="-1"></a>}</span>
<span id="cb98-24"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-24" tabindex="-1"></a></span>
<span id="cb98-25"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-25" tabindex="-1"></a>get_reward <span class="ot">&lt;-</span> <span class="cf">function</span>(loads, server_choice) {</span>
<span id="cb98-26"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-26" tabindex="-1"></a>  new_loads <span class="ot">&lt;-</span> loads</span>
<span id="cb98-27"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-27" tabindex="-1"></a>  new_loads[server_choice] <span class="ot">&lt;-</span> <span class="fu">min</span>(new_loads[server_choice] <span class="sc">+</span> <span class="dv">1</span>, max_load)</span>
<span id="cb98-28"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-28" tabindex="-1"></a>  </span>
<span id="cb98-29"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-29" tabindex="-1"></a>  <span class="co"># Response time model: linear in load</span></span>
<span id="cb98-30"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-30" tabindex="-1"></a>  avg_load <span class="ot">&lt;-</span> <span class="fu">mean</span>(new_loads)</span>
<span id="cb98-31"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-31" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> <span class="sc">-</span>avg_load</span>
<span id="cb98-32"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-32" tabindex="-1"></a>  </span>
<span id="cb98-33"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-33" tabindex="-1"></a>  <span class="co"># Simulate request completion</span></span>
<span id="cb98-34"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-34" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_servers) {</span>
<span id="cb98-35"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-35" tabindex="-1"></a>    <span class="cf">if</span> (new_loads[i] <span class="sc">&gt;</span> <span class="dv">0</span> <span class="sc">&amp;&amp;</span> <span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> <span class="fl">0.3</span>) {</span>
<span id="cb98-36"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-36" tabindex="-1"></a>      new_loads[i] <span class="ot">&lt;-</span> new_loads[i] <span class="sc">-</span> <span class="dv">1</span></span>
<span id="cb98-37"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-37" tabindex="-1"></a>    }</span>
<span id="cb98-38"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-38" tabindex="-1"></a>  }</span>
<span id="cb98-39"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-39" tabindex="-1"></a>  </span>
<span id="cb98-40"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-40" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">reward =</span> reward, <span class="at">new_loads =</span> new_loads)</span>
<span id="cb98-41"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-41" tabindex="-1"></a>}</span>
<span id="cb98-42"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-42" tabindex="-1"></a></span>
<span id="cb98-43"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-43" tabindex="-1"></a><span class="co"># Initialize parameters</span></span>
<span id="cb98-44"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-44" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> (max_load <span class="sc">+</span> <span class="dv">1</span>)<span class="sc">^</span>n_servers</span>
<span id="cb98-45"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-45" tabindex="-1"></a>Q <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_servers))</span>
<span id="cb98-46"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-46" tabindex="-1"></a>rho_hat <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb98-47"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-47" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb98-48"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-48" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fl">0.001</span></span>
<span id="cb98-49"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-49" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb98-50"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-50" tabindex="-1"></a></span>
<span id="cb98-51"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-51" tabindex="-1"></a><span class="co"># Training loop</span></span>
<span id="cb98-52"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-52" tabindex="-1"></a>loads <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_servers)</span>
<span id="cb98-53"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-53" tabindex="-1"></a>state_id <span class="ot">&lt;-</span> <span class="fu">encode_state</span>(loads)</span>
<span id="cb98-54"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-54" tabindex="-1"></a>rewards_history <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_steps)</span>
<span id="cb98-55"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-55" tabindex="-1"></a>rho_history <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_steps)</span>
<span id="cb98-56"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-56" tabindex="-1"></a></span>
<span id="cb98-57"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb98-57" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;Training average reward Q-learning agent...</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## Training average reward Q-learning agent...</code></pre>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb100-1"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-1" tabindex="-1"></a><span class="cf">for</span> (step <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_steps) {</span>
<span id="cb100-2"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-2" tabindex="-1"></a>  <span class="co"># Epsilon-greedy action selection</span></span>
<span id="cb100-3"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-3" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb100-4"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-4" tabindex="-1"></a>    action <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_servers, <span class="dv">1</span>)</span>
<span id="cb100-5"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-5" tabindex="-1"></a>  } <span class="cf">else</span> {</span>
<span id="cb100-6"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-6" tabindex="-1"></a>    action <span class="ot">&lt;-</span> <span class="fu">which.max</span>(Q[state_id, ])</span>
<span id="cb100-7"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-7" tabindex="-1"></a>  }</span>
<span id="cb100-8"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-8" tabindex="-1"></a>  </span>
<span id="cb100-9"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-9" tabindex="-1"></a>  <span class="co"># Take action and observe reward</span></span>
<span id="cb100-10"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-10" tabindex="-1"></a>  result <span class="ot">&lt;-</span> <span class="fu">get_reward</span>(loads, action)</span>
<span id="cb100-11"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-11" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> result<span class="sc">$</span>reward</span>
<span id="cb100-12"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-12" tabindex="-1"></a>  loads <span class="ot">&lt;-</span> result<span class="sc">$</span>new_loads</span>
<span id="cb100-13"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-13" tabindex="-1"></a>  next_state_id <span class="ot">&lt;-</span> <span class="fu">encode_state</span>(loads)</span>
<span id="cb100-14"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-14" tabindex="-1"></a>  </span>
<span id="cb100-15"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-15" tabindex="-1"></a>  <span class="co"># Q-learning update</span></span>
<span id="cb100-16"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-16" tabindex="-1"></a>  td_error <span class="ot">&lt;-</span> reward <span class="sc">-</span> rho_hat <span class="sc">+</span> <span class="fu">max</span>(Q[next_state_id, ]) <span class="sc">-</span> Q[state_id, action]</span>
<span id="cb100-17"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-17" tabindex="-1"></a>  Q[state_id, action] <span class="ot">&lt;-</span> Q[state_id, action] <span class="sc">+</span> alpha <span class="sc">*</span> td_error</span>
<span id="cb100-18"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-18" tabindex="-1"></a>  rho_hat <span class="ot">&lt;-</span> rho_hat <span class="sc">+</span> beta <span class="sc">*</span> td_error</span>
<span id="cb100-19"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-19" tabindex="-1"></a>  </span>
<span id="cb100-20"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-20" tabindex="-1"></a>  <span class="co"># Record progress</span></span>
<span id="cb100-21"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-21" tabindex="-1"></a>  rewards_history[step] <span class="ot">&lt;-</span> reward</span>
<span id="cb100-22"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-22" tabindex="-1"></a>  rho_history[step] <span class="ot">&lt;-</span> rho_hat</span>
<span id="cb100-23"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-23" tabindex="-1"></a>  </span>
<span id="cb100-24"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-24" tabindex="-1"></a>  <span class="co"># Update state</span></span>
<span id="cb100-25"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-25" tabindex="-1"></a>  state_id <span class="ot">&lt;-</span> next_state_id</span>
<span id="cb100-26"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-26" tabindex="-1"></a>  </span>
<span id="cb100-27"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-27" tabindex="-1"></a>  <span class="co"># Decay exploration</span></span>
<span id="cb100-28"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-28" tabindex="-1"></a>  <span class="cf">if</span> (step <span class="sc">%%</span> <span class="dv">1000</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb100-29"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-29" tabindex="-1"></a>    epsilon <span class="ot">&lt;-</span> <span class="fu">max</span>(<span class="fl">0.01</span>, epsilon <span class="sc">*</span> <span class="fl">0.995</span>)</span>
<span id="cb100-30"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-30" tabindex="-1"></a>    <span class="cf">if</span> (step <span class="sc">%%</span> <span class="dv">10000</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb100-31"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-31" tabindex="-1"></a>      <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Step %d: Average reward estimate = %.4f</span><span class="sc">\n</span><span class="st">&quot;</span>, step, rho_hat))</span>
<span id="cb100-32"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-32" tabindex="-1"></a>    }</span>
<span id="cb100-33"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-33" tabindex="-1"></a>  }</span>
<span id="cb100-34"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb100-34" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Step 10000: Average reward estimate = -1.5199
## Step 20000: Average reward estimate = -1.6557
## Step 30000: Average reward estimate = -1.6660
## Step 40000: Average reward estimate = -1.6685
## Step 50000: Average reward estimate = -1.6553</code></pre>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb102-1"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-1" tabindex="-1"></a><span class="co"># Evaluate learned policy</span></span>
<span id="cb102-2"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-2" tabindex="-1"></a>eval_steps <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb102-3"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-3" tabindex="-1"></a>loads <span class="ot">&lt;-</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_servers)</span>
<span id="cb102-4"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-4" tabindex="-1"></a>state_id <span class="ot">&lt;-</span> <span class="fu">encode_state</span>(loads)</span>
<span id="cb102-5"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-5" tabindex="-1"></a>eval_rewards <span class="ot">&lt;-</span> <span class="fu">numeric</span>(eval_steps)</span>
<span id="cb102-6"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-6" tabindex="-1"></a></span>
<span id="cb102-7"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-7" tabindex="-1"></a><span class="cf">for</span> (step <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>eval_steps) {</span>
<span id="cb102-8"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-8" tabindex="-1"></a>  action <span class="ot">&lt;-</span> <span class="fu">which.max</span>(Q[state_id, ])</span>
<span id="cb102-9"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-9" tabindex="-1"></a>  result <span class="ot">&lt;-</span> <span class="fu">get_reward</span>(loads, action)</span>
<span id="cb102-10"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-10" tabindex="-1"></a>  eval_rewards[step] <span class="ot">&lt;-</span> result<span class="sc">$</span>reward</span>
<span id="cb102-11"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-11" tabindex="-1"></a>  loads <span class="ot">&lt;-</span> result<span class="sc">$</span>new_loads</span>
<span id="cb102-12"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-12" tabindex="-1"></a>  state_id <span class="ot">&lt;-</span> <span class="fu">encode_state</span>(loads)</span>
<span id="cb102-13"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-13" tabindex="-1"></a>}</span>
<span id="cb102-14"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-14" tabindex="-1"></a></span>
<span id="cb102-15"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-15" tabindex="-1"></a>final_avg_reward <span class="ot">&lt;-</span> <span class="fu">mean</span>(eval_rewards)</span>
<span id="cb102-16"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb102-16" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Final average reward: %.4f</span><span class="sc">\n</span><span class="st">&quot;</span>, final_avg_reward))</span></code></pre></div>
<pre><code>## Final average reward: -1.6658</code></pre>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb104-1"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb104-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Training estimate: %.4f</span><span class="sc">\n</span><span class="st">&quot;</span>, rho_hat))</span></code></pre></div>
<pre><code>## Training estimate: -1.6553</code></pre>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb106-1"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb106-1" tabindex="-1"></a><span class="co"># Policy analysis</span></span>
<span id="cb106-2"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb106-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">Learned policy examples:</span><span class="sc">\n</span><span class="st">&quot;</span>)</span></code></pre></div>
<pre><code>## 
## Learned policy examples:</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-1" tabindex="-1"></a>example_states <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb108-2"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-2" tabindex="-1"></a>  <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">0</span>),  <span class="co"># All servers idle</span></span>
<span id="cb108-3"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-3" tabindex="-1"></a>  <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>),  <span class="co"># Server 1 heavily loaded</span></span>
<span id="cb108-4"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-4" tabindex="-1"></a>  <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>),  <span class="co"># Balanced load</span></span>
<span id="cb108-5"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-5" tabindex="-1"></a>  <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">0</span>, <span class="dv">0</span>),  <span class="co"># Server 1 at capacity</span></span>
<span id="cb108-6"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-6" tabindex="-1"></a>  <span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">3</span>, <span class="dv">4</span>)   <span class="co"># High overall load</span></span>
<span id="cb108-7"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-7" tabindex="-1"></a>)</span>
<span id="cb108-8"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-8" tabindex="-1"></a></span>
<span id="cb108-9"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-9" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(example_states)) {</span>
<span id="cb108-10"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-10" tabindex="-1"></a>  loads <span class="ot">&lt;-</span> example_states[[i]]</span>
<span id="cb108-11"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-11" tabindex="-1"></a>  state_id <span class="ot">&lt;-</span> <span class="fu">encode_state</span>(loads)</span>
<span id="cb108-12"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-12" tabindex="-1"></a>  q_values <span class="ot">&lt;-</span> Q[state_id, ]</span>
<span id="cb108-13"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-13" tabindex="-1"></a>  best_action <span class="ot">&lt;-</span> <span class="fu">which.max</span>(q_values)</span>
<span id="cb108-14"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-14" tabindex="-1"></a>  </span>
<span id="cb108-15"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-15" tabindex="-1"></a>  <span class="fu">cat</span>(<span class="fu">sprintf</span>(<span class="st">&quot;Loads %s -&gt; Route to server %d (Q-values: %.3f, %.3f, %.3f)</span><span class="sc">\n</span><span class="st">&quot;</span>,</span>
<span id="cb108-16"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-16" tabindex="-1"></a>              <span class="fu">paste</span>(loads, <span class="at">collapse=</span><span class="st">&quot;, &quot;</span>), best_action,</span>
<span id="cb108-17"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-17" tabindex="-1"></a>              q_values[<span class="dv">1</span>], q_values[<span class="dv">2</span>], q_values[<span class="dv">3</span>]))</span>
<span id="cb108-18"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#cb108-18" tabindex="-1"></a>}</span></code></pre></div>
<pre><code>## Loads 0, 0, 0 -&gt; Route to server 2 (Q-values: -0.033, 0.000, 0.000)
## Loads 3, 1, 1 -&gt; Route to server 3 (Q-values: -0.249, -0.171, -0.116)
## Loads 2, 2, 2 -&gt; Route to server 2 (Q-values: -0.434, -0.355, -0.408)
## Loads 5, 0, 0 -&gt; Route to server 3 (Q-values: -0.166, -0.214, -0.154)
## Loads 3, 3, 4 -&gt; Route to server 1 (Q-values: -0.884, -1.003, -0.918)</code></pre>
</div>
<div id="implementation-considerations" class="section level2 hasAnchor" number="13.7">
<h2><span class="header-section-number">13.7</span> Implementation Considerations<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#implementation-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The choice of step sizes <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> is crucial for average reward algorithms. The general principle is to use different timescales: a fast timescale (<span class="math inline">\(\alpha\)</span>) for updates to value functions or policy parameters, and a slow timescale (<span class="math inline">\(\beta\)</span>) for updates to the average reward estimate. A common choice is <span class="math inline">\(\beta = c \cdot \alpha\)</span> where <span class="math inline">\(c \in [0.01, 0.1]\)</span>. This ensures the average reward estimate stabilizes while allowing value functions to adapt to policy changes.</p>
<p>Unlike discounted RL, where initialization typically has limited impact on final performance, average reward methods can be sensitive to initial conditions. Poor initialization of <span class="math inline">\(\hat{\rho}\)</span> can lead to slow convergence or temporary instability. Effective strategies include running a random policy for some steps to get an initial estimate of <span class="math inline">\(\hat{\rho}\)</span>, starting with a conservative estimate based on domain knowledge, or using adaptive step sizes that are larger initially and then decay as estimates stabilize.</p>
<p>Exploration-exploitation trade-offs require special consideration in average reward settings. Since there’s no discounting to naturally prioritize immediate rewards, exploration strategies must be designed to maintain long-term effectiveness. Optimistic initialization works particularly well: initializing value functions optimistically encourages exploration while the agent discovers the true reward structure.</p>
</div>
<div id="when-to-choose-average-reward-over-discounting" class="section level2 hasAnchor" number="13.8">
<h2><span class="header-section-number">13.8</span> When to Choose Average Reward Over Discounting<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#when-to-choose-average-reward-over-discounting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The choice between discounted and average reward formulations should be based on problem characteristics rather than mathematical convenience. Consider average reward when the task is genuinely continuing with no natural episodes, long-term steady-state performance is the primary concern, the discount factor feels arbitrary or its choice significantly affects learned behavior, or fairness across time steps is important.</p>
<p>Moderate indicators include when the environment is relatively stable over long horizons, exploration can be maintained without artificial urgency from discounting, or the system will operate for extended periods without reset. However, choose against average reward when the task has clear episodes or terminal states, short-term performance is crucial (e.g., real-time systems), the environment is highly non-stationary, or mathematical tractability is paramount.</p>
<p>Based on empirical studies and practical applications, server systems and manufacturing typically find average reward superior, while games with clear episodes often find discounting more natural. Financial trading uses average reward for long-term strategies and discounting for short-term tactics. In robotics, the choice depends on task duration and safety requirements, while recommendation systems use average reward for user satisfaction and discounting for engagement.</p>
</div>
<div id="appendix-a-mathematical-proofs-and-derivations" class="section level2 hasAnchor" number="13.9">
<h2><span class="header-section-number">13.9</span> Appendix A: Mathematical Proofs and Derivations<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#appendix-a-mathematical-proofs-and-derivations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="convergence-of-average-reward-td-learning" class="section level3 hasAnchor" number="13.9.1">
<h3><span class="header-section-number">13.9.1</span> Convergence of Average Reward TD Learning<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#convergence-of-average-reward-td-learning" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Theorem</strong>: Under standard assumptions (finite state space, ergodic Markov chains, diminishing step sizes satisfying <span class="math inline">\(\sum_t \alpha_t = \infty\)</span>, <span class="math inline">\(\sum_t \alpha_t^2 &lt; \infty\)</span>, and <span class="math inline">\(\beta_t = o(\alpha_t)\)</span>), the average reward TD(0) algorithm converges almost surely to the true differential value function and average reward.</p>
<p><strong>Proof sketch</strong>: The proof uses two-timescale stochastic approximation theory. The key insight is that the average reward estimate evolves on a slower timescale (smaller step size), allowing it to track the average while the differential values adapt to the current estimate.</p>
<p>Let <span class="math inline">\(\{\alpha_t\}\)</span> and <span class="math inline">\(\{\beta_t\}\)</span> be the step size sequences for differential values and average reward, respectively, satisfying <span class="math inline">\(\beta_t = o(\alpha_t)\)</span>.</p>
<p>The updates can be written as:
<span class="math display">\[h_{t+1}(s) = h_t(s) + \alpha_t \mathbf{1}_{S_t = s} [R_{t+1} - \rho_t + h_t(S_{t+1}) - h_t(S_t)]\]</span>
<span class="math display">\[\rho_{t+1} = \rho_t + \beta_t [R_{t+1} - \rho_t + h_t(S_{t+1}) - h_t(S_t)]\]</span></p>
<p>Using the two-timescale framework, we first analyze the faster timescale (differential values) assuming the average reward estimate is fixed. This leads to the standard convergence analysis for policy evaluation. Then, we analyze the slower timescale (average reward) and show it tracks the true average reward under the converged policy.</p>
</div>
<div id="policy-gradient-theorem-for-average-reward" class="section level3 hasAnchor" number="13.9.2">
<h3><span class="header-section-number">13.9.2</span> Policy Gradient Theorem for Average Reward<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#policy-gradient-theorem-for-average-reward" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Theorem (Average Reward Policy Gradient)</strong>: For a parameterized policy <span class="math inline">\(\pi_\theta\)</span>, the gradient of the average reward with respect to parameters is:</p>
<p><span class="math display">\[\nabla_\theta \rho(\pi_\theta) = \sum_s d^{\pi_\theta}(s) \sum_a \nabla_\theta \pi_\theta(a|s) q^{\pi_\theta}(s,a)\]</span></p>
<p>where <span class="math inline">\(d^{\pi_\theta}(s)\)</span> is the stationary distribution under policy <span class="math inline">\(\pi_\theta\)</span> and <span class="math inline">\(q^{\pi_\theta}(s,a)\)</span> is the differential action-value function.</p>
<p><strong>Proof</strong>: The proof parallels the discounted case but requires careful handling of the limit defining average reward. The key steps involve expressing average reward in terms of stationary distribution, differentiating with respect to <span class="math inline">\(\theta\)</span>, applying the chain rule noting that both the stationary distribution and immediate rewards depend on the policy, using the fundamental matrix approach to handle the derivative of the stationary distribution, and simplifying using the relationship between immediate rewards and differential action-values.</p>
</div>
<div id="optimality-equations-derivation" class="section level3 hasAnchor" number="13.9.3">
<h3><span class="header-section-number">13.9.3</span> Optimality Equations Derivation<a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-equations-derivation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Starting from the definition of differential value function:
<span class="math display">\[h^*(s) = \max_\pi h^\pi(s)\]</span></p>
<p>where
<span class="math display">\[h^\pi(s) = \mathbb{E}^\pi \left[ \sum_{t=0}^\infty (R_{t+1} - \rho(\pi)) \mid S_0 = s \right]\]</span></p>
<p>For the optimal policy <span class="math inline">\(\pi^*\)</span>, we have:
<span class="math display">\[h^*(s) = \mathbb{E}^{\pi^*} \left[ \sum_{t=0}^\infty (R_{t+1} - \rho^*) \mid S_0 = s \right]\]</span></p>
<p>Expanding the first term and using the Markov property, since <span class="math inline">\(\pi^*\)</span> is optimal, it must choose actions to maximize:
<span class="math display">\[\sum_{s&#39;} P(s&#39;|s,a) [R(s,a,s&#39;) - \rho^* + h^*(s&#39;)]\]</span></p>
<p>This gives us the optimality equation:
<span class="math display">\[\rho^* + h^*(s) = \max_a \sum_{s&#39;} P(s&#39;|s,a) [R(s,a,s&#39;) + h^*(s&#39;)]\]</span></p>
<p>The average reward formulation of reinforcement learning provides a principled alternative to discounted approaches that better captures the objectives of many real-world applications. While mathematically more challenging than discounted methods, the conceptual clarity and practical benefits make it worthy of broader adoption. The choice between formulations should reflect true problem objectives rather than mathematical convenience, and for many continuing tasks, average reward optimization represents the most appropriate objective for creating systems that serve genuine long-term value.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="learning-policies-versus-learning-values.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="eligibility-traces.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/edit/main/13-Average_Reward.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/blob/main/13-Average_Reward.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
