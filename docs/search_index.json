[["function-approximation-q-learning-with-linear-models.html", "Chapter 6 Function Approximation Q-Learning with Linear Models 6.1 Introduction 6.2 R Implementation", " Chapter 6 Function Approximation Q-Learning with Linear Models 6.1 Introduction In reinforcement learning (RL), tabular methods like SARSA and Q-Learning store a separate Q-value for each state-action pair, which becomes infeasible in large or continuous state spaces. Function approximation addresses this by representing the action-value function \\(Q(s, a)\\) as a parameterized function \\(Q(s, a; \\theta)\\), enabling generalization across states and scalability. This post explores Q-Learning with linear function approximation, using a 10-state, 2-action environment to demonstrate how it learns policies compared to tabular methods. We provide mathematical formulations, R code, and comparisons with tabular Q-Learning, focusing on generalization, scalability, and practical implications. Function approximation in RL aims to estimate the action-value function: \\[ Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a \\right] \\] where \\(\\gamma \\in [0,1]\\) is the discount factor, and \\(R_{t+1}\\) is the reward at time \\(t+1\\). Instead of storing \\(Q(s, a)\\) in a table, we approximate it as: \\[ Q(s, a; \\theta) = \\phi(s, a)^T \\theta \\] where \\(\\phi(s, a)\\) is a feature vector for state-action pair \\((s, a)\\), and \\(\\theta\\) is a parameter vector learned via optimization, typically stochastic gradient descent (SGD). 6.1.1 Q-Learning with Function Approximation Q-Learning with function approximation is an off-policy method that learns the optimal policy by updating \\(\\theta\\) to minimize the temporal difference (TD) error. The update rule is: \\[ \\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta Q(s, a; \\theta) \\] where \\(\\alpha\\) is the learning rate, and the TD error \\(\\delta\\) is: \\[ \\delta = r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;; \\theta) - Q(s, a; \\theta) \\] Here, \\(r\\) is the reward, \\(s&#39;\\) is the next state, and \\(\\max_{a&#39;} Q(s&#39;, a&#39;; \\theta)\\) estimates the value of the next state assuming the optimal action. For linear function approximation, the gradient is: \\[ \\nabla_\\theta Q(s, a; \\theta) = \\phi(s, a) \\] Thus, the update becomes: \\[ \\theta \\leftarrow \\theta + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;; \\theta) - Q(s, a; \\theta) \\right) \\phi(s, a) \\] In our 10-state environment, we use one-hot encoding for \\(\\phi(s, a)\\), mimicking tabular Q-Learning for simplicity but demonstrating the framework’s potential for generalization with more complex features. 6.1.2 Comparison with Tabular Q-Learning Tabular Q-Learning updates a table of Q-values directly: \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s, a) \\right) \\] Function approximation generalizes across states via \\(\\phi(s, a)\\), reducing memory requirements and enabling learning in large or continuous spaces. However, it introduces approximation errors and requires careful feature design to ensure convergence. Aspect Tabular Q-Learning Q-Learning with Function Approximation Representation Table of \\(Q(s, a)\\) values \\(Q(s, a; \\theta) = \\phi(s, a)^T \\theta\\) Memory \\(O(|\\mathcal{S}| \\cdot |\\mathcal{A}|)\\) \\(O(|\\theta|)\\), depends on feature size Generalization None; state-action specific Yes; depends on feature design Scalability Poor for large/continuous spaces Good for large/continuous spaces with proper features Update Rule Direct Q-value update Parameter update via gradient descent Convergence Guaranteed to optimal \\(Q^*\\) under conditions Converges to approximation of \\(Q^*\\); depends on features 6.2 R Implementation We implement Q-Learning with linear function approximation in a 10-state, 2-action environment, using one-hot encoding for \\(\\phi(s, a)\\). The environment mirrors the previous post, with action 1 yielding a 1.0 reward at the terminal state (state 10) and action 2 yielding 0.5. # Common settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) # Build transition and reward models for (s in 1:(n_states - 1)) { # Action 1: 90% to s+1, 10% to a different random state transition_model[s, 1, s + 1] &lt;- 0.9 other_states &lt;- setdiff(1:n_states, s + 1) random_state &lt;- sample(other_states, 1) transition_model[s, 1, random_state] &lt;- 0.1 # Action 2: 80% and 20% to two different random states two_states &lt;- sample(1:n_states, 2, replace = FALSE) transition_model[s, 2, two_states[1]] &lt;- 0.8 transition_model[s, 2, two_states[2]] &lt;- 0.2 } # Make terminal state absorbing (stays in itself) transition_model[n_states, , n_states] &lt;- 1 # Set rewards (fixed, not random each time) set.seed(42) for (s in 1:(n_states - 1)) { for (a in 1:n_actions) { for (s_prime in 1:n_states) { if (transition_model[s, a, s_prime] &gt; 0) { if (s_prime == n_states) { reward_model[s, a, s_prime] &lt;- ifelse(a == 1, 1.0, 0.5) } else { reward_model[s, a, s_prime] &lt;- ifelse(a == 1, 0.1 * runif(1), 0.05 * runif(1)) } } } } } # Terminal state has no rewards reward_model[n_states, , ] &lt;- 0 # Verify transition probabilities sum to 1 for (s in 1:n_states) { for (a in 1:n_actions) { prob_sum &lt;- sum(transition_model[s, a, ]) if (abs(prob_sum - 1.0) &gt; 1e-10) { warning(sprintf(&quot;State %d, Action %d: probabilities sum to %.4f&quot;, s, a, prob_sum)) } } } # Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } # Create one-hot features for (state, action) pairs create_features &lt;- function(s, a, n_states, n_actions) { vec &lt;- rep(0, n_states * n_actions) index &lt;- (a - 1) * n_states + s vec[index] &lt;- 1 return(vec) } # Initialize weights n_features &lt;- n_states * n_actions theta &lt;- rep(0, n_features) # Q-value approximation function q_hat &lt;- function(s, a, theta) { x &lt;- create_features(s, a, n_states, n_actions) return(sum(x * theta)) } # Q-Learning with function approximation q_learning_fa &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) { theta &lt;- rep(0, n_features) rewards &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 step_count &lt;- 0 max_steps &lt;- 1000 # Prevent infinite loops while (step_count &lt; max_steps) { step_count &lt;- step_count + 1 # Epsilon-greedy action selection a &lt;- if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { q_vals &lt;- sapply(1:n_actions, function(a_) q_hat(s, a_, theta)) which.max(q_vals) } out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime r &lt;- out$reward episode_reward &lt;- episode_reward + r # Compute TD target and error q_current &lt;- q_hat(s, a, theta) q_next &lt;- if (s_prime == terminal_state) { 0 } else { max(sapply(1:n_actions, function(a_) q_hat(s_prime, a_, theta))) } target &lt;- r + gamma * q_next error &lt;- target - q_current # Gradient update x &lt;- create_features(s, a, n_states, n_actions) theta &lt;- theta + alpha * error * x if (s_prime == terminal_state) break s &lt;- s_prime } rewards[ep] &lt;- episode_reward } # Derive policy policy &lt;- sapply(1:n_states, function(s) { if (s == terminal_state) { NA } else { which.max(sapply(1:n_actions, function(a) q_hat(s, a, theta))) } }) list(theta = theta, policy = policy, rewards = rewards) } # Run Q-Learning with function approximation set.seed(42) fa_result &lt;- q_learning_fa(episodes = 1000, alpha = 0.1, epsilon = 0.1) fa_policy &lt;- fa_result$policy fa_rewards &lt;- fa_result$rewards # Visualize policy library(ggplot2) policy_df &lt;- data.frame( State = 1:n_states, Policy = fa_policy, Algorithm = &quot;Q-Learning FA&quot; ) policy_plot &lt;- ggplot(policy_df[!is.na(policy_df$Policy), ], aes(x = State, y = Policy)) + geom_point(size = 3, color = &quot;deepskyblue&quot;) + geom_line(color = &quot;deepskyblue&quot;) + theme_minimal() + labs(title = &quot;Policy from Q-Learning with Function Approximation&quot;, x = &quot;State&quot;, y = &quot;Action&quot;) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;)) + theme(legend.position = &quot;none&quot;) # Visualize cumulative rewards reward_df &lt;- data.frame( Episode = 1:1000, CumulativeReward = cumsum(fa_rewards), Algorithm = &quot;Q-Learning FA&quot; ) reward_plot &lt;- ggplot(reward_df, aes(x = Episode, y = CumulativeReward)) + geom_line(color = &quot;deepskyblue&quot;, linewidth = 1) + theme_minimal() + labs(title = &quot;Cumulative Rewards Over Episodes&quot;, x = &quot;Episode&quot;, y = &quot;Cumulative Reward&quot;) # Display plots print(policy_plot) print(reward_plot) # Print summary statistics cat(&quot;\\nPolicy Summary:\\n&quot;) ## ## Policy Summary: print(table(fa_policy, useNA = &quot;ifany&quot;)) ## fa_policy ## 1 2 &lt;NA&gt; ## 8 1 1 cat(&quot;\\nFinal 10 Episode Rewards:\\n&quot;) ## ## Final 10 Episode Rewards: print(tail(fa_rewards, 10)) ## [1] 1.415547 1.316729 1.145381 1.352685 1.181336 1.110396 1.382692 1.234756 ## [9] 1.158847 1.008244 cat(&quot;\\nMean Episode Reward (last 100 episodes):\\n&quot;) ## ## Mean Episode Reward (last 100 episodes): print(mean(tail(fa_rewards, 100))) ## [1] 1.187619 Environment settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states n_states and n_actions: size of the MDP (10 states, 2 actions). gamma: discount factor (0.9). terminal_state: the index of the terminal state (state 10). Many later pieces use this to stop episodes and to set q_next = 0 for transitions into terminal. Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { # Action 1: 90% to s+1, 10% to a different random state transition_model[s, 1, s + 1] &lt;- 0.9 other_states &lt;- setdiff(1:n_states, s + 1) random_state &lt;- sample(other_states, 1) transition_model[s, 1, random_state] &lt;- 0.1 # Action 2: 80% and 20% to two different random states two_states &lt;- sample(1:n_states, 2, replace = FALSE) transition_model[s, 2, two_states[1]] &lt;- 0.8 transition_model[s, 2, two_states[2]] &lt;- 0.2 } # Make terminal state absorbing (stays in itself) transition_model[n_states, , n_states] &lt;- 1 # Set rewards (fixed, not random each time) set.seed(42) for (s in 1:(n_states - 1)) { for (a in 1:n_actions) { for (s_prime in 1:n_states) { if (transition_model[s, a, s_prime] &gt; 0) { if (s_prime == n_states) { reward_model[s, a, s_prime] &lt;- ifelse(a == 1, 1.0, 0.5) } else { reward_model[s, a, s_prime] &lt;- ifelse(a == 1, 0.1 * runif(1), 0.05 * runif(1)) } } } } } reward_model[n_states, , ] &lt;- 0 transition_model[s, a, s'] is the probability of transitioning from state s with action a to next state s'. reward_model[s, a, s'] is the reward received for that transition. Key behavioral details: Seeding: set.seed(42) fixes random draws so behavior is reproducible. Action 1 transitions: For non-terminal s, assigns 0.9 probability to go to s+1. Uses setdiff() to exclude s+1 from the random selection, ensuring the 10% probability goes to a different state. Result: Each row for action 1 correctly sums to 1.0 with exactly two non-zero entries (0.9 and 0.1). Action 2 transitions: Uses sample(1:n_states, 2, replace = FALSE) to select two different states. Assigns 0.8 to the first state and 0.2 to the second. Result: Each row for action 2 correctly sums to 1.0 with exactly two non-zero entries, and they’re guaranteed to be different states. Validation check: for (s in 1:n_states) { for (a in 1:n_actions) { prob_sum &lt;- sum(transition_model[s, a, ]) if (abs(prob_sum - 1.0) &gt; 1e-10) { warning(sprintf(&quot;State %d, Action %d: probabilities sum to %.4f&quot;, s, a, prob_sum)) } } } Validates that all transition probability rows sum to 1.0 (within numerical tolerance). Issues warnings if any row is malformed. Rewards: For action 1: if s' is terminal reward = 1.0, else 0.1 * runif(1) (random small reward). For action 2: terminal reward 0.5, else 0.05 * runif(1) (even smaller). Efficiency improvement: Rewards are only assigned for reachable states (where transition_model[s, a, s_prime] &gt; 0). Rewards are generated once during initialization and remain fixed during training (deterministic environment). Terminal state: transition_model[n_states, , n_states] &lt;- 1 makes terminal state absorbing (transitions to itself with probability 1.0). This is the standard MDP formulation for terminal states. reward_model[n_states, , ] &lt;- 0 ensures no rewards from terminal state. Episodes still end when terminal is reached, so this absorbing property isn’t exercised during training. Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } Given (s, a), samples s' according to the probability vector probs. Returns a list with s_prime and reward = reward_model[s,a,s_prime]. Now safe: Since all transition rows properly sum to 1.0, sample() will work correctly without needing to normalize. The terminal state absorbing property means if sample_env(n_states, a) were called, it would always return s_prime = n_states, but episodes terminate before this occurs. Feature creation (one-hot for (state,action)) create_features &lt;- function(s, a, n_states, n_actions) { vec &lt;- rep(0, n_states * n_actions) index &lt;- (a - 1) * n_states + s vec[index] &lt;- 1 return(vec) } Creates a one-hot feature vector of length n_states * n_actions = 20. The mapping index = (a - 1) * n_states + s organizes features in action blocks: entries 1-10 for action 1 (states 1-10), entries 11-20 for action 2 (states 1-10). Because features are one-hot, linear function approximation q(s,a) = x^T theta is mathematically equivalent to tabular Q-learning (each (s,a) has its own independent weight in theta). Weights and Q-value function n_features &lt;- n_states * n_actions theta &lt;- rep(0, n_features) q_hat &lt;- function(s, a, theta) { x &lt;- create_features(s, a, n_states, n_actions) return(sum(x * theta)) } theta holds 20 parameters for the linear approximator; initialized at zeros. q_hat(s,a,theta) computes Q̂(s,a) = x^T θ. With one-hot x, this returns the single theta entry corresponding to (s,a). Q-learning with function approximation q_learning_fa &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) { theta &lt;- rep(0, n_features) rewards &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 step_count &lt;- 0 max_steps &lt;- 1000 # Prevent infinite loops while (step_count &lt; max_steps) { step_count &lt;- step_count + 1 # Epsilon-greedy action selection a &lt;- if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { q_vals &lt;- sapply(1:n_actions, function(a_) q_hat(s, a_, theta)) which.max(q_vals) } out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime r &lt;- out$reward episode_reward &lt;- episode_reward + r # Compute TD target and error q_current &lt;- q_hat(s, a, theta) q_next &lt;- if (s_prime == terminal_state) { 0 } else { max(sapply(1:n_actions, function(a_) q_hat(s_prime, a_, theta))) } target &lt;- r + gamma * q_next error &lt;- target - q_current # Gradient update x &lt;- create_features(s, a, n_states, n_actions) theta &lt;- theta + alpha * error * x if (s_prime == terminal_state) break s &lt;- s_prime } rewards[ep] &lt;- episode_reward } # Derive policy policy &lt;- sapply(1:n_states, function(s) { if (s == terminal_state) { NA } else { which.max(sapply(1:n_actions, function(a) q_hat(s, a, theta))) } }) list(theta = theta, policy = policy, rewards = rewards) } Algorithm details: Initialization: New theta vector (zeros) and rewards vector to store total reward per episode. Episode start: s &lt;- sample(1:(n_states - 1), 1) picks a random non-terminal start state. Safety mechanism: max_steps &lt;- 1000 with step_count counter prevents infinite loops in case of implementation errors. Action selection (epsilon-greedy): With probability epsilon = 0.1, pick a random action (exploration). Otherwise compute Q-values q_hat(s,a,theta) for all actions and pick argmax_a Q(s,a) (exploitation). Note: which.max() returns the first maximum if there’s a tie. Environment interaction: Call sample_env(s,a) to get next state and reward. TD target and error: q_current = Q̂(s,a; θ) q_next = 0 if terminal, else max_a' Q̂(s',a'; θ) target = r + γ · q_next error = target - q_current (TD error) Semi-gradient update: For linear Q with features x, the gradient ∇_θ Q̂(s,a) = x. Update: θ ← θ + α · error · x Because x is one-hot, only one entry of θ changes, making this equivalent to tabular Q-learning: θ[s,a] ← θ[s,a] + α(r + γ max_a&#39; Q(s&#39;,a&#39;) - Q(s,a)) Episode termination: Break if s_prime == terminal_state; otherwise continue from s &lt;- s_prime. Record episode reward: Store total episode reward in rewards[ep]. Policy extraction: After all episodes, derive greedy policy from final θ: for each non-terminal state, select argmax_a Q̂(s,a; θ). Terminal state gets NA. Running the algorithm set.seed(42) fa_result &lt;- q_learning_fa(episodes = 1000, alpha = 0.1, epsilon = 0.1) fa_policy &lt;- fa_result$policy fa_rewards &lt;- fa_result$rewards Trains for 1000 episodes using learning rate alpha=0.1 and exploration rate epsilon=0.1. Returns learned policy, final theta weights, and episode rewards. Visualization library(ggplot2) policy_df &lt;- data.frame( State = 1:n_states, Policy = fa_policy, Algorithm = &quot;Q-Learning FA&quot; ) policy_plot &lt;- ggplot(policy_df[!is.na(policy_df$Policy), ], aes(x = State, y = Policy)) + geom_point(size = 3, color = &quot;deepskyblue&quot;) + geom_line(color = &quot;deepskyblue&quot;) + theme_minimal() + labs(title = &quot;Policy from Q-Learning with Function Approximation&quot;, x = &quot;State&quot;, y = &quot;Action&quot;) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;)) + theme(legend.position = &quot;none&quot;) reward_df &lt;- data.frame( Episode = 1:1000, CumulativeReward = cumsum(fa_rewards), Algorithm = &quot;Q-Learning FA&quot; ) reward_plot &lt;- ggplot(reward_df, aes(x = Episode, y = CumulativeReward)) + geom_line(color = &quot;deepskyblue&quot;, linewidth = 1) + theme_minimal() + labs(title = &quot;Cumulative Rewards Over Episodes&quot;, x = &quot;Episode&quot;, y = &quot;Cumulative Reward&quot;) print(policy_plot) print(reward_plot) Policy plot: Uses policy_df[!is.na(policy_df$Policy), ] to filter out the terminal state NA value before plotting. Prevents ggplot2 warnings about missing values. Shows the greedy action (1 or 2) for each non-terminal state as points connected by lines. Reward plot : Creates and displays cumulative rewards over episodes. Shows learning progress: cumulative reward should increase as the agent learns better policies. Both plots are explicitly printed with print(). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
