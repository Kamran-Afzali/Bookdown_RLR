[["index.html", "Reinforcement Learning in R Chapter 1 Understanding Reinforcement Learning: From Bandits to Policy Optimization 1.1 Introduction to Reinforcement Learning 1.2 The Multi-Armed Bandit: The Simplest Case 1.3 Transition to Markov Decision Processes 1.4 Comparing Reinforcement Learning Methods 1.5 Conclusion and Further Directions 1.6 References", " Reinforcement Learning in R Kamran Afzalui 2025-07-04 Chapter 1 Understanding Reinforcement Learning: From Bandits to Policy Optimization 1.1 Introduction to Reinforcement Learning Reinforcement Learning (RL) is a dynamic subfield of artificial intelligence concerned with how agents ought to take actions in an environment to maximize cumulative reward. It is inspired by behavioral psychology and decision theory, involving a learning paradigm where feedback from the environment is neither supervised (as in classification tasks) nor completely unsupervised, but rather in the form of scalar rewards. The foundational concepts of RL can be understood by progressing from simple problems, such as the multi-armed bandit, to more sophisticated frameworks like Markov Decision Processes (MDPs) and their many solution methods. 1.2 The Multi-Armed Bandit: The Simplest Case The journey begins with the multi-armed bandit problem, a classic formulation that captures the essence of the exploration-exploitation dilemma. In this setting, an agent must choose among several actions (arms), each yielding stochastic rewards from an unknown distribution. There are no state transitions or temporal dependencies—just the immediate outcome of the chosen action. The objective is to maximize the expected reward over time. Despite its simplicity, the bandit framework introduces crucial ideas such as reward estimation, uncertainty, and exploration strategies. Algorithms like ε-greedy methods introduce random exploration, while Upper Confidence Bound (UCB) techniques adjust choices based on uncertainty estimates. Thompson Sampling applies Bayesian reasoning to balance exploration and exploitation. Though limited in scope, these strategies establish foundational principles that generalize to more complex environments. In bandits, action selection strategies serve a role similar to policies in full RL problems, but without dependence on state transitions. 1.3 Transition to Markov Decision Processes The limitations of bandit models become evident when we consider sequential decision-making problems where actions influence future states and rewards. This leads to the formalism of Markov Decision Processes (MDPs), which model environments through states \\(S\\), actions \\(A\\), transition probabilities \\(P(s&#39;|s, a)\\), rewards \\(R(s, a)\\), and a discount factor \\(\\gamma \\in [0, 1]\\). The Markov property assumes that the future is independent of the past given the present state, simplifying the dynamics and enabling tractable analysis. The agent’s goal is to learn an optimal policy \\(\\pi^*(s)\\) that maximizes the expected cumulative return: \\(G_t = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\right]\\) This process relies on value functions such as: \\(V^\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t | S_t = s \\right], \\quad Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ G_t | S_t = s, A_t = a \\right]\\) Solving MDPs involves either learning these functions or directly learning \\(\\pi\\). 1.4 Comparing Reinforcement Learning Methods To frame this spectrum of approaches clearly, the table below summarizes key RL method families, highlighting whether they rely on an explicit model of the environment, whether they learn policies, value functions, or both, and notes on convergence and sample efficiency. Method Type Uses Model? Learns Policy? Learns Value? Sample Efficient? Converges to Optimal? Suitable For Example Algorithms Multi-Armed Bandits No Action Rule No Yes Yes (in expectation) Stateless problems ε-Greedy, UCB, Thompson Dynamic Programming Yes Yes Yes No Yes Known model Value Iteration, Policy Iteration Monte Carlo No Yes Yes No Yes (episodic) Episodic tasks MC Prediction, MC Control TD Learning No Yes Yes Moderate Yes Ongoing tasks SARSA, Q-Learning Q-Learning + Function Approx. No Yes Yes Moderate Not always (non-linear) High-dimensional spaces DQN Policy Gradient No Yes Maybe Low Local Optimum Continuous action spaces REINFORCE, PPO, A2C Actor-Critic No Yes Yes Moderate Local Optimum Most RL settings A2C, DDPG, SAC Model-Based RL Yes (learned) Yes Yes High Not guaranteed Data-limited tasks Dreamer, MBPO, MuZero This classification illustrates the diversity of RL approaches and underscores the flexibility of the RL paradigm. Some methods assume access to a perfect model, while others learn entirely from data. Some directly optimize policies, while others estimate values to guide policy improvement. 1.4.1 Dynamic Programming: Model-Based Learning Dynamic Programming (DP) methods such as Value Iteration and Policy Iteration assume complete knowledge of the environment’s dynamics. These algorithms exploit the Bellman equations to iteratively compute optimal value functions and policies. The Bellman optimality equation is given by: \\(V^*(s) = \\max_a \\sum_{s&#39;} P(s&#39;|s,a) [R(s,a) + \\gamma V^*(s&#39;)]\\) Value Iteration applies this update directly, while Policy Iteration alternates between evaluating the current policy and improving it by acting greedily with respect to the value function. Although DP methods guarantee convergence to the optimal policy, they are rarely applicable to real-world problems due to their assumption of known transitions and the computational infeasibility of operating over large or continuous state spaces. 1.4.2 Model-Free Approaches: Monte Carlo and TD Learning When the model is unknown, we turn to model-free methods that learn directly from sampled experience. Monte Carlo methods estimate the value of policies by averaging the total return over multiple episodes. These methods are simple and intuitive, suitable for episodic environments, but suffer from high variance and are not efficient in online learning scenarios. Temporal Difference (TD) learning bridges the gap between Monte Carlo and DP by updating value estimates based on partial returns. Algorithms like SARSA and Q-learning fall into this category. SARSA is on-policy, updating values based on the actual trajectory taken, while Q-learning is off-policy, learning about the greedy policy regardless of the agent’s current behavior. These methods do not require waiting until the end of an episode and are thus applicable in ongoing tasks. They offer a tradeoff between bias and variance in value estimation. 1.4.3 Q-Learning and Function Approximation Q-learning is one of the most widely used RL algorithms due to its simplicity and theoretical guarantees. However, when dealing with large state-action spaces, tabular Q-learning becomes infeasible. Function approximation, particularly using neural networks, allows Q-learning to scale to high-dimensional problems. This gave rise to Deep Q-Networks (DQNs), where a neural network is trained to approximate the Q-function. DQNs introduced mechanisms like experience replay—storing and reusing past interactions to reduce correlation between updates—and target networks—fixed Q-value targets updated slowly to stabilize learning. These enhancements enabled RL to tackle complex environments like Atari games directly from pixels. Nonetheless, deep RL methods often suffer from sample inefficiency and training instability, especially when generalizing to new environments. 1.4.4 Policy Gradient and Actor-Critic Methods While value-based methods derive policies from value functions, policy-based methods directly parameterize and optimize the policy itself. Policy Gradient methods compute the gradient of expected return with respect to policy parameters and perform gradient ascent. The REINFORCE algorithm is the archetype of this approach, but it often suffers from high variance in gradient estimates. The Policy Gradient Theorem provides the theoretical foundation: \\(\\nabla J(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^{\\pi}(s,a)]\\) To address variance, Actor-Critic methods introduce a second component: the critic, which estimates value functions to inform and stabilize the updates of the actor (policy). Algorithms like Advantage Actor-Critic (A2C), Deep Deterministic Policy Gradient (DDPG), and Soft Actor-Critic (SAC) build on this architecture, each adding unique elements to improve performance and stability. 1.4.5 Advanced Policy Optimization Techniques More recent advances in policy optimization have focused on improving training stability and sample efficiency. Trust Region Policy Optimization (TRPO) constrains policy updates to stay within a trust region defined by the KL divergence, ensuring small, safe steps in parameter space. Proximal Policy Optimization (PPO) simplifies TRPO with a clipped objective function, striking a balance between ease of implementation and empirical performance. Soft Actor-Critic (SAC), on the other hand, incorporates an entropy maximization objective, encouraging exploration by maintaining a degree of randomness in the policy. This leads to better performance in environments with sparse or deceptive rewards and is particularly effective in continuous control tasks. Model-based approaches, such as MuZero or Dreamer, offer a complementary strategy: by learning a model of the environment dynamics and reward structure, they can generate synthetic experiences to improve sample efficiency. However, model inaccuracies can lead to cascading errors and suboptimal policies. 1.5 Conclusion and Further Directions Reinforcement Learning has evolved into a mature and diverse field, offering a rich set of tools for decision-making under uncertainty. From simple bandit strategies to deep policy optimization and model-based reasoning, RL provides a versatile framework for learning from interaction. However, key challenges remain: training instability, sample inefficiency, reward mis-specification, and generalization across tasks. These remain active areas of research. A solid understanding of the main categories—such as those outlined in the comparative table—is essential for navigating the RL landscape. Whether one is interested in theoretical foundations, algorithm development, or practical deployment, the key ideas of exploration, value estimation, policy optimization, and model learning form the backbone of modern RL. For further reading, Reinforcement Learning: An Introduction by Sutton and Barto remains the canonical text. Online resources like OpenAI’s Spinning Up, DeepMind’s technical blog, and repositories of papers on arXiv are excellent for staying current with the latest advancements. As artificial agents continue to tackle more complex and dynamic environments, reinforcement learning stands at the forefront of AI research and application. 1.6 References Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). MIT Press. http://incompleteideas.net/book/the-book-2nd.html Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236 Schulman, J., Levine, S., Abbeel, P., Jordan, M., &amp; Moritz, P. (2015). Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (pp. 1889–1897). https://proceedings.mlr.press/v37/schulman15.html Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. https://arxiv.org/abs/1707.06347 Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning. https://arxiv.org/abs/1801.01290 Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., … &amp; Hassabis, D. (2020). Mastering the game of Go without human knowledge. Nature, 550(7676), 354–359. https://doi.org/10.1038/nature24270 Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., … &amp; Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419), 1140–1144. https://doi.org/10.1126/science.aar6404 OpenAI. (2018). Spinning Up in Deep RL. https://spinningup.openai.com Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., … &amp; Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971. https://arxiv.org/abs/1509.02971 Auer, P., Cesa-Bianchi, N., &amp; Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47, 235–256. https://doi.org/10.1023/A:1013689704352 Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4), 285–294. https://doi.org/10.2307/2332286 "],["the-multi-armed-bandit-problem.html", "Chapter 2 The Multi-Armed Bandit Problem 2.1 Introduction 2.2 Mathematical Formalism 2.3 Frequentist Approach: UCB1 Algorithm 2.4 Bayesian Approach: Thompson Sampling 2.5 Epsilon-Greedy Strategy 2.6 Summary Table 2.7 Conclusion", " Chapter 2 The Multi-Armed Bandit Problem 2.1 Introduction The multi-armed bandit (MAB) problem is a foundational model in the study of sequential decision-making under uncertainty. Representing the trade-off between exploration (gathering information) and exploitation (maximizing known rewards), MAB problems are central to reinforcement learning, online optimization, and adaptive experimental design. An agent is faced with a choice among multiple options—arms—each producing stochastic rewards with unknown distributions. The objective is to maximize cumulative reward, or equivalently, to minimize the regret incurred by not always choosing the best arm. This post presents a rigorous treatment of the MAB problem, comparing frequentist and Bayesian approaches. We offer formal mathematical foundations, develop regret bounds, and implement both Upper Confidence Bound (UCB) and Thompson Sampling algorithms in R. A summary table is provided at the end. 2.2 Mathematical Formalism Let \\(K\\) denote the number of arms, and each arm \\(k \\in \\{1, \\dots, K\\}\\) has an unknown reward distribution \\(P_k\\), with mean \\(\\mu_k\\). Define the optimal arm: \\[ k^* = \\arg\\max_{k} \\mu_k. \\] At each time \\(t \\in \\{1, \\dots, T\\}\\), the agent chooses arm \\(A_t \\in \\{1, \\dots, K\\}\\) and receives a stochastic reward \\(R_t \\sim P_{A_t}\\). The cumulative expected regret is: \\[ \\mathcal{R}(T) = T\\mu^* - \\mathbb{E}\\left[ \\sum_{t=1}^T R_t \\right] = \\sum_{k=1}^K \\Delta_k \\, \\mathbb{E}[N_k(T)], \\] where \\(\\Delta_k = \\mu^* - \\mu_k\\) and \\(N_k(T)\\) is the number of times arm \\(k\\) was played. 2.3 Frequentist Approach: UCB1 Algorithm Frequentist methods estimate expected rewards using empirical means. The UCB1 algorithm, based on Hoeffding’s inequality, constructs an upper confidence bound: \\[ A_t = \\arg\\max_{k} \\left[ \\hat{\\mu}_{k,t} + \\sqrt{ \\frac{2 \\log t}{N_k(t)} } \\right]. \\] This ensures logarithmic regret in expectation. 2.3.1 R Code for UCB1 set.seed(42) K &lt;- 3 T &lt;- 1000 mu &lt;- c(0.3, 0.5, 0.7) # true means counts &lt;- rep(0, K) values &lt;- rep(0, K) regret &lt;- numeric(T) # Play each arm once for (k in 1:K) { reward &lt;- rbinom(1, 1, mu[k]) counts[k] &lt;- 1 values[k] &lt;- reward regret[k] &lt;- max(mu) - mu[k] } for (t in (K+1):T) { ucb &lt;- values + sqrt(2 * log(t) / counts) a &lt;- which.max(ucb) reward &lt;- rbinom(1, 1, mu[a]) counts[a] &lt;- counts[a] + 1 values[a] &lt;- values[a] + (reward - values[a]) / counts[a] regret[t] &lt;- max(mu) - mu[a] } cum_regret_ucb &lt;- cumsum(regret) plot(cum_regret_ucb, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, ylab = &quot;Cumulative Regret&quot;, xlab = &quot;Time&quot;, main = &quot;UCB1 Regret&quot;) 2.4 Bayesian Approach: Thompson Sampling Bayesian bandits model reward distributions probabilistically, updating beliefs via Bayes’ rule. For Bernoulli rewards, we assume Beta priors: \\[ \\mu_k \\sim \\text{Beta}(\\alpha_k, \\beta_k). \\] After observing a reward \\(r \\in \\{0, 1\\}\\), the posterior update is: \\[ \\alpha_k \\leftarrow \\alpha_k + r, \\quad \\beta_k \\leftarrow \\beta_k + 1 - r. \\] The Thompson Sampling algorithm draws a sample \\(\\tilde{\\mu}_k \\sim \\text{Beta}(\\alpha_k, \\beta_k)\\) and selects the arm with the highest sample. 2.4.1 R Code for Thompson Sampling set.seed(42) alpha &lt;- rep(1, K) beta &lt;- rep(1, K) regret &lt;- numeric(T) for (t in 1:T) { sampled_means &lt;- rbeta(K, alpha, beta) a &lt;- which.max(sampled_means) reward &lt;- rbinom(1, 1, mu[a]) alpha[a] &lt;- alpha[a] + reward beta[a] &lt;- beta[a] + (1 - reward) regret[t] &lt;- max(mu) - mu[a] } cum_regret_ts &lt;- cumsum(regret) lines(cum_regret_ts, col = &quot;red&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;UCB1&quot;, &quot;Thompson Sampling&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 2) The UCB1 algorithm guarantees a regret bound of: \\[ \\mathcal{R}(T) \\leq \\sum_{k: \\Delta_k &gt; 0} \\left( \\frac{8 \\log T}{\\Delta_k} + C_k \\right), \\] where \\(C_k\\) is a problem-dependent constant. Thompson Sampling achieves comparable performance. Under certain regularity conditions, its Bayesian regret is bounded by: \\[ \\mathbb{E}[\\mathcal{R}(T)] = O\\left( \\sqrt{KT \\log T} \\right), \\] and often outperforms UCB1 in practice due to its adaptive exploration. 2.5 Epsilon-Greedy Strategy The epsilon-greedy algorithm is a simple and intuitive approach to balancing exploration and exploitation. At each time step, with probability \\(\\epsilon\\), the agent chooses a random arm (exploration), and with probability \\(1 - \\epsilon\\), it selects the arm with the highest empirical mean (exploitation). Let \\(\\hat{\\mu}_{k,t}\\) denote the empirical mean reward for arm \\(k\\) at time \\(t\\). Then: \\[ A_t = \\begin{cases} \\text{random choice} &amp; \\text{with probability } \\epsilon, \\\\ \\arg\\max_k \\hat{\\mu}_{k,t} &amp; \\text{with probability } 1 - \\epsilon. \\end{cases} \\] While this algorithm is not optimal in the theoretical sense, it often performs well in practice for problems with stationary reward distributions when the exploration rate \\(\\epsilon\\) is properly tuned. Regret under a fixed \\(\\epsilon\\) is linear in \\(T\\), i.e., \\(\\mathcal{R}(T) = O(T)\\), unless \\(\\epsilon\\) is decayed over time (e.g., \\(\\epsilon_t = 1/t\\)), which introduces a trade-off between convergence speed and variance. 2.5.1 R Code for Epsilon-Greedy set.seed(42) epsilon &lt;- 0.1 counts &lt;- rep(0, K) values &lt;- rep(0, K) regret &lt;- numeric(T) for (t in 1:T) { if (runif(1) &lt; epsilon) { a &lt;- sample(1:K, 1) # Exploration } else { a &lt;- which.max(values) # Exploitation } reward &lt;- rbinom(1, 1, mu[a]) counts[a] &lt;- counts[a] + 1 values[a] &lt;- values[a] + (reward - values[a]) / counts[a] regret[t] &lt;- max(mu) - mu[a] } cum_regret_eps &lt;- cumsum(regret) lines(cum_regret_eps, col = &quot;darkgreen&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;UCB1&quot;, &quot;Thompson Sampling&quot;, &quot;Epsilon-Greedy&quot;), col = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;), lwd = 2) 2.6 Summary Table Method Paradigm Assumptions Exploration Mechanism Regret Bound Strengths Weaknesses UCB1 Frequentist Stationary, bounded rewards Upper Confidence Bound \\(O(\\log T)\\) Simple, provable guarantees Conservative, suboptimal in practice Thompson Sampling Bayesian Prior over reward distributions Posterior sampling \\(O(\\sqrt{KT})\\), empirically better Adaptive, efficient with good priors Sensitive to prior misspecification KL-UCB Frequentist Known reward distributions KL-divergence bounds \\(O(\\log T)\\) (tighter) Distribution-aware More complex implementation Epsilon-Greedy Heuristic None Random exploration \\(O(T)\\) if \\(\\epsilon\\) fixed Very simple Inefficient long-term 2.7 Conclusion The multi-armed bandit problem remains an essential model for studying decision-making under uncertainty. While frequentist methods like UCB1 provide rigorous guarantees and conceptual clarity, Bayesian approaches like Thompson Sampling offer greater flexibility and empirical performance. The choice between them hinges on the trade-offs between interpretability, adaptivity, and prior knowledge. The R implementations provided here allow for practical experimentation and benchmarking. In real-world applications, such as clinical trial design, online recommendations, and adaptive A/B testing, these algorithms offer principled foundations for learning and acting in uncertain environments. "],["markov-decision-processes-and-dynamic-programming.html", "Chapter 3 Markov Decision Processes and Dynamic Programming 3.1 Introduction 3.2 Constructing the MDP in R 3.3 Value Iteration Algorithm 3.4 Evaluation and Interpretation 3.5 Theoretical Properties of Value Iteration 3.6 Summary Table 3.7 Conclusion", " Chapter 3 Markov Decision Processes and Dynamic Programming 3.1 Introduction Markov Decision Processes (MDPs) constitute a formal framework for modeling sequential decision-making under uncertainty. Widely applied in operations research, control theory, economics, and artificial intelligence, MDPs encapsulate the dynamics of environments where outcomes are partly stochastic and partly under an agent’s control. At their core, MDPs unify probabilistic transitions, state-contingent rewards, and long-term optimization goals. This post explores MDPs from a computational standpoint, emphasizing Dynamic Programming (DP) methods—particularly Value Iteration—for solving them when the model is fully known. We proceed by defining the mathematical components of MDPs, implementing them in R, and illustrating policy derivation using value iteration. A comparative summary of key aspects of DP methods concludes the post. An MDP is formally defined by the tuple \\((S, A, P, R, \\gamma)\\), where: \\(S\\): a finite set of states. \\(A\\): a finite set of actions. \\(P(s&#39;|s, a)\\): the transition probability function—probability of moving to state \\(s&#39;\\) given current state \\(s\\) and action \\(a\\). \\(R(s, a, s&#39;)\\): the reward received after transitioning from state \\(s\\) to \\(s&#39;\\) via action \\(a\\). \\(\\gamma \\in [0,1]\\): the discount factor, representing the agent’s preference for immediate versus delayed rewards. The agent’s objective is to learn a policy \\(\\pi: S \\to A\\) that maximizes the expected cumulative discounted reward: \\[ V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(S_t, A_t, S_{t+1}) \\,\\bigg|\\, S_0 = s \\right] \\] The optimal value function \\(V^*\\) satisfies the Bellman optimality equation: $$ V^*(s) = {a A} {s’ S} P(s’|s,a) $$ Once \\(V^*\\) is computed, the optimal policy \\(\\pi^*\\) is obtained via: $$ ^*(s) = {a A} {s’} P(s’|s,a) $$ 3.2 Constructing the MDP in R We now implement an environment with 10 states and 2 actions per state, following stochastic transition and reward dynamics. The final state is absorbing, with no further transitions or rewards. n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 This setup specifies an MDP with stochastic transitions favoring forward progression and asymmetric reward structures across actions. State 10 is a terminal state. We define a function to sample from the environment: sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } 3.3 Value Iteration Algorithm Value Iteration is a fundamental DP method for computing the optimal value function and deriving the optimal policy. It exploits the Bellman optimality equation through successive approximation. value_iteration &lt;- function() { V &lt;- rep(0, n_states) policy &lt;- rep(1, n_states) theta &lt;- 1e-4 repeat { delta &lt;- 0 for (s in 1:(n_states - 1)) { v &lt;- V[s] q_values &lt;- numeric(n_actions) for (a in 1:n_actions) { for (s_prime in 1:n_states) { q_values[a] &lt;- q_values[a] + transition_model[s, a, s_prime] * (reward_model[s, a, s_prime] + gamma * V[s_prime]) } } V[s] &lt;- max(q_values) policy[s] &lt;- which.max(q_values) delta &lt;- max(delta, abs(v - V[s])) } if (delta &lt; theta) break } list(V = V, policy = policy) } This implementation iteratively updates value estimates until convergence, determined by a small threshold \\(\\theta\\). At each iteration, the value of state \\(s\\) is updated using the maximum expected return across available actions. We now apply the algorithm and extract the resulting value function and policy: dp_result &lt;- value_iteration() dp_value &lt;- dp_result$V dp_policy &lt;- dp_result$policy dp_policy The returned policy is a vector of optimal actions indexed by state. The value function can be visualized to reveal which states yield higher expected returns under the optimal policy. 3.4 Evaluation and Interpretation The policy and value function obtained via value iteration provide complete guidance for optimal behavior in the modeled environment. In this setting: The forward action (Action 1) is generally rewarded with higher long-term return due to its tendency to reach the terminal (high-reward) state. The second action (Action 2) introduces randomness and lower rewards, thus is optimal only in specific states where it offers a better expected value. Such interpretation emphasizes the Bellman principle of optimality: every sub-policy of an optimal policy must itself be optimal for the corresponding subproblem. We can visualize the value function: plot(dp_value, type = &quot;b&quot;, col = &quot;blue&quot;, pch = 19, xlab = &quot;State&quot;, ylab = &quot;Value&quot;, main = &quot;Optimal State Values via Value Iteration&quot;) 3.5 Theoretical Properties of Value Iteration Value Iteration exhibits the following theoretical guarantees: Convergence: The algorithm is guaranteed to converge to \\(V^*\\) for any finite MDP with bounded rewards and \\(0 \\leq \\gamma &lt; 1\\). Policy Derivation: Once \\(V^*\\) is known, the greedy policy is optimal. Computational Complexity: For state space size \\(S\\) and action space size \\(A\\), each iteration requires \\(O(S^2 A)\\) operations due to the summation over all successor states. However, in practice, the applicability of DP methods is restricted by: The need for full knowledge of \\(P\\) and \\(R\\), Infeasibility in large or continuous state spaces (the “curse of dimensionality”). These challenges motivate the use of approximation, sampling-based methods, and model-free approaches in reinforcement learning contexts. 3.6 Summary Table The following table summarizes the key elements and tradeoffs of the value iteration algorithm: Property Value Iteration (DP) Model Required Yes (transition probabilities and rewards) State Representation Tabular (explicit state-value storage) Action Selection Greedy w.r.t. value estimates Convergence Guarantee Yes (under finite \\(S, A\\), \\(\\gamma &lt; 1\\)) Sample Efficiency High (uses full model, no sampling error) Scalability Poor in large or continuous state spaces Output Optimal value function and policy Computational Complexity \\(O(S^2 A)\\) per iteration 3.7 Conclusion Dynamic Programming offers elegant and theoretically grounded algorithms for solving MDPs when the environment model is fully specified. Value Iteration, as illustrated, leverages the recursive Bellman optimality equation to iteratively compute the value function and derive the optimal policy. While its practical scope is limited by scalability and model assumptions, DP remains foundational in the study of decision processes. Understanding these principles is essential for extending to model-free reinforcement learning, function approximation, and policy gradient methods. Future posts in this series will explore Temporal Difference learning, Monte Carlo methods, and the transition to policy optimization. These approaches lift the strict requirements of model knowledge and allow learning from interaction, thereby opening the door to real-world applications. "],["model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html", "Chapter 4 Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R 4.1 Introduction 4.2 Theoretical Background 4.3 Interpretation and Discussion 4.4 Conclusion", " Chapter 4 Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R 4.1 Introduction In many real-world decision-making problems, the environment model—comprising transition probabilities and reward functions—is unknown or too complex to specify explicitly. Model-free reinforcement learning (RL) methods address this by learning optimal policies directly from experience or sample trajectories, without requiring full knowledge of the underlying Markov Decision Process (MDP). Two foundational model-free methods are Temporal Difference (TD) learning and Monte Carlo (MC) methods. TD learning updates value estimates online based on one-step lookahead and bootstrapping, while MC methods learn from complete episodes by averaging returns. This post presents these approaches with mathematical intuition, R implementations, and an illustrative environment. We also compare how learned policies adapt—or fail to adapt—when rewards are changed after training, illuminating the distinction between goal-directed and habitual learning. 4.2 Theoretical Background Suppose an agent interacts with an environment defined by states \\(S\\), actions \\(A\\), a discount factor \\(\\gamma \\in [0,1]\\), and an unknown MDP dynamics. The goal is to learn the action-value function \\(Q^\\pi(s,a)\\), the expected discounted return starting from state \\(s\\), taking action \\(a\\), and following policy \\(\\pi\\) thereafter: \\[ Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a \\right] \\] 4.2.1 Temporal Difference Learning (Q-Learning) Q-Learning is an off-policy TD control method that updates the estimate \\(Q(s,a)\\) incrementally after each transition: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right) \\] where \\(\\alpha\\) is the learning rate, \\(r\\) the reward received, and \\(s&#39;\\) the next state. This update uses the current estimate of \\(Q\\) at \\(s&#39;\\) (bootstrapping). 4.2.2 Monte Carlo Methods Monte Carlo methods learn \\(Q\\) by averaging returns observed after visiting \\((s,a)\\). Every-visit MC estimates \\(Q\\) by averaging the total discounted return \\(G_t\\) following each occurrence of \\((s,a)\\) within complete episodes: \\[ Q(s,a) \\approx \\frac{1}{N(s,a)} \\sum_{i=1}^{N(s,a)} G_t^{(i)} \\] where \\(N(s,a)\\) is the count of visits to \\((s,a)\\) and \\(G_t^{(i)}\\) is the return following the \\(i\\)-th visit. 4.2.3 Step 1: Defining the Environment in R We use a 10-state, 2-action environment with stochastic transitions and rewards, as in previous work: # Common settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition + reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } 4.2.4 Step 2: Q-Learning Implementation in R The Q-Learning function runs episodes, selects actions using \\(\\epsilon\\)-greedy policy, updates Q-values using the TD rule, and outputs a policy by greedy action selection: q_learning &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) for (ep in 1:episodes) { s &lt;- 1 while (TRUE) { a &lt;- if (runif(1) &lt; epsilon) sample(1:n_actions, 1) else which.max(Q[s, ]) out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime reward &lt;- out$reward Q[s, a] &lt;- Q[s, a] + alpha * (reward + gamma * max(Q[s_prime, ]) - Q[s, a]) if (s_prime == n_states) break s &lt;- s_prime } } apply(Q, 1, which.max) } Running this yields the learned policy: ql_policy_before &lt;- q_learning() 4.2.5 Step 3: Monte Carlo Every-Visit Implementation This MC method generates episodes, stores the full sequence of state-action-reward tuples, and updates \\(Q\\) by averaging discounted returns for every visit of \\((s,a)\\): monte_carlo &lt;- function(episodes = 1000, epsilon = 0.1) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) returns &lt;- vector(&quot;list&quot;, n_states * n_actions) names(returns) &lt;- paste(rep(1:n_states, each = n_actions), rep(1:n_actions, n_states), sep = &quot;_&quot;) for (ep in 1:episodes) { episode &lt;- list() s &lt;- 1 while (TRUE) { a &lt;- if (runif(1) &lt; epsilon) sample(1:n_actions, 1) else which.max(Q[s, ]) out &lt;- sample_env(s, a) episode[[length(episode) + 1]] &lt;- list(state = s, action = a, reward = out$reward) if (out$s_prime == n_states) break s &lt;- out$s_prime } G &lt;- 0 for (t in length(episode):1) { s &lt;- episode[[t]]$state a &lt;- episode[[t]]$action r &lt;- episode[[t]]$reward G &lt;- gamma * G + r key &lt;- paste(s, a, sep = &quot;_&quot;) returns[[key]] &lt;- c(returns[[key]], G) Q[s, a] &lt;- mean(returns[[key]]) } } apply(Q, 1, which.max) } The resulting policy is computed by: mc_policy_before &lt;- monte_carlo() 4.2.6 Step 4: Simulating Outcome Devaluation We now alter the environment by removing the reward for reaching the terminal state, simulating a devaluation: # Devalue terminal reward for (s in 1:(n_states - 1)) { reward_model[s, 1, n_states] &lt;- 0 reward_model[s, 2, n_states] &lt;- 0 } 4.2.7 Step 5: Comparing Policies Before and After Devaluation We compare policies derived via: Dynamic Programming (DP) which has full model knowledge and updates instantly after reward changes, Q-Learning and Monte Carlo which keep their previously learned policies (habitual behavior without retraining). # DP recomputes policy based on updated reward model dp_policy_after &lt;- value_iteration()$policy # Q-learning and MC keep previous policy (habitual) ql_policy_after &lt;- ql_policy_before mc_policy_after &lt;- mc_policy_before 4.2.8 Step 6: Visualizing the Policies plot_policy &lt;- function(policy, label, col = &quot;skyblue&quot;) { barplot(policy, names.arg = 1:n_states, col = col, ylim = c(0, 3), ylab = &quot;Action (1=A1, 2=A2)&quot;, main = label) abline(h = 1.5, lty = 2, col = &quot;gray&quot;) } par(mfrow = c(3, 2)) plot_policy(dp_policy_before, &quot;DP Policy Before&quot;) plot_policy(dp_policy_after, &quot;DP Policy After&quot;, &quot;lightgreen&quot;) plot_policy(ql_policy_before, &quot;Q-Learning Policy Before&quot;) plot_policy(ql_policy_after, &quot;Q-Learning Policy After&quot;, &quot;orange&quot;) plot_policy(mc_policy_before, &quot;MC Policy Before&quot;) plot_policy(mc_policy_after, &quot;MC Policy After&quot;, &quot;orchid&quot;) 4.3 Interpretation and Discussion Dynamic Programming adapts immediately after devaluation because it recalculates the policy using the updated reward model. In contrast, Q-Learning and Monte Carlo methods, which are model-free and learn from past experience, maintain their prior policies unless explicitly retrained. This reflects habitual behavior — a policy learned from experience that does not flexibly adjust to changed outcomes without further learning. This illustrates a core difference: Model-based methods (like DP) support goal-directed behavior, recomputing optimal decisions as the environment changes. Model-free methods (like Q-Learning and MC) support habitual behavior, relying on cached values learned from past rewards. 4.4 Conclusion Temporal Difference and Monte Carlo methods provide powerful approaches to reinforcement learning when the environment is unknown. TD learning’s bootstrapping allows online updates after each transition, while Monte Carlo averages full returns after complete episodes. Both learn policies from experience rather than models. Future posts will explore extensions including function approximation and policy gradient methods. Aspect Monte Carlo (MC) Temporal Difference (Q-Learning) Learning Approach Learns from complete episodes by averaging returns after each episode. Learns incrementally after each state-action transition using bootstrapping. Update Rule Updates Q-value as the mean of observed returns: \\(Q(s,a) \\approx \\frac{1}{N(s,a)} \\sum_{i=1}^{N(s,a)} G_t^{(i)}\\) Updates Q-value using TD error: \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right)\\) Episode Requirement Requires complete episodes to compute returns (\\(G_t\\)). Does not require complete episodes; updates online after each step. Bias and Variance Unbiased estimate of Q-value, but high variance due to full episode returns. Biased due to bootstrapping (relies on current Q estimates), but lower variance. Policy Type Typically on-policy (e.g., with ε-greedy exploration), but can be adapted for off-policy. Off-policy; learns optimal policy regardless of exploration policy. Computational Efficiency Less efficient; must wait for episode completion before updating. More efficient; updates Q-values immediately after each transition. Adaptation to Change Slow to adapt to environment changes without retraining, as it relies on past episode returns. Slow to adapt without retraining, but incremental updates allow faster response to changes. Implementation in Code Stores state-action-reward sequences, computes discounted returns backward. Updates Q-values online using immediate reward and next state’s Q-value. Example in Provided Code Every-visit MC: averages returns for each \\((s,a)\\) visit in an episode. Q-Learning: updates Q-values after each transition using TD rule. "],["on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html", "Chapter 5 On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R 5.1 Introduction 5.2 SARSA (On-Policy) 5.3 Q-Learning (Off-Policy) 5.4 Off-Policy Monte Carlo with Importance Sampling 5.5 Key Differences 5.6 Interpretation and Discussion 5.7 Conclusion 5.8 Comparison Table", " Chapter 5 On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R 5.1 Introduction In model-free reinforcement learning (RL), agents learn optimal policies directly from experience without a model of the environment’s dynamics. Two key approaches are on-policy and off-policy methods, exemplified by SARSA (State-Action-Reward-State-Action) and Q-Learning, respectively. Additionally, off-policy Monte Carlo methods leverage importance sampling to learn optimal policies from exploratory data. This post explores these methods, focusing on their theoretical foundations, practical implications, and implementation in R. We use a 10-state, 2-action environment to compare how SARSA, Q-Learning, and off-policy Monte Carlo learn policies and adapt to environmental changes, such as outcome devaluation. Mathematical formulations and R code are provided to illustrate the concepts. SARSA, Q-Learning, and off-policy Monte Carlo aim to estimate the action-value function \\(Q^\\pi(s,a)\\), the expected discounted return for taking action \\(a\\) in state \\(s\\) and following policy \\(\\pi\\): \\[ Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a \\right] \\] where \\(\\gamma \\in [0,1]\\) is the discount factor, and \\(R_{t+1}\\) is the reward at time \\(t+1\\). 5.2 SARSA (On-Policy) SARSA is an on-policy method, meaning it learns the value of the policy being followed, including exploration. The update rule is: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma Q(s&#39;, a&#39;) - Q(s,a) \\right) \\] where \\(\\alpha\\) is the learning rate, \\(r\\) is the reward, \\(s&#39;\\) is the next state, and \\(a&#39;\\) is the action actually taken in \\(s&#39;\\) according to the current policy (e.g., \\(\\epsilon\\)-greedy). SARSA updates \\(Q\\) based on the next state-action pair \\((s&#39;, a&#39;)\\), making it sensitive to the exploration policy. In the 10-state environment, SARSA learns a policy that accounts for exploratory actions, potentially avoiding risky moves that lead to lower rewards. 5.3 Q-Learning (Off-Policy) Q-Learning is an off-policy method, meaning it learns the optimal policy \\(\\pi^*\\) regardless of the exploration policy (e.g., \\(\\epsilon\\)-greedy). The update rule is: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right) \\] where \\(\\max_{a&#39;} Q(s&#39;, a&#39;)\\) estimates the value of the next state assuming the optimal action. This bootstrapping makes Q-Learning converge to the optimal action-value function \\(Q^*(s,a)\\). In the 10-state environment, Q-Learning favors actions that maximize future rewards (e.g., action 1 in state 9, yielding a 1.0 reward at the terminal state), ignoring exploration effects. 5.4 Off-Policy Monte Carlo with Importance Sampling Off-policy Monte Carlo uses importance sampling to learn the value of a target policy (e.g., greedy) from episodes generated by a behavior policy (e.g., random). The return \\(G_t\\) (cumulative discounted reward from time \\(t\\) onward) is weighted by the importance sampling ratio: \\[ \\rho_t = \\prod_{k=t}^T \\frac{\\pi(a_k|s_k)}{\\mu(a_k|s_k)} \\] where \\(\\pi\\) is the target policy, \\(\\mu\\) is the behavior policy, and \\(T\\) is the episode length. The Q-value update is: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( \\rho_t G_t - Q(s,a) \\right) \\] Importance Sampling Mechanics: In the 10-state environment, suppose the behavior policy is random (0.5 probability for actions 1 and 2), but the target policy is greedy (choosing the action with the highest Q-value). If the agent in state 9 takes action 2 (reward 0.5 at the terminal state), but the greedy policy prefers action 1 (reward 1.0), the importance sampling ratio \\(\\rho_t\\) is low (e.g., 0 if the greedy policy assigns zero probability to action 2), reducing the update’s impact. This allows learning the optimal policy from exploratory trajectories, but high variance can occur if the policies diverge significantly. Weighted importance sampling (as in the code below) normalizes weights to reduce variance, and early termination (stopping when \\(\\rho_t = 0\\)) improves efficiency. 5.5 Key Differences Aspect SARSA (On-Policy) Q-Learning (Off-Policy) Off-Policy Monte Carlo Update Rule Uses \\(Q(s&#39;, a&#39;)\\), where \\(a&#39;\\) is sampled from the current policy. Uses \\(\\max_{a&#39;} Q(s&#39;, a&#39;)\\), assuming the optimal action. Uses \\(\\rho_t G_t\\), where \\(\\rho_t\\) reweights returns based on policy likelihoods. Policy Learning Learns the value of the policy being followed (including exploration). Learns the optimal policy, independent of exploration. Learns the optimal policy using importance sampling from exploratory trajectories. Exploration Impact Exploration affects learned Q-values. Exploration does not affect learned Q-values. Exploration affects returns, reweighted by importance sampling. Convergence Converges to the policy’s value if exploration decreases (e.g., \\(\\epsilon \\to 0\\)). Converges to the optimal policy even with fixed exploration. Converges to the optimal policy, but variance depends on policy similarity. Behavior More conservative, accounts for exploration risks. More aggressive, assumes optimal future actions. Aggressive, but variance can lead to unstable learning if policies differ significantly. # Common settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Helper function: Epsilon-greedy policy epsilon_greedy &lt;- function(Q, state, epsilon) { if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { which.max(Q[state, ]) } } # Helper function: Simulate environment simulate_step &lt;- function(state, action) { probs &lt;- transition_model[state, action, ] next_state &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[state, action, next_state] list(next_state = next_state, reward = reward) } # SARSA sarsa &lt;- function(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, n_states, n_actions) policy &lt;- rep(0, n_states) rewards &lt;- numeric(n_episodes) for (episode in 1:n_episodes) { state &lt;- sample(1:(n_states - 1), 1) action &lt;- epsilon_greedy(Q, state, epsilon) episode_reward &lt;- 0 while (state != terminal_state) { step &lt;- simulate_step(state, action) next_state &lt;- step$next_state reward &lt;- step$reward next_action &lt;- epsilon_greedy(Q, next_state, epsilon) Q[state, action] &lt;- Q[state, action] + alpha * ( reward + gamma * Q[next_state, next_action] - Q[state, action] ) state &lt;- next_state action &lt;- next_action episode_reward &lt;- episode_reward + reward } rewards[episode] &lt;- episode_reward } policy[1:(n_states - 1)] &lt;- apply(Q[1:(n_states - 1), ], 1, which.max) list(Q = Q, policy = policy, rewards = rewards) } # Q-Learning q_learning &lt;- function(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, n_states, n_actions) policy &lt;- rep(0, n_states) rewards &lt;- numeric(n_episodes) for (episode in 1:n_episodes) { state &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 while (state != terminal_state) { action &lt;- epsilon_greedy(Q, state, epsilon) step &lt;- simulate_step(state, action) next_state &lt;- step$next_state reward &lt;- step$reward Q[state, action] &lt;- Q[state, action] + alpha * ( reward + gamma * max(Q[next_state, ]) - Q[state, action] ) state &lt;- next_state episode_reward &lt;- episode_reward + reward } rewards[episode] &lt;- episode_reward } policy[1:(n_states - 1)] &lt;- apply(Q[1:(n_states - 1), ], 1, which.max) list(Q = Q, policy = policy, rewards = rewards) } # Off-Policy Monte Carlo off_policy_mc &lt;- function(n_episodes = 1000, epsilon = 0.1) { Q &lt;- matrix(0, n_states, n_actions) C &lt;- matrix(0, n_states, n_actions) # Cumulative weights policy &lt;- rep(0, n_states) rewards &lt;- numeric(n_episodes) for (episode in 1:n_episodes) { # Generate episode using behavior policy (epsilon-greedy) states &lt;- numeric(0) actions &lt;- numeric(0) rewards_ep &lt;- numeric(0) state &lt;- sample(1:(n_states - 1), 1) while (state != terminal_state) { action &lt;- sample(1:n_actions, 1) # Behavior policy: random step &lt;- simulate_step(state, action) next_state &lt;- step$next_state reward &lt;- step$reward states &lt;- c(states, state) actions &lt;- c(actions, action) rewards_ep &lt;- c(rewards_ep, reward) state &lt;- next_state } rewards[episode] &lt;- sum(rewards_ep) # Update Q using importance sampling G &lt;- 0 W &lt;- 1 for (t in length(states):1) { state &lt;- states[t] action &lt;- actions[t] reward &lt;- rewards_ep[t] G &lt;- gamma * G + reward C[state, action] &lt;- C[state, action] + W Q[state, action] &lt;- Q[state, action] + (W / C[state, action]) * (G - Q[state, action]) pi_action &lt;- which.max(Q[state, ]) if (action != pi_action) break W &lt;- W / (1 / n_actions) # Importance sampling ratio } } policy[1:(n_states - 1)] &lt;- apply(Q[1:(n_states - 1), ], 1, which.max) list(Q = Q, policy = policy, rewards = rewards) } # Value Iteration (from DP) value_iteration &lt;- function(transition_model, reward_model, gamma, epsilon = 1e-6, max_iter = 1000) { V &lt;- rep(0, n_states) policy &lt;- rep(0, n_states) delta &lt;- Inf iter &lt;- 0 while (delta &gt; epsilon &amp;&amp; iter &lt; max_iter) { delta &lt;- 0 V_old &lt;- V for (s in 1:(n_states - 1)) { Q &lt;- numeric(n_actions) for (a in 1:n_actions) { Q[a] &lt;- sum(transition_model[s, a, ] * (reward_model[s, a, ] + gamma * V)) } V[s] &lt;- max(Q) policy[s] &lt;- which.max(Q) delta &lt;- max(delta, abs(V[s] - V_old[s])) } iter &lt;- iter + 1 } # Evaluate DP policy rewards &lt;- numeric(1000) for (episode in 1:1000) { state &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 while (state != terminal_state) { action &lt;- policy[state] step &lt;- simulate_step(state, action) episode_reward &lt;- episode_reward + step$reward state &lt;- step$next_state } rewards[episode] &lt;- episode_reward } list(V = V, policy = policy, rewards = rewards) } # Run algorithms set.seed(42) dp_result &lt;- value_iteration(transition_model, reward_model, gamma) sarsa_result &lt;- sarsa(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) qlearn_result &lt;- q_learning(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) mc_result &lt;- off_policy_mc(n_episodes = 1000, epsilon = 0.1) # Visualization library(ggplot2) library(gridExtra) # Policy comparison policy_df &lt;- data.frame( State = rep(1:n_states, 4), Policy = c(dp_result$policy, sarsa_result$policy, qlearn_result$policy, mc_result$policy), Algorithm = rep(c(&quot;DP&quot;, &quot;SARSA&quot;, &quot;Q-Learning&quot;, &quot;Off-Policy MC&quot;), each = n_states) ) policy_df$Policy[n_states * 0:3 + n_states] &lt;- NA # Terminal state policy_plot &lt;- ggplot(policy_df, aes(x = State, y = Policy, color = Algorithm)) + geom_point(size = 3) + geom_line(aes(group = Algorithm), na.rm = TRUE) + theme_minimal() + labs(title = &quot;Optimal Policies by Algorithm&quot;, x = &quot;State&quot;, y = &quot;Action&quot;) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;)) + theme(legend.position = &quot;bottom&quot;) # Reward comparison reward_df &lt;- data.frame( Episode = rep(1:1000, 4), Reward = c( cumsum(dp_result$rewards), cumsum(sarsa_result$rewards), cumsum(qlearn_result$rewards), cumsum(mc_result$rewards) ), Algorithm = rep(c(&quot;DP&quot;, &quot;SARSA&quot;, &quot;Q-Learning&quot;, &quot;Off-Policy MC&quot;), each = 1000) ) reward_plot &lt;- ggplot(reward_df, aes(x = Episode, y = Reward, color = Algorithm)) + geom_line() + theme_minimal() + labs(title = &quot;Cumulative Reward Comparison&quot;, x = &quot;Episode&quot;, y = &quot;Cumulative Reward&quot;) + theme(legend.position = &quot;bottom&quot;) # Display plots grid.arrange(policy_plot, reward_plot, ncol = 1) # Print performance metrics cat(&quot;Average Cumulative Reward per Episode:\\n&quot;) cat(&quot;DP:&quot;, mean(dp_result$rewards), &quot;\\n&quot;) cat(&quot;SARSA:&quot;, mean(sarsa_result$rewards), &quot;\\n&quot;) cat(&quot;Q-Learning:&quot;, mean(qlearn_result$rewards), &quot;\\n&quot;) cat(&quot;Off-Policy MC:&quot;, mean(mc_result$rewards), &quot;\\n&quot;) 5.6 Interpretation and Discussion 5.6.0.1 Policy Differences SARSA: As an on-policy method, it learns the value of the \\(\\epsilon\\)-greedy policy, which includes exploratory actions. In the 10-state environment, SARSA may balance between actions 1 and 2, reflecting the impact of random exploration, leading to a more conservative policy. Q-Learning: As an off-policy method, it learns the optimal policy, favoring action 1 in state 9 (higher terminal reward of 1.0) due to its greedy updates. Its policy is less sensitive to exploration noise, as it assumes optimal future actions. Off-Policy Monte Carlo: Also off-policy, it learns the optimal policy using importance sampling to reweight returns from a random behavior policy. It may align closely with Q-Learning’s policy but can exhibit variability due to high variance in importance sampling ratios, especially if the random policy frequently selects action 2 (lower reward). 5.6.0.2 Devaluation All methods exhibit habitual behavior without retraining, retaining their original policies after the terminal reward is removed. This highlights a limitation of model-free methods compared to model-based approaches (e.g., dynamic programming), which adapt instantly to reward changes. 5.6.0.3 Practical Implications SARSA: Better suited for environments where the exploration policy must be accounted for, such as safety-critical systems (e.g., robotics), where risky exploratory actions could lead to poor outcomes. Q-Learning: Ideal for scenarios where the optimal policy is desired regardless of exploration, such as games or simulations where exploration does not incur real-world costs. Off-Policy Monte Carlo: Suitable for offline learning from logged data (e.g., recommendation systems), but high variance can make it less stable than Q-Learning in dynamic environments. 5.6.0.4 Experimental Observations Before devaluation, Q-Learning and off-policy Monte Carlo likely favor action 1 in state 9 due to its higher terminal reward, while SARSA’s policy may show more variability due to exploration. After devaluation, all policies remain unchanged without retraining, illustrating their reliance on cached Q-values. Off-policy Monte Carlo’s performance depends on the similarity between the random behavior policy and the greedy target policy, with high variance potentially leading to less consistent policies compared to Q-Learning. 5.7 Conclusion SARSA, Q-Learning, and off-policy Monte Carlo represent distinct paradigms in model-free RL. SARSA’s on-policy updates reflect the exploration policy, making it conservative. Q-Learning’s off-policy updates target the optimal policy, ignoring exploration effects. Off-policy Monte Carlo uses importance sampling to learn from diverse trajectories, enabling offline learning but introducing variance. The R implementations demonstrate these differences in a 10-state environment, and the devaluation experiment underscores their habitual nature. Future posts could explore advanced topics, such as SARSA(\\(\\lambda\\)), deep RL extensions, or variance reduction in off-policy Monte Carlo. 5.8 Comparison Table Aspect SARSA (On-Policy) Q-Learning (Off-Policy) Off-Policy Monte Carlo Learning Approach Learns incrementally, updates based on action taken by behavior policy. Learns incrementally, updates based on best action in next state. Learns from complete episodes, using importance sampling. Update Rule \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma Q(s&#39;, a&#39;) - Q(s,a) \\right)\\) \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right)\\) \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( \\rho_t G_t - Q(s,a) \\right)\\) Episode Requirement Updates online, no episode completion needed. Updates online, no episode completion needed. Requires complete episodes for returns and importance weights. Bias and Variance Biased due to bootstrapping, moderate variance. Biased due to bootstrapping, lower variance. Unbiased but high variance due to importance sampling. Policy Type On-policy; learns value of behavior policy. Off-policy; learns optimal policy via max Q-value. Off-policy; learns greedy policy using importance sampling. Exploration Impact Exploration affects learned Q-values. Exploration does not affect learned Q-values. Exploration affects returns, reweighted by importance sampling. Convergence Converges to policy’s value if \\(\\epsilon \\to 0\\). Converges to optimal policy even with fixed \\(\\epsilon\\). Converges to optimal policy, but variance depends on policy similarity. Behavior Conservative, accounts for exploration risks. Aggressive, assumes optimal future actions. Aggressive, but variance can lead to instability. Example in Environment Balances actions 1 and 2, sensitive to exploration. Favors action 1 (higher reward) in state 9. Favors action 1, but variance may cause variability. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
