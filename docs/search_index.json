[["index.html", "Reinforcement Learning in R Chapter 1 Understanding Reinforcement Learning: From Bandits to Policy Optimization 1.1 Introduction to Reinforcement Learning 1.2 The Multi-Armed Bandit: The Simplest Case 1.3 Transition to Markov Decision Processes 1.4 Comparing Reinforcement Learning Methods 1.5 Further Directions 1.6 References", " Reinforcement Learning in R Kamran Afzalui 2025-10-19 Chapter 1 Understanding Reinforcement Learning: From Bandits to Policy Optimization 1.1 Introduction to Reinforcement Learning Reinforcement Learning (RL) is a dynamic subfield of artificial intelligence concerned with how agents ought to take actions in an environment to maximize cumulative reward. It is inspired by behavioral psychology and decision theory, involving a learning paradigm where feedback from the environment is neither supervised (as in classification tasks) nor completely unsupervised, but rather in the form of scalar rewards. The foundational concepts of RL can be understood by progressing from simple problems, such as the multi-armed bandit, to more sophisticated frameworks like Markov Decision Processes (MDPs) and their many solution methods. 1.2 The Multi-Armed Bandit: The Simplest Case The journey begins with the multi-armed bandit problem, a classic formulation that captures the essence of the exploration-exploitation dilemma. In this setting, an agent must choose among several actions (arms), each yielding stochastic rewards from an unknown distribution. There are no state transitions or temporal dependencies—just the immediate outcome of the chosen action. The objective is to maximize the expected reward over time. Despite its simplicity, the bandit framework introduces crucial ideas such as reward estimation, uncertainty, and exploration strategies. Algorithms like ε-greedy methods introduce random exploration, while Upper Confidence Bound (UCB) techniques adjust choices based on uncertainty estimates. Thompson Sampling applies Bayesian reasoning to balance exploration and exploitation. Though limited in scope, these strategies establish foundational principles that generalize to more complex environments. In bandits, action selection strategies serve a role similar to policies in full RL problems, but without dependence on state transitions. 1.3 Transition to Markov Decision Processes The limitations of bandit models become evident when we consider sequential decision-making problems where actions influence future states and rewards. This leads to the formalism of Markov Decision Processes (MDPs), which model environments through states \\(S\\), actions \\(A\\), transition probabilities \\(P(s&#39;|s, a)\\), rewards \\(R(s, a)\\), and a discount factor \\(\\gamma \\in [0, 1]\\). The Markov property assumes that the future is independent of the past given the present state, simplifying the dynamics and enabling tractable analysis. The agent’s goal is to learn an optimal policy \\(\\pi^*(s)\\) that maximizes the expected cumulative return: \\(G_t = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\right]\\) This process relies on value functions such as: \\(V^\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t | S_t = s \\right], \\quad Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ G_t | S_t = s, A_t = a \\right]\\) Solving MDPs involves either learning these functions or directly learning \\(\\pi\\). 1.4 Comparing Reinforcement Learning Methods To frame this spectrum of approaches clearly, the table below summarizes key RL method families, highlighting whether they rely on an explicit model of the environment, whether they learn policies, value functions, or both, and notes on convergence and sample efficiency. Method Type Uses Model? Learns Policy? Learns Value? Sample Efficient? Converges to Optimal? Suitable For Example Algorithms Multi-Armed Bandits No Action Rule No Yes Yes (in expectation) Stateless problems ε-Greedy, UCB, Thompson Dynamic Programming Yes Yes Yes No Yes Known model Value Iteration, Policy Iteration Monte Carlo No Yes Yes No Yes (episodic) Episodic tasks MC Prediction, MC Control TD Learning No Yes Yes Moderate Yes Ongoing tasks SARSA, Q-Learning Dyna-style Methods Yes (learned) Yes Yes High Yes (with perfect model) Sample-limited tasks Dyna-Q, Dyna+, Prioritized Sweeping Q-Learning + Function Approx. No Yes Yes Moderate Not always (non-linear) High-dimensional spaces DQN Policy Gradient No Yes Maybe Low Local Optimum Continuous action spaces REINFORCE, PPO, A2C Actor-Critic No Yes Yes Moderate Local Optimum Most RL settings A2C, DDPG, SAC Model-Based RL Yes (learned) Yes Yes High Not guaranteed Data-limited tasks Dreamer, MBPO, MuZero This classification illustrates the diversity of RL approaches and underscores the flexibility of the RL paradigm. Some methods assume access to a perfect model, while others learn entirely from data. Some directly optimize policies, while others estimate values to guide policy improvement. 1.4.1 Dynamic Programming: Model-Based Learning Dynamic Programming (DP) methods such as Value Iteration and Policy Iteration assume complete knowledge of the environment’s dynamics. These algorithms exploit the Bellman equations to iteratively compute optimal value functions and policies. The Bellman optimality equation is given by: \\(V^*(s) = \\max_a \\sum_{s&#39;} P(s&#39;|s,a) [R(s,a) + \\gamma V^*(s&#39;)]\\) Value Iteration applies this update directly, while Policy Iteration alternates between evaluating the current policy and improving it by acting greedily with respect to the value function. Although DP methods guarantee convergence to the optimal policy, they are rarely applicable to real-world problems due to their assumption of known transitions and the computational infeasibility of operating over large or continuous state spaces. 1.4.2 Model-Free Approaches: Monte Carlo and TD Learning When the model is unknown, we turn to model-free methods that learn directly from sampled experience. Monte Carlo methods estimate the value of policies by averaging the total return over multiple episodes. These methods are simple and intuitive, suitable for episodic environments, but suffer from high variance and are not efficient in online learning scenarios. Temporal Difference (TD) learning bridges the gap between Monte Carlo and DP by updating value estimates based on partial returns. Algorithms like SARSA and Q-learning fall into this category. SARSA is on-policy, updating values based on the actual trajectory taken, while Q-learning is off-policy, learning about the greedy policy regardless of the agent’s current behavior. These methods do not require waiting until the end of an episode and are thus applicable in ongoing tasks. They offer a tradeoff between bias and variance in value estimation. 1.4.3 Dyna: Bridging Model-Free and Model-Based Learning The Dyna architecture, introduced by Richard Sutton, represents an elegant solution to combining model-free learning with model-based planning. Dyna methods simultaneously learn a model of the environment while using that model to generate synthetic experiences for additional learning. This approach addresses a fundamental limitation of pure model-free methods: their sample inefficiency. The core Dyna algorithm operates through three interleaved processes: Direct RL: Learn from real experience using standard model-free methods (e.g., Q-learning) Model Learning: Update the learned model based on observed transitions Planning: Use the learned model to generate simulated experiences and update the value function The basic Dyna-Q algorithm combines Q-learning with a simple tabular model. After each real experience \\((s, a, r, s&#39;)\\), the agent updates both its Q-values and its model estimates \\(\\hat{R}(s,a)\\) and \\(\\hat{T}(s,a)\\). The agent then performs several planning steps by randomly sampling previously visited state-action pairs and using the model to generate simulated transitions for additional Q-learning updates. Dyna+ and Exploration Bonuses: The original Dyna-Q algorithm can suffer when the environment changes, as the learned model becomes outdated. Dyna+ addresses this by adding exploration bonuses to state-action pairs that haven’t been visited recently, encouraging the agent to re-explore and update its model. The bonus term is typically: \\(\\text{bonus}(s,a) = \\kappa \\sqrt{\\tau(s,a)}\\) where \\(\\kappa\\) is a small constant and \\(\\tau(s,a)\\) is the time since state-action pair \\((s,a)\\) was last visited. Prioritized Sweeping: Rather than randomly selecting state-action pairs for planning updates, prioritized sweeping focuses computational resources on the most important updates. It maintains a priority queue of state-action pairs, prioritizing those where the model predicts large changes in value. This targeted approach can significantly improve learning efficiency compared to uniform random planning. The Dyna framework demonstrates that the dichotomy between model-free and model-based methods is not absolute. By learning and using simple models alongside direct RL, Dyna methods can achieve much better sample efficiency than pure model-free approaches while remaining more robust than methods that rely entirely on learned models. 1.4.4 Q-Learning and Function Approximation Q-learning is one of the most widely used RL algorithms due to its simplicity and theoretical guarantees. However, when dealing with large state-action spaces, tabular Q-learning becomes infeasible. Function approximation, particularly using neural networks, allows Q-learning to scale to high-dimensional problems. This gave rise to Deep Q-Networks (DQNs), where a neural network is trained to approximate the Q-function. DQNs introduced mechanisms like experience replay—storing and reusing past interactions to reduce correlation between updates—and target networks—fixed Q-value targets updated slowly to stabilize learning. These enhancements enabled RL to tackle complex environments like Atari games directly from pixels. Nonetheless, deep RL methods often suffer from sample inefficiency and training instability, especially when generalizing to new environments. 1.4.5 Policy Gradient and Actor-Critic Methods While value-based methods derive policies from value functions, policy-based methods directly parameterize and optimize the policy itself. Policy Gradient methods compute the gradient of expected return with respect to policy parameters and perform gradient ascent. The REINFORCE algorithm is the archetype of this approach, but it often suffers from high variance in gradient estimates. The Policy Gradient Theorem provides the theoretical foundation: \\(\\nabla J(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^{\\pi}(s,a)]\\) To address variance, Actor-Critic methods introduce a second component: the critic, which estimates value functions to inform and stabilize the updates of the actor (policy). Algorithms like Advantage Actor-Critic (A2C), Deep Deterministic Policy Gradient (DDPG), and Soft Actor-Critic (SAC) build on this architecture, each adding unique elements to improve performance and stability. 1.4.6 Advanced Policy Optimization Techniques More recent advances in policy optimization have focused on improving training stability and sample efficiency. Trust Region Policy Optimization (TRPO) constrains policy updates to stay within a trust region defined by the KL divergence, ensuring small, safe steps in parameter space. Proximal Policy Optimization (PPO) simplifies TRPO with a clipped objective function, striking a balance between ease of implementation and empirical performance. Soft Actor-Critic (SAC), on the other hand, incorporates an entropy maximization objective, encouraging exploration by maintaining a degree of randomness in the policy. This leads to better performance in environments with sparse or deceptive rewards and is particularly effective in continuous control tasks. Model-based approaches, such as MuZero or Dreamer, offer a complementary strategy: by learning a model of the environment dynamics and reward structure, they can generate synthetic experiences to improve sample efficiency. However, model inaccuracies can lead to cascading errors and suboptimal policies. 1.5 Further Directions Reinforcement Learning has evolved into a mature and diverse field, offering a rich set of tools for decision-making under uncertainty. From simple bandit strategies to deep policy optimization and model-based reasoning, RL provides a versatile framework for learning from interaction. The Dyna architecture exemplifies how combining different learning paradigms can yield methods that are both sample-efficient and robust, highlighting the value of hybrid approaches in RL. However, key challenges remain: training instability, sample inefficiency, reward mis-specification, and generalization across tasks. These remain active areas of research, with promising directions including meta-learning, hierarchical RL, and more sophisticated model learning techniques. A solid understanding of the main categories—such as those outlined in the comparative table—is essential for navigating the RL landscape. Whether one is interested in theoretical foundations, algorithm development, or practical deployment, the key ideas of exploration, value estimation, policy optimization, and model learning form the backbone of modern RL. For further reading, Reinforcement Learning: An Introduction by Sutton and Barto remains the canonical text. As artificial agents continue to do more complex and dynamic environments, reinforcement learning stands at the forefront of AI research and application. 1.6 References Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). MIT Press. http://incompleteideas.net/book/the-book-2nd.html Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236 Schulman, J., Levine, S., Abbeel, P., Jordan, M., &amp; Moritz, P. (2015). Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (pp. 1889–1897). https://proceedings.mlr.press/v37/schulman15.html Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. https://arxiv.org/abs/1707.06347 Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning. https://arxiv.org/abs/1801.01290 Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., … &amp; Hassabis, D. (2020). Mastering the game of Go without human knowledge. Nature, 550(7676), 354–359. https://doi.org/10.1038/nature24270 Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., … &amp; Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419), 1140–1144. https://doi.org/10.1126/science.aar6404 OpenAI. (2018). Spinning Up in Deep RL. https://spinningup.openai.com Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., … &amp; Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971. https://arxiv.org/abs/1509.02971 Auer, P., Cesa-Bianchi, N., &amp; Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47, 235–256. https://doi.org/10.1023/A:1013689704352 Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4), 285–294. https://doi.org/10.2307/2332286 "],["the-multi-armed-bandit-problem.html", "Chapter 2 The Multi-Armed Bandit Problem 2.1 Introduction 2.2 Mathematical Formalism 2.3 Frequentist Approach: UCB1 Algorithm 2.4 Bayesian Approach: Thompson Sampling 2.5 Epsilon-Greedy Strategy 2.6 Summary Table 2.7 Conclusion", " Chapter 2 The Multi-Armed Bandit Problem 2.1 Introduction The multi-armed bandit (MAB) problem is a foundational model in the study of sequential decision-making under uncertainty. Representing the trade-off between exploration (gathering information) and exploitation (maximizing known rewards), MAB problems are central to reinforcement learning, online optimization, and adaptive experimental design. An agent is faced with a choice among multiple options—arms—each producing stochastic rewards with unknown distributions. The objective is to maximize cumulative reward, or equivalently, to minimize the regret incurred by not always choosing the best arm. This post presents a rigorous treatment of the MAB problem, comparing frequentist and Bayesian approaches. We offer formal mathematical foundations, develop regret bounds, and implement both Upper Confidence Bound (UCB) and Thompson Sampling algorithms in R. A summary table is provided at the end. 2.2 Mathematical Formalism Let \\(K\\) denote the number of arms, and each arm \\(k \\in \\{1, \\dots, K\\}\\) has an unknown reward distribution \\(P_k\\), with mean \\(\\mu_k\\). Define the optimal arm: \\[ k^* = \\arg\\max_{k} \\mu_k. \\] At each time \\(t \\in \\{1, \\dots, T\\}\\), the agent chooses arm \\(A_t \\in \\{1, \\dots, K\\}\\) and receives a stochastic reward \\(R_t \\sim P_{A_t}\\). The cumulative expected regret is: \\[ \\mathcal{R}(T) = T\\mu^* - \\mathbb{E}\\left[ \\sum_{t=1}^T R_t \\right] = \\sum_{k=1}^K \\Delta_k \\, \\mathbb{E}[N_k(T)], \\] where \\(\\Delta_k = \\mu^* - \\mu_k\\) and \\(N_k(T)\\) is the number of times arm \\(k\\) was played. 2.3 Frequentist Approach: UCB1 Algorithm Frequentist methods estimate expected rewards using empirical means. The UCB1 algorithm, based on Hoeffding’s inequality, constructs an upper confidence bound: \\[ A_t = \\arg\\max_{k} \\left[ \\hat{\\mu}_{k,t} + \\sqrt{ \\frac{2 \\log t}{N_k(t)} } \\right]. \\] This ensures logarithmic regret in expectation. 2.3.1 R Code for UCB1 set.seed(42) K &lt;- 3 T &lt;- 1000 mu &lt;- c(0.3, 0.5, 0.7) # true means counts &lt;- rep(0, K) values &lt;- rep(0, K) regret &lt;- numeric(T) # Play each arm once for (k in 1:K) { reward &lt;- rbinom(1, 1, mu[k]) counts[k] &lt;- 1 values[k] &lt;- reward regret[k] &lt;- max(mu) - mu[k] } for (t in (K+1):T) { ucb &lt;- values + sqrt(2 * log(t) / counts) a &lt;- which.max(ucb) reward &lt;- rbinom(1, 1, mu[a]) counts[a] &lt;- counts[a] + 1 values[a] &lt;- values[a] + (reward - values[a]) / counts[a] regret[t] &lt;- max(mu) - mu[a] } cum_regret_ucb &lt;- cumsum(regret) plot(cum_regret_ucb, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, ylab = &quot;Cumulative Regret&quot;, xlab = &quot;Time&quot;, main = &quot;UCB1 Regret&quot;) 2.4 Bayesian Approach: Thompson Sampling Bayesian bandits model reward distributions probabilistically, updating beliefs via Bayes’ rule. For Bernoulli rewards, we assume Beta priors: \\[ \\mu_k \\sim \\text{Beta}(\\alpha_k, \\beta_k). \\] After observing a reward \\(r \\in \\{0, 1\\}\\), the posterior update is: \\[ \\alpha_k \\leftarrow \\alpha_k + r, \\quad \\beta_k \\leftarrow \\beta_k + 1 - r. \\] The Thompson Sampling algorithm draws a sample \\(\\tilde{\\mu}_k \\sim \\text{Beta}(\\alpha_k, \\beta_k)\\) and selects the arm with the highest sample. 2.4.1 R Code for Thompson Sampling set.seed(42) alpha &lt;- rep(1, K) beta &lt;- rep(1, K) regret &lt;- numeric(T) for (t in 1:T) { sampled_means &lt;- rbeta(K, alpha, beta) a &lt;- which.max(sampled_means) reward &lt;- rbinom(1, 1, mu[a]) alpha[a] &lt;- alpha[a] + reward beta[a] &lt;- beta[a] + (1 - reward) regret[t] &lt;- max(mu) - mu[a] } cum_regret_ts &lt;- cumsum(regret) plot(cum_regret_ucb, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, ylab = &quot;Cumulative Regret&quot;, xlab = &quot;Time&quot;, main = &quot;UCB1 Regret&quot;) lines(cum_regret_ts, col = &quot;red&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;UCB1&quot;, &quot;Thompson Sampling&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 2) The UCB1 algorithm guarantees a regret bound of: \\[ \\mathcal{R}(T) \\leq \\sum_{k: \\Delta_k &gt; 0} \\left( \\frac{8 \\log T}{\\Delta_k} + C_k \\right), \\] where \\(C_k\\) is a problem-dependent constant. Thompson Sampling achieves comparable performance. Under certain regularity conditions, its Bayesian regret is bounded by: \\[ \\mathbb{E}[\\mathcal{R}(T)] = O\\left( \\sqrt{KT \\log T} \\right), \\] and often outperforms UCB1 in practice due to its adaptive exploration. 2.5 Epsilon-Greedy Strategy The epsilon-greedy algorithm is a simple and intuitive approach to balancing exploration and exploitation. At each time step, with probability \\(\\epsilon\\), the agent chooses a random arm (exploration), and with probability \\(1 - \\epsilon\\), it selects the arm with the highest empirical mean (exploitation). Let \\(\\hat{\\mu}_{k,t}\\) denote the empirical mean reward for arm \\(k\\) at time \\(t\\). Then: \\[ A_t = \\begin{cases} \\text{random choice} &amp; \\text{with probability } \\epsilon, \\\\ \\arg\\max_k \\hat{\\mu}_{k,t} &amp; \\text{with probability } 1 - \\epsilon. \\end{cases} \\] While this algorithm is not optimal in the theoretical sense, it often performs well in practice for problems with stationary reward distributions when the exploration rate \\(\\epsilon\\) is properly tuned. Regret under a fixed \\(\\epsilon\\) is linear in \\(T\\), i.e., \\(\\mathcal{R}(T) = O(T)\\), unless \\(\\epsilon\\) is decayed over time (e.g., \\(\\epsilon_t = 1/t\\)), which introduces a trade-off between convergence speed and variance. 2.5.1 R Code for Epsilon-Greedy set.seed(42) epsilon &lt;- 0.1 counts &lt;- rep(0, K) values &lt;- rep(0, K) regret &lt;- numeric(T) for (t in 1:T) { if (runif(1) &lt; epsilon) { a &lt;- sample(1:K, 1) # Exploration } else { a &lt;- which.max(values) # Exploitation } reward &lt;- rbinom(1, 1, mu[a]) counts[a] &lt;- counts[a] + 1 values[a] &lt;- values[a] + (reward - values[a]) / counts[a] regret[t] &lt;- max(mu) - mu[a] } cum_regret_eps &lt;- cumsum(regret) plot(cum_regret_ucb, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, ylab = &quot;Cumulative Regret&quot;, xlab = &quot;Time&quot;, main = &quot;UCB1 Regret&quot;) lines(cum_regret_eps, col = &quot;darkgreen&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;UCB1&quot;, &quot;Thompson Sampling&quot;, &quot;Epsilon-Greedy&quot;), col = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;), lwd = 2) 2.6 Summary Table Method Paradigm Assumptions Exploration Mechanism Regret Bound Strengths Weaknesses UCB1 Frequentist Stationary, bounded rewards Upper Confidence Bound \\(O(\\log T)\\) Simple, provable guarantees Conservative, suboptimal in practice Thompson Sampling Bayesian Prior over reward distributions Posterior sampling \\(O(\\sqrt{KT})\\), empirically better Adaptive, efficient with good priors Sensitive to prior misspecification KL-UCB Frequentist Known reward distributions KL-divergence bounds \\(O(\\log T)\\) (tighter) Distribution-aware More complex implementation Epsilon-Greedy Heuristic None Random exploration \\(O(T)\\) if \\(\\epsilon\\) fixed Very simple Inefficient long-term 2.7 Conclusion The multi-armed bandit problem remains an essential model for studying decision-making under uncertainty. While frequentist methods like UCB1 provide rigorous guarantees and conceptual clarity, Bayesian approaches like Thompson Sampling offer greater flexibility and empirical performance. The choice between them hinges on the trade-offs between interpretability, adaptivity, and prior knowledge. The R implementations provided here allow for practical experimentation and benchmarking. In real-world applications, such as clinical trial design, online recommendations, and adaptive A/B testing, these algorithms offer principled foundations for learning and acting in uncertain environments. "],["markov-decision-processes-and-dynamic-programming.html", "Chapter 3 Markov Decision Processes and Dynamic Programming 3.1 Introduction 3.2 Constructing the MDP in R 3.3 Value Iteration Algorithm 3.4 Evaluation and Interpretation 3.5 Theoretical Properties of Value Iteration 3.6 Summary Table 3.7 Conclusion", " Chapter 3 Markov Decision Processes and Dynamic Programming 3.1 Introduction Markov Decision Processes (MDPs) constitute a formal framework for modeling sequential decision-making under uncertainty. Widely applied in operations research, control theory, economics, and artificial intelligence, MDPs encapsulate the dynamics of environments where outcomes are partly stochastic and partly under an agent’s control. At their core, MDPs unify probabilistic transitions, state-contingent rewards, and long-term optimization goals. This post explores MDPs from a computational standpoint, emphasizing Dynamic Programming (DP) methods—particularly Value Iteration—for solving them when the model is fully known. We proceed by defining the mathematical components of MDPs, implementing them in R, and illustrating policy derivation using value iteration. A comparative summary of key aspects of DP methods concludes the post. An MDP is formally defined by the tuple $(S, A, P, R, )$, where: $S$: a finite set of states. $A$: a finite set of actions. $P(s’|s, a)$: the transition probability function—probability of moving to state $s’$ given current state $s$ and action $a$. $R(s, a, s’)$: the reward received after transitioning from state $s$ to $s’$ via action $a$. $[0,1]$: the discount factor, representing the agent’s preference for immediate versus delayed rewards. The agent’s objective is to learn a policy $: S A$ that maximizes the expected cumulative discounted reward: \\[ V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(S_t, A_t, S_{t+1}) \\,\\bigg|\\, S_0 = s \\right] \\] The optimal value function $V^*$ satisfies the Bellman optimality equation: $$ V^*(s) = {a A} {s’ S} P(s’|s,a) $$ Once $V^$ is computed, the optimal policy $^$ is obtained via: $$ ^*(s) = {a A} {s’} P(s’|s,a) $$ 3.2 Constructing the MDP in R We now implement an environment with 10 states and 2 actions per state, following stochastic transition and reward dynamics. The final state is absorbing, with no further transitions or rewards. n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 Explanation: This block defines the MDP. We create n_states = 10 with n_actions = 2. The transition probabilities are stored in a 3D array, where the first dimension is the current state, the second is the action, and the third is the next state. Action 1 strongly favors moving forward to the next state (90% chance), while Action 2 introduces more randomness (different probabilities to jump to random states). Rewards are drawn randomly, but reaching the terminal state yields higher rewards (1.0 for Action 1 and 0.5 for Action 2). Finally, the terminal state has no outgoing transitions or rewards, making it absorbing. We define a function to sample from the environment: sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } Explanation: This function simulates environment dynamics. Given a current state s and action a, it samples the next state s_prime according to the transition probabilities and retrieves the corresponding reward. The function outputs both the next state and the reward, mimicking how an agent would experience interaction with the MDP. 3.3 Value Iteration Algorithm Value Iteration is a fundamental DP method for computing the optimal value function and deriving the optimal policy. It exploits the Bellman optimality equation through successive approximation. value_iteration &lt;- function() { V &lt;- rep(0, n_states) policy &lt;- rep(1, n_states) theta &lt;- 1e-4 repeat { delta &lt;- 0 for (s in 1:(n_states - 1)) { v &lt;- V[s] q_values &lt;- numeric(n_actions) for (a in 1:n_actions) { for (s_prime in 1:n_states) { q_values[a] &lt;- q_values[a] + transition_model[s, a, s_prime] * (reward_model[s, a, s_prime] + gamma * V[s_prime]) } } V[s] &lt;- max(q_values) policy[s] &lt;- which.max(q_values) delta &lt;- max(delta, abs(v - V[s])) } if (delta &lt; theta) break } list(V = V, policy = policy) } Explanation: This function implements value iteration. It initializes all state values V to zero and assigns a default policy of choosing action 1. At each iteration, the algorithm updates the value of each state by computing expected returns for all actions (q_values) and taking the maximum. The associated action becomes the new policy for that state. The process continues until the maximum change in values (delta) is below the convergence threshold theta. The function outputs both the value function and the derived policy. We now apply the algorithm and extract the resulting value function and policy: dp_result &lt;- value_iteration() dp_value &lt;- dp_result$V dp_policy &lt;- dp_result$policy dp_policy ## [1] 2 2 1 1 2 1 1 1 1 1 Explanation: Here we run the value iteration function and extract the results. The dp_value vector contains the optimal state values, while dp_policy indicates the best action to take at each state. Printing dp_policy gives us a direct mapping from state to optimal action. 3.4 Evaluation and Interpretation The policy and value function obtained via value iteration provide complete guidance for optimal behavior in the modeled environment. In this setting: The forward action (Action 1) is generally rewarded with higher long-term return due to its tendency to reach the terminal (high-reward) state. The second action (Action 2) introduces randomness and lower rewards, thus is optimal only in specific states where it offers a better expected value. Such interpretation emphasizes the Bellman principle of optimality: every sub-policy of an optimal policy must itself be optimal for the corresponding subproblem. We can visualize the value function: plot(dp_value, type = &quot;b&quot;, col = &quot;blue&quot;, pch = 19, xlab = &quot;State&quot;, ylab = &quot;Value&quot;, main = &quot;Optimal State Values via Value Iteration&quot;) Explanation: This plot shows the computed value function across all states. Higher values indicate states with greater expected long-term returns under the optimal policy. The shape of the curve helps us see how proximity to the terminal state (and the possibility of reaching it through Action 1) drives up the expected value. 3.5 Theoretical Properties of Value Iteration Value Iteration exhibits the following theoretical guarantees: Convergence: The algorithm is guaranteed to converge to $V^*$ for any finite MDP with bounded rewards and $0 &lt; 1$. Policy Derivation: Once $V^*$ is known, the greedy policy is optimal. Computational Complexity: For state space size $S$ and action space size $A$, each iteration requires $O(S^2 A)$ operations due to the summation over all successor states. However, in practice, the applicability of DP methods is restricted by: The need for full knowledge of $P$ and $R$, Infeasibility in large or continuous state spaces (the “curse of dimensionality”). These challenges motivate the use of approximation, sampling-based methods, and model-free approaches in reinforcement learning contexts. 3.6 Summary Table The following table summarizes the key elements and tradeoffs of the value iteration algorithm: Property Value Iteration (DP) Model Required Yes (transition probabilities and rewards) State Representation Tabular (explicit state-value storage) Action Selection Greedy w.r.t. value estimates Convergence Guarantee Yes (under finite $S, A$, $&lt; 1$) Sample Efficiency High (uses full model, no sampling error) Scalability Poor in large or continuous state spaces Output Optimal value function and policy Computational Complexity $O(S^2 A)$ per iteration 3.7 Conclusion Dynamic Programming offers elegant and theoretically grounded algorithms for solving MDPs when the environment model is fully specified. Value Iteration, as illustrated, leverages the recursive Bellman optimality equation to iteratively compute the value function and derive the optimal policy. While its practical scope is limited by scalability and model assumptions, DP remains foundational in the study of decision processes. Understanding these principles is essential for extending to model-free reinforcement learning, function approximation, and policy gradient methods. Future posts in this series will explore Temporal Difference learning, Monte Carlo methods, and the transition to policy optimization. These approaches lift the strict requirements of model knowledge and allow learning from interaction, thereby opening the door to real-world applications. Of course. Here is the provided text with detailed explanations added below each code chunk. "],["model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html", "Chapter 4 Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R 4.1 Introduction 4.2 Theoretical Background 4.3 Interpretation and Discussion 4.4 Conclusion", " Chapter 4 Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R 4.1 Introduction In many real-world decision-making problems, the environment model—comprising transition probabilities and reward functions—is unknown or too complex to specify explicitly. Model-free reinforcement learning (RL) methods address this by learning optimal policies directly from experience or sample trajectories, without requiring full knowledge of the underlying Markov Decision Process (MDP). Two foundational model-free methods are Temporal Difference (TD) learning and Monte Carlo (MC) methods. TD learning updates value estimates online based on one-step lookahead and bootstrapping, while MC methods learn from complete episodes by averaging returns. This post presents these approaches with mathematical intuition, R implementations, and an illustrative environment. We also compare how learned policies adapt—or fail to adapt—when rewards are changed after training, illuminating the distinction between goal-directed and habitual learning. 4.2 Theoretical Background Suppose an agent interacts with an environment defined by states \\(S\\), actions \\(A\\), a discount factor \\(\\gamma \\in [0,1]\\), and an unknown MDP dynamics. The goal is to learn the action-value function \\(Q^\\pi(s,a)\\), the expected discounted return starting from state \\(s\\), taking action \\(a\\), and following policy \\(\\pi\\) thereafter: \\[Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a \\right]\\] 4.2.1 Temporal Difference Learning (Q-Learning) Q-Learning is an off-policy TD control method that updates the estimate \\(Q(s,a)\\) incrementally after each transition: \\[Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right)\\] where \\(\\alpha\\) is the learning rate, \\(r\\) the reward received, and \\(s&#39;\\) the next state. This update uses the current estimate of \\(Q\\) at \\(s&#39;\\) (bootstrapping). 4.2.2 Monte Carlo Methods Monte Carlo methods learn \\(Q\\) by averaging returns observed after visiting \\((s,a)\\). Every-visit MC estimates \\(Q\\) by averaging the total discounted return \\(G_t\\) following each occurrence of \\((s,a)\\) within complete episodes: \\[Q(s,a) \\approx \\frac{1}{N(s,a)} \\sum_{i=1}^{N(s,a)} G_t^{(i)}\\] where \\(N(s,a)\\) is the count of visits to \\((s,a)\\) and \\(G_t^{(i)}\\) is the return following the \\(i\\)-th visit. 4.2.3 Step 1: Defining the Environment in R We use a 10-state, 2-action environment with stochastic transitions and rewards, as in previous work: # Common settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition + reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } This code chunk sets up the simulation environment. Common settings: It begins by defining the core parameters of the Markov Decision Process (MDP): 10 states, 2 actions, a discount factor gamma of 0.9, and defines the 10th state as the terminal state. Environment Models: set.seed(42) is used to ensure the random elements are reproducible. It then initializes two 3D arrays, transition_model and reward_model, to store the environment’s dynamics. These arrays define the probability of moving from a state s to a next state s_prime given an action a, and the reward received for doing so. Populating Models: The for loop defines the transitions and rewards for all non-terminal states. For each state s: Action 1 has a 90% chance of moving to the next state (s + 1) and a 10% chance of moving to a random state. This introduces stochasticity. Action 2 has an 80% chance of moving to one random state and a 20% chance of moving to another. The reward for transitioning to the terminal state is high (1.0 for action 1, 0.5 for action 2), while all other transitions yield small, random rewards. Terminal State: The transitions and rewards from the terminal state are set to zero, as the episode ends there. Sampling Function: The sample_env function simulates an agent’s interaction with the environment. Given a state s and action a, it uses the defined probabilities in transition_model to sample a next state s_prime and returns it along with the corresponding reward from reward_model. This function allows the model-free agents to get experience without having direct access to the underlying model arrays. 4.2.4 Step 2: Q-Learning Implementation in R The Q-Learning function runs episodes, selects actions using \\(\\epsilon\\)-greedy policy, updates Q-values using the TD rule, and outputs a policy by greedy action selection: q_learning &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) for (ep in 1:episodes) { s &lt;- 1 while (TRUE) { a &lt;- if (runif(1) &lt; epsilon) sample(1:n_actions, 1) else which.max(Q[s, ]) out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime reward &lt;- out$reward Q[s, a] &lt;- Q[s, a] + alpha * (reward + gamma * max(Q[s_prime, ]) - Q[s, a]) if (s_prime == n_states) break s &lt;- s_prime } } apply(Q, 1, which.max) } This code implements the Q-learning algorithm. Initialization: The function initializes a Q-table, Q, as a matrix of zeros. Rows represent states, and columns represent actions. This table will store the learned action-value estimates. Episode Loop: The outer for loop runs the learning process for a specified number of episodes. Step Loop: The inner while loop simulates a single episode, starting from state 1. It continues until the agent reaches the terminal state. Action Selection: Inside the loop, an action a is chosen using an \\(\\epsilon\\)-greedy strategy. With probability epsilon, a random action is selected (exploration); otherwise, the action with the highest current Q-value for state s is chosen (exploitation). Environment Interaction: The agent takes action a by calling sample_env, which returns the next state s_prime and a reward. Q-Value Update: This is the core of the algorithm. The Q-value for the state-action pair (s, a) is updated using the TD update rule: Q[s, a] &lt;- Q[s, a] + alpha * (reward + gamma * max(Q[s_prime, ]) - Q[s, a]). The update is based on the immediate reward and the discounted maximum Q-value of the next state (bootstrapping). Termination: The episode breaks if the s_prime is the terminal state. Otherwise, the current state s is updated to s_prime. Policy Extraction: After all episodes, the apply function is used to extract the final policy. For each state (row), it finds the action (column index) with the maximum Q-value, resulting in the learned optimal policy. Running this yields the learned policy: ql_policy_before &lt;- q_learning() ql_policy_before ## [1] 1 2 2 2 1 1 1 1 1 1 This line executes the q_learning function with its default parameters (1000 episodes, a learning rate of 0.1, and an epsilon of 0.1). The resulting optimal policy, which is a vector indicating the best action for each state, is stored in the variable ql_policy_before. 4.2.5 Step 3: Monte Carlo Every-Visit Implementation This MC method generates episodes, stores the full sequence of state-action-reward tuples, and updates \\(Q\\) by averaging discounted returns for every visit of \\((s,a)\\): monte_carlo &lt;- function(episodes = 1000, epsilon = 0.1) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) returns &lt;- vector(&quot;list&quot;, n_states * n_actions) names(returns) &lt;- paste(rep(1:n_states, each = n_actions), rep(1:n_actions, n_states), sep = &quot;_&quot;) for (ep in 1:episodes) { episode &lt;- list() s &lt;- 1 while (TRUE) { a &lt;- if (runif(1) &lt; epsilon) sample(1:n_actions, 1) else which.max(Q[s, ]) out &lt;- sample_env(s, a) episode[[length(episode) + 1]] &lt;- list(state = s, action = a, reward = out$reward) if (out$s_prime == n_states) break s &lt;- out$s_prime } G &lt;- 0 for (t in length(episode):1) { s &lt;- episode[[t]]$state a &lt;- episode[[t]]$action r &lt;- episode[[t]]$reward G &lt;- gamma * G + r key &lt;- paste(s, a, sep = &quot;_&quot;) returns[[key]] &lt;- c(returns[[key]], G) Q[s, a] &lt;- mean(returns[[key]]) } } apply(Q, 1, which.max) } This code implements the every-visit Monte Carlo control algorithm. Initialization: It initializes a Q table similar to Q-learning. It also creates a named list called returns, where each element will store a vector of all returns observed for a specific state-action pair. Episode Generation: The first part of the main for loop generates a complete episode. Using an \\(\\epsilon\\)-greedy policy based on the current Q table, it simulates steps until the terminal state is reached. The entire trajectory of states, actions, and rewards is stored in the episode list. Return Calculation: After an episode is complete, the second for loop iterates backward from the last step to the first. It initializes the total discounted return, G, to 0. In each step t of the backward loop, it updates G using the formula G &lt;- gamma * G + r. This correctly calculates the discounted return from that time step to the end of the episode. Q-Value Update: For each state-action pair (s, a) encountered in the episode, the calculated return G is appended to the corresponding list in returns. The Q-value Q[s, a] is then updated to be the average of all returns collected so far for that pair. This is the defining feature of the Monte Carlo method. Policy Extraction: Finally, after all episodes, the optimal policy is extracted from the Q table by selecting the action with the maximum value for each state. The resulting policy is computed by: mc_policy_before &lt;- monte_carlo() mc_policy_before ## [1] 1 2 1 2 2 1 1 1 1 1 This line calls the monte_carlo function with its default parameters. It runs the simulation for 1000 episodes and stores the final learned policy in the mc_policy_before variable. 4.2.6 Step 4: Simulating Outcome Devaluation We now alter the environment by removing the reward for reaching the terminal state, simulating a devaluation: # Devalue terminal reward for (s in 1:(n_states - 1)) { reward_model[s, 1, n_states] &lt;- 0 reward_model[s, 2, n_states] &lt;- 0 } This code block directly modifies the environment’s reward_model. The for loop iterates through all non-terminal states. For each state s, it sets the reward for any action that leads to the terminal state (n_states) to 0. This simulates “outcome devaluation,” where the primary goal of the task is no longer rewarding. 4.2.7 Step 5: Comparing Policies Before and After Devaluation We compare policies derived via: Dynamic Programming (DP) which has full model knowledge and updates instantly after reward changes, Q-Learning and Monte Carlo which keep their previously learned policies (habitual behavior without retraining). # Q-learning and MC keep previous policy (habitual) ql_policy_after &lt;- ql_policy_before mc_policy_after &lt;- mc_policy_before This chunk prepares the policies for comparison after the reward devaluation. Dynamic Programming: A call is made to a value_iteration() function (assumed to exist elsewhere) to compute the new optimal policy. Because DP is a model-based method, it uses the modified reward_model to instantly calculate the best policy for the new circumstances. Q-Learning and Monte Carlo: For the model-free methods, no retraining occurs. The “after” policies (ql_policy_after, mc_policy_after) are simply assigned the values of the policies learned before the devaluation (ql_policy_before, mc_policy_before). This demonstrates that without new experience, these agents will continue to follow their previously learned, now suboptimal, “habitual” policies. 4.2.8 Step 6: Visualizing the Policies plot_policy &lt;- function(policy, label, col = &quot;skyblue&quot;) { barplot(policy, names.arg = 1:n_states, col = col, ylim = c(0, 3), ylab = &quot;Action (1=A1, 2=A2)&quot;, main = label) abline(h = 1.5, lty = 2, col = &quot;gray&quot;) } par(mfrow = c(3, 2)) plot_policy(dp_policy_before, &quot;DP Policy Before&quot;) plot_policy(dp_policy_after, &quot;DP Policy After&quot;, &quot;lightgreen&quot;) plot_policy(ql_policy_before, &quot;Q-Learning Policy Before&quot;) plot_policy(ql_policy_after, &quot;Q-Learning Policy After&quot;, &quot;orange&quot;) plot_policy(mc_policy_before, &quot;MC Policy Before&quot;) plot_policy(mc_policy_after, &quot;MC Policy After&quot;, &quot;orchid&quot;) This R code is for visualizing and comparing the different policies. plot_policy function: This is a helper function created to generate a consistent bar plot for any given policy. It takes a policy vector, a title (label), and a color (col) as input. It creates a bar plot where the x-axis represents the states and the y-axis represents the chosen action (1 or 2). A dashed line is added at y=1.5 to clearly separate the two actions. par(mfrow = c(3, 2)): This command sets up the graphical parameters to arrange the subsequent plots in a grid of 3 rows and 2 columns. This allows for a direct side-by-side comparison of all six policies. Plotting Calls: The six calls to plot_policy generate the visualizations for the policies from Dynamic Programming, Q-Learning, and Monte Carlo, both before and after the reward devaluation, each with a distinct color and title. 4.3 Interpretation and Discussion Dynamic Programming adapts immediately after devaluation because it recalculates the policy using the updated reward model. In contrast, Q-Learning and Monte Carlo methods, which are model-free and learn from past experience, maintain their prior policies unless explicitly retrained. This reflects habitual behavior — a policy learned from experience that does not flexibly adjust to changed outcomes without further learning. This illustrates a core difference: Model-based methods (like DP) support goal-directed behavior, recomputing optimal decisions as the environment changes. Model-free methods (like Q-Learning and MC) support habitual behavior, relying on cached values learned from past rewards. 4.4 Conclusion Temporal Difference and Monte Carlo methods provide powerful approaches to reinforcement learning when the environment is unknown. TD learning’s bootstrapping allows online updates after each transition, while Monte Carlo averages full returns after complete episodes. Both learn policies from experience rather than models. Future posts will explore extensions including function approximation and policy gradient methods. Aspect Monte Carlo (MC) Temporal Difference (Q-Learning) Learning Approach Learns from complete episodes by averaging returns after each episode. Learns incrementally after each state-action transition using bootstrapping. Update Rule Updates Q-value as the mean of observed returns: \\(Q(s,a) \\approx \\frac{1}{N(s,a)} \\sum_{i=1}^{N(s,a)} G_t^{(i)}\\) Updates Q-value using TD error: \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right)\\) Episode Requirement Requires complete episodes to compute returns (\\(G_t\\)). Does not require complete episodes; updates online after each step. Bias and Variance Unbiased estimate of Q-value, but high variance due to full episode returns. Biased due to bootstrapping (relies on current Q estimates), but lower variance. Policy Type Typically on-policy (e.g., with ε-greedy exploration), but can be adapted for off-policy. Off-policy; learns optimal policy regardless of exploration policy. Computational Efficiency Less efficient; must wait for episode completion before updating. More efficient; updates Q-values immediately after each transition. Adaptation to Change Slow to adapt to environment changes without retraining, as it relies on past episode returns. Slow to adapt without retraining, but incremental updates allow faster response to changes. Implementation in Code Stores state-action-reward sequences, computes discounted returns backward. Updates Q-values online using immediate reward and next state’s Q-value. Example in Provided Code Every-visit MC: averages returns for each \\((s,a)\\) visit in an episode. Q-Learning: updates Q-values after each transition using TD rule. "],["on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html", "Chapter 5 On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R 5.1 Introduction 5.2 SARSA (On-Policy) 5.3 Q-Learning (Off-Policy) 5.4 Off-Policy Monte Carlo with Importance Sampling 5.5 Key Differences 5.6 Interpretation and Discussion 5.7 Conclusion 5.8 Comparison Table", " Chapter 5 On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R 5.1 Introduction In model-free reinforcement learning (RL), agents learn optimal policies directly from experience without a model of the environment’s dynamics. Two key approaches are on-policy and off-policy methods, exemplified by SARSA (State-Action-Reward-State-Action) and Q-Learning, respectively. Additionally, off-policy Monte Carlo methods leverage importance sampling to learn optimal policies from exploratory data. This post explores these methods, focusing on their theoretical foundations, practical implications, and implementation in R. We use a 10-state, 2-action environment to compare how SARSA, Q-Learning, and off-policy Monte Carlo learn policies and adapt to environmental changes, such as outcome devaluation. Mathematical formulations and R code are provided to illustrate the concepts. SARSA, Q-Learning, and off-policy Monte Carlo aim to estimate the action-value function \\(Q^\\pi(s,a)\\), the expected discounted return for taking action \\(a\\) in state \\(s\\) and following policy \\(\\pi\\): \\[ Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a \\right] \\] where \\(\\gamma \\in [0,1]\\) is the discount factor, and \\(R_{t+1}\\) is the reward at time \\(t+1\\). 5.2 SARSA (On-Policy) SARSA is an on-policy method, meaning it learns the value of the policy being followed, including exploration. The update rule is: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma Q(s&#39;, a&#39;) - Q(s,a) \\right) \\] where \\(\\alpha\\) is the learning rate, \\(r\\) is the reward, \\(s&#39;\\) is the next state, and \\(a&#39;\\) is the action actually taken in \\(s&#39;\\) according to the current policy (e.g., \\(\\epsilon\\)-greedy). SARSA updates \\(Q\\) based on the next state-action pair \\((s&#39;, a&#39;)\\), making it sensitive to the exploration policy. In the 10-state environment, SARSA learns a policy that accounts for exploratory actions, potentially avoiding risky moves that lead to lower rewards. 5.3 Q-Learning (Off-Policy) Q-Learning is an off-policy method, meaning it learns the optimal policy \\(\\pi^*\\) regardless of the exploration policy (e.g., \\(\\epsilon\\)-greedy). The update rule is: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right) \\] where \\(\\max_{a&#39;} Q(s&#39;, a&#39;)\\) estimates the value of the next state assuming the optimal action. This bootstrapping makes Q-Learning converge to the optimal action-value function \\(Q^*(s,a)\\). In the 10-state environment, Q-Learning favors actions that maximize future rewards (e.g., action 1 in state 9, yielding a 1.0 reward at the terminal state), ignoring exploration effects. 5.4 Off-Policy Monte Carlo with Importance Sampling Off-policy Monte Carlo uses importance sampling to learn the value of a target policy (e.g., greedy) from episodes generated by a behavior policy (e.g., random). The return \\(G_t\\) (cumulative discounted reward from time \\(t\\) onward) is weighted by the importance sampling ratio: \\[ \\rho_t = \\prod_{k=t}^T \\frac{\\pi(a_k|s_k)}{\\mu(a_k|s_k)} \\] where \\(\\pi\\) is the target policy, \\(\\mu\\) is the behavior policy, and \\(T\\) is the episode length. The Q-value update is: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( \\rho_t G_t - Q(s,a) \\right) \\] Importance Sampling Mechanics: In the 10-state environment, suppose the behavior policy is random (0.5 probability for actions 1 and 2), but the target policy is greedy (choosing the action with the highest Q-value). If the agent in state 9 takes action 2 (reward 0.5 at the terminal state), but the greedy policy prefers action 1 (reward 1.0), the importance sampling ratio \\(\\rho_t\\) is low (e.g., 0 if the greedy policy assigns zero probability to action 2), reducing the update’s impact. This allows learning the optimal policy from exploratory trajectories, but high variance can occur if the policies diverge significantly. Weighted importance sampling (as in the code below) normalizes weights to reduce variance, and early termination (stopping when \\(\\rho_t = 0\\)) improves efficiency. 5.5 Key Differences Aspect SARSA (On-Policy) Q-Learning (Off-Policy) Off-Policy Monte Carlo Update Rule Uses \\(Q(s&#39;, a&#39;)\\), where \\(a&#39;\\) is sampled from the current policy. Uses \\(\\max_{a&#39;} Q(s&#39;, a&#39;)\\), assuming the optimal action. Uses \\(\\rho_t G_t\\), where \\(\\rho_t\\) reweights returns based on policy likelihoods. Policy Learning Learns the value of the policy being followed (including exploration). Learns the optimal policy, independent of exploration. Learns the optimal policy using importance sampling from exploratory trajectories. Exploration Impact Exploration affects learned Q-values. Exploration does not affect learned Q-values. Exploration affects returns, reweighted by importance sampling. Convergence Converges to the policy’s value if exploration decreases (e.g., \\(\\epsilon \\to 0\\)). Converges to the optimal policy even with fixed exploration. Converges to the optimal policy, but variance depends on policy similarity. Behavior More conservative, accounts for exploration risks. More aggressive, assumes optimal future actions. Aggressive, but variance can lead to unstable learning if policies differ significantly. # Common settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Helper function: Epsilon-greedy policy epsilon_greedy &lt;- function(Q, state, epsilon) { if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { which.max(Q[state, ]) } } # Helper function: Simulate environment simulate_step &lt;- function(state, action) { probs &lt;- transition_model[state, action, ] next_state &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[state, action, next_state] list(next_state = next_state, reward = reward) } # SARSA sarsa &lt;- function(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, n_states, n_actions) policy &lt;- rep(0, n_states) rewards &lt;- numeric(n_episodes) for (episode in 1:n_episodes) { state &lt;- sample(1:(n_states - 1), 1) action &lt;- epsilon_greedy(Q, state, epsilon) episode_reward &lt;- 0 while (state != terminal_state) { step &lt;- simulate_step(state, action) next_state &lt;- step$next_state reward &lt;- step$reward next_action &lt;- epsilon_greedy(Q, next_state, epsilon) Q[state, action] &lt;- Q[state, action] + alpha * ( reward + gamma * Q[next_state, next_action] - Q[state, action] ) state &lt;- next_state action &lt;- next_action episode_reward &lt;- episode_reward + reward } rewards[episode] &lt;- episode_reward } policy[1:(n_states - 1)] &lt;- apply(Q[1:(n_states - 1), ], 1, which.max) list(Q = Q, policy = policy, rewards = rewards) } This chunk sets up the simulated environment for the reinforcement learning agent. This environment is a Markov Decision Process (MDP). Common Settings: n_states &lt;- 10: Defines a world with 10 possible states. n_actions &lt;- 2: The agent can choose between 2 actions in any state. gamma &lt;- 0.9: Sets the discount factor to 0.9. This means rewards received in the future are valued slightly less than immediate rewards. terminal_state &lt;- n_states: State 10 is defined as the terminal state. Once the agent reaches this state, an episode ends. Environment Models: set.seed(42): This function ensures that any random numbers generated are the same every time the code is run, making the results reproducible. transition_model and reward_model: These are 3D arrays that define the environment’s dynamics. transition_model[s, a, s']: Stores the probability of transitioning to state s' after taking action a in state s. reward_model[s, a, s']: Stores the reward received when transitioning from state s to s' via action a. Populating the Models: The for loop defines the rules of the environment for all non-terminal states. For Action 1: There’s a 90% chance of moving to the next state sequentially (s + 1) and a 10% chance of being sent to a random state. This is a relatively predictable action. For Action 2: There’s an 80% chance of moving to one random state and a 20% chance of moving to another random state. This action is less predictable. Rewards: The agent gets a large positive reward (1.0 for Action 1, 0.5 for Action 2) for reaching the terminal state. All other transitions yield a small, random positive reward. Terminal State Logic: The final two lines ensure that if the agent is in the terminal state (n_states), all transition probabilities and rewards are zero, effectively ending the episode. epsilon_greedy: This function implements the ε-greedy strategy, which balances exploration and exploitation. Exploration: With a small probability epsilon (ε), the agent ignores what it has learned and chooses a random action. This helps discover new, potentially better, strategies. Exploitation: With probability 1 - epsilon, the agent chooses the action that it currently believes is the best, based on the highest action-value (Q) for the given state. which.max(Q[state, ]) finds the index (action) of the maximum value in that state’s row. simulate_step: This function simulates the agent taking a single step in the environment. It takes the current state and chosen action as input. It looks up the transition probabilities for that state-action pair from the transition_model. It then samples a next_state based on those probabilities. Finally, it retrieves the corresponding reward from the reward_model for that specific transition. It returns the next_state and reward as a list. This function implements the SARSA algorithm, which stands for State-Action-Reward-State-Action. It’s an on-policy temporal-difference learning algorithm. Initialization: It starts with a Q matrix (the action-value function) filled with zeros. Main Loop: The algorithm runs for a set number of n_episodes. Inside an Episode: The agent starts in a random state. It chooses an action using the ε-greedy policy. The agent takes the action, and the environment returns a reward and a next_state. Crucially for SARSA, it then chooses the next_action it will take from the next_state, again using the ε-greedy policy. The Q-value update is performed. It updates the value of the original state-action pair (Q[state, action]) using the tuple \\((S, A, R, S&#39;, A&#39;)\\). The update rule is: \\[ \\]\\[Q(S, A) \\\\leftarrow Q(S, A) + \\\\alpha [R + \\\\gamma Q(S&#39;, A&#39;) - Q(S, A)] \\] $$$\\(Here, `alpha` (\\)$) is the learning rate. This update moves the current estimate of Q[state, action] slightly towards the target value reward + gamma * Q[next_state, next_action]. Because the update uses the Q-value of the action (next_action) that the current policy actually chooses, it is considered “on-policy.” The agent moves to the next state (state &lt;- next_state), and the chosen next action becomes the current action (action &lt;- next_action). The loop continues until the terminal state is reached. Output: After all episodes, a final deterministic policy is created by choosing the action with the highest Q-value for each state. The function returns the learned Q table, the final policy, and the history of rewards. # Q-Learning q_learning &lt;- function(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, n_states, n_actions) policy &lt;- rep(0, n_states) rewards &lt;- numeric(n_episodes) for (episode in 1:n_episodes) { state &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 while (state != terminal_state) { action &lt;- epsilon_greedy(Q, state, epsilon) step &lt;- simulate_step(state, action) next_state &lt;- step$next_state reward &lt;- step$reward Q[state, action] &lt;- Q[state, action] + alpha * ( reward + gamma * max(Q[next_state, ]) - Q[state, action] ) state &lt;- next_state episode_reward &lt;- episode_reward + reward } rewards[episode] &lt;- episode_reward } policy[1:(n_states - 1)] &lt;- apply(Q[1:(n_states - 1), ], 1, which.max) list(Q = Q, policy = policy, rewards = rewards) } This function implements the Q-Learning algorithm. It is an off-policy temporal-difference learning algorithm. The structure is very similar to SARSA, with one critical difference. The Q-value Update: Unlike SARSA, Q-learning does not need to choose the next action (A') to perform its update. Instead, it updates its Q-value based on the maximum possible Q-value in the next state. This represents the value of acting optimally (greedily) from the next state, regardless of what the ε-greedy policy might do. The update rule is: \\[ \\]\\[Q(S, A) \\\\leftarrow Q(S, A) + \\\\alpha [R + \\\\gamma \\\\max\\_{a&#39;} Q(S&#39;, a&#39;) - Q(S, A)] \\] $$$$The term max(Q[next_state, ]) finds the best possible future value from next_state. Because the agent learns about the optimal policy while following a different (ε-greedy) policy, it is considered “off-policy.” # Off-Policy Monte Carlo off_policy_mc &lt;- function(n_episodes = 1000, epsilon = 0.1) { Q &lt;- matrix(0, n_states, n_actions) C &lt;- matrix(0, n_states, n_actions) # Cumulative weights policy &lt;- rep(0, n_states) rewards &lt;- numeric(n_episodes) for (episode in 1:n_episodes) { # Generate episode using behavior policy (epsilon-greedy) states &lt;- numeric(0) actions &lt;- numeric(0) rewards_ep &lt;- numeric(0) state &lt;- sample(1:(n_states - 1), 1) while (state != terminal_state) { action &lt;- sample(1:n_actions, 1) # Behavior policy: random step &lt;- simulate_step(state, action) next_state &lt;- step$next_state reward &lt;- step$reward states &lt;- c(states, state) actions &lt;- c(actions, action) rewards_ep &lt;- c(rewards_ep, reward) state &lt;- next_state } rewards[episode] &lt;- sum(rewards_ep) # Update Q using importance sampling G &lt;- 0 W &lt;- 1 for (t in length(states):1) { state &lt;- states[t] action &lt;- actions[t] reward &lt;- rewards_ep[t] G &lt;- gamma * G + reward C[state, action] &lt;- C[state, action] + W Q[state, action] &lt;- Q[state, action] + (W / C[state, action]) * (G - Q[state, action]) pi_action &lt;- which.max(Q[state, ]) if (action != pi_action) break W &lt;- W / (1 / n_actions) # Importance sampling ratio } } policy[1:(n_states - 1)] &lt;- apply(Q[1:(n_states - 1), ], 1, which.max) list(Q = Q, policy = policy, rewards = rewards) } This function implements an Off-Policy Monte Carlo (MC) control method using importance sampling. Monte Carlo Method: Unlike SARSA and Q-Learning, MC methods do not update values after every step. Instead, they run a complete episode and then update the values of the state-action pairs visited during that episode based on the total observed return. Off-Policy Learning: The goal is to learn the optimal (greedy) policy, called the target policy. However, to ensure exploration, the agent generates episodes using a different behavior policy. In this code, the behavior policy is to choose actions completely at random. Episode Generation: The first while loop generates a complete episode using the random behavior policy and stores all the states, actions, and rewards. Update with Importance Sampling: The second for loop iterates backward through the episode’s steps. G: This is the return, or the cumulative discounted reward from time step t to the end of the episode. W: This is the importance sampling ratio. It corrects for the fact that we are learning about the target policy while observing actions from the behavior policy. It’s the ratio of the probabilities of a trajectory occurring under the two policies. Q and C are updated using a weighted average formula. C accumulates the weights to ensure a stable average. if (action != pi_action) break: This is a key step. The target policy is greedy. If at any step the action taken under the random policy is not the one the greedy policy would have chosen, the probability of the rest of the trajectory under the target policy becomes zero. The importance sampling ratio W would become 0, so we can stop updating for this episode. # Value Iteration (from DP) value_iteration &lt;- function(transition_model, reward_model, gamma, epsilon = 1e-6, max_iter = 1000) { V &lt;- rep(0, n_states) policy &lt;- rep(0, n_states) delta &lt;- Inf iter &lt;- 0 while (delta &gt; epsilon &amp;&amp; iter &lt; max_iter) { delta &lt;- 0 V_old &lt;- V for (s in 1:(n_states - 1)) { Q &lt;- numeric(n_actions) for (a in 1:n_actions) { Q[a] &lt;- sum(transition_model[s, a, ] * (reward_model[s, a, ] + gamma * V)) } V[s] &lt;- max(Q) policy[s] &lt;- which.max(Q) delta &lt;- max(delta, abs(V[s] - V_old[s])) } iter &lt;- iter + 1 } # Evaluate DP policy rewards &lt;- numeric(1000) for (episode in 1:1000) { state &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 while (state != terminal_state) { action &lt;- policy[state] step &lt;- simulate_step(state, action) episode_reward &lt;- episode_reward + step$reward state &lt;- step$next_state } rewards[episode] &lt;- episode_reward } list(V = V, policy = policy, rewards = rewards) } This function implements Value Iteration, a classic Dynamic Programming (DP) algorithm. Model-Based: Unlike the previous methods, DP is model-based, meaning it requires full knowledge of the environment’s dynamics (the transition_model and reward_model). It doesn’t learn from interaction but calculates the optimal policy directly from the model. State-Value Function (V): Value Iteration computes the optimal state-value function, V(s), which is the expected return starting from state s and following the optimal policy thereafter. Main Loop: The algorithm repeatedly sweeps through all states. In each sweep, it updates the value of each state s using the Bellman optimality equation: \\[ \\]\\[V\\_{k+1}(s) \\\\leftarrow \\\\max\\_a \\\\sum\\_{s&#39;} P(s&#39;|s, a) [R(s,a,s&#39;) + \\\\gamma V\\_k(s&#39;)] \\] $$$$The code calculates the Q-value for each action (Q[a] &lt;- sum(...)) and then takes the maximum of these to update V[s]. The loop continues until the value function converges, meaning the maximum change (delta) in any state’s value between iterations is smaller than a tiny threshold epsilon. Policy Extraction: Once the optimal value function V is found, the optimal policy is extracted by choosing the action that maximizes the expected return from each state. Evaluation: Since DP doesn’t generate reward data during learning, a separate loop is run at the end to evaluate the performance of the found policy for fair comparison with the other algorithms. # Run algorithms set.seed(42) dp_result &lt;- value_iteration(transition_model, reward_model, gamma) sarsa_result &lt;- sarsa(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) qlearn_result &lt;- q_learning(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) mc_result &lt;- off_policy_mc(n_episodes = 1000, epsilon = 0.1) # Visualization library(ggplot2) library(gridExtra) # Policy comparison policy_df &lt;- data.frame( State = rep(1:n_states, 4), Policy = c(dp_result$policy, sarsa_result$policy, qlearn_result$policy, mc_result$policy), Algorithm = rep(c(&quot;DP&quot;, &quot;SARSA&quot;, &quot;Q-Learning&quot;, &quot;Off-Policy MC&quot;), each = n_states) ) policy_df$Policy[n_states * 0:3 + n_states] &lt;- NA # Terminal state policy_plot &lt;- ggplot(policy_df, aes(x = State, y = Policy, color = Algorithm)) + geom_point(size = 3) + geom_line(aes(group = Algorithm), na.rm = TRUE) + theme_minimal() + labs(title = &quot;Optimal Policies by Algorithm&quot;, x = &quot;State&quot;, y = &quot;Action&quot;) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;)) + theme(legend.position = &quot;bottom&quot;) # Reward comparison reward_df &lt;- data.frame( Episode = rep(1:1000, 4), Reward = c( cumsum(dp_result$rewards), cumsum(sarsa_result$rewards), cumsum(qlearn_result$rewards), cumsum(mc_result$rewards) ), Algorithm = rep(c(&quot;DP&quot;, &quot;SARSA&quot;, &quot;Q-Learning&quot;, &quot;Off-Policy MC&quot;), each = 1000) ) reward_plot &lt;- ggplot(reward_df, aes(x = Episode, y = Reward, color = Algorithm)) + geom_line() + theme_minimal() + labs(title = &quot;Cumulative Reward Comparison&quot;, x = &quot;Episode&quot;, y = &quot;Cumulative Reward&quot;) + theme(legend.position = &quot;bottom&quot;) # Display plots grid.arrange(policy_plot, reward_plot, ncol = 1) ## Warning: Removed 4 rows containing missing values or values outside the scale range (`geom_point()`). # Print performance metrics cat(&quot;Average Cumulative Reward per Episode:\\n&quot;) ## Average Cumulative Reward per Episode: cat(&quot;DP:&quot;, mean(dp_result$rewards), &quot;\\n&quot;) ## DP: 1.084506 cat(&quot;SARSA:&quot;, mean(sarsa_result$rewards), &quot;\\n&quot;) ## SARSA: 1.147303 cat(&quot;Q-Learning:&quot;, mean(qlearn_result$rewards), &quot;\\n&quot;) ## Q-Learning: 0.9781511 cat(&quot;Off-Policy MC:&quot;, mean(mc_result$rewards), &quot;\\n&quot;) ## Off-Policy MC: 1.081865 This final chunk executes all the defined algorithms and visualizes their results for comparison. Execution: Each of the four algorithms is run with specified parameters, and their results (policies, rewards, etc.) are stored. set.seed(42) is used again to ensure consistent starting conditions for all methods. Policy Comparison Plot: A data frame is created to hold the final policies from all algorithms. ggplot2 is used to create a plot that shows the action chosen by each algorithm for each state. This allows for a direct visual comparison of how the learned strategies differ. Reward Comparison Plot: A second data frame is created to track the cumulative reward over the training episodes. cumsum calculates the running total of rewards, which is a good indicator of learning performance over time. A line plot shows how the cumulative reward for each algorithm increases with each episode. A steeper slope indicates faster and more effective learning. Display and Metrics: grid.arrange from the gridExtra package combines the two plots into a single output. Finally, the average reward per episode is calculated and printed for each algorithm, providing a simple, quantitative summary of their overall performance. 5.6 Interpretation and Discussion 5.6.0.1 Policy Differences SARSA: As an on-policy method, it learns the value of the \\(\\epsilon\\)-greedy policy, which includes exploratory actions. In the 10-state environment, SARSA may balance between actions 1 and 2, reflecting the impact of random exploration, leading to a more conservative policy. Q-Learning: As an off-policy method, it learns the optimal policy, favoring action 1 in state 9 (higher terminal reward of 1.0) due to its greedy updates. Its policy is less sensitive to exploration noise, as it assumes optimal future actions. Off-Policy Monte Carlo: Also off-policy, it learns the optimal policy using importance sampling to reweight returns from a random behavior policy. It may align closely with Q-Learning’s policy but can exhibit variability due to high variance in importance sampling ratios, especially if the random policy frequently selects action 2 (lower reward). 5.6.0.2 Devaluation All methods exhibit habitual behavior without retraining, retaining their original policies after the terminal reward is removed. This highlights a limitation of model-free methods compared to model-based approaches (e.g., dynamic programming), which adapt instantly to reward changes. 5.6.0.3 Practical Implications SARSA: Better suited for environments where the exploration policy must be accounted for, such as safety-critical systems (e.g., robotics), where risky exploratory actions could lead to poor outcomes. Q-Learning: Ideal for scenarios where the optimal policy is desired regardless of exploration, such as games or simulations where exploration does not incur real-world costs. Off-Policy Monte Carlo: Suitable for offline learning from logged data (e.g., recommendation systems), but high variance can make it less stable than Q-Learning in dynamic environments. 5.6.0.4 Experimental Observations Before devaluation, Q-Learning and off-policy Monte Carlo likely favor action 1 in state 9 due to its higher terminal reward, while SARSA’s policy may show more variability due to exploration. After devaluation, all policies remain unchanged without retraining, illustrating their reliance on cached Q-values. Off-policy Monte Carlo’s performance depends on the similarity between the random behavior policy and the greedy target policy, with high variance potentially leading to less consistent policies compared to Q-Learning. 5.7 Conclusion SARSA, Q-Learning, and off-policy Monte Carlo represent distinct paradigms in model-free RL. SARSA’s on-policy updates reflect the exploration policy, making it conservative. Q-Learning’s off-policy updates target the optimal policy, ignoring exploration effects. Off-policy Monte Carlo uses importance sampling to learn from diverse trajectories, enabling offline learning but introducing variance. The R implementations demonstrate these differences in a 10-state environment, and the devaluation experiment underscores their habitual nature. Future posts could explore advanced topics, such as SARSA(\\(\\lambda\\)), deep RL extensions, or variance reduction in off-policy Monte Carlo. 5.8 Comparison Table Aspect SARSA (On-Policy) Q-Learning (Off-Policy) Off-Policy Monte Carlo Learning Approach Learns incrementally, updates based on action taken by behavior policy. Learns incrementally, updates based on best action in next state. Learns from complete episodes, using importance sampling. Update Rule \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma Q(s&#39;, a&#39;) - Q(s,a) \\right)\\) \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right)\\) \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( \\rho_t G_t - Q(s,a) \\right)\\) Episode Requirement Updates online, no episode completion needed. Updates online, no episode completion needed. Requires complete episodes for returns and importance weights. Bias and Variance Biased due to bootstrapping, moderate variance. Biased due to bootstrapping, lower variance. Unbiased but high variance due to importance sampling. Policy Type On-policy; learns value of behavior policy. Off-policy; learns optimal policy via max Q-value. Off-policy; learns greedy policy using importance sampling. Exploration Impact Exploration affects learned Q-values. Exploration does not affect learned Q-values. Exploration affects returns, reweighted by importance sampling. Convergence Converges to policy’s value if \\(\\epsilon \\to 0\\). Converges to optimal policy even with fixed \\(\\epsilon\\). Converges to optimal policy, but variance depends on policy similarity. Behavior Conservative, accounts for exploration risks. Aggressive, assumes optimal future actions. Aggressive, but variance can lead to instability. Example in Environment Balances actions 1 and 2, sensitive to exploration. Favors action 1 (higher reward) in state 9. Favors action 1, but variance may cause variability. "],["function-approximation-q-learning-with-linear-models.html", "Chapter 6 Function Approximation Q-Learning with Linear Models 6.1 Introduction 6.2 Theoretical Background 6.3 R Implementation", " Chapter 6 Function Approximation Q-Learning with Linear Models 6.1 Introduction In reinforcement learning (RL), tabular methods like SARSA and Q-Learning store a separate Q-value for each state-action pair, which becomes infeasible in large or continuous state spaces. Function approximation addresses this by representing the action-value function \\(Q(s, a)\\) as a parameterized function \\(Q(s, a; \\theta)\\), enabling generalization across states and scalability. This post explores Q-Learning with linear function approximation, using a 10-state, 2-action environment to demonstrate how it learns policies compared to tabular methods. We provide mathematical formulations, R code, and comparisons with tabular Q-Learning, focusing on generalization, scalability, and practical implications. 6.2 Theoretical Background Function approximation in RL aims to estimate the action-value function: \\[ Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a \\right] \\] where \\(\\gamma \\in [0,1]\\) is the discount factor, and \\(R_{t+1}\\) is the reward at time \\(t+1\\). Instead of storing \\(Q(s, a)\\) in a table, we approximate it as: \\[ Q(s, a; \\theta) = \\phi(s, a)^T \\theta \\] where \\(\\phi(s, a)\\) is a feature vector for state-action pair \\((s, a)\\), and \\(\\theta\\) is a parameter vector learned via optimization, typically stochastic gradient descent (SGD). 6.2.1 Q-Learning with Function Approximation Q-Learning with function approximation is an off-policy method that learns the optimal policy by updating \\(\\theta\\) to minimize the temporal difference (TD) error. The update rule is: \\[ \\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta Q(s, a; \\theta) \\] where \\(\\alpha\\) is the learning rate, and the TD error \\(\\delta\\) is: \\[ \\delta = r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;; \\theta) - Q(s, a; \\theta) \\] Here, \\(r\\) is the reward, \\(s&#39;\\) is the next state, and \\(\\max_{a&#39;} Q(s&#39;, a&#39;; \\theta)\\) estimates the value of the next state assuming the optimal action. For linear function approximation, the gradient is: \\[ \\nabla_\\theta Q(s, a; \\theta) = \\phi(s, a) \\] Thus, the update becomes: \\[ \\theta \\leftarrow \\theta + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;; \\theta) - Q(s, a; \\theta) \\right) \\phi(s, a) \\] In our 10-state environment, we use one-hot encoding for \\(\\phi(s, a)\\), mimicking tabular Q-Learning for simplicity but demonstrating the framework’s potential for generalization with more complex features. 6.2.2 Comparison with Tabular Q-Learning Tabular Q-Learning updates a table of Q-values directly: \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s, a) \\right) \\] Function approximation generalizes across states via \\(\\phi(s, a)\\), reducing memory requirements and enabling learning in large or continuous spaces. However, it introduces approximation errors and requires careful feature design to ensure convergence. Aspect Tabular Q-Learning Q-Learning with Function Approximation Representation Table of \\(Q(s, a)\\) values \\(Q(s, a; \\theta) = \\phi(s, a)^T \\theta\\) Memory \\(O(|\\mathcal{S}| \\cdot |\\mathcal{A}|)\\) \\(O(|\\theta|)\\), depends on feature size Generalization None; state-action specific Yes; depends on feature design Scalability Poor for large/continuous spaces Good for large/continuous spaces with proper features Update Rule Direct Q-value update Parameter update via gradient descent Convergence Guaranteed to optimal \\(Q^*\\) under conditions Converges to approximation of \\(Q^*\\); depends on features 6.3 R Implementation We implement Q-Learning with linear function approximation in a 10-state, 2-action environment, using one-hot encoding for \\(\\phi(s, a)\\). The environment mirrors the previous post, with action 1 yielding a 1.0 reward at the terminal state (state 10) and action 2 yielding 0.5. # Common settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } # Create one-hot features for (state, action) pairs create_features &lt;- function(s, a, n_states, n_actions) { vec &lt;- rep(0, n_states * n_actions) index &lt;- (a - 1) * n_states + s vec[index] &lt;- 1 return(vec) } # Initialize weights n_features &lt;- n_states * n_actions theta &lt;- rep(0, n_features) # Q-value approximation function q_hat &lt;- function(s, a, theta) { x &lt;- create_features(s, a, n_states, n_actions) return(sum(x * theta)) } # Q-Learning with function approximation q_learning_fa &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) { theta &lt;- rep(0, n_features) rewards &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 while (TRUE) { # Epsilon-greedy action selection a &lt;- if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { q_vals &lt;- sapply(1:n_actions, function(a_) q_hat(s, a_, theta)) which.max(q_vals) } out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime r &lt;- out$reward episode_reward &lt;- episode_reward + r # Compute TD target and error q_current &lt;- q_hat(s, a, theta) q_next &lt;- if (s_prime == terminal_state) 0 else max(sapply(1:n_actions, function(a_) q_hat(s_prime, a_, theta))) target &lt;- r + gamma * q_next error &lt;- target - q_current # Gradient update x &lt;- create_features(s, a, n_states, n_actions) theta &lt;- theta + alpha * error * x if (s_prime == terminal_state) break s &lt;- s_prime } rewards[ep] &lt;- episode_reward } # Derive policy policy &lt;- sapply(1:n_states, function(s) { if (s == terminal_state) NA else which.max(sapply(1:n_actions, function(a) q_hat(s, a, theta))) }) list(theta = theta, policy = policy, rewards = rewards) } # Run Q-Learning with function approximation set.seed(42) fa_result &lt;- q_learning_fa(episodes = 1000, alpha = 0.1, epsilon = 0.1) fa_policy &lt;- fa_result$policy fa_rewards &lt;- fa_result$rewards # Visualize policy library(ggplot2) policy_df &lt;- data.frame( State = 1:n_states, Policy = fa_policy, Algorithm = &quot;Q-Learning FA&quot; ) policy_df$Policy[n_states] &lt;- NA # Terminal state policy_plot &lt;- ggplot(policy_df, aes(x = State, y = Policy)) + geom_point(size = 3, color = &quot;deepskyblue&quot;) + geom_line(na.rm = TRUE, color = &quot;deepskyblue&quot;) + theme_minimal() + labs(title = &quot;Policy from Q-Learning with Function Approximation&quot;, x = &quot;State&quot;, y = &quot;Action&quot;) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;)) + theme(legend.position = &quot;none&quot;) # Visualize cumulative rewards reward_df &lt;- data.frame( Episode = 1:1000, Reward = cumsum(fa_rewards), Algorithm = &quot;Q-Learning FA&quot; ) policy_plot ## Warning: Removed 1 row containing missing values or values outside the scale range (`geom_point()`). Nice — I’ll walk through this chunk by chunk, explain what each piece does, point out subtle behaviors/bugs, and finish with a few short, practical fixes and suggestions. I’ll keep it factual and focused. 6.3.0.1 Top-level settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states n_states and n_actions: size of the MDP (10 states, 2 actions). gamma: discount factor (0.9). terminal_state: the index of the terminal state (state 10). Many later pieces use this to stop episodes and to set q_next = 0 for transitions into terminal. 6.3.0.2 Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 What this builds: transition_model[s, a, s'] is the probability of transitioning from state s with action a to next state s'. reward_model[s, a, s'] is the reward received for that transition. Important behavioral details and issues: Seeding: set.seed(42) fixes random draws so behavior is reproducible. Action 1 transitions: For non-terminal s the code assigns 0.9 to go deterministically to s+1 most of the time. Then it assigns 0.1 to a random state (sample(1:n_states, 1)). If the sampled random s' equals s+1 you end up with transition_model[s,1,s+1] = 0.9 + 0.1 = 1.0. If it’s different, two entries are non-zero (0.9 and 0.1). Action 2 transitions — probable bug / unintended overwrite: The code calls sample() twice for action 2, assigning 0.8 and then 0.2 to (possibly) two different random indices. That is allowed, but these two sample() calls are independent and may pick the same s' — if they pick the same index the entry becomes 1.0. More subtle: because the two sample() calls pick indices independently, it’s possible that only one index gets both probabilities or one gets 0.8 and another 0.2. This is probably not what the author intended (likely they wanted two different explicitly chosen next states). Normalization: transition_model rows are not explicitly normalized. In R, sample() will accept a prob vector that does not sum to 1 (it normalizes internally) — so sampling will still work even if rows don’t sum exactly to 1, provided the vector is non-negative and at least one positive entry exists. But for clarity and safety it’s better to ensure each (s,a,·) sums to 1. Rewards: For action 1: if s' is terminal (s_prime == n_states) reward = 1.0, else 0.1 * runif(1) (random small reward). For action 2: terminal reward 0.5, else 0.05 * runif(1) (even smaller). So action 1 gives larger terminal reward than action 2; also non-terminal rewards are noisy. Terminal state rows zeroed: transition_model[n_states,,] &lt;- 0 and reward_model[n_states,,] &lt;- 0 make terminal state absorbing with no outgoing transitions/rewards. The code later treats terminal specially (stops episodes when reached). 6.3.0.3 Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } Given (s, a), it samples s' according to the probability vector probs (R will normalize probabilities if they don’t sum to 1). Returns a list with s_prime and reward = reward_model[s,a,s_prime]. If probs is all zeros (possible for terminal state row), sample() will error — but the code avoids calling sample_env from terminal because episodes end when s_prime == terminal_state. Still, careful code should check for zero-prob rows. 6.3.0.4 Feature creation (one-hot for (state,action)) create_features &lt;- function(s, a, n_states, n_actions) { vec &lt;- rep(0, n_states * n_actions) index &lt;- (a - 1) * n_states + s vec[index] &lt;- 1 return(vec) } Creates a one-hot feature vector of length n_states * n_actions. The mapping index = (a - 1) * n_states + s puts action blocks in the feature vector: first n_states entries correspond to action 1, next n_states to action 2, etc. Because features are one-hot, linear function approximation q(s,a) = x^T theta is mathematically equivalent to a tabular Q-value representation (each (s,a) has its own weight in theta). 6.3.0.5 Weights and Q-value function n_features &lt;- n_states * n_actions theta &lt;- rep(0, n_features) q_hat &lt;- function(s, a, theta) { x &lt;- create_features(s, a, n_states, n_actions) return(sum(x * theta)) } theta holds parameters for the linear approximator; initialized at zeros. q_hat(s,a,theta) computes ( (s,a) = x^). With one-hot x this returns the single theta entry corresponding to (s,a). 6.3.0.6 Q-learning with function approximation q_learning_fa &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) { theta &lt;- rep(0, n_features) rewards &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 while (TRUE) { # Epsilon-greedy action selection a &lt;- if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { q_vals &lt;- sapply(1:n_actions, function(a_) q_hat(s, a_, theta)) which.max(q_vals) } out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime r &lt;- out$reward episode_reward &lt;- episode_reward + r # Compute TD target and error q_current &lt;- q_hat(s, a, theta) q_next &lt;- if (s_prime == terminal_state) 0 else max(sapply(1:n_actions, function(a_) q_hat(s_prime, a_, theta))) target &lt;- r + gamma * q_next error &lt;- target - q_current # Gradient update x &lt;- create_features(s, a, n_states, n_actions) theta &lt;- theta + alpha * error * x if (s_prime == terminal_state) break s &lt;- s_prime } rewards[ep] &lt;- episode_reward } # Derive policy policy &lt;- sapply(1:n_states, function(s) { if (s == terminal_state) NA else which.max(sapply(1:n_actions, function(a) q_hat(s, a, theta))) }) list(theta = theta, policy = policy, rewards = rewards) } Step-by-step explanation: Initialization: new theta vector and a rewards vector to store total reward per episode. Episode start: s &lt;- sample(1:(n_states - 1), 1) picks a random non-terminal start state for each episode. Action selection (epsilon-greedy): With probability epsilon pick a random action. Else compute Q-values q_hat(s,a,theta) for all actions and pick the action with maximum Q. Note: which.max() returns the index of the first maximum if there’s a tie. Interact with environment using sample_env. TD target and error: q_current is the estimate for (s,a). q_next is 0 if s_prime is terminal, otherwise max_a' q_hat(s', a'). target = r + gamma * q_next error = target - q_current Gradient (linear FA) update: For linear Q with features x, gradient of q_hat w.r.t theta is x. So the parameter update theta &lt;- theta + alpha * error * x is standard semi-gradient Q-learning. Because x is a one-hot, only the (s,a)-entry in theta is changed, identical to a tabular Q-learning update: ({s,a} {s,a} + (r + _{a’} Q(s’,a’) - Q(s,a)).) Episode termination: break if s_prime == terminal_state; otherwise continue from s &lt;- s_prime. Record episode total reward in rewards[ep]. After all episodes, derive greedy policy from final theta: for each non-terminal state pick argmax_a q_hat(s,a,theta). 6.3.0.7 Running the learner set.seed(42) fa_result &lt;- q_learning_fa(episodes = 1000, alpha = 0.1, epsilon = 0.1) fa_policy &lt;- fa_result$policy fa_rewards &lt;- fa_result$rewards Trains for 1000 episodes using alpha=0.1 and epsilon=0.1. Stores learned policy and episode rewards. 6.3.0.8 Visualization library(ggplot2) policy_df &lt;- data.frame( State = 1:n_states, Policy = fa_policy, Algorithm = &quot;Q-Learning FA&quot; ) policy_df$Policy[n_states] &lt;- NA # Terminal state policy_plot &lt;- ggplot(policy_df, aes(x = State, y = Policy)) + geom_point(size = 3, color = &quot;deepskyblue&quot;) + geom_line(na.rm = TRUE, color = &quot;deepskyblue&quot;) + theme_minimal() + labs(title = &quot;Policy from Q-Learning with Function Approximation&quot;, x = &quot;State&quot;, y = &quot;Action&quot;) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;)) + theme(legend.position = &quot;none&quot;) # Visualize cumulative rewards reward_df &lt;- data.frame( Episode = 1:1000, Reward = cumsum(fa_rewards), Algorithm = &quot;Q-Learning FA&quot; ) policy_plot policy_plot shows the greedy action (1 or 2) for each state as points/line. reward_df computes cumulative rewards, but note: the code builds reward_df but never plots it. Only policy_plot is shown at the end. If you want episode-by-episode reward or moving average, you’d need to add a ggplot call for reward_df (see suggestions below). 6.3.0.9 Overall algorithmic behavior &amp; complexity Because features are one-hot, this is function approximation that reduces exactly to tabular Q-learning. Per update cost: O(n_actions) for computing max over next-state actions. Per episode cost depends on episode length (random here), so total complexity roughly O(episodes * average_episode_length * n_actions). "],["beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html", "Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R 7.1 Introduction 7.2 Theoretical Background 7.3 R Implementation 7.4 Analysis and Insights 7.5 Comparison with Linear Approximation 7.6 Conclusion", " Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R 7.1 Introduction While linear function approximation provides a solid foundation for scaling reinforcement learning beyond tabular methods, it assumes a linear relationship between features and Q-values. Real-world problems often exhibit complex, non-linear patterns that linear models cannot capture effectively. This post extends our previous exploration by implementing Q-Learning with Random Forest function approximation, demonstrating how ensemble methods can learn intricate state-action value relationships while maintaining interpretability and robust generalization. Random Forests offer several advantages over linear approximation: they handle non-linear relationships naturally, provide built-in feature importance measures, resist overfitting through ensemble averaging, and require minimal hyperparameter tuning. We’ll implement this approach using the same 10-state, 2-action environment, comparing the learned policies and examining the unique characteristics of tree-based function approximation. 7.2 Theoretical Background Random Forest function approximation replaces the linear parameterization with an ensemble of decision trees. Instead of: \\[ Q(s, a; \\theta) = \\phi(s, a)^T \\theta \\] we now approximate the action-value function as: \\[ Q(s, a) = \\frac{1}{B} \\sum_{b=1}^{B} T_b(\\phi(s, a)) \\] where \\(T_b\\) represents the \\(b\\)-th tree in the ensemble, \\(B\\) is the number of trees, and \\(\\phi(s, a)\\) is our feature representation. Each tree \\(T_b\\) is trained on a bootstrap sample of the data with random feature subsets at each split, providing natural regularization and variance reduction. 7.2.1 Q-Learning with Random Forest Approximation The Q-Learning update process with Random Forest approximation involves: Experience Collection: Gather state-action-reward-next state tuples \\((s, a, r, s&#39;)\\) Target Computation: Calculate TD targets \\(y = r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;)\\) Model Training: Fit Random Forest regressor to predict \\(Q(s, a)\\) from features \\(\\phi(s, a)\\) Policy Update: Use updated model for epsilon-greedy action selection Unlike linear methods with continuous parameter updates, Random Forest approximation requires periodic model retraining on accumulated experience. This batch-like approach trades computational efficiency for modeling flexibility. 7.2.2 Feature Engineering for Tree-Based Models For our implementation, we use a simple concatenation of one-hot encoded state and action vectors: \\[ \\phi(s, a) = [e_s^{(state)} \\; || \\; e_a^{(action)}] \\] where \\(e_s^{(state)}\\) is a one-hot vector for state \\(s\\) and \\(e_a^{(action)}\\) is a one-hot vector for action \\(a\\). This encoding allows trees to learn complex interactions between states and actions while maintaining interpretability. 7.2.3 Comparison with Previous Methods Aspect Tabular Q-Learning Linear Function Approximation Random Forest Function Approximation Model Complexity None; direct storage Linear combinations Non-linear ensemble Feature Interactions Implicit None (unless engineered) Automatic discovery Interpretability Full Moderate (weights) High (tree structures) Training Online updates Gradient descent Batch retraining Overfitting Risk None Low Low (ensemble averaging) Computational Cost \\(O(1)\\) lookup \\(O(d)\\) linear algebra \\(O(B \\cdot \\log n)\\) prediction 7.3 R Implementation Our implementation builds upon the previous environment while introducing Random Forest-based Q-value approximation. The key innovation lies in accumulating training examples and periodically retraining the forest to incorporate new experience. # Load required libraries library(randomForest) library(ggplot2) # Environment setup (same as previous implementation) n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } # Feature encoding for Random Forest encode_features &lt;- function(s, a, n_states, n_actions) { state_vec &lt;- rep(0, n_states) action_vec &lt;- rep(0, n_actions) state_vec[s] &lt;- 1 action_vec[a] &lt;- 1 return(c(state_vec, action_vec)) } n_features &lt;- n_states + n_actions # Q-Learning with Random Forest function approximation q_learning_rf &lt;- function(episodes = 1000, epsilon = 0.1, retrain_freq = 10, min_samples = 50) { # Initialize training data storage rf_data_x &lt;- matrix(nrow = 0, ncol = n_features) rf_data_y &lt;- numeric(0) rf_model &lt;- NULL rewards &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- sample(1:(n_states - 1), 1) # Start from non-terminal state episode_reward &lt;- 0 while (TRUE) { # Predict Q-values for all actions q_preds &lt;- sapply(1:n_actions, function(a) { x &lt;- encode_features(s, a, n_states, n_actions) if (!is.null(rf_model)) { predict(rf_model, as.data.frame(t(x))) } else { runif(1) # Random initialization } }) # Epsilon-greedy action selection a &lt;- if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { which.max(q_preds) } # Take action and observe outcome out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime r &lt;- out$reward episode_reward &lt;- episode_reward + r # Compute TD target q_next &lt;- if (s_prime == terminal_state) { 0 } else { max(sapply(1:n_actions, function(a_) { x_next &lt;- encode_features(s_prime, a_, n_states, n_actions) if (!is.null(rf_model)) { predict(rf_model, as.data.frame(t(x_next))) } else { 0 } })) } target &lt;- r + gamma * q_next # Store training example x &lt;- encode_features(s, a, n_states, n_actions) rf_data_x &lt;- rbind(rf_data_x, x) rf_data_y &lt;- c(rf_data_y, target) # Retrain Random Forest periodically if (nrow(rf_data_x) &gt;= min_samples &amp;&amp; ep %% retrain_freq == 0) { rf_model &lt;- randomForest( x = as.data.frame(rf_data_x), y = rf_data_y, ntree = 100, nodesize = 5, mtry = max(1, floor(n_features / 3)) ) } if (s_prime == terminal_state) break s &lt;- s_prime } rewards[ep] &lt;- episode_reward } # Derive final policy policy &lt;- sapply(1:(n_states-1), function(s) { if (!is.null(rf_model)) { q_vals &lt;- sapply(1:n_actions, function(a) { x &lt;- encode_features(s, a, n_states, n_actions) predict(rf_model, as.data.frame(t(x))) }) which.max(q_vals) } else { 1 # Default action } }) list(model = rf_model, policy = c(policy, NA), rewards = rewards, training_data = list(x = rf_data_x, y = rf_data_y)) } # Run Q-Learning with Random Forest approximation set.seed(42) rf_result &lt;- q_learning_rf(episodes = 1000, epsilon = 0.1, retrain_freq = 10) rf_policy &lt;- rf_result$policy rf_rewards &lt;- rf_result$rewards # Create policy visualization policy_df &lt;- data.frame( State = 1:n_states, Policy = rf_policy, Algorithm = &quot;Q-Learning RF&quot; ) policy_plot_rf &lt;- ggplot(policy_df[1:(n_states-1), ], aes(x = State, y = Policy)) + geom_point(size = 4, color = &quot;forestgreen&quot;) + geom_line(color = &quot;forestgreen&quot;, linewidth = 1) + theme_minimal() + labs( title = &quot;Policy from Q-Learning with Random Forest Approximation&quot;, x = &quot;State&quot;, y = &quot;Action&quot; ) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;), limits = c(0.5, 2.5)) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) # Compare cumulative rewards with moving average rewards_smooth &lt;- numeric(length(rf_rewards)) window_size &lt;- 50 for (i in 1:length(rf_rewards)) { start_idx &lt;- max(1, i - window_size + 1) rewards_smooth[i] &lt;- mean(rf_rewards[start_idx:i]) } reward_df_rf &lt;- data.frame( Episode = 1:1000, Reward = rewards_smooth, Algorithm = &quot;Q-Learning RF&quot; ) reward_plot_rf &lt;- ggplot(reward_df_rf, aes(x = Episode, y = Reward)) + geom_line(color = &quot;forestgreen&quot;, linewidth = 1) + theme_minimal() + labs( title = &quot;Learning Curve: Q-Learning with Random Forest (50-episode moving average)&quot;, x = &quot;Episode&quot;, y = &quot;Average Reward&quot; ) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) # Display plots print(policy_plot_rf) print(reward_plot_rf) # Feature importance analysis if (!is.null(rf_result$model)) { importance_df &lt;- data.frame( Feature = c(paste(&quot;State&quot;, 1:n_states), paste(&quot;Action&quot;, 1:n_actions)), Importance = importance(rf_result$model)[, 1] ) importance_plot &lt;- ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) + geom_col(fill = &quot;forestgreen&quot;, alpha = 0.7) + coord_flip() + theme_minimal() + labs( title = &quot;Feature Importance in Random Forest Q-Function&quot;, x = &quot;Feature&quot;, y = &quot;Importance (Mean Decrease in MSE)&quot; ) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) print(importance_plot) } # Model diagnostics cat(&quot;Random Forest Model Summary:\\n&quot;) cat(&quot;Number of trees:&quot;, rf_result$model$ntree, &quot;\\n&quot;) cat(&quot;Training examples:&quot;, nrow(rf_result$training_data$x), &quot;\\n&quot;) cat(&quot;Final OOB error:&quot;, tail(rf_result$model$mse, 1), &quot;\\n&quot;) 7.4 Analysis and Insights 7.4.1 Policy Learning Characteristics Random Forest function approximation exhibits several characteristics that distinguish it from linear methods. Trees can capture non-linear decision boundaries, enabling the model to learn state-action relationships that linear approaches cannot represent. The random feature sampling at each split performs automatic feature selection, focusing computational resources on the most informative variables. Ensemble averaging across multiple trees reduces overfitting and provides stable predictions across different training samples. Individual trees maintain interpretable decision paths that show how Q-values are estimated for specific state-action pairs. 7.4.2 Computational Considerations The batch retraining approach creates distinct computational trade-offs that affect implementation decisions. Training frequency must balance responsiveness against computational cost, as more frequent updates improve adaptation but require additional processing time. Trees need sufficient data to learn meaningful patterns, which can slow initial learning compared to methods that update continuously. Memory requirements increase over time as training examples accumulate, requiring careful management of historical data. 7.4.3 Feature Importance Insights Random Forest methods naturally generate feature importance measures that reveal which states and actions most influence Q-value predictions. This interpretability provides diagnostic capabilities for understanding learning issues and analyzing policy decisions. The feature ranking can guide state representation choices and help identify redundant or irrelevant variables in the problem formulation. 7.4.4 Practical Implications Random Forest function approximation occupies a position between simple linear models and neural networks in terms of complexity and capability. The method handles larger state spaces more effectively than tabular approaches while remaining computationally tractable. It captures non-linear patterns without requiring extensive feature engineering or domain expertise. The approach shows less sensitivity to hyperparameter choices compared to neural networks while maintaining stability across different problem instances. The inherent interpretability provides insights into the decision-making process that can be valuable for debugging and analysis. 7.5 Comparison with Linear Approximation Random Forest methods demonstrate several advantages and trade-offs when compared to linear function approximation. The tree-based approach excels at pattern recognition, learning state-action relationships that linear models cannot capture due to their representational limitations. However, initial learning proceeds more slowly as trees require sufficient data to construct meaningful decision boundaries. Computational costs are higher due to periodic retraining requirements, contrasting with the continuous gradient updates used in linear methods. Generalization performance tends to be superior, as ensemble averaging provides natural regularization that reduces overfitting tendencies. 7.6 Conclusion Random Forest function approximation extends linear methods by offering enhanced modeling flexibility while preserving interpretability characteristics. The approach performs particularly well in environments with non-linear state-action relationships and provides regularization through ensemble averaging. Several key observations emerge from this analysis. Non-linear function approximation can capture patterns that linear models miss, enabling better policy learning in complex environments. Batch learning approaches require careful consideration of training frequency and sample requirements to balance performance with computational efficiency. Feature importance analysis provides insights into learned policies that can guide problem formulation and debugging efforts. Tree-based methods offer an interpretable alternative to neural network approaches while maintaining theoretical foundations. This exploration demonstrates how ensemble methods can enhance reinforcement learning without abandoning the established principles of Q-Learning. Future work could investigate online tree learning algorithms that avoid batch retraining requirements, adaptive schedules that optimize training frequency based on performance metrics, or hybrid approaches that combine strengths from different function approximation methods. "],["deep-function-approximation-q-learning-with-neural-networks-in-r.html", "Chapter 8 Deep Function Approximation: Q-Learning with Neural Networks in R 8.1 Introduction 8.2 Theoretical Foundation 8.3 R Implementation 8.4 Analysis and Interpretation 8.5 Practical Considerations 8.6 Comparison Across Function Approximation Methods 8.7 Future Directions 8.8 Conclusion", " Chapter 8 Deep Function Approximation: Q-Learning with Neural Networks in R 8.1 Introduction Our exploration of function approximation in reinforcement learning has progressed from linear models to ensemble methods, each offering increasing sophistication in capturing complex relationships between states, actions, and their values. Neural networks represent the natural next step in this evolution, providing the theoretical foundation for modern deep reinforcement learning while maintaining practical implementability in R. Neural network function approximation transcends the limitations of both linear models and tree-based methods by learning hierarchical feature representations automatically. Where linear models assume additive relationships and Random Forests rely on axis-aligned splits, neural networks can discover arbitrary non-linear transformations of the input space. This capability proves particularly valuable in reinforcement learning, where the optimal action-value function often exhibits complex dependencies that resist simple parametric forms. This post demonstrates Q-Learning with neural network function approximation using R’s nnet package, continuing our 10-state environment while examining how artificial neural networks learn Q-value approximations. We explore the theoretical foundations, implementation challenges, and practical considerations that distinguish neural network approaches from their predecessors. 8.2 Theoretical Foundation Neural network function approximation replaces our previous parameterizations with a multi-layered composition of non-linear transformations. The action-value function becomes: \\[ Q(s, a; \\theta) = f_L(W_L f_{L-1}(W_{L-1} \\cdots f_1(W_1 \\phi(s, a) + b_1) \\cdots + b_{L-1}) + b_L) \\] where \\(f_i\\) represents the activation function at layer \\(i\\), \\(W_i\\) and \\(b_i\\) are weight matrices and bias vectors, and \\(\\theta = \\{W_1, b_1, \\ldots, W_L, b_L\\}\\) encompasses all trainable parameters. This hierarchical structure enables the network to learn increasingly abstract representations of the state-action space. 8.2.1 Universal Approximation and Expressivity The theoretical appeal of neural networks stems from universal approximation theorems, which guarantee that feedforward networks with sufficient hidden units can approximate any continuous function to arbitrary precision. In the context of Q-Learning, this suggests that neural networks can, in principle, represent any action-value function arising from a Markov decision process. For our implementation, we employ a single hidden layer architecture with sigmoid activation functions: \\[ Q(s, a; \\theta) = W_2 \\sigma(W_1 \\phi(s, a) + b_1) + b_2 \\] where \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function, providing the non-linearity necessary for complex function approximation. 8.2.2 Gradient-Based Learning Neural network training relies on backpropagation to compute gradients of the temporal difference error with respect to all network parameters. The loss function for a single transition becomes: \\[ L(\\theta) = \\frac{1}{2}(y - Q(s, a; \\theta))^2 \\] where \\(y = r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;; \\theta)\\) is the TD target. The gradient with respect to parameters \\(\\theta\\) follows the chain rule: \\[ \\nabla_\\theta L(\\theta) = (Q(s, a; \\theta) - y) \\nabla_\\theta Q(s, a; \\theta) \\] This gradient guides parameter updates through standard optimization algorithms, though the non-convex nature of neural network loss surfaces introduces challenges absent in linear approximation. 8.2.3 Comparison with Previous Approaches Neural networks offer several theoretical advantages over linear and tree-based methods. Unlike linear approximation, they can learn feature interactions without explicit engineering. Unlike Random Forests, they provide smooth function approximations suitable for gradient-based optimization. However, this flexibility comes with increased computational complexity and potential instability during training. Characteristic Linear Approximation Random Forest Neural Network Function Class Linear combinations Piecewise constant Universal approximators Feature Learning None Implicit via splits Explicit representation learning Optimization Convex (guaranteed convergence) Non-parametric Non-convex (local minima) Interpretability High (weight inspection) Moderate (tree visualization) Low (distributed representations) Sample Efficiency High Moderate Variable (depends on architecture) 8.3 R Implementation Our neural network implementation builds upon the established environment while introducing the complexities of gradient-based optimization and network training. The nnet package provides a lightweight implementation suitable for demonstrating core concepts without the overhead of deep learning frameworks. # Load required libraries library(nnet) library(ggplot2) # Environment setup (consistent with previous implementations) n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } # Feature encoding for neural network input encode_features &lt;- function(s, a, n_states, n_actions) { state_vec &lt;- rep(0, n_states) action_vec &lt;- rep(0, n_actions) state_vec[s] &lt;- 1 action_vec[a] &lt;- 1 return(c(state_vec, action_vec)) } n_features &lt;- n_states + n_actions # Q-Learning with neural network function approximation q_learning_nn &lt;- function(episodes = 1000, epsilon = 0.1, hidden_size = 10, retrain_freq = 10, min_samples = 50) { # Initialize training data storage q_data_x &lt;- matrix(nrow = 0, ncol = n_features) q_data_y &lt;- numeric(0) q_model &lt;- NULL rewards &lt;- numeric(episodes) training_losses &lt;- numeric() for (ep in 1:episodes) { s &lt;- sample(1:(n_states - 1), 1) # Start from non-terminal state episode_reward &lt;- 0 while (TRUE) { # Predict Q-values for all actions q_preds &lt;- sapply(1:n_actions, function(a) { x &lt;- encode_features(s, a, n_states, n_actions) if (!is.null(q_model)) { as.numeric(predict(q_model, as.data.frame(t(x)))) } else { runif(1) # Random initialization } }) # Epsilon-greedy action selection a &lt;- if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { which.max(q_preds) } # Take action and observe outcome out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime r &lt;- out$reward episode_reward &lt;- episode_reward + r # Compute TD target q_next &lt;- if (s_prime == terminal_state) { 0 } else { max(sapply(1:n_actions, function(a_) { x_next &lt;- encode_features(s_prime, a_, n_states, n_actions) if (!is.null(q_model)) { as.numeric(predict(q_model, as.data.frame(t(x_next)))) } else { 0 } })) } target &lt;- r + gamma * q_next # Store training example x &lt;- encode_features(s, a, n_states, n_actions) q_data_x &lt;- rbind(q_data_x, x) q_data_y &lt;- c(q_data_y, target) # Train neural network periodically if (nrow(q_data_x) &gt;= min_samples &amp;&amp; ep %% retrain_freq == 0) { # Suppress nnet output for cleaner execution capture.output({ q_model &lt;- nnet( x = q_data_x, y = q_data_y, size = hidden_size, linout = TRUE, maxit = 100, decay = 0.01, trace = FALSE ) }) # Track training loss if (!is.null(q_model)) { predictions &lt;- predict(q_model, as.data.frame(q_data_x)) mse &lt;- mean((predictions - q_data_y)^2) training_losses &lt;- c(training_losses, mse) } } if (s_prime == terminal_state) break s &lt;- s_prime } rewards[ep] &lt;- episode_reward } # Derive final policy policy &lt;- sapply(1:(n_states-1), function(s) { if (!is.null(q_model)) { q_vals &lt;- sapply(1:n_actions, function(a) { x &lt;- encode_features(s, a, n_states, n_actions) as.numeric(predict(q_model, as.data.frame(t(x)))) }) which.max(q_vals) } else { 1 # Default action } }) list(model = q_model, policy = c(policy, NA), rewards = rewards, training_losses = training_losses, training_data = list(x = q_data_x, y = q_data_y)) } # Run Q-Learning with neural network approximation set.seed(42) nn_result &lt;- q_learning_nn(episodes = 1000, epsilon = 0.1, hidden_size = 10) nn_policy &lt;- nn_result$policy nn_rewards &lt;- nn_result$rewards # Visualize learned policy policy_df &lt;- data.frame( State = 1:n_states, Policy = nn_policy, Algorithm = &quot;Q-Learning NN&quot; ) policy_plot_nn &lt;- ggplot(policy_df[1:(n_states-1), ], aes(x = State, y = Policy)) + geom_point(size = 4, color = &quot;coral&quot;) + geom_line(color = &quot;coral&quot;, linewidth = 1) + theme_minimal() + labs( title = &quot;Policy from Q-Learning with Neural Network Approximation&quot;, x = &quot;State&quot;, y = &quot;Action&quot; ) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;), limits = c(0.5, 2.5)) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) # Learning curve with smoothing rewards_smooth &lt;- numeric(length(nn_rewards)) window_size &lt;- 50 for (i in 1:length(nn_rewards)) { start_idx &lt;- max(1, i - window_size + 1) rewards_smooth[i] &lt;- mean(nn_rewards[start_idx:i]) } reward_df_nn &lt;- data.frame( Episode = 1:1000, Reward = rewards_smooth, Algorithm = &quot;Q-Learning NN&quot; ) reward_plot_nn &lt;- ggplot(reward_df_nn, aes(x = Episode, y = Reward)) + geom_line(color = &quot;coral&quot;, linewidth = 1) + theme_minimal() + labs( title = &quot;Learning Curve: Q-Learning with Neural Network (50-episode moving average)&quot;, x = &quot;Episode&quot;, y = &quot;Average Reward&quot; ) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) # Training loss evolution if (length(nn_result$training_losses) &gt; 0) { loss_df &lt;- data.frame( Update = 1:length(nn_result$training_losses), Loss = nn_result$training_losses ) loss_plot &lt;- ggplot(loss_df, aes(x = Update, y = Loss)) + geom_line(color = &quot;darkred&quot;, linewidth = 1) + theme_minimal() + labs( title = &quot;Neural Network Training Loss Evolution&quot;, x = &quot;Training Update&quot;, y = &quot;Mean Squared Error&quot; ) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) print(loss_plot) } # Display main plots print(policy_plot_nn) print(reward_plot_nn) # Model diagnostics and analysis if (!is.null(nn_result$model)) { cat(&quot;Neural Network Model Summary:\\n&quot;) cat(&quot;Architecture: Input(&quot;, n_features, &quot;) -&gt; Hidden(&quot;, nn_result$model$n[2], &quot;) -&gt; Output(1)\\n&quot;) cat(&quot;Total parameters:&quot;, length(nn_result$model$wts), &quot;\\n&quot;) cat(&quot;Training examples:&quot;, nrow(nn_result$training_data$x), &quot;\\n&quot;) cat(&quot;Final training loss:&quot;, tail(nn_result$training_losses, 1), &quot;\\n&quot;) # Weight analysis weights &lt;- nn_result$model$wts cat(&quot;Weight statistics:\\n&quot;) cat(&quot;Mean:&quot;, round(mean(weights), 4), &quot;\\n&quot;) cat(&quot;Standard deviation:&quot;, round(sd(weights), 4), &quot;\\n&quot;) cat(&quot;Range: [&quot;, round(min(weights), 4), &quot;,&quot;, round(max(weights), 4), &quot;]\\n&quot;) } 8.4 Analysis and Interpretation 8.4.1 Learning Dynamics Neural network function approximation introduces several unique characteristics compared to linear and tree-based methods. The non-convex optimization landscape means that training can exhibit complex dynamics, including periods of rapid improvement followed by plateaus. The learning curve often shows more volatility than linear methods due to the continuous parameter updates and potential for local minima. 8.4.2 Function Representation Unlike Random Forests that learn piecewise constant approximations, neural networks produce smooth function approximations. This continuity can be advantageous for policy learning, as small changes in state typically result in small changes in Q-values. The hidden layer learns intermediate representations that capture relevant features for action-value estimation. 8.4.3 Generalization Properties Neural networks excel at discovering relevant patterns in the state-action space without explicit feature engineering. The hidden units automatically learn combinations of input features that prove useful for Q-value prediction. This automatic feature discovery becomes increasingly valuable as problem complexity grows. 8.4.4 Training Stability The batch retraining approach helps stabilize learning compared to online neural network updates, which can suffer from catastrophic forgetting. However, the periodic retraining introduces discontinuities in the learned function that can temporarily disrupt policy performance. 8.5 Practical Considerations 8.5.1 Architecture Selection The choice of network architecture significantly impacts performance. Too few hidden units may underfit the true Q-function, while too many can lead to overfitting with limited training data. Our single hidden layer with 10 units provides a reasonable balance for the 10-state environment. 8.5.2 Training Frequency The retraining frequency presents a trade-off between computational efficiency and learning responsiveness. More frequent retraining provides better adaptation to new experience but increases computational cost. The optimal frequency depends on the environment complexity and available computational resources. 8.5.3 Regularization Neural networks benefit from regularization techniques to prevent overfitting. Our implementation includes weight decay (L2 regularization) to encourage smaller weights and improve generalization. Other techniques like dropout or early stopping could further enhance performance. 8.5.4 Initialization and Convergence Neural network training depends critically on weight initialization and optimization parameters. Poor initialization can trap the network in suboptimal local minima, while inappropriate learning rates can cause divergence or slow convergence. 8.6 Comparison Across Function Approximation Methods Our progression from tabular to linear to ensemble to neural network methods illustrates the evolution of function approximation in reinforcement learning. Each method offers distinct advantages for different problem characteristics. Tabular methods provide exact representation but fail to scale. Linear methods offer guaranteed convergence and interpretability but assume additive relationships. Random Forests handle non-linearities while maintaining interpretability but produce discontinuous approximations. Neural networks provide universal approximation capabilities and smooth functions but introduce optimization challenges and reduced interpretability. The choice among methods depends on problem requirements, available data, computational constraints, and interpretability needs. Neural networks shine when function complexity exceeds simpler methods’ capabilities and sufficient training data is available. 8.7 Future Directions This exploration establishes the foundation for more advanced neural network approaches in reinforcement learning. Extensions could include deeper architectures, convolutional networks for spatial problems, recurrent networks for partially observable environments, or modern techniques like attention mechanisms. The theoretical framework developed here scales naturally to these more complex architectures, with the core principles of temporal difference learning and gradient-based optimization remaining constant while the function approximation capabilities expand dramatically. 8.8 Conclusion Neural network function approximation represents a significant step toward the sophisticated methods underlying modern deep reinforcement learning. While maintaining the theoretical foundations of Q-Learning, neural networks provide the flexibility to tackle complex environments that challenge simpler approximation methods. The implementation demonstrates how classical reinforcement learning principles extend naturally to neural network settings, preserving core algorithmic structure while enhancing representational power. This foundation enables practitioners to understand and implement more advanced methods building on these fundamental concepts. The journey through different function approximation approaches reveals the rich landscape of reinforcement learning methods, each contributing unique insights and capabilities. Neural networks, as universal approximators, provide the theoretical and practical foundation for tackling increasingly complex decision-making problems across diverse domains. "],["dyna-and-dynaq.html", "Chapter 9 Dyna and DynaQ 9.1 Introduction 9.2 Theoretical Framework 9.3 Implementation in R 9.4 Experimental Analysis 9.5 Discussion 9.6 Implementation Considerations and Conclusion", " Chapter 9 Dyna and DynaQ 9.1 Introduction Traditional reinforcement learning methods fall into two categories: model-free approaches like SARSA and Q-Learning that learn directly from experience, and model-based methods that first learn environment dynamics then use planning algorithms. Dyna, introduced by Sutton (1990), bridges this gap by combining direct reinforcement learning with indirect learning through an internal model of the environment. The key insight behind Dyna is that real experience can serve dual purposes: updating value functions directly and improving an internal model that generates simulated experience for additional learning. This architecture allows agents to benefit from both the sample efficiency of planning and the robustness of direct learning, making it particularly effective in environments where experience is costly or limited. 9.2 Theoretical Framework 9.2.1 The Dyna Architecture Dyna integrates three key components within a unified learning system: Direct RL: Learning from real experience using standard temporal difference methods Model Learning: Building an internal model of environment dynamics from experience Planning: Using the learned model to generate simulated experience for additional value function updates The complete Dyna update cycle can be formalized as follows. For each real experience tuple \\((s, a, r, s&#39;)\\): Direct Learning (Q-Learning): \\[Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right]\\] Model Update: \\[\\hat{T}(s,a) \\leftarrow s&#39;\\] \\[\\hat{R}(s,a) \\leftarrow r\\] Planning Phase (repeat \\(n\\) times): \\[s \\leftarrow \\text{random previously visited state}\\] \\[a \\leftarrow \\text{random action previously taken in } s\\] \\[r \\leftarrow \\hat{R}(s,a)\\] \\[s&#39; \\leftarrow \\hat{T}(s,a)\\] \\[Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right]\\] The parameter \\(n\\) controls the number of planning steps per real experience, representing the computational budget available for internal simulation. 9.2.2 Model Representation In its simplest form, Dyna uses a deterministic table-based model where \\(\\hat{T}(s,a)\\) stores the last observed next state for state-action pair \\((s,a)\\), and \\(\\hat{R}(s,a)\\) stores the last observed reward. This approach works well for deterministic environments but can be extended to handle stochastic dynamics through sampling-based representations. 9.2.3 Convergence Properties Under standard assumptions (all state-action pairs visited infinitely often, appropriate learning rates), Dyna inherits the convergence guarantees of its underlying RL algorithm. The addition of planning typically accelerates convergence by allowing each real experience to propagate information more widely through the value function. 9.3 Implementation in R We implement Dyna-Q using the same 10-state environment from previous posts, allowing direct comparison with pure Q-Learning and SARSA approaches. 9.3.1 Environment Setup # Environment parameters n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Transition and reward models (same as previous posts) set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Environment interaction function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } 9.3.2 Dyna-Q Implementation # Dyna-Q algorithm with episode-wise performance tracking dyna_q &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1, n_planning = 5) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) model_T &lt;- array(NA, dim = c(n_states, n_actions)) model_R &lt;- array(NA, dim = c(n_states, n_actions)) visited_sa &lt;- list() episode_rewards &lt;- numeric(episodes) episode_steps &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- 1 total_reward &lt;- 0 steps &lt;- 0 while (s != terminal_state &amp;&amp; steps &lt; 100) { # Add max steps to prevent infinite loops if (runif(1) &lt; epsilon) { a &lt;- sample(1:n_actions, 1) } else { a &lt;- which.max(Q[s, ]) } outcome &lt;- sample_env(s, a) s_prime &lt;- outcome$s_prime r &lt;- outcome$reward total_reward &lt;- total_reward + r steps &lt;- steps + 1 Q[s, a] &lt;- Q[s, a] + alpha * (r + gamma * max(Q[s_prime, ]) - Q[s, a]) model_T[s, a] &lt;- s_prime model_R[s, a] &lt;- r sa_key &lt;- paste(s, a, sep = &quot;_&quot;) if (!(sa_key %in% names(visited_sa))) { visited_sa[[sa_key]] &lt;- c(s, a) } if (length(visited_sa) &gt; 0) { for (i in 1:n_planning) { sa_sample &lt;- sample(visited_sa, 1)[[1]] s_plan &lt;- sa_sample[1] a_plan &lt;- sa_sample[2] if (!is.na(model_T[s_plan, a_plan])) { s_prime_plan &lt;- model_T[s_plan, a_plan] r_plan &lt;- model_R[s_plan, a_plan] Q[s_plan, a_plan] &lt;- Q[s_plan, a_plan] + alpha * (r_plan + gamma * max(Q[s_prime_plan, ]) - Q[s_plan, a_plan]) } } } s &lt;- s_prime } episode_rewards[ep] &lt;- total_reward episode_steps[ep] &lt;- steps } list(Q = Q, policy = apply(Q, 1, which.max), episode_rewards = episode_rewards, episode_steps = episode_steps) } 9.3.3 Standard Q-Learning for Comparison # Q-Learning with performance tracking q_learning &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) episode_rewards &lt;- numeric(episodes) episode_steps &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- 1 total_reward &lt;- 0 steps &lt;- 0 while (s != terminal_state &amp;&amp; steps &lt; 100) { a &lt;- if (runif(1) &lt; epsilon) sample(1:n_actions, 1) else which.max(Q[s, ]) outcome &lt;- sample_env(s, a) s_prime &lt;- outcome$s_prime r &lt;- outcome$reward total_reward &lt;- total_reward + r steps &lt;- steps + 1 Q[s, a] &lt;- Q[s, a] + alpha * (r + gamma * max(Q[s_prime, ]) - Q[s, a]) s &lt;- s_prime } episode_rewards[ep] &lt;- total_reward episode_steps[ep] &lt;- steps } list(Q = Q, policy = apply(Q, 1, which.max), episode_rewards = episode_rewards, episode_steps = episode_steps) } 9.4 Experimental Analysis 9.4.1 Learning Efficiency Comparison We compare Dyna-Q against standard Q-Learning across different numbers of planning steps: # Run comprehensive experiments set.seed(123) n_runs &lt;- 20 episodes &lt;- 300 # Initialize results storage all_results &lt;- data.frame() print(&quot;Running experiments...&quot;) for (run in 1:n_runs) { cat(&quot;Run&quot;, run, &quot;of&quot;, n_runs, &quot;\\n&quot;) # Run algorithms ql_result &lt;- q_learning(episodes = episodes) dyna5_result &lt;- dyna_q(episodes = episodes, n_planning = 5) dyna10_result &lt;- dyna_q(episodes = episodes, n_planning = 10) dyna20_result &lt;- dyna_q(episodes = episodes, n_planning = 20) # Store results run_data &lt;- data.frame( episode = rep(1:episodes, 4), run = run, algorithm = rep(c(&quot;Q-Learning&quot;, &quot;Dyna-Q (n=5)&quot;, &quot;Dyna-Q (n=10)&quot;, &quot;Dyna-Q (n=20)&quot;), each = episodes), reward = c(ql_result$episode_rewards, dyna5_result$episode_rewards, dyna10_result$episode_rewards, dyna20_result$episode_rewards), steps = c(ql_result$episode_steps, dyna5_result$episode_steps, dyna10_result$episode_steps, dyna20_result$episode_steps) ) all_results &lt;- rbind(all_results, run_data) } # Compute smoothed averages smoothed_results &lt;- all_results %&gt;% group_by(algorithm, episode) %&gt;% summarise( mean_reward = mean(reward), se_reward = sd(reward) / sqrt(n()), mean_steps = mean(steps), se_steps = sd(steps) / sqrt(n()), .groups = &#39;drop&#39; ) %&gt;% group_by(algorithm) %&gt;% mutate( smooth_reward = stats::filter(mean_reward, rep(1/10, 10), sides = 2), smooth_steps = stats::filter(mean_steps, rep(1/10, 10), sides = 2) ) # Create comprehensive visualization # Plot 1: Learning Curves (Rewards) p1 &lt;- ggplot(smoothed_results, aes(x = episode, y = mean_reward, color = algorithm)) + geom_line(size = 1.2) + geom_ribbon(aes(ymin = mean_reward - se_reward, ymax = mean_reward + se_reward, fill = algorithm), alpha = 0.2, color = NA) + scale_color_manual(values = c(&quot;Q-Learning&quot; = &quot;#E31A1C&quot;, &quot;Dyna-Q (n=5)&quot; = &quot;#1F78B4&quot;, &quot;Dyna-Q (n=10)&quot; = &quot;#33A02C&quot;, &quot;Dyna-Q (n=20)&quot; = &quot;#FF7F00&quot;)) + scale_fill_manual(values = c(&quot;Q-Learning&quot; = &quot;#E31A1C&quot;, &quot;Dyna-Q (n=5)&quot; = &quot;#1F78B4&quot;, &quot;Dyna-Q (n=10)&quot; = &quot;#33A02C&quot;, &quot;Dyna-Q (n=20)&quot; = &quot;#FF7F00&quot;)) + labs(title = &quot;Learning Performance: Average Episode Rewards&quot;, x = &quot;Episode&quot;, y = &quot;Average Reward per Episode&quot;, color = &quot;Algorithm&quot;, fill = &quot;Algorithm&quot;) + theme_minimal() + theme(plot.title = element_text(size = 14, face = &quot;bold&quot;), legend.position = &quot;bottom&quot;) # Plot 2: Steps to Terminal (Efficiency) p2 &lt;- ggplot(smoothed_results, aes(x = episode, y = mean_steps, color = algorithm)) + geom_line(size = 1.2) + geom_ribbon(aes(ymin = mean_steps - se_steps, ymax = mean_steps + se_steps, fill = algorithm), alpha = 0.2, color = NA) + scale_color_manual(values = c(&quot;Q-Learning&quot; = &quot;#E31A1C&quot;, &quot;Dyna-Q (n=5)&quot; = &quot;#1F78B4&quot;, &quot;Dyna-Q (n=10)&quot; = &quot;#33A02C&quot;, &quot;Dyna-Q (n=20)&quot; = &quot;#FF7F00&quot;)) + scale_fill_manual(values = c(&quot;Q-Learning&quot; = &quot;#E31A1C&quot;, &quot;Dyna-Q (n=5)&quot; = &quot;#1F78B4&quot;, &quot;Dyna-Q (n=10)&quot; = &quot;#33A02C&quot;, &quot;Dyna-Q (n=20)&quot; = &quot;#FF7F00&quot;)) + labs(title = &quot;Learning Efficiency: Steps to Reach Terminal State&quot;, x = &quot;Episode&quot;, y = &quot;Average Steps per Episode&quot;, color = &quot;Algorithm&quot;, fill = &quot;Algorithm&quot;) + theme_minimal() + theme(plot.title = element_text(size = 14, face = &quot;bold&quot;), legend.position = &quot;bottom&quot;) # Plot 3: Final performance comparison (last 50 episodes) final_performance &lt;- all_results %&gt;% filter(episode &gt; episodes - 50) %&gt;% group_by(algorithm, run) %&gt;% summarise(avg_reward = mean(reward), .groups = &#39;drop&#39;) p3 &lt;- ggplot(final_performance, aes(x = algorithm, y = avg_reward, fill = algorithm)) + geom_boxplot(alpha = 0.7) + geom_point(position = position_jitter(width = 0.2), alpha = 0.5) + scale_fill_manual(values = c(&quot;Q-Learning&quot; = &quot;#E31A1C&quot;, &quot;Dyna-Q (n=5)&quot; = &quot;#1F78B4&quot;, &quot;Dyna-Q (n=10)&quot; = &quot;#33A02C&quot;, &quot;Dyna-Q (n=20)&quot; = &quot;#FF7F00&quot;)) + labs(title = &quot;Final Performance Comparison (Last 50 Episodes)&quot;, x = &quot;Algorithm&quot;, y = &quot;Average Reward&quot;, fill = &quot;Algorithm&quot;) + theme_minimal() + theme(plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot;) # Display all plots print(p1) print(p2) print(p3) 9.5 Discussion Dyna demonstrates a fundamental trade-off between sample efficiency and computational cost, requiring \\((n+1)\\) times the computation per step compared to model-free methods due to additional planning updates. This computational investment typically yields faster convergence, particularly when real experience is expensive or dangerous to obtain, though optimal planning steps depend on problem characteristics—moderate values (5-10) generally provide good improvements without excessive overhead while very large values can produce diminishing returns or hurt performance with inaccurate models. The algorithm’s effectiveness critically depends on model quality, with our deterministic table-based implementation becoming increasingly accurate as more state-action pairs are visited, though this approach assumes deterministic transitions and uses simple model representation (storing only last observed transitions) that works well for stationary environments but struggles with non-stationary dynamics where more sophisticated representations maintaining transition probability distributions could improve robustness at increased computational cost. The interaction between exploration and planning creates a distinctive advantage where \\(\\epsilon\\)-greedy exploration ensures diverse state-action coverage that directly improves model quality and planning effectiveness, establishing a positive feedback loop where better exploration enhances the model, making planning more effective and leading to better policies with potentially more informed exploration. Relative to pure model-free methods like Q-Learning, Dyna typically shows faster convergence and better sample efficiency by allowing each real experience to propagate information more widely through planning, while compared to pure model-based approaches, it maintains robustness through continued direct learning even with imperfect models. However, basic Dyna has notable limitations including poor representation of stochastic environments through deterministic models and suboptimal uniform sampling for planning, though modern extensions like Dyna-Q+ with exploration bonuses, prioritized sweeping focusing on high-value updates, and model-based variants maintaining probability distributions over transitions address many of these constraints. 9.6 Implementation Considerations and Conclusion Dyna requires additional memory to store the learned model alongside the value function. In our implementation, this doubles the memory requirements compared to pure Q-Learning. For larger state-action spaces, this can become a significant consideration, potentially requiring sparse representations or function approximation techniques. The planning phase introduces variable computational demands that can complicate real-time applications. While the number of planning steps can be adjusted based on available computational budget, this flexibility requires careful consideration of timing constraints in online learning scenarios. Dyna introduces additional hyperparameters, particularly the number of planning steps \\(n\\). This parameter requires tuning based on the specific problem characteristics and computational constraints. Unlike some hyperparameters that can be set based on theoretical considerations, \\(n\\) often requires empirical validation. Dyna represents an elegant solution to integrating learning and planning in reinforcement learning, combining the sample efficiency of model-based methods with the robustness of model-free approaches. Our R implementation demonstrates both the benefits and challenges of this integration, showing improved learning speed at the cost of increased computational and memory requirements. The key insight of using real experience for both direct learning and model improvement creates a powerful synergy that can significantly accelerate learning in many environments. However, the approach requires careful consideration of model representation, computational constraints, and hyperparameter tuning to achieve optimal performance. Future extensions could explore more sophisticated model representations, prioritized planning strategies, or integration with function approximation techniques for handling larger state spaces. The fundamental principle of combining direct and indirect learning remains a valuable paradigm in modern reinforcement learning research. "],["dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html", "Chapter 10 Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning 10.1 Introduction 10.2 Theoretical Framework 10.3 Implementation in R 10.4 Experimental Analysis 10.5 Discussion and Implementation Considerations 10.6 Conclusion", " Chapter 10 Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning 10.1 Introduction While Dyna successfully bridges model-free and model-based reinforcement learning, it carries an inherent assumption that can limit its effectiveness in changing environments: that the world remains static. When an agent has learned to navigate one version of an environment, what happens if the rules suddenly change? Standard Dyna may find itself stuck, continuing to plan based on outdated information while failing to adequately explore the new reality. Dyna-Q+, introduced by Sutton (1990) alongside the original Dyna framework, addresses this limitation through a deceptively simple yet powerful mechanism: it rewards curiosity. By providing exploration bonuses for state-action pairs that haven’t been tried recently, Dyna-Q+ maintains a healthy skepticism about its model’s continued accuracy. This approach proves particularly valuable in non-stationary environments where adaptation speed can mean the difference between success and failure. The enhancement might seem minor—just an additional term in the reward calculation—but its implications run deep. Dyna-Q+ acknowledges that in a changing world, forgetting can be as important as remembering, and that an agent’s confidence in its model should decay over time unless continually refreshed by recent experience. 10.2 Theoretical Framework 10.2.1 The Exploration Bonus Mechanism Dyna-Q+ modifies the planning phase of standard Dyna by augmenting rewards with an exploration bonus based on the time elapsed since each state-action pair was last visited. The core insight lies in treating the passage of time as information: the longer an agent hasn’t verified a particular transition, the less confident it should be about that transition’s current validity. For each state-action pair \\((s,a)\\), we maintain a timestamp \\(\\\\tau(s,a)\\) recording when it was last experienced. During planning, instead of using the stored reward \\(\\\\hat{R}(s,a)\\) directly, we calculate an augmented reward: \\[r_{augmented} = \\hat{R}(s,a) + \\kappa \\sqrt{t - \\tau(s,a)}\\] where \\(t\\) represents the current time step, and \\(\\\\kappa\\) is a parameter controlling the strength of the exploration bonus. The square root function provides a diminishing bonus that grows with time but at a decreasing rate, reflecting the intuition that uncertainty about a transition increases with time but not linearly. 10.2.2 Complete Dyna-Q+ Algorithm The full algorithm extends standard Dyna with minimal modifications. For each real experience tuple \\((s, a, r, s&#39;)\\): Direct Learning: \\[Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right]\\] Model and Timestamp Updates: \\[\\hat{T}(s,a) \\leftarrow s&#39;\\]\\[\\hat{R}(s,a) \\leftarrow r\\]\\[\\tau(s,a) \\leftarrow t\\] Planning Phase (repeat \\(n\\) times): \\[s_{plan} \\leftarrow \\text{random previously visited state}\\] \\[a_{plan} \\leftarrow \\text{random action previously taken in } s_{plan}\\] \\[r_{plan} \\leftarrow \\hat{R}(s_{plan},a_{plan}) + \\kappa \\sqrt{t - \\tau(s_{plan},a_{plan})}\\] \\[s&#39;_{plan} \\leftarrow \\hat{T}(s_{plan},a_{plan})\\] \\[Q(s_{plan},a_{plan}) \\leftarrow Q(s_{plan},a_{plan}) + \\alpha \\left[ r_{plan} + \\gamma \\max_{a&#39;} Q(s&#39;_{plan}, a&#39;) - Q(s_{plan},a_{plan}) \\right]\\] 10.2.3 Convergence and Stability The theoretical properties of Dyna-Q+ are more complex than those of standard Dyna due to the non-stationary nature of the augmented rewards. In stationary environments, the exploration bonuses for frequently visited state-action pairs will remain small, and convergence properties approach those of standard Dyna. However, the algorithm sacrifices some theoretical guarantees about convergence to optimal policies in exchange for improved adaptability. The parameter \\(\\\\kappa\\) requires careful tuning. Too small, and the exploration bonus becomes negligible, reducing Dyna-Q+ to standard Dyna. Too large, and the algorithm may exhibit excessive exploration even in stable environments, potentially degrading performance. The square root scaling helps moderate this trade-off by providing significant bonuses for truly neglected state-action pairs while keeping bonuses manageable for recently visited ones. 10.3 Implementation in R Building on our previous Dyna implementation, we extend the framework to include timestamp tracking and exploration bonuses. 10.3.1 Environment Setup We’ll use the same 10-state environment as before, but we’ll also create scenarios with environmental changes to demonstrate Dyna-Q+’s adaptive capabilities: # Environment parameters (same as before) n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Environment interaction function with optional modification capability sample_env &lt;- function(s, a, modified = FALSE) { if (modified) { # Simulate environmental change by blocking previously optimal path if (s == 5 &amp;&amp; a == 1) { return(list(s_prime = 1, reward = -0.5)) # Penalty for blocked path } } probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } 10.3.2 Dyna-Q+ Implementation The key modification involves maintaining timestamps and calculating exploration bonuses during planning: dyna_q_plus &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1, n_planning = 5, kappa = 0.1, change_episode = NULL) { # Initialize Q-values, model, and timestamps Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) model_T &lt;- array(NA, dim = c(n_states, n_actions)) model_R &lt;- array(NA, dim = c(n_states, n_actions)) timestamps &lt;- array(0, dim = c(n_states, n_actions)) # When last visited visited_sa &lt;- list() current_time &lt;- 0 environment_changed &lt;- FALSE for (ep in 1:episodes) { # Check if we should change the environment if (!is.null(change_episode) &amp;&amp; ep == change_episode) { environment_changed &lt;- TRUE } s &lt;- 1 while (s != terminal_state) { current_time &lt;- current_time + 1 # Action selection (epsilon-greedy) if (runif(1) &lt; epsilon) { a &lt;- sample(1:n_actions, 1) } else { # Break ties randomly a &lt;- sample(which(Q[s, ] == max(Q[s, ])), 1) } # Take action and observe outcome outcome &lt;- sample_env(s, a, modified = environment_changed) s_prime &lt;- outcome$s_prime r &lt;- outcome$reward # Direct learning (Q-Learning update) Q[s, a] &lt;- Q[s, a] + alpha * (r + gamma * max(Q[s_prime, ]) - Q[s, a]) # Model learning and timestamp update model_T[s, a] &lt;- s_prime model_R[s, a] &lt;- r timestamps[s, a] &lt;- current_time # Track visited state-action pairs sa_key &lt;- paste(s, a, sep = &quot;_&quot;) if (!(sa_key %in% names(visited_sa))) { visited_sa[[sa_key]] &lt;- c(s, a) } # Planning phase with exploration bonuses if (length(visited_sa) &gt; 0) { for (i in 1:n_planning) { # Sample random previously visited state-action pair sa_sample &lt;- sample(visited_sa, 1)[[1]] s_plan &lt;- sa_sample[1] a_plan &lt;- sa_sample[2] # Get simulated experience from model if (!is.na(model_T[s_plan, a_plan])) { s_prime_plan &lt;- model_T[s_plan, a_plan] r_base &lt;- model_R[s_plan, a_plan] # Calculate exploration bonus time_since_visit &lt;- current_time - timestamps[s_plan, a_plan] exploration_bonus &lt;- kappa * sqrt(time_since_visit) r_plan &lt;- r_base + exploration_bonus # Planning update with augmented reward Q[s_plan, a_plan] &lt;- Q[s_plan, a_plan] + alpha * (r_plan + gamma * max(Q[s_prime_plan, ]) - Q[s_plan, a_plan]) } } } s &lt;- s_prime } } list(Q = Q, policy = apply(Q, 1, which.max), model_T = model_T, model_R = model_R, timestamps = timestamps) } 10.3.3 Standard Dyna for Comparison We also implement standard Dyna to highlight the differences: dyna_q_standard &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1, n_planning = 5, change_episode = NULL) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) model_T &lt;- array(NA, dim = c(n_states, n_actions)) model_R &lt;- array(NA, dim = c(n_states, n_actions)) visited_sa &lt;- list() environment_changed &lt;- FALSE for (ep in 1:episodes) { if (!is.null(change_episode) &amp;&amp; ep == change_episode) { environment_changed &lt;- TRUE } s &lt;- 1 while (s != terminal_state) { # Action selection if (runif(1) &lt; epsilon) { a &lt;- sample(1:n_actions, 1) } else { # Break ties randomly a &lt;- sample(which(Q[s, ] == max(Q[s, ])), 1) } # Take action outcome &lt;- sample_env(s, a, modified = environment_changed) s_prime &lt;- outcome$s_prime r &lt;- outcome$reward # Direct learning Q[s, a] &lt;- Q[s, a] + alpha * (r + gamma * max(Q[s_prime, ]) - Q[s, a]) # Model learning (no timestamp tracking) model_T[s, a] &lt;- s_prime model_R[s, a] &lt;- r # Track visited pairs sa_key &lt;- paste(s, a, sep = &quot;_&quot;) if (!(sa_key %in% names(visited_sa))) { visited_sa[[sa_key]] &lt;- c(s, a) } # Standard planning (no exploration bonus) if (length(visited_sa) &gt; 0) { for (i in 1:n_planning) { sa_sample &lt;- sample(visited_sa, 1)[[1]] s_plan &lt;- sa_sample[1] a_plan &lt;- sa_sample[2] if (!is.na(model_T[s_plan, a_plan])) { s_prime_plan &lt;- model_T[s_plan, a_plan] r_plan &lt;- model_R[s_plan, a_plan] # No bonus here Q[s_plan, a_plan] &lt;- Q[s_plan, a_plan] + alpha * (r_plan + gamma * max(Q[s_prime_plan, ]) - Q[s_plan, a_plan]) } } } s &lt;- s_prime } } list(Q = Q, policy = apply(Q, 1, which.max)) } 10.4 Experimental Analysis 10.4.1 Adaptation to Environmental Changes The most compelling demonstration of Dyna-Q+’s advantages comes from scenarios where the environment changes mid-learning. We’ll compare how quickly both algorithms adapt: # Function to evaluate policy performance evaluate_policy_performance &lt;- function(Q, episodes = 100, modified = FALSE) { total_reward &lt;- 0 total_steps &lt;- 0 for (ep in 1:episodes) { s &lt;- 1 episode_reward &lt;- 0 steps &lt;- 0 while (s != terminal_state &amp;&amp; steps &lt; 50) { # Prevent infinite loops a &lt;- sample(which(Q[s, ] == max(Q[s, ])), 1) # Break ties randomly outcome &lt;- sample_env(s, a, modified = modified) episode_reward &lt;- episode_reward + outcome$reward s &lt;- outcome$s_prime steps &lt;- steps + 1 } total_reward &lt;- total_reward + episode_reward total_steps &lt;- total_steps + steps } list(avg_reward = total_reward / episodes, avg_steps = total_steps / episodes) } # Comparative experiment with environmental change adaptation_experiment &lt;- function() { set.seed(123) n_runs &lt;- 20 change_point &lt;- 500 total_episodes &lt;- 1000 # Storage for results results &lt;- data.frame( episode = rep(1:total_episodes, 2), algorithm = rep(c(&quot;Dyna-Q&quot;, &quot;Dyna-Q+&quot;), each = total_episodes), performance = numeric(total_episodes * 2), run = rep(1, total_episodes * 2) ) for (run in 1:n_runs) { # Train both algorithms # For a full experiment, you would re-initialize Q here for each run dyna_standard_result &lt;- dyna_q_standard(episodes = total_episodes, change_episode = change_point) dyna_plus_result &lt;- dyna_q_plus(episodes = total_episodes, change_episode = change_point, kappa = 0.1) # Evaluate performance at each episode (simplified for illustration) for (ep in 1:total_episodes) { modified_env &lt;- ep &gt;= change_point # This is a simplified evaluation - in practice, you&#39;d want to # track performance throughout training if (ep %% 50 == 0) { std_perf &lt;- evaluate_policy_performance(dyna_standard_result$Q, episodes = 10, modified = modified_env) plus_perf &lt;- evaluate_policy_performance(dyna_plus_result$Q, episodes = 10, modified = modified_env) # Store results (this is simplified - you&#39;d want better tracking) idx_std &lt;- (run - 1) * total_episodes * 2 + ep idx_plus &lt;- (run - 1) * total_episodes * 2 + total_episodes + ep if (run == 1) { # Just store first run for illustration results$performance[ep] &lt;- std_perf$avg_reward results$performance[total_episodes + ep] &lt;- plus_perf$avg_reward } } } } return(results) } 10.4.1.1 Example: Running the Adaptation Experiment Here we execute the experiment defined above. The resulting plot shows that both algorithms perform similarly until the environment changes at episode 500. After the change, Dyna-Q (kappa = 0) fails to adapt because its model is outdated, leading to a sharp drop in performance. In contrast, Dyna-Q+ uses its exploration bonus to re-evaluate old paths, quickly discovering the change and finding a new optimal policy, thus recovering its performance. # Setup chunk for libraries if (!require(&quot;ggplot2&quot;, quietly = TRUE)) install.packages(&quot;ggplot2&quot;) if (!require(&quot;dplyr&quot;, quietly = TRUE)) install.packages(&quot;dplyr&quot;) if (!require(&quot;tidyr&quot;, quietly = TRUE)) install.packages(&quot;tidyr&quot;) library(ggplot2) library(dplyr) library(tidyr) # Run the experiment. # Note: The original experiment function evaluates performance every 50 episodes, # resulting in a plot that connects these discrete data points. adaptation_results_df &lt;- adaptation_experiment() # Prepare data for plotting by filtering out unevaluated episodes plot_data &lt;- adaptation_results_df %&gt;% filter(performance != 0) # Generate the plot ggplot(plot_data, aes(x = episode, y = performance, color = algorithm)) + geom_line(linewidth = 1.2) + geom_point(size = 2.5) + geom_vline(xintercept = 500, linetype = &quot;dashed&quot;, color = &quot;black&quot;, linewidth = 1) + annotate(&quot;text&quot;, x = 480, y = min(plot_data$performance, na.rm=TRUE) * 1.1, label = &quot;Environment\\nChange&quot;, vjust = 0, hjust = 1, color = &quot;black&quot;, size=3.5) + labs( title = &quot;Dyna-Q+ vs. Standard Dyna-Q in a Changing Environment&quot;, subtitle = &quot;Performance comparison before and after an environmental change at episode 500.&quot;, x = &quot;Episode&quot;, y = &quot;Average Reward&quot;, color = &quot;Algorithm&quot; ) + theme_minimal(base_size = 14) + scale_color_manual(values = c(&quot;Dyna-Q&quot; = &quot;#d95f02&quot;, &quot;Dyna-Q+&quot; = &quot;#1b9e77&quot;)) + theme( legend.position = &quot;bottom&quot;, plot.title = element_text(face = &quot;bold&quot;), plot.subtitle = element_text(color = &quot;grey30&quot;) ) 10.4.2 Parameter Sensitivity Analysis The exploration parameter \\(\\\\kappa\\) significantly influences Dyna-Q+’s behavior. Let’s examine its effects: kappa_sensitivity_analysis &lt;- function() { kappa_values &lt;- c(0, 0.01, 0.05, 0.1, 0.2, 0.5) change_point &lt;- 300 total_episodes &lt;- 600 results &lt;- list() for (i in seq_along(kappa_values)) { set.seed(42) # Consistent conditions result &lt;- dyna_q_plus(episodes = total_episodes, change_episode = change_point, kappa = kappa_values[i], n_planning = 10) # Create a temporary Q matrix for evaluation before the change # We can&#39;t know the exact Q before the change without modifying the main loop, # so we run a separate short training for pre-change evaluation. pre_change_result &lt;- dyna_q_plus(episodes = change_point, kappa = kappa_values[i], n_planning=10) pre_change_perf &lt;- evaluate_policy_performance(pre_change_result$Q, modified = FALSE) post_change_perf &lt;- evaluate_policy_performance(result$Q, modified = TRUE) # Handle division by zero or near-zero rewards adaptation_ratio &lt;- if (pre_change_perf$avg_reward &gt; 1e-5) { post_change_perf$avg_reward / pre_change_perf$avg_reward } else { NA # Avoid meaningless ratios } results[[i]] &lt;- list( kappa = kappa_values[i], pre_change_reward = pre_change_perf$avg_reward, post_change_reward = post_change_perf$avg_reward, adaptation_ratio = adaptation_ratio ) } # Convert to data frame for analysis sensitivity_df &lt;- do.call(rbind, lapply(results, function(x) { data.frame(kappa = x$kappa, pre_change = x$pre_change_reward, post_change = x$post_change_reward, adaptation = x$adaptation_ratio) })) return(sensitivity_df) } # Visualization function plot_kappa_sensitivity &lt;- function(data) { par(mfrow = c(1, 2), mar = c(5, 4, 4, 2) + 0.1) # Plot 1: Performance vs kappa plot(data$kappa, data$post_change, type = &quot;b&quot;, pch = 16, col = &quot;darkred&quot;, xlab = &quot;Kappa Value (κ)&quot;, ylab = &quot;Post-Change Average Reward&quot;, main = &quot;Performance After Change&quot;, ylim=c(min(data$post_change, na.rm=T)*0.9, max(data$post_change, na.rm=T)*1.1)) grid(lty = 1, col = &quot;gray90&quot;) # Plot 2: Adaptation ratio vs kappa plot(data$kappa, data$adaptation, type = &quot;b&quot;, pch = 16, col = &quot;darkblue&quot;, xlab = &quot;Kappa Value (κ)&quot;, ylab = &quot;Adaptation Ratio (Post/Pre)&quot;, main = &quot;Adaptation vs Exploration&quot;) grid(lty = 1, col = &quot;gray90&quot;) par(mfrow = c(1, 1)) } 10.4.2.1 Example: Running the Sensitivity Analysis We run the analysis for different values of \\(\\\\kappa\\). A value of \\(\\\\kappa = 0\\) corresponds to standard Dyna-Q. The table shows the average reward before and after the environmental change. The plots visualize how post-change performance and the adaptation ratio change with \\(\\\\kappa\\). There is a sweet spot for \\(\\\\kappa\\) (around 0.1-0.2 in this case) that provides the best adaptation. If \\(\\\\kappa\\) is too low, adaptation is slow; if it’s too high, the agent explores too much, which can also hurt performance. # 1. Generate the data sensitivity_data &lt;- kappa_sensitivity_analysis() # 2. Display the data as a formatted table if (!require(&quot;knitr&quot;, quietly = TRUE)) install.packages(&quot;knitr&quot;) knitr::kable(sensitivity_data, caption = &quot;Kappa Parameter Sensitivity Analysis Results&quot;, col.names = c(&quot;Kappa (κ)&quot;, &quot;Pre-Change Reward&quot;, &quot;Post-Change Reward&quot;, &quot;Adaptation Ratio&quot;), digits = 3, align = &#39;c&#39;) # 3. Generate the plots using the provided function plot_kappa_sensitivity(sensitivity_data) 10.4.3 Exploration Pattern Analysis One way to understand Dyna-Q+’s behavior is to examine how exploration bonuses evolve over time: analyze_exploration_patterns &lt;- function() { # Run Dyna-Q+ and track exploration bonuses set.seed(123) Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) timestamps &lt;- array(0, dim = c(n_states, n_actions)) visited_sa &lt;- list() current_time &lt;- 0 kappa &lt;- 0.1 # Storage for bonus tracking bonus_history &lt;- list() # Define time points for snapshotting bonus values time_points &lt;- seq(100, 2000, by=100) # Run for a fixed number of time steps instead of episodes # to get a clearer view of bonus evolution over time. while(current_time &lt; 2000) { s &lt;- 1 # Reset to start state for each &quot;pseudo-episode&quot; while (s != terminal_state &amp;&amp; current_time &lt; 2000) { current_time &lt;- current_time + 1 # Simple action selection for this analysis a &lt;- sample(1:n_actions, 1) outcome &lt;- sample_env(s, a) s_prime &lt;- outcome$s_prime # Update timestamps timestamps[s, a] &lt;- current_time # Track bonuses at specific time points if (current_time %in% time_points) { bonuses &lt;- array(0, dim = c(n_states, n_actions)) for (state in 1:n_states) { for (action in 1:n_actions) { if (timestamps[state, action] &gt; 0) { time_diff &lt;- current_time - timestamps[state, action] bonuses[state, action] &lt;- kappa * sqrt(time_diff) } } } bonus_history[[as.character(current_time)]] &lt;- bonuses } s &lt;- s_prime } } return(bonus_history) } # Visualization of exploration bonus evolution plot_bonus_evolution &lt;- function() { bonus_data &lt;- analyze_exploration_patterns() # Extract bonus magnitudes over time time_points &lt;- as.numeric(names(bonus_data)) max_bonuses &lt;- sapply(bonus_data, function(x) max(x, na.rm = TRUE)) mean_bonuses &lt;- sapply(bonus_data, function(x) mean(x[x &gt; 0], na.rm = TRUE)) plot(time_points, max_bonuses, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, xlab = &quot;Time Steps&quot;, ylab = &quot;Exploration Bonus Magnitude&quot;, main = &quot;Evolution of Exploration Bonuses&quot;, ylim = c(0, max(max_bonuses, na.rm = TRUE))) lines(time_points, mean_bonuses, col = &quot;blue&quot;, lwd = 2, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;Maximum Bonus&quot;, &quot;Average Bonus&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = c(1, 2), lwd = 2, bty=&quot;n&quot;) grid(lty = 1, col = &quot;gray90&quot;) } 10.4.3.1 Example: Visualizing Exploration Bonuses This plot shows how the exploration bonuses change over time. As the agent explores, timestamps are updated, and the time since the last visit (t - τ) for any given state-action pair can grow. The Maximum Bonus corresponds to the state-action pair that has been unvisited for the longest time, showing the agent’s growing “curiosity” about that specific part of the environment. The Average Bonus (for visited pairs) tends to stay lower, indicating that most parts of the model are kept relatively fresh through planning and exploration. plot_bonus_evolution() 10.5 Discussion and Implementation Considerations Dyna-Q+ can be understood as a form of algorithmic curiosity that parallels aspects of human learning. Just as people grow uneasy about facts they have not revisited in some time, the algorithm gradually discounts its own model’s accuracy as intervals between visits lengthen. This built-in doubt is advantageous in non-stationary settings, where relying on yesterday’s truths can be costly. The exploration bonus, scaled by a square root of elapsed time, encodes an important nuance: uncertainty should increase with neglect, but at a diminishing rate. This prevents the system from sliding into perpetual skepticism while keeping enough pressure to revisit older assumptions. The extra bookkeeping is minimal—simply a timestamp for each state–action pair—but it changes the decision-making problem. The agent now balances three forces: exploiting current knowledge, exploring new possibilities, and re-exploring known areas to keep the model current. This is more complex than the standard explore–exploit trade-off in Dyna. For large state–action spaces, the linear scaling of timestamp storage may require function approximation or selective retention, especially in continuous or high-dimensional domains. Empirically, Dyna-Q+ tends to shine in environments that evolve over time. In stable conditions, bonuses for well-visited states remain small and the algorithm behaves much like standard Dyna. But when conditions shift, the systematic revisiting of old transitions enables faster adaptation. The parameter \\(\\\\kappa\\) sets the level of “model anxiety”: small values create a trusting system, large values a more suspicious one. The best setting depends on how quickly the world changes and on the relative costs of exploration and exploitation errors. The method rests on an implicit assumption—that environmental change is the main cause of model inaccuracy. When inaccuracy stems instead from intrinsic difficulty, such as noisy transitions or highly complex dynamics, the uniform bonuses may encourage needless exploration. Similarly, applying the same bonus across all state–action pairs ignores that some regions may be more volatile or strategically important than others. More refined variants might weight bonuses according to change likelihood or the expected impact of outdated information. Later research has broadened these ideas. In deep reinforcement learning, uncertainty-driven exploration often uses learned uncertainty estimates rather than timestamps. Meta-learning approaches aim to optimise exploration strategies across related environments. Curiosity-driven methods extend the spirit of Dyna-Q+ beyond temporal doubt, rewarding novelty in prediction error, information gain, or visitation patterns. The shared thread is that learning systems should actively seek information that improves their internal models. In practice, Dyna-Q+ is well suited to domains with gradual, structured change—financial markets with shifting regimes, or mobile robots navigating spaces where obstacles occasionally move. It is less effective in environments with rapid or chaotic dynamics, where maintaining a model may be futile or the bonus insufficient to trigger timely adaptation. Implementation choices often start with \\(\\\\kappa\\) between 0.01 and 0.1, tuning from there. More volatile settings generally warrant larger values. Planning steps \\(n\\) interact with \\(\\\\kappa\\): increasing \\(n\\) amplifies bonus effects and may require reducing \\(\\\\kappa\\). Large-scale use can demand timestamp approximations—such as storing them only for a subset of pairs or grouping times into bins—to save memory while preserving adaptivity. The extra computation from bonuses is usually negligible compared to value updates, though in time-critical systems, even the square-root calculation may be replaced by lookup tables or cheaper approximations. 10.6 Conclusion Dyna-Q+ represents a elegant solution to a fundamental challenge in reinforcement learning: how to maintain confidence in learned models while remaining appropriately skeptical about their continued accuracy. By treating time as information and systematically rewarding curiosity about neglected state-action pairs, the algorithm achieves a sophisticated balance between stability and adaptability. The approach’s strength lies not just in its technical effectiveness but in its conceptual clarity. The idea that confidence should decay over time unless refreshed by recent experience resonates across many domains beyond reinforcement learning. This principle finds echoes in human psychology, scientific methodology, and even social institutions that require periodic validation of their foundational assumptions. While modern deep reinforcement learning has developed more sophisticated approaches to uncertainty and exploration, Dyna-Q+’s core insights remain relevant. The tension between trusting learned models and maintaining healthy skepticism about their accuracy continues to challenge contemporary algorithms. In an era of rapidly changing environments and non-stationary dynamics, the principle of time-decaying confidence may prove even more valuable than when originally proposed. Looking forward, the integration of Dyna-Q+’s temporal curiosity with modern uncertainty estimation techniques presents intriguing possibilities. Neural networks that maintain both predictive models and confidence estimates could incorporate exploration bonuses based on both temporal factors and model uncertainty, potentially creating more robust and adaptive learning systems. The simplicity of Dyna-Q+’s modification to standard Dyna—just adding a single term to the planning rewards—belies its conceptual sophistication. Sometimes the most profound advances in artificial intelligence come not from complex new architectures but from simple changes that embody deep insights about learning, adaptation, and the nature of knowledge itself. "],["function-approximation-and-feature-engineering.html", "Chapter 11 Function Approximation And Feature Engineering 11.1 Feature Engineering and State Representation 11.2 Mathematical Foundations of Linear Function Approximation 11.3 Classical Basis Function Methods 11.4 Comparative Analysis and Practical Considerations 11.5 Bridging Classical and Modern Approaches", " Chapter 11 Function Approximation And Feature Engineering Reinforcement Learning has evolved dramatically from its tabular origins, where algorithms stored explicit values for every state-action pair. Modern deep RL agents navigate vast continuous environments that would be impossible to represent in lookup tables. This transformation rests on a crucial foundation that bridges tabular methods and neural networks: classical function approximation. Understanding linear function approximation, basis functions, and feature engineering illuminates why deep RL succeeds, helps diagnose failures, and guides the design of effective agent representations. The curse of dimensionality makes tabular RL impractical for realistic problems. Consider a robot with ten continuous state variables, each discretized into just 100 bins—the resulting state space contains 10^20 states, far exceeding what any computer could store. Function approximation solves this by learning parameterized functions that generalize knowledge across similar states. This generalization creates both opportunities and challenges, introducing a fundamental tension between discrimination (the ability to distinguish important differences) and generalization (the capacity to share knowledge) that shapes every modern RL algorithm. This appendix provides the conceptual bridge between simple tabular methods and sophisticated neural networks. We explore how to engineer effective features, examine the mathematical principles governing linear function approximation, and investigate classical basis functions that remain relevant in contemporary RL systems. 11.1 Feature Engineering and State Representation The quality of state representation often determines the success or failure of RL algorithms more than the choice of learning method itself. Raw environmental observations—sensor readings, pixel values, or game states—rarely provide the optimal format for learning. Effective feature engineering transforms these high-dimensional, noisy inputs into compact, informative representations that accelerate learning and improve policy performance. Consider a autonomous vehicle navigating city streets. Raw sensor data might include thousands of LIDAR points, camera pixels, and instrument readings. However, the agent’s success depends on recognizing higher-level patterns: the presence of pedestrians, the curvature of the road ahead, or the relative positions of nearby vehicles. Good features capture these essential abstractions while discarding irrelevant details like the exact shade of a building or the precise timestamp of the observation. 11.1.1 The Discrimination vs. Generalization Tradeoff At the heart of feature design lies a fundamental tension that permeates all of machine learning but becomes particularly acute in RL. Discrimination enables the agent to recognize when seemingly similar states require different actions. A chess program must distinguish between positions that appear nearly identical but have vastly different strategic implications. Generalization allows the agent to apply lessons learned in one context to new, similar situations. The same chess program should recognize that tactical patterns transfer across different board positions. Mathematically, we seek to approximate the true value function V*(s) using a parameterized function V_θ(s). In the linear case, this takes the form: V_θ(s) = φ(s)^T θ = Σ_{i=1}^d φ_i(s) θ_i The feature vector φ(s) determines what information the agent can access about state s, while the weight vector θ contains the learned parameters. The approximation quality depends critically on how well the feature space can represent the true value function. The theoretical limit of linear approximation is given by the projection error |V* - ΠV*|^2, where Π projects onto the space spanned by the features. This mathematical framework reveals why feature selection is so crucial. No amount of sophisticated learning can overcome fundamentally inadequate features. If the feature vector φ(s) cannot distinguish between states with different optimal values, the agent cannot learn the correct policy regardless of the learning algorithm used. 11.1.2 Principles of Effective Feature Design Successful RL features often encode temporal structure, highlighting how the environment evolves over time. In physical systems, this means capturing both position and velocity. In financial applications, it involves representing price trends alongside current values. The key insight is that optimal actions depend not just on the current state, but on the trajectory that led to it. Features should focus on action-relevant information while filtering out distracting details. A poker-playing agent needs to represent card values, betting patterns, and opponent tendencies, but the color of the cards or the time of day are irrelevant. This filtering process requires domain knowledge and careful analysis of what information actually influences optimal decision-making. Hierarchical structure in features enables agents to reason at multiple levels of abstraction. A household robot benefits from features that represent both fine-grained spatial information (“near the kitchen counter”) and higher-level semantic concepts (“in the cooking area”). This hierarchical representation supports both precise control and strategic planning. 11.2 Mathematical Foundations of Linear Function Approximation Linear function approximation provides the theoretical foundation for understanding more complex methods. Its mathematical simplicity enables rigorous analysis while its computational efficiency makes it practical for many applications. Most importantly, the insights gained from linear methods transfer directly to neural network approaches. 11.2.1 Linear Value Functions and Their Properties The linear model represents value functions as weighted combinations of features: V_θ(s) = φ(s)^T θ Q_θ(s,a) = φ(s,a)^T θ This representation has several appealing properties. The gradient with respect to parameters is simply the feature vector itself: ∇_θ V_θ(s) = φ(s). This makes gradient-based learning algorithms straightforward to implement and analyze. The update rules become linear in the features, ensuring computational efficiency even for high-dimensional feature spaces. Linear models also provide theoretical guarantees. Under certain conditions, they converge to the best possible approximation within the representational constraints. This convergence behavior can be analyzed using techniques from linear algebra and optimization theory, providing insights that remain valuable when working with more complex function approximators. 11.2.2 Temporal Difference Learning with Linear Approximation The temporal difference learning framework adapts naturally to linear function approximation. The TD(0) update rule becomes: θ_{t+1} ← θ_t + α [R_{t+1} + γ V_θ(S_{t+1}) - V_θ(S_t)] φ(S_t) This update adjusts the weight vector to reduce the temporal difference error for the observed transition. Since all states share the same parameter vector θ, each update influences value estimates throughout the entire state space. The degree of influence depends on the similarity of feature vectors between states. For action-value learning, the Q-learning update becomes: θ_{t+1} ← θ_t + α [R_{t+1} + γ max_{a’} Q_θ(S_{t+1}, a’) - Q_θ(S_t, A_t)] φ(S_t, A_t) These updates have intuitive interpretations. When the agent discovers that a state is more valuable than expected (positive TD error), it increases the weights corresponding to active features. States with similar feature representations receive proportional increases in their estimated values. 11.2.3 The Deadly Triad and Stability Concerns Linear function approximation with on-policy TD learning enjoys strong convergence guarantees, but stability becomes problematic under certain combinations of conditions. The infamous “deadly triad” consists of off-policy learning (updating from data generated by a different policy), function approximation (using parameterized models rather than tabular representations), and bootstrapping (updating estimates based on other estimates rather than complete returns). This instability isn’t merely a theoretical curiosity—it represents a fundamental challenge that motivated much of the research leading to modern deep RL algorithms. Techniques like experience replay, target networks, and careful exploration strategies all address various aspects of this stability problem. Understanding the deadly triad provides crucial intuition for diagnosing and fixing problems in deep RL systems. 11.3 Classical Basis Function Methods Basis functions provide systematic approaches to feature construction, bridging the gap between hand-crafted features and automatically learned representations. These classical methods remain relevant both as standalone techniques for appropriate problems and as conceptual frameworks for understanding more sophisticated approaches. 11.3.1 Coarse Coding: Overlapping Receptive Fields Coarse coding represents the state space using overlapping binary features, where each feature corresponds to a region or “receptive field.” A feature activates when the state falls within its designated region and remains inactive otherwise. The power of this approach lies in the overlapping structure—nearby states activate similar sets of features, creating natural generalization. The degree of overlap controls the smoothness of the learned function. Greater overlap produces broader generalization but reduces discrimination. The optimal balance depends on the problem structure and the complexity of the target function. Coarse coding mimics biological neural representations, where individual neurons respond to ranges of stimuli with overlapping tuning curves. # Load required libraries library(ggplot2) library(ggforce) library(dplyr) # Coarse Coding Implementation create_coarse_coder &lt;- function(state_dims, num_features, overlap_factor = 1.5, seed = 123) { set.seed(seed) # Generate random centers uniformly across state space centers &lt;- data.frame( x = runif(num_features, 0, state_dims[1]), y = runif(num_features, 0, state_dims[2]) ) # Calculate radius to ensure appropriate overlap avg_spacing &lt;- sqrt(prod(state_dims) / num_features) radius &lt;- overlap_factor * avg_spacing / 2 # Function to compute binary feature vector for given state encode &lt;- function(state) { state_vec &lt;- as.numeric(state) distances &lt;- sqrt((centers$x - state_vec[1])^2 + (centers$y - state_vec[2])^2) features &lt;- as.numeric(distances &lt;= radius) return(features) } # Visualization function visualize &lt;- function(query_state = NULL) { # Determine active features for query state active_features &lt;- NULL active_indices &lt;- integer(0) if (!is.null(query_state)) { active_features &lt;- encode(query_state) active_indices &lt;- which(active_features == 1) } # Create base plot p &lt;- ggplot() + coord_fixed(xlim = c(0, state_dims[1]), ylim = c(0, state_dims[2])) # Add all receptive fields (inactive) p &lt;- p + geom_circle( data = centers, aes(x0 = x, y0 = y, r = radius), color = &quot;grey70&quot;, fill = &quot;grey90&quot;, alpha = 0.3, linetype = &quot;dashed&quot; ) # Highlight active receptive fields if (length(active_indices) &gt; 0) { p &lt;- p + geom_circle( data = centers[active_indices, ], aes(x0 = x, y0 = y, r = radius), color = &quot;#e41a1c&quot;, fill = &quot;#e41a1c&quot;, alpha = 0.4, size = 1 ) } # Add center points p &lt;- p + geom_point( data = centers, aes(x = x, y = y), color = &quot;grey40&quot;, size = 1.5, alpha = 0.8 ) # Add query state point if (!is.null(query_state)) { p &lt;- p + geom_point( aes(x = query_state[1], y = query_state[2]), color = &quot;#377eb8&quot;, size = 4, shape = 17 ) num_active &lt;- length(active_indices) subtitle &lt;- paste(num_active, &quot;of&quot;, num_features, &quot;features active&quot;) } else { subtitle &lt;- &quot;Overlapping receptive fields&quot; } p + labs( title = &quot;Coarse Coding&quot;, subtitle = subtitle, x = &quot;State Dimension 1&quot;, y = &quot;State Dimension 2&quot; ) + theme_minimal(base_size = 12) + theme( panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;white&quot;, color = NA) ) } return(list( encode = encode, visualize = visualize, centers = centers, radius = radius, state_dims = state_dims, num_features = num_features )) } # Example usage coarse_coder &lt;- create_coarse_coder( state_dims = c(10, 10), num_features = 50, overlap_factor = 1.5 ) # Visualize with query state print(coarse_coder$visualize(query_state = c(3.2, 7.8))) 11.3.2 Tile Coding: Structured Overlapping Grids Tile coding, also known as CMAC (Cerebellar Model Articulation Controller), provides a more structured approach to creating overlapping features. Instead of random circular regions, tile coding partitions the state space using multiple offset grids called tilings. Each tiling covers the entire state space with non-overlapping rectangular tiles, but the tilings are offset from each other to create overlapping coverage. For any given state, exactly one tile from each tiling will be active, ensuring a constant number of active features regardless of the state location. This property provides computational advantages and makes the learning dynamics more predictable. The offsets between tilings create fine-grained discrimination capability, while the tile sizes control the degree of generalization. Tile coding’s rectangular structure creates asymmetric generalization that aligns with coordinate axes. This asymmetry is often desirable in control problems where different state dimensions represent distinct physical quantities with different units and scaling properties. A robot’s position and velocity, for example, should generalize differently along each dimension. # Tile Coding Implementation create_tile_coder &lt;- function(state_low, state_high, num_tilings, tiles_per_dim) { state_dims &lt;- length(state_low) state_range &lt;- state_high - state_low tile_widths &lt;- state_range / tiles_per_dim # Generate offsets for each tiling offsets &lt;- array(0, dim = c(num_tilings, state_dims)) for (dim in 1:state_dims) { offsets[, dim] &lt;- (0:(num_tilings - 1)) * tile_widths[dim] / num_tilings } # Function to get indices of active tiles for given state encode &lt;- function(state) { state_vec &lt;- as.numeric(state) active_tiles &lt;- list() for (t in 1:num_tilings) { # Apply offset and compute tile indices offset_state &lt;- state_vec - state_low + offsets[t, ] tile_indices &lt;- floor(offset_state / tile_widths) # Clip to valid range tile_indices &lt;- pmax(0, pmin(tile_indices, tiles_per_dim - 1)) active_tiles[[t]] &lt;- tile_indices } return(active_tiles) } # Visualization function (2D only) visualize &lt;- function(query_state = NULL) { if (state_dims != 2) { stop(&quot;Visualization only supports 2D state spaces&quot;) } # Create base plot p &lt;- ggplot() + coord_fixed(xlim = state_low, ylim = state_high, expand = FALSE) # Generate colors for tilings colors &lt;- RColorBrewer::brewer.pal(min(num_tilings, 8), &quot;Set1&quot;) if (num_tilings &gt; 8) colors &lt;- rep(colors, ceiling(num_tilings / 8)) # Draw grid lines for each tiling for (t in 1:num_tilings) { color &lt;- colors[t] # Vertical lines x_lines &lt;- seq( state_low[1] - offsets[t, 1], state_high[1] + tile_widths[1], by = tile_widths[1] ) valid_x &lt;- x_lines[x_lines &gt;= state_low[1] &amp; x_lines &lt;= state_high[1]] if (length(valid_x) &gt; 0) { p &lt;- p + geom_vline( xintercept = valid_x, color = color, alpha = 0.7, linetype = &quot;dashed&quot;, size = 0.5 ) } # Horizontal lines y_lines &lt;- seq( state_low[2] - offsets[t, 2], state_high[2] + tile_widths[2], by = tile_widths[2] ) valid_y &lt;- y_lines[y_lines &gt;= state_low[2] &amp; y_lines &lt;= state_high[2]] if (length(valid_y) &gt; 0) { p &lt;- p + geom_hline( yintercept = valid_y, color = color, alpha = 0.7, linetype = &quot;dashed&quot;, size = 0.5 ) } } # Highlight active tiles for query state if (!is.null(query_state)) { active_tiles_indices &lt;- encode(query_state) for (t in 1:num_tilings) { color &lt;- colors[t] tile_indices &lt;- active_tiles_indices[[t]] # Calculate tile boundaries tile_x &lt;- state_low[1] + tile_indices[1] * tile_widths[1] - offsets[t, 1] tile_y &lt;- state_low[2] + tile_indices[2] * tile_widths[2] - offsets[t, 2] # Create rectangle data rect_data &lt;- data.frame( xmin = tile_x, xmax = tile_x + tile_widths[1], ymin = tile_y, ymax = tile_y + tile_widths[2] ) # Add highlighted tile p &lt;- p + geom_rect( data = rect_data, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax), fill = color, alpha = 0.4, color = color, size = 1.2 ) } # Mark query state p &lt;- p + geom_point( aes(x = query_state[1], y = query_state[2]), color = &quot;black&quot;, size = 4, shape = 17 ) subtitle &lt;- paste(num_tilings, &quot;active tiles (one per tiling)&quot;) } else { subtitle &lt;- paste(num_tilings, &quot;overlapping tilings&quot;) } p + labs( title = &quot;Tile Coding&quot;, subtitle = subtitle, x = &quot;State Dimension 1&quot;, y = &quot;State Dimension 2&quot; ) + theme_minimal(base_size = 12) + theme( panel.grid.minor = element_blank(), panel.background = element_rect(fill = &quot;white&quot;, color = NA) ) } return(list( encode = encode, visualize = visualize, state_low = state_low, state_high = state_high, num_tilings = num_tilings, tiles_per_dim = tiles_per_dim, tile_widths = tile_widths, offsets = offsets )) } # Example usage tile_coder &lt;- create_tile_coder( state_low = c(0, 0), state_high = c(10, 10), num_tilings = 4, tiles_per_dim = c(5, 5) ) # Visualize with query state print(tile_coder$visualize(query_state = c(3.2, 7.8))) 11.3.3 Radial Basis Functions: Smooth Continuous Features Radial Basis Functions represent a fundamentally different approach to feature construction. Instead of binary activations, RBFs provide smooth, continuous responses that depend on the distance from fixed center points. The most common form uses Gaussian functions: φ_i(s) = exp(-||s - c_i||^2 / (2σ_i^2)) Here c_i represents the center of the i-th RBF, while σ_i controls its width or bandwidth. Unlike the sharp boundaries of tile coding or coarse coding, RBFs create smooth activation landscapes that vary continuously with state changes. This smoothness often produces more stable learning dynamics and can lead to better generalization in certain domains. The width parameter σ plays a crucial role in controlling the locality of influence. Narrow RBFs (small σ) create peaked functions that provide high discrimination but limited generalization. Wide RBFs (large σ) create broad functions that generalize extensively but may lack the discrimination needed for complex tasks. The optimal choice depends on the smoothness of the target function and the density of training data. RBF networks possess powerful theoretical properties. They are universal approximators, meaning they can represent any continuous function to arbitrary accuracy given sufficient basis functions. The placement of centers and selection of widths determine the network’s effectiveness. Centers can be positioned on regular grids, placed randomly, or adapted during learning using techniques like k-means clustering on experienced states. # Load additional libraries for RBF network library(viridis) library(RColorBrewer) # Radial Basis Function Network Implementation create_rbf_network &lt;- function(centers, widths) { # Convert centers to matrix format if (is.data.frame(centers)) { centers &lt;- as.matrix(centers) } n_rbf &lt;- nrow(centers) # Handle scalar or vector widths if (length(widths) == 1) { widths &lt;- rep(widths, n_rbf) } # Function to compute RBF activations for given state encode &lt;- function(state) { state_vec &lt;- as.numeric(state) distances_sq &lt;- rowSums((centers - matrix(rep(state_vec, n_rbf), nrow = n_rbf, byrow = TRUE))^2) activations &lt;- exp(-distances_sq / (2 * widths^2)) return(activations) } # Function to create adaptive RBF centers using k-means-like placement create_adaptive_centers &lt;- function(state_dims, n_centers, n_samples = 1000, seed = 42) { set.seed(seed) # Generate random samples from state space samples &lt;- data.frame( x = runif(n_samples, 0, state_dims[1]), y = runif(n_samples, 0, state_dims[2]) ) # Simple k-means implementation centers &lt;- samples[sample(nrow(samples), n_centers), ] for (iter in 1:50) { # Assign points to closest centers distances &lt;- as.matrix(dist(rbind(centers, samples))) distances &lt;- distances[(n_centers + 1):(n_centers + n_samples), 1:n_centers] assignments &lt;- apply(distances, 1, which.min) # Update centers new_centers &lt;- centers for (k in 1:n_centers) { if (sum(assignments == k) &gt; 0) { cluster_points &lt;- samples[assignments == k, ] new_centers[k, ] &lt;- colMeans(cluster_points) } } # Check convergence if (sum((new_centers - centers)^2) &lt; 1e-6) break centers &lt;- new_centers } return(as.matrix(centers)) } # Visualization function for activation landscape visualize_activation_landscape &lt;- function(state_dims, resolution = 100) { # Create evaluation grid x_seq &lt;- seq(0, state_dims[1], length.out = resolution) y_seq &lt;- seq(0, state_dims[2], length.out = resolution) grid_df &lt;- expand.grid(x = x_seq, y = y_seq) # Compute total activation at each point grid_df$activation &lt;- apply(grid_df, 1, function(point) { sum(encode(point)) }) # Create visualization p &lt;- ggplot(grid_df, aes(x = x, y = y, fill = activation)) + geom_raster(interpolate = TRUE) + geom_contour(aes(z = activation), color = &quot;white&quot;, alpha = 0.4, size = 0.3) + geom_point( data = as.data.frame(centers), aes(x = V1, y = V2), color = &quot;white&quot;, shape = 4, size = 3, stroke = 1.5, inherit.aes = FALSE ) + scale_fill_viridis_c(name = &quot;Total\\nActivation&quot;, option = &quot;plasma&quot;) + coord_fixed(xlim = c(0, state_dims[1]), ylim = c(0, state_dims[2]), expand = FALSE) + labs( title = &quot;RBF Network: Combined Activation Landscape&quot;, subtitle = paste(&quot;White crosses indicate&quot;, n_rbf, &quot;RBF centers&quot;), x = &quot;State Dimension 1&quot;, y = &quot;State Dimension 2&quot; ) + theme_minimal(base_size = 12) + theme( panel.grid = element_blank(), panel.background = element_rect(fill = &quot;white&quot;, color = NA) ) return(p) } # Visualization function for individual RBFs visualize_individual_rbfs &lt;- function(state_dims, resolution = 50) { # Determine subplot layout n_cols &lt;- ceiling(sqrt(n_rbf)) n_rows &lt;- ceiling(n_rbf / n_cols) # Create evaluation grid x_seq &lt;- seq(0, state_dims[1], length.out = resolution) y_seq &lt;- seq(0, state_dims[2], length.out = resolution) grid_df &lt;- expand.grid(x = x_seq, y = y_seq) # Prepare data for all RBFs all_rbf_data &lt;- list() for (i in 1:n_rbf) { # Compute activation for this RBF only distances_sq &lt;- rowSums((grid_df - matrix(rep(centers[i, ], nrow(grid_df)), nrow = nrow(grid_df), byrow = TRUE))^2) activations &lt;- exp(-distances_sq / (2 * widths[i]^2)) rbf_data &lt;- grid_df rbf_data$activation &lt;- activations rbf_data$rbf_id &lt;- i rbf_data$rbf_label &lt;- paste0(&quot;RBF &quot;, i, &quot; (σ=&quot;, round(widths[i], 1), &quot;)&quot;) all_rbf_data[[i]] &lt;- rbf_data } # Combine all data combined_data &lt;- do.call(rbind, all_rbf_data) # Add center information center_data &lt;- data.frame( x = rep(centers[, 1], each = 1), y = rep(centers[, 2], each = 1), rbf_id = 1:n_rbf, rbf_label = paste0(&quot;RBF &quot;, 1:n_rbf, &quot; (σ=&quot;, round(widths, 1), &quot;)&quot;) ) # Create faceted plot p &lt;- ggplot(combined_data, aes(x = x, y = y, fill = activation)) + geom_raster(interpolate = TRUE) + geom_point( data = center_data, aes(x = x, y = y), color = &quot;white&quot;, shape = 4, size = 2, stroke = 1, inherit.aes = FALSE ) + facet_wrap(~ rbf_label, ncol = n_cols) + scale_fill_viridis_c(option = &quot;plasma&quot;, name = &quot;Activation&quot;) + coord_fixed(expand = FALSE) + labs( title = &quot;Individual RBF Activations&quot;, x = &quot;State Dimension 1&quot;, y = &quot;State Dimension 2&quot; ) + theme_minimal(base_size = 10) + theme( panel.grid = element_blank(), panel.background = element_rect(fill = &quot;white&quot;, color = NA), strip.background = element_rect(fill = &quot;grey90&quot;, color = NA), axis.text = element_text(size = 8) ) return(p) } return(list( encode = encode, visualize_activation_landscape = visualize_activation_landscape, visualize_individual_rbfs = visualize_individual_rbfs, centers = centers, widths = widths, n_rbf = n_rbf, create_adaptive_centers = create_adaptive_centers )) } # Example usage with adaptive center placement set.seed(42) # Create RBF centers using k-means-like clustering rbf_net_temp &lt;- create_rbf_network(matrix(c(0, 0), nrow = 1), 1.0) rbf_centers &lt;- rbf_net_temp$create_adaptive_centers( state_dims = c(10, 10), n_centers = 12 ) # Create RBF network with adaptive centers rbf_network &lt;- create_rbf_network( centers = rbf_centers, widths = 1.5 # Uniform width for all RBFs ) # Visualize activation landscape print(rbf_network$visualize_activation_landscape(state_dims = c(10, 10))) # Visualize individual RBFs print(rbf_network$visualize_individual_rbfs(state_dims = c(10, 10))) 11.4 Comparative Analysis and Practical Considerations The choice between different basis function methods involves tradeoffs across multiple dimensions that depend on the specific requirements of the learning problem. Computational efficiency varies significantly between approaches. Tile coding typically offers the best computational performance because it requires only simple arithmetic operations and guarantees a fixed, small number of active features per state. This predictable activation pattern makes memory access patterns efficient and simplifies the implementation of learning algorithms. Coarse coding can be computationally expensive when implemented naively, requiring distance calculations to all receptive field centers. However, efficient implementations using spatial data structures can dramatically reduce this cost. RBF networks generally require the most computation since they must evaluate all basis functions for every state, though this cost can be acceptable for problems with moderate numbers of basis functions. Memory requirements also differ substantially. Tile coding with N tilings and T tiles per tiling requires N×T features, but the sparse activation pattern means that storage can be optimized. RBF networks must store all centers and widths explicitly, and the dense activation patterns require more memory during computation. The control over generalization represents perhaps the most important distinction between methods. RBF networks provide the finest-grained control through careful placement of centers and tuning of widths. This flexibility makes them particularly suitable for problems where the optimal policy varies smoothly across the state space. Tile coding offers structured, axis-aligned generalization that can be controlled through the number and arrangement of tilings. This asymmetric generalization often matches the structure of control problems where different state dimensions have different physical meanings and scaling properties. Coarse coding provides the most intuitive approach to generalization control but offers less precise tuning than the alternatives. The circular or spherical regions create symmetric generalization that may not align well with the natural structure of some problems. 11.5 Bridging Classical and Modern Approaches These classical basis function methods are not mere historical artifacts but continue to play important roles in modern RL systems. They provide interpretable building blocks that can be combined with neural networks to create hybrid architectures. For instance, RBF networks can serve as the final layer of a deep network, providing smooth interpolation over a learned feature space. Tile coding remains competitive for many control problems, particularly those with moderate dimensionality where its computational advantages outweigh the representational limitations. Perhaps most importantly, these methods provide the conceptual foundation for understanding deep RL. The discrimination-generalization tradeoff that appears explicitly in classical function approximation operates implicitly in neural networks. The stability issues that arise in linear TD learning become even more complex in the nonlinear setting but follow similar patterns. The feature engineering principles that guide classical basis function design inform the architecture choices and training procedures used in deep networks. Understanding how to balance computational efficiency, representational power, and generalization control in classical methods provides invaluable intuition for navigating the much larger design space of modern deep RL systems. The journey from tabular methods to neural networks necessarily passes through this classical territory, making these concepts essential knowledge for any serious RL practitioner. The mathematical principles, design tradeoffs, and practical considerations explored in this appendix provide a solid foundation for understanding both current RL systems and future developments in the field. As the field continues to evolve, these fundamental concepts remain relevant, offering both practical tools for appropriate problems and conceptual frameworks for understanding more sophisticated approaches. "],["learning-policies-versus-learning-values.html", "Chapter 12 Learning Policies versus Learning Values: 12.1 The Two Paradigms of Reinforcement Learning", " Chapter 12 Learning Policies versus Learning Values: In the landscape of reinforcement learning (RL), an agent seeks to learn a strategy—a policy—to maximize cumulative rewards in an uncertain environment. Historically, much of the foundational literature centered on value-based methods. These algorithms, like Q-Learning, focus on learning the value of being in a state or taking an action, and then derive a policy from these value estimates. However, another powerful and more direct paradigm exists: policy-based methods. Instead of learning values first, these methods directly parameterize and optimize the policy itself. This approach is particularly effective in high-dimensional or continuous action spaces and forms the basis for many state-of-the-art algorithms. This post explores the distinction between these two paradigms, delves into the mathematics of the Policy Gradient Theorem—the theoretical cornerstone of policy-based RL—and examines the fundamental differences that make each approach suited to different problem domains. 12.1 The Two Paradigms of Reinforcement Learning At its core, RL is about finding an optimal policy, denoted \\(\\pi(a|s)\\), which is a probability distribution over actions \\(a\\) given a state \\(s\\). The two primary approaches tackle this optimization from fundamentally different angles, each with distinct advantages and theoretical foundations. 12.1.1 Value-Based Methods: Learning Worth Before Action Value-based methods learn a value function that estimates the expected return from any given state or state-action pair. The most fundamental is the action-value function, \\(Q^{\\pi}(s,a)\\), which represents the expected cumulative reward from taking action \\(a\\) in state \\(s\\) and then following policy \\(\\pi\\). This function satisfies the Bellman equation: \\[Q^{\\pi}(s,a) = \\mathbb{E}_{\\pi} \\left[ \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1} \\, \\Big| \\, S_t=s, A_t=a \\right]\\] where \\(\\gamma \\in [0, 1)\\) is the discount factor that weights future rewards. The ultimate goal is to find the optimal action-value function, \\(Q^*(s,a)\\), which satisfies the Bellman optimality equation. Once \\(Q^*(s,a)\\) is known, the optimal policy \\(\\pi^*\\) emerges implicitly by acting greedily at every state: \\[\\pi^*(s) = \\arg\\max_a Q^*(s,a)\\] This indirect approach to policy optimization has proven remarkably successful in discrete environments. Value-based methods often demonstrate superior sample efficiency and come with strong convergence guarantees in tabular settings. However, they face significant challenges when dealing with continuous action spaces, requiring either discretization or complex optimization procedures over the action space at each decision point. Additionally, the policy derived from value estimates can exhibit instability when the underlying value function is noisy or when combined with function approximation. 12.1.2 Policy-Based Methods: Direct Optimization of Behavior Policy-based methods take a fundamentally different approach by bypassing the intermediate step of learning a value function. Instead, they directly parameterize the policy as \\(\\pi_\\theta(a|s)\\), where \\(\\theta\\) is a vector of parameters that might represent the weights of a neural network or coefficients in a linear model. The optimization objective becomes finding the parameters \\(\\theta\\) that maximize a performance measure, typically the expected return from a starting state distribution: \\[J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ G_0 \\right] = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R_{t+1} \\, \\Big| \\, S_0 \\sim \\mu_0 \\right]\\] where \\(\\mu_0\\) represents the initial state distribution. Optimization proceeds through gradient ascent on this objective: \\[\\theta \\leftarrow \\theta + \\alpha \\nabla_\\theta J(\\theta)\\] where \\(\\alpha\\) is the learning rate and \\(\\nabla_\\theta J(\\theta)\\) is the policy gradient. This direct approach naturally accommodates stochastic policies and excels in continuous or high-dimensional action spaces where the greedy action selection of value-based methods becomes computationally intractable. Policy-based methods often exhibit better convergence properties when combined with function approximation, as they avoid the instabilities that can arise from the indirect policy derivation in value-based approaches. However, they typically suffer from high variance in gradient estimates, which can lead to slower convergence and may converge to local rather than global optima. The contrast between these paradigms extends beyond computational considerations. Value-based methods excel when we can accurately estimate the worth of different choices, making them particularly suited to environments with well-defined state-action values. Policy-based methods shine when the optimal behavior is inherently stochastic or when the action space complexity makes value-based approaches impractical. 12.1.2.1 The Policy Gradient Theorem The central challenge in policy-based methods lies in computing the gradient \\(\\nabla_\\theta J(\\theta)\\). The objective function \\(J(\\theta)\\) depends on the complex interplay between the policy parameters, the state and action distributions they generate, and the environment’s dynamics. A naive differentiation approach is intractable because the gradient affects not only the immediate policy decisions but also the distribution of future states encountered. The Policy Gradient Theorem provides an elegant and computationally tractable solution to this challenge. For episodic tasks, the theorem establishes that: \\[\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} G_t \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t) \\right]\\] where \\(G_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} R_{k+1}\\) represents the discounted return from time step \\(t\\) onward. The theorem’s elegance lies in its intuitive interpretation. The term \\(\\nabla_\\theta \\log \\pi_\\theta(A_t|S_t)\\) is known as the score function or eligibility vector. This vector points in the direction within parameter space that most increases the log-probability of taking action \\(A_t\\) from state \\(S_t\\). The policy gradient update pushes the parameters in this direction, weighted by the return \\(G_t\\). When \\(G_t\\) is high, indicating a favorable outcome, the update increases the probability of taking action \\(A_t\\) in state \\(S_t\\). Conversely, when \\(G_t\\) is low, representing an unfavorable outcome, the update decreases this probability. This mechanism provides a direct connection between outcomes and policy adjustments without requiring explicit value function estimates. The mathematical derivation begins by considering the objective as an expectation over all possible trajectories \\(\\tau = (S_0, A_0, R_1, S_1, A_1, \\ldots)\\): \\[J(\\theta) = \\sum_{\\tau} P(\\tau; \\theta) R(\\tau)\\] where \\(R(\\tau)\\) is the total return of trajectory \\(\\tau\\) and \\(P(\\tau; \\theta)\\) is its probability under policy \\(\\pi_\\theta\\). The gradient becomes: \\[\\nabla_\\theta J(\\theta) = \\sum_{\\tau} R(\\tau) \\nabla_\\theta P(\\tau; \\theta)\\] Applying the log-derivative trick, \\(\\nabla_x f(x) = f(x) \\nabla_x \\log f(x)\\), transforms this into: \\[\\nabla_\\theta J(\\theta) = \\sum_{\\tau} R(\\tau) P(\\tau; \\theta) \\nabla_\\theta \\log P(\\tau; \\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ R(\\tau) \\nabla_\\theta \\log P(\\tau; \\theta) \\right]\\] The trajectory probability decomposes as: \\[P(\\tau; \\theta) = p(S_0) \\prod_{t=0}^{T-1} \\pi_\\theta(A_t|S_t) p(S_{t+1}|S_t, A_t)\\] Taking the logarithm and gradient: \\[\\nabla_\\theta \\log P(\\tau; \\theta) = \\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t)\\] The environment dynamics and initial state distribution vanish from the gradient because they don’t depend on \\(\\theta\\). This crucial insight allows us to compute policy gradients without requiring knowledge of the environment model. 12.1.2.2 Variance Reduction Through Baselines While theoretically sound, the basic policy gradient estimate suffers from high variance. The return \\(G_t\\) can vary dramatically between episodes even with identical policies, leading to noisy and unstable learning. The theorem can be enhanced by subtracting a baseline \\(b(S_t)\\) from the return: \\[\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta} \\left[ \\sum_{t=0}^{T-1} (G_t - b(S_t)) \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t) \\right]\\] This modification introduces no bias because: \\[\\mathbb{E}_{\\pi_\\theta} [b(S_t) \\nabla_\\theta \\log \\pi_\\theta(A_t|S_t)] = \\sum_s d^\\pi(s) \\sum_a b(s) \\nabla_\\theta \\pi_\\theta(a|s) = \\sum_s d^\\pi(s) b(s) \\nabla_\\theta \\sum_a \\pi_\\theta(a|s) = 0\\] where \\(d^\\pi(s)\\) is the state visitation distribution under policy \\(\\pi\\), and the final equality follows because \\(\\sum_a \\pi_\\theta(a|s) = 1\\) for all states. The optimal baseline choice is the state-value function \\(V^{\\pi_\\theta}(S_t)\\), which leads to using the advantage function: \\[A^{\\pi_\\theta}(S_t, A_t) = Q^{\\pi_\\theta}(S_t, A_t) - V^{\\pi_\\theta}(S_t) \\approx G_t - V^{\\pi_\\theta}(S_t)\\] This insight bridges the gap between value-based and policy-based methods, forming the theoretical foundation for Actor-Critic architectures. 12.1.2.3 Conclusion The dichotomy between value-based and policy-based methods has largely given way to hybrid approaches that leverage the strengths of both paradigms. The most prominent example is the Actor-Critic family of algorithms, where a critic learns value function estimates to provide low-variance signals for a actor that maintains and updates the policy parameters. In Actor-Critic methods, the critic learns an approximation to the value function \\(V^{\\pi_\\theta}(s)\\), which serves as the baseline in the policy gradient update. This combination addresses the high variance problem of pure policy methods while maintaining the direct policy optimization advantages. Advanced variants like Proximal Policy Optimization (PPO) and Actor-Critic with Experience Replay (ACER) further enhance this synthesis by incorporating additional variance reduction techniques and sample efficiency improvements. The REINFORCE algorithm, one of the earliest and most fundamental policy gradient methods, implements the basic policy gradient theorem directly by using Monte Carlo returns as unbiased estimates of \\(G_t\\). While conceptually important and mathematically elegant, REINFORCE suffers from the high variance issues discussed above. Modern policy gradient methods have largely superseded pure REINFORCE through the incorporation of value function baselines and more sophisticated variance reduction techniques. We will explore these advanced Actor-Critic methods and their practical implementations in detail in subsequent posts. The evolution from value-based to policy-based methods, and ultimately to their synthesis in Actor-Critic architectures, represents a fundamental progression in reinforcement learning theory and practice. Value-based methods provide a solid foundation with strong theoretical guarantees and sample efficiency in appropriate domains. Policy-based methods offer direct optimization of the desired behavior with natural handling of stochastic policies and continuous action spaces. The Policy Gradient Theorem stands as one of the most elegant results in reinforcement learning, providing a principled foundation for direct policy optimization while revealing deep connections between policy improvement and value estimation. Its mathematical beauty lies not just in its derivation, but in how it transforms an apparently intractable optimization problem into a practical algorithm that can learn complex behaviors through experience. Modern reinforcement learning has embraced the complementary nature of these approaches. The most successful algorithms combine the stability and efficiency of value learning with the direct optimization power of policy gradients. This synthesis has enabled breakthroughs in complex domains from robotics to game playing, demonstrating that the apparent dichotomy between learning values and learning policies is better understood as a spectrum of approaches, each with its place in the reinforcement learning toolkit. Understanding both paradigms and their theoretical foundations provides the knowledge necessary to choose appropriate methods for specific problems and to appreciate the sophisticated algorithms that drive today’s most impressive reinforcement learning applications. "],["average-reward-in-reinforcement-learning-a-comprehensive-guide.html", "Chapter 13 Average Reward in Reinforcement Learning: A Comprehensive Guide 13.1 The Limitations of Discounted Reward Formulations 13.2 The Average Reward Alternative: Theoretical Foundations 13.3 Value Functions in the Average Reward Setting 13.4 Optimality Theory for Average Reward 13.5 Learning Algorithms for Average Reward 13.6 Practical Implementation: Server Load Balancing 13.7 Implementation Considerations 13.8 When to Choose Average Reward Over Discounting 13.9 Appendix A: Mathematical Proofs and Derivations", " Chapter 13 Average Reward in Reinforcement Learning: A Comprehensive Guide The overwhelming majority of reinforcement learning (RL) literature centers on maximizing discounted cumulative rewards. This approach, while mathematically elegant and computationally convenient, represents just one way to formalize sequential decision-making problems. For many real-world applications—particularly those involving continuous operation without natural episode boundaries—an alternative formulation based on average reward per time step offers both theoretical advantages and practical benefits that deserve serious consideration. 13.1 The Limitations of Discounted Reward Formulations Traditional RL formulates the control problem around maximizing the expected discounted return: \\[G_t = \\sum_{k=0}^{\\infty} \\gamma^k R_{t+k+1}\\] where \\(0 \\leq \\gamma &lt; 1\\) is the discount factor. The policy evaluation objective becomes: \\[J(\\pi) = \\mathbb{E}_\\pi [G_t] = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\right]\\] This formulation dominates the field for compelling mathematical reasons: it ensures convergence of infinite sums, provides contraction properties for Bellman operators, and offers computational tractability. However, these advantages come with conceptual costs that become apparent when we examine real-world applications. Consider a server load balancing system that must operate continuously for months or years. In such systems, the choice of discount factor becomes arbitrary and potentially misleading. A discount factor of 0.9 implies that a reward received 22 steps in the future is worth only 10% of an immediate reward—a perspective that may not align with the true operational objectives. Similarly, in medical treatment scheduling, financial portfolio management, or industrial control systems, the fundamental goal is often to maximize long-term average performance rather than to optimize a present-value calculation with an artificially imposed time preference. The prevalence of discounted formulations partly stems from RL’s historical focus on episodic tasks with clear terminal states. Games, navigation problems, and many benchmark environments naturally decompose into episodes with definite endings. However, this episodic perspective may be the exception rather than the rule in real-world applications. Manufacturing systems, recommendation engines, traffic control networks, and autonomous vehicles operate in continuing environments where the notion of “episodes” is artificial. 13.2 The Average Reward Alternative: Theoretical Foundations The average reward formulation abandons discounting entirely, focusing instead on the long-run average payoff: \\[\\rho(\\pi) = \\lim_{T \\to \\infty} \\frac{1}{T} \\; \\mathbb{E}_\\pi \\left[ \\sum_{t=1}^T R_t \\right]\\] This quantity represents the steady-state reward rate under policy \\(\\pi\\)—the expected reward per time step in the long run. The control problem becomes: \\[\\pi^* = \\arg\\max_\\pi \\rho(\\pi)\\] Unlike discounted formulations, this objective requires no arbitrary parameters and treats all time steps equally. The interpretation is straightforward: find the policy that maximizes long-term throughput. The average reward formulation relies on ergodic theory for its theoretical foundation. Under standard assumptions—primarily that the Markov chains induced by policies are finite, irreducible, and aperiodic—several important properties hold. The well-definedness property ensures the limit defining \\(\\rho(\\pi)\\) exists and is finite. Initial state independence means the average reward is independent of the starting state. Finally, uniqueness of stationary distribution guarantees each policy induces a unique long-run state distribution. These conditions are satisfied by most practical MDPs, making the average reward criterion broadly applicable. However, care must be taken with absorbing states or poorly connected state spaces, where ergodicity assumptions may fail. A key insight is that average reward directly relates to the stationary distribution induced by a policy. If \\(d^\\pi(s)\\) denotes the long-run probability of being in state \\(s\\) under policy \\(\\pi\\), then: \\[\\rho(\\pi) = \\sum_s d^\\pi(s) \\sum_a \\pi(a|s) \\sum_{s&#39;} P(s&#39;|s,a) R(s,a,s&#39;)\\] This expression reveals that optimizing average reward requires finding policies that induce favorable stationary distributions—those that concentrate probability mass on high-reward regions of the state space. 13.3 Value Functions in the Average Reward Setting Without discounting, the traditional notion of value function becomes problematic. The infinite sum \\(\\sum_{k=0}^\\infty R_{t+k+1}\\) generally diverges, making it impossible to define meaningful state values in the conventional sense. The solution is to work with relative values rather than absolute ones. The differential value function captures the relative advantage of starting in each state: \\[h_\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty (R_{t+1} - \\rho(\\pi)) \\;\\big|\\; S_0 = s \\right]\\] This quantity measures how much better (or worse) it is to start in state \\(s\\) compared to the long-run average. Crucially, this sum converges under ergodic assumptions because the centered rewards \\((R_{t+1} - \\rho(\\pi))\\) have zero mean in the long run. The differential value function satisfies a modified Bellman equation: \\[h_\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s&#39;} P(s&#39;|s,a) \\left[ R(s,a,s&#39;) - \\rho(\\pi) + h_\\pi(s&#39;) \\right]\\] This equation is structurally similar to the discounted case but with two key differences: the immediate reward is adjusted by subtracting the average reward, and no discount factor appears before the future differential value. We can similarly define differential action-value functions: \\[q_\\pi(s,a) = \\sum_{s&#39;} P(s&#39;|s,a) \\left[ R(s,a,s&#39;) - \\rho(\\pi) + h_\\pi(s&#39;) \\right]\\] These satisfy the consistency relation: \\(h_\\pi(s) = \\sum_a \\pi(a|s) q_\\pi(s,a)\\). 13.4 Optimality Theory for Average Reward Optimal policies satisfy the average reward optimality equations: \\[\\rho^* + h^*(s) = \\max_a \\sum_{s&#39;} P(s&#39;|s,a) \\left[ R(s,a,s&#39;) + h^*(s&#39;) \\right]\\] Here, \\(\\rho^*\\) is the optimal average reward (achieved by any optimal policy), and \\(h^*\\) is the optimal differential value function. Unlike the discounted case, all optimal policies achieve the same average reward, though they may differ in their differential values. The policy improvement theorem extends naturally to the average reward setting. Given the differential action-values under the current policy, improvement is achieved by acting greedily: \\[\\pi&#39;(s) = \\arg\\max_a q_\\pi(s,a)\\] This greedy policy is guaranteed to achieve average reward \\(\\rho(\\pi&#39;) \\geq \\rho(\\pi)\\), with equality only when \\(\\pi\\) is already optimal. 13.5 Learning Algorithms for Average Reward The most direct extension of TD learning to average reward settings involves updating both the differential value function and an estimate of the average reward. The average reward TD(0) algorithm proceeds as follows: Initialize: \\(h(s) = 0\\) for all \\(s\\), \\(\\hat{\\rho} = 0\\) For each time step \\(t\\): Observe \\(S_t\\), take action \\(A_t\\), observe \\(R_{t+1}\\) and \\(S_{t+1}\\) Compute TD error: \\(\\delta_t = R_{t+1} - \\hat{\\rho} + h(S_{t+1}) - h(S_t)\\) Update differential value: \\(h(S_t) \\leftarrow h(S_t) + \\alpha \\delta_t\\) Update average reward estimate: \\(\\hat{\\rho} \\leftarrow \\hat{\\rho} + \\beta \\delta_t\\) The step sizes \\(\\alpha\\) and \\(\\beta\\) control the learning rates for differential values and average reward, respectively. Typical choices satisfy \\(\\beta \\ll \\alpha\\) to ensure the average reward estimate changes more slowly than differential values. Convergence of average reward TD methods requires more delicate analysis than their discounted counterparts. The key challenge is that both \\(h\\) and \\(\\rho\\) are being estimated simultaneously, creating a coupled system of stochastic approximations. Under standard assumptions (diminishing step sizes, sufficient exploration, ergodic Markov chains), convergence can be established using two-timescale analysis. The intuition is that \\(\\rho\\) is estimated on a slower timescale (smaller step size), allowing the differential values to track the instantaneous policy while the average reward estimate provides a stable reference point. Q-learning extends to average reward settings through the average reward Q-learning algorithm: Initialize: \\(q(s,a) = 0\\) for all \\((s,a)\\), \\(\\hat{\\rho} = 0\\) For each time step \\(t\\): Observe \\(S_t\\), choose \\(A_t\\) (e.g., \\(\\epsilon\\)-greedily) Observe \\(R_{t+1}\\) and \\(S_{t+1}\\) Compute TD error: \\(\\delta_t = R_{t+1} - \\hat{\\rho} + \\max_a q(S_{t+1}, a) - q(S_t, A_t)\\) Update: \\(q(S_t, A_t) \\leftarrow q(S_t, A_t) + \\alpha \\delta_t\\) Update: \\(\\hat{\\rho} \\leftarrow \\hat{\\rho} + \\beta \\delta_t\\) This algorithm learns differential action-values directly, avoiding the need to maintain a separate state value function. 13.6 Practical Implementation: Server Load Balancing Consider a web server load balancing system with three servers. Requests arrive continuously and must be routed to minimize average response time. This is naturally a continuing task where episodes don’t exist, making it ideal for average reward formulation. # Server Load Balancing with Average Reward Q-Learning library(ggplot2) # Environment setup set.seed(42) n_servers &lt;- 3 max_load &lt;- 5 n_steps &lt;- 50000 # State encoding and reward function encode_state &lt;- function(loads) { sum(loads * (max_load + 1)^(0:(length(loads)-1))) + 1 } decode_state &lt;- function(state_id) { state_id &lt;- state_id - 1 loads &lt;- numeric(n_servers) for (i in 1:n_servers) { loads[i] &lt;- state_id %% (max_load + 1) state_id &lt;- state_id %/% (max_load + 1) } loads } get_reward &lt;- function(loads, server_choice) { new_loads &lt;- loads new_loads[server_choice] &lt;- min(new_loads[server_choice] + 1, max_load) # Response time model: linear in load avg_load &lt;- mean(new_loads) reward &lt;- -avg_load # Simulate request completion for (i in 1:n_servers) { if (new_loads[i] &gt; 0 &amp;&amp; runif(1) &lt; 0.3) { new_loads[i] &lt;- new_loads[i] - 1 } } list(reward = reward, new_loads = new_loads) } # Initialize parameters n_states &lt;- (max_load + 1)^n_servers Q &lt;- array(0, dim = c(n_states, n_servers)) rho_hat &lt;- 0 alpha &lt;- 0.1 beta &lt;- 0.001 epsilon &lt;- 0.1 # Training loop loads &lt;- rep(0, n_servers) state_id &lt;- encode_state(loads) rewards_history &lt;- numeric(n_steps) rho_history &lt;- numeric(n_steps) cat(&quot;Training average reward Q-learning agent...\\n&quot;) ## Training average reward Q-learning agent... for (step in 1:n_steps) { # Epsilon-greedy action selection if (runif(1) &lt; epsilon) { action &lt;- sample(1:n_servers, 1) } else { action &lt;- which.max(Q[state_id, ]) } # Take action and observe reward result &lt;- get_reward(loads, action) reward &lt;- result$reward loads &lt;- result$new_loads next_state_id &lt;- encode_state(loads) # Q-learning update td_error &lt;- reward - rho_hat + max(Q[next_state_id, ]) - Q[state_id, action] Q[state_id, action] &lt;- Q[state_id, action] + alpha * td_error rho_hat &lt;- rho_hat + beta * td_error # Record progress rewards_history[step] &lt;- reward rho_history[step] &lt;- rho_hat # Update state state_id &lt;- next_state_id # Decay exploration if (step %% 1000 == 0) { epsilon &lt;- max(0.01, epsilon * 0.995) if (step %% 10000 == 0) { cat(sprintf(&quot;Step %d: Average reward estimate = %.4f\\n&quot;, step, rho_hat)) } } } ## Step 10000: Average reward estimate = -1.5199 ## Step 20000: Average reward estimate = -1.6557 ## Step 30000: Average reward estimate = -1.6660 ## Step 40000: Average reward estimate = -1.6685 ## Step 50000: Average reward estimate = -1.6553 # Evaluate learned policy eval_steps &lt;- 10000 loads &lt;- rep(0, n_servers) state_id &lt;- encode_state(loads) eval_rewards &lt;- numeric(eval_steps) for (step in 1:eval_steps) { action &lt;- which.max(Q[state_id, ]) result &lt;- get_reward(loads, action) eval_rewards[step] &lt;- result$reward loads &lt;- result$new_loads state_id &lt;- encode_state(loads) } final_avg_reward &lt;- mean(eval_rewards) cat(sprintf(&quot;Final average reward: %.4f\\n&quot;, final_avg_reward)) ## Final average reward: -1.6658 cat(sprintf(&quot;Training estimate: %.4f\\n&quot;, rho_hat)) ## Training estimate: -1.6553 # Policy analysis cat(&quot;\\nLearned policy examples:\\n&quot;) ## ## Learned policy examples: example_states &lt;- list( c(0, 0, 0), # All servers idle c(3, 1, 1), # Server 1 heavily loaded c(2, 2, 2), # Balanced load c(5, 0, 0), # Server 1 at capacity c(3, 3, 4) # High overall load ) for (i in seq_along(example_states)) { loads &lt;- example_states[[i]] state_id &lt;- encode_state(loads) q_values &lt;- Q[state_id, ] best_action &lt;- which.max(q_values) cat(sprintf(&quot;Loads %s -&gt; Route to server %d (Q-values: %.3f, %.3f, %.3f)\\n&quot;, paste(loads, collapse=&quot;, &quot;), best_action, q_values[1], q_values[2], q_values[3])) } ## Loads 0, 0, 0 -&gt; Route to server 2 (Q-values: -0.033, 0.000, 0.000) ## Loads 3, 1, 1 -&gt; Route to server 3 (Q-values: -0.249, -0.171, -0.116) ## Loads 2, 2, 2 -&gt; Route to server 2 (Q-values: -0.434, -0.355, -0.408) ## Loads 5, 0, 0 -&gt; Route to server 3 (Q-values: -0.166, -0.214, -0.154) ## Loads 3, 3, 4 -&gt; Route to server 1 (Q-values: -0.884, -1.003, -0.918) 13.7 Implementation Considerations The choice of step sizes \\(\\alpha\\) and \\(\\beta\\) is crucial for average reward algorithms. The general principle is to use different timescales: a fast timescale (\\(\\alpha\\)) for updates to value functions or policy parameters, and a slow timescale (\\(\\beta\\)) for updates to the average reward estimate. A common choice is \\(\\beta = c \\cdot \\alpha\\) where \\(c \\in [0.01, 0.1]\\). This ensures the average reward estimate stabilizes while allowing value functions to adapt to policy changes. Unlike discounted RL, where initialization typically has limited impact on final performance, average reward methods can be sensitive to initial conditions. Poor initialization of \\(\\hat{\\rho}\\) can lead to slow convergence or temporary instability. Effective strategies include running a random policy for some steps to get an initial estimate of \\(\\hat{\\rho}\\), starting with a conservative estimate based on domain knowledge, or using adaptive step sizes that are larger initially and then decay as estimates stabilize. Exploration-exploitation trade-offs require special consideration in average reward settings. Since there’s no discounting to naturally prioritize immediate rewards, exploration strategies must be designed to maintain long-term effectiveness. Optimistic initialization works particularly well: initializing value functions optimistically encourages exploration while the agent discovers the true reward structure. 13.8 When to Choose Average Reward Over Discounting The choice between discounted and average reward formulations should be based on problem characteristics rather than mathematical convenience. Consider average reward when the task is genuinely continuing with no natural episodes, long-term steady-state performance is the primary concern, the discount factor feels arbitrary or its choice significantly affects learned behavior, or fairness across time steps is important. Moderate indicators include when the environment is relatively stable over long horizons, exploration can be maintained without artificial urgency from discounting, or the system will operate for extended periods without reset. However, choose against average reward when the task has clear episodes or terminal states, short-term performance is crucial (e.g., real-time systems), the environment is highly non-stationary, or mathematical tractability is paramount. Based on empirical studies and practical applications, server systems and manufacturing typically find average reward superior, while games with clear episodes often find discounting more natural. Financial trading uses average reward for long-term strategies and discounting for short-term tactics. In robotics, the choice depends on task duration and safety requirements, while recommendation systems use average reward for user satisfaction and discounting for engagement. 13.9 Appendix A: Mathematical Proofs and Derivations 13.9.1 Convergence of Average Reward TD Learning Theorem: Under standard assumptions (finite state space, ergodic Markov chains, diminishing step sizes satisfying \\(\\sum_t \\alpha_t = \\infty\\), \\(\\sum_t \\alpha_t^2 &lt; \\infty\\), and \\(\\beta_t = o(\\alpha_t)\\)), the average reward TD(0) algorithm converges almost surely to the true differential value function and average reward. Proof sketch: The proof uses two-timescale stochastic approximation theory. The key insight is that the average reward estimate evolves on a slower timescale (smaller step size), allowing it to track the average while the differential values adapt to the current estimate. Let \\(\\{\\alpha_t\\}\\) and \\(\\{\\beta_t\\}\\) be the step size sequences for differential values and average reward, respectively, satisfying \\(\\beta_t = o(\\alpha_t)\\). The updates can be written as: \\[h_{t+1}(s) = h_t(s) + \\alpha_t \\mathbf{1}_{S_t = s} [R_{t+1} - \\rho_t + h_t(S_{t+1}) - h_t(S_t)]\\] \\[\\rho_{t+1} = \\rho_t + \\beta_t [R_{t+1} - \\rho_t + h_t(S_{t+1}) - h_t(S_t)]\\] Using the two-timescale framework, we first analyze the faster timescale (differential values) assuming the average reward estimate is fixed. This leads to the standard convergence analysis for policy evaluation. Then, we analyze the slower timescale (average reward) and show it tracks the true average reward under the converged policy. 13.9.2 Policy Gradient Theorem for Average Reward Theorem (Average Reward Policy Gradient): For a parameterized policy \\(\\pi_\\theta\\), the gradient of the average reward with respect to parameters is: \\[\\nabla_\\theta \\rho(\\pi_\\theta) = \\sum_s d^{\\pi_\\theta}(s) \\sum_a \\nabla_\\theta \\pi_\\theta(a|s) q^{\\pi_\\theta}(s,a)\\] where \\(d^{\\pi_\\theta}(s)\\) is the stationary distribution under policy \\(\\pi_\\theta\\) and \\(q^{\\pi_\\theta}(s,a)\\) is the differential action-value function. Proof: The proof parallels the discounted case but requires careful handling of the limit defining average reward. The key steps involve expressing average reward in terms of stationary distribution, differentiating with respect to \\(\\theta\\), applying the chain rule noting that both the stationary distribution and immediate rewards depend on the policy, using the fundamental matrix approach to handle the derivative of the stationary distribution, and simplifying using the relationship between immediate rewards and differential action-values. 13.9.3 Optimality Equations Derivation Starting from the definition of differential value function: \\[h^*(s) = \\max_\\pi h^\\pi(s)\\] where \\[h^\\pi(s) = \\mathbb{E}^\\pi \\left[ \\sum_{t=0}^\\infty (R_{t+1} - \\rho(\\pi)) \\mid S_0 = s \\right]\\] For the optimal policy \\(\\pi^*\\), we have: \\[h^*(s) = \\mathbb{E}^{\\pi^*} \\left[ \\sum_{t=0}^\\infty (R_{t+1} - \\rho^*) \\mid S_0 = s \\right]\\] Expanding the first term and using the Markov property, since \\(\\pi^*\\) is optimal, it must choose actions to maximize: \\[\\sum_{s&#39;} P(s&#39;|s,a) [R(s,a,s&#39;) - \\rho^* + h^*(s&#39;)]\\] This gives us the optimality equation: \\[\\rho^* + h^*(s) = \\max_a \\sum_{s&#39;} P(s&#39;|s,a) [R(s,a,s&#39;) + h^*(s&#39;)]\\] The average reward formulation of reinforcement learning provides a principled alternative to discounted approaches that better captures the objectives of many real-world applications. While mathematically more challenging than discounted methods, the conceptual clarity and practical benefits make it worthy of broader adoption. The choice between formulations should reflect true problem objectives rather than mathematical convenience, and for many continuing tasks, average reward optimization represents the most appropriate objective for creating systems that serve genuine long-term value. "],["eligibility-traces.html", "Chapter 14 Eligibility Traces 14.1 From One-Step to Multi-Step Learning 14.2 The Mechanics of Eligibility Traces: A Backward View 14.3 The TD(\\(\\\\lambda\\)) Algorithm for Prediction 14.4 Control with Eligibility Traces: Average Reward Sarsa(\\(\\\\lambda\\)) 14.5 Practical Implementation: Server Load Balancing with Sarsa(\\(\\\\lambda\\)) 14.6 Computational and Performance Considerations 14.7 Conclusion", " Chapter 14 Eligibility Traces 14.1 From One-Step to Multi-Step Learning The preceding discussion on average reward reinforcement learning focused on algorithms like TD(0) and Q-learning, which update value estimates based on a one-step lookahead. These methods, known as one-step temporal-difference (TD) methods, perform a “bootstrap” from the value of the very next state. While computationally efficient, this approach can be slow to propagate information through the state space. An error discovered in one state only trickles back to its predecessor on the next visit. For tasks with long causal chains between actions and significant rewards, this one-step credit assignment process becomes a bottleneck, leading to slow convergence. Eligibility traces offer a powerful and elegant mechanism to address this limitation. They provide a bridge between one-step TD methods and computationally expensive Monte Carlo (MC) methods, which update state values only at the end of an episode based on the full sequence of rewards. By maintaining a memory of recently visited states, eligibility traces allow a single TD error to be used to update the values of multiple preceding states simultaneously, dramatically accelerating the learning process. This follow-up explores the theory and application of eligibility traces, with a particular focus on their integration into the average reward framework. 14.2 The Mechanics of Eligibility Traces: A Backward View At its core, an eligibility trace is a short-term memory vector, denoted \\(e\\_t \\\\in \\\\mathbb{R}^{|\\\\mathcal{S}|}\\) for state-value functions or \\(e\\_t \\\\in \\\\mathbb{R}^{|\\\\mathcal{S}| \\\\times |\\\\mathcal{A}|}\\) for action-value functions. Each component of this vector corresponds to a state (or state-action pair) and records a measure of its “eligibility” for learning from the current TD error. A state’s eligibility is highest immediately after it is visited and then decays exponentially over time. The most common form of trace is the accumulating trace. At each time step \\(t\\), the trace for the current state \\(S\\_t\\) is incremented, while the traces for all other states decay. The update rule is given by: \\[e_t(s) = \\begin{cases} \\lambda e_{t-1}(s) + 1 &amp; \\text{if } s = S_t \\\\ \\lambda e_{t-1}(s) &amp; \\text{if } s \\neq S_t \\end{cases}\\] for all \\(s \\\\in \\\\mathcal{S}\\). The parameter \\(\\\\lambda \\\\in [0, 1]\\) is the trace-decay parameter. It controls the rate at which the trace fades. When \\(\\\\lambda = 0\\), the trace vector is non-zero only for the current state, \\(e\\_t(S\\_t)=1\\), which reduces the algorithm to a one-step TD method. As \\(\\\\lambda\\) approaches 1, the traces of past states decay more slowly, and credit from a TD error is assigned more broadly to states visited further in the past. When \\(\\\\lambda=1\\), the update becomes closely related to Monte Carlo methods, where credit is distributed across all prior states in an episode. This formulation is known as the backward view of learning with eligibility traces. The term “backward” refers to the mechanism: at time \\(t+1\\), the TD error \\(\\\\delta\\_t\\) is computed and then used to update the value estimates of states visited before time \\(t\\), with the magnitude of the update for each state weighted by its current trace value. 14.3 The TD(\\(\\\\lambda\\)) Algorithm for Prediction The canonical prediction algorithm using eligibility traces is TD(\\(\\\\lambda\\)). It combines the trace mechanism with the standard TD update. For policy evaluation in the average reward setting, the goal is to learn the differential value function \\(h\\_\\\\pi(s)\\). The algorithm proceeds as follows: Initialize \\(h(s)\\) for all \\(s \\\\in \\\\mathcal{S}\\), and the average reward estimate \\(\\\\hat{\\\\rho}\\). Initialize the eligibility trace vector \\(e(s) = 0\\) for all \\(s\\). For each time step \\(t\\): Observe state \\(S\\_t\\), take action \\(A\\_t\\) (according to policy \\(\\\\pi\\)), and observe reward \\(R\\_{t+1}\\) and next state \\(S\\_{t+1}\\). Compute the differential TD error: \\[\\delta_t = R_{t+1} - \\hat{\\rho} + h(S_{t+1}) - h(S_t)\\] Decay all traces and increment the trace for the current state: \\[e(s) \\leftarrow \\lambda e(s) \\text{ for all } s \\in \\mathcal{S}\\] \\[e(S_t) \\leftarrow e(S_t) + 1\\] Update the value function and average reward estimate for all states: \\[h(s) \\leftarrow h(s) + \\alpha \\delta_t e(s) \\text{ for all } s \\in \\mathcal{S}\\] \\[\\hat{\\rho} \\leftarrow \\hat{\\rho} + \\beta \\delta_t\\] The crucial difference from the one-step TD(0) algorithm lies in step 3d. Instead of updating only the value of the current state \\(S\\_t\\), TD(\\(\\\\lambda\\)) updates the value of every state in proportion to its eligibility trace. A state visited many time steps ago, but whose trace has not fully decayed, will still receive a portion of the learning update from the current TD error. This allows a single surprising outcome (a large \\(\\\\delta\\_t\\)) to immediately inform the values of all states in the causal chain leading up to it. The theoretical justification for this backward mechanism comes from the forward view, which defines a compound target called the \\(\\\\lambda\\)-return. The \\(\\\\lambda\\)-return \\(G\\_t^\\\\lambda\\) is an exponentially weighted average of n-step returns. In the average reward setting, the n-step differential return is: \\[G_{t:t+n} = \\sum_{k=0}^{n-1} (R_{t+k+1} - \\hat{\\rho}) + h(S_{t+n})\\] The TD(\\(\\\\lambda\\)) update, as implemented via the backward view with eligibility traces, has been shown to be an efficient and unbiased way to approximate an update toward the \\(\\\\lambda\\)-return, making it a sound and powerful learning rule. 14.4 Control with Eligibility Traces: Average Reward Sarsa(\\(\\\\lambda\\)) Extending eligibility traces from prediction to control requires applying them to action-value functions. The most direct on-policy control algorithm is Sarsa(\\(\\\\lambda\\)). It follows the same principles as TD(\\(\\\\lambda\\)) but learns a differential action-value function \\(q(s,a)\\) instead of a state-value function \\(h(s)\\). The name Sarsa derives from the quintuple of events that form the basis of its update: \\((S\\_t, A\\_t, R\\_{t+1}, S\\_{t+1}, A\\_{t+1})\\). In the average reward context, the Sarsa(\\(\\\\lambda\\)) algorithm maintains an eligibility trace for each state-action pair, \\(e(s,a)\\). The update proceeds as follows: Initialize \\(q(s,a)\\) for all \\((s,a)\\), and the average reward estimate \\(\\\\hat{\\\\rho}\\). Initialize the eligibility trace vector \\(e(s,a) = 0\\) for all \\((s,a)\\). Choose an initial state \\(S\\_0\\) and an initial action \\(A\\_0\\) (e.g., \\(\\\\epsilon\\)-greedily from \\(q(S\\_0, \\\\cdot)\\)). For each time step \\(t\\): Take action \\(A\\_t\\), observe reward \\(R\\_{t+1}\\) and next state \\(S\\_{t+1}\\). Choose the next action \\(A\\_{t+1}\\) from \\(S\\_{t+1}\\) using the policy derived from \\(q\\) (e.g., \\(\\\\epsilon\\)-greedy). Compute the differential TD error: \\[\\delta_t = R_{t+1} - \\hat{\\rho} + q(S_{t+1}, A_{t+1}) - q(S_t, A_t)\\] Decay all traces and increment the trace for the current state-action pair: \\[e(s,a) \\leftarrow \\lambda e(s,a) \\text{ for all } (s,a)\\] \\[e(S_t, A_t) \\leftarrow e(S_t, A_t) + 1\\] Update the action-value function and average reward estimate for all pairs: \\[q(s,a) \\leftarrow q(s,a) + \\alpha \\delta_t e(s,a) \\text{ for all } (s,a)\\] \\[\\hat{\\rho} \\leftarrow \\hat{\\rho} + \\beta \\delta_t\\] It is important to note the distinction from Q-learning. Sarsa(\\(\\\\lambda\\)) is an on-policy algorithm because the action \\(A\\_{t+1}\\) used to form the TD error is the same action the agent actually takes in the next step. This makes the integration with eligibility traces straightforward. Off-policy algorithms like Q-learning are more complex to combine with traces (e.g., Watkins’s Q(\\(\\\\lambda\\))) because the greedy action used in the update (\\(\\\\max\\_a q(S\\_{t+1}, a)\\)) may not be the one taken by the behavior policy, requiring the trace to be cut or managed differently. 14.5 Practical Implementation: Server Load Balancing with Sarsa(\\(\\\\lambda\\)) We can adapt the previous server load balancing example to use Average Reward Sarsa(\\(\\\\lambda\\)). This modification should demonstrate faster convergence to an optimal policy due to the more efficient credit assignment enabled by eligibility traces. # Server Load Balancing with Average Reward Sarsa(lambda) library(ggplot2) # Environment setup (identical to previous example) set.seed(42) n_servers &lt;- 3 max_load &lt;- 5 n_steps &lt;- 50000 # State encoding and reward function encode_state &lt;- function(loads) { sum(loads * (max_load + 1)^(0:(length(loads)-1))) + 1 } decode_state &lt;- function(state_id) { state_id &lt;- state_id - 1 loads &lt;- numeric(n_servers) for (i in 1:n_servers) { loads[i] &lt;- state_id %% (max_load + 1) state_id &lt;- state_id %/% (max_load + 1) } loads } get_reward &lt;- function(loads, server_choice) { new_loads &lt;- loads new_loads[server_choice] &lt;- min(new_loads[server_choice] + 1, max_load) avg_load &lt;- mean(new_loads) reward &lt;- -avg_load for (i in 1:n_servers) { if (new_loads[i] &gt; 0 &amp;&amp; runif(1) &lt; 0.3) { new_loads[i] &lt;- new_loads[i] - 1 } } list(reward = reward, new_loads = new_loads) } # Initialize parameters n_states &lt;- (max_load + 1)^n_servers Q &lt;- array(0, dim = c(n_states, n_servers)) E &lt;- array(0, dim = c(n_states, n_servers)) # Eligibility traces rho_hat &lt;- 0 alpha &lt;- 0.1 beta &lt;- 0.001 epsilon &lt;- 0.1 lambda &lt;- 0.9 # Trace decay parameter # --- Helper function for epsilon-greedy action selection --- choose_action &lt;- function(state_id, q_table, eps) { if (runif(1) &lt; eps) { return(sample(1:n_servers, 1)) } else { return(which.max(q_table[state_id, ])) } } # Training loop loads &lt;- rep(0, n_servers) state_id &lt;- encode_state(loads) action &lt;- choose_action(state_id, Q, epsilon) rho_history_sarsa_lambda &lt;- numeric(n_steps) cat(&quot;Training average reward Sarsa(lambda) agent...\\n&quot;) ## Training average reward Sarsa(lambda) agent... for (step in 1:n_steps) { # Take action and observe outcome result &lt;- get_reward(loads, action) reward &lt;- result$reward next_loads &lt;- result$new_loads next_state_id &lt;- encode_state(next_loads) # Choose next action (A_{t+1}) for the Sarsa update next_action &lt;- choose_action(next_state_id, Q, epsilon) # Sarsa(lambda) update td_error &lt;- reward - rho_hat + Q[next_state_id, next_action] - Q[state_id, action] # Increment trace for the current state-action pair E[state_id, action] &lt;- E[state_id, action] + 1 # Update Q-values and rho using the trace Q &lt;- Q + alpha * td_error * E rho_hat &lt;- rho_hat + beta * td_error # Decay eligibility traces E &lt;- lambda * E # Record progress and update state/action for next iteration rho_history_sarsa_lambda[step] &lt;- rho_hat state_id &lt;- next_state_id action &lt;- next_action if (step %% 10000 == 0) { cat(sprintf(&quot;Step %d: Average reward estimate = %.4f\\n&quot;, step, rho_hat)) epsilon &lt;- max(0.01, epsilon * 0.95) # Faster decay for demonstration } } ## Step 10000: Average reward estimate = -0.3333 ## Step 20000: Average reward estimate = -0.3333 ## Step 30000: Average reward estimate = -0.3333 ## Step 40000: Average reward estimate = -0.3333 ## Step 50000: Average reward estimate = -0.3333 # Final evaluation and policy analysis (similar to before) # ... [code for evaluation and printing policy examples] ... final_avg_reward_sarsa_lambda &lt;- tail(cumsum(rho_history_sarsa_lambda), 1) / n_steps cat(sprintf(&quot;Final average reward estimate from training: %.4f\\n&quot;, rho_hat)) ## Final average reward estimate from training: -0.3333 14.6 Computational and Performance Considerations The primary advantage of using eligibility traces is improved data efficiency. By propagating information from a single experience to multiple preceding states, TD(\\(\\\\lambda\\)) and Sarsa(\\(\\\\lambda\\)) often converge significantly faster than their one-step counterparts, especially in problems with sparse rewards or long-delayed consequences. This can be critical in real-world applications where data collection is expensive or time-consuming. However, this benefit comes at a computational cost. The one-step update only modifies a single entry in the value table. In contrast, the eligibility trace update requires iterating through and modifying every entry in the value table at each time step. For problems with very large state-action spaces, this can be prohibitively expensive. In practice, this is often mitigated by only updating traces that are above a small threshold, as very old traces contribute negligibly to the update. With the advent of deep reinforcement learning, more sophisticated methods inspired by eligibility traces are used to propagate credit through neural network weights. Furthermore, eligibility traces introduce a new hyperparameter, \\(\\\\lambda\\). The optimal value of \\(\\\\lambda\\) is problem-dependent and often requires tuning. A high \\(\\\\lambda\\) can speed up learning but may also introduce higher variance in the updates, as they become more like Monte Carlo updates. A low \\(\\\\lambda\\) provides more stable but potentially slower learning. The choice of \\(\\\\lambda\\) reflects a bias-variance trade-off, where higher values reduce the bias of bootstrapping but increase the variance from relying on longer reward sequences. 14.7 Conclusion Eligibility traces represent a fundamental and powerful concept in reinforcement learning, elevating simple one-step TD methods into more sophisticated algorithms capable of multi-step credit assignment. By maintaining a decaying record of past states, they enable a single TD error to update a whole chain of preceding value estimates, effectively accelerating the propagation of learned information. As demonstrated, this mechanism is not limited to the traditional discounted setting; it integrates seamlessly with the average reward formulation by using the differential TD error as its learning signal. Algorithms like Average Reward Sarsa(\\(\\\\lambda\\)) are therefore highly effective tools for solving continuing tasks where long-term performance is paramount. While they introduce additional computational complexity and a new hyperparameter, the resulting improvements in data efficiency and convergence speed make eligibility traces an indispensable component of the modern reinforcement learning toolkit. "],["intrinsic-rewards.html", "Chapter 15 Intrinsic Rewards 15.1 The Sparse Reward Problem and Its Implications 15.2 Mathematical Foundations of Intrinsic Motivation 15.3 Practical Implementation: Curiosity-Driven Grid World Navigation 15.4 Advanced Intrinsic Motivation Mechanisms 15.5 Implementation Considerations and Practical Guidelines 15.6 Theoretical Analysis and Convergence Properties 15.7 Applications and Empirical Results 15.8 Limitations and Future Directions", " Chapter 15 Intrinsic Rewards The standard reinforcement learning paradigm assumes agents learn solely from extrinsic rewards provided by the environment. While this approach has achieved remarkable successes in game-playing and robotics, it fundamentally relies on carefully engineered reward functions that capture all aspects of desired behavior. For many real-world applications, designing such reward functions proves either impractical or impossible. Sparse reward environments, where meaningful feedback occurs infrequently, present particular challenges that have motivated the development of intrinsic reward mechanisms. Intrinsic motivation addresses these limitations by endowing agents with internal reward signals that supplement or replace environmental rewards. These mechanisms draw inspiration from biological systems, where curiosity, novelty-seeking, and information-gathering behaviors emerge without explicit external reinforcement. The mathematical formalization of these concepts has led to practical algorithms that demonstrate superior exploration capabilities and learning efficiency across diverse domains. 15.1 The Sparse Reward Problem and Its Implications Traditional RL assumes the existence of informative reward signals that guide learning toward desired behaviors. The agent’s objective is to maximize expected cumulative reward: \\[J(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^T \\gamma^t R_t \\right]\\] where \\(\\tau\\) represents trajectories sampled under policy \\(\\pi\\). This formulation works well when rewards are dense and informative, providing frequent feedback about action quality. However, many environments provide only sparse, delayed, or uninformative rewards. Consider a robot learning to navigate to a goal location in a large maze. The environment provides reward only upon reaching the target, offering no guidance during exploration. Random exploration becomes inefficient as state space size grows, leading to sample complexity that scales exponentially with problem dimension. The fundamental issue lies in the exploration-exploitation dilemma. Without intermediate rewards, agents have no basis for preferring one action over another until accidentally discovering successful strategies. This creates a bootstrapping problem where learning cannot begin until rare rewarding events occur through random chance. Intrinsic motivation mechanisms address this challenge by providing internal reward signals that encourage systematic exploration, information gathering, and skill development. These signals augment the learning process by rewarding potentially useful behaviors even in the absence of extrinsic feedback. 15.2 Mathematical Foundations of Intrinsic Motivation Intrinsic reward mechanisms can be formalized as functions that map agent experiences to internal reward signals. The general framework augments the standard RL objective with intrinsic terms: \\[J_{\\text{total}}(\\pi) = J_{\\text{extrinsic}}(\\pi) + \\lambda J_{\\text{intrinsic}}(\\pi)\\] where \\(\\lambda\\) controls the relative importance of intrinsic versus extrinsic motivation. The intrinsic component takes the form: \\[J_{\\text{intrinsic}}(\\pi) = \\mathbb{E}_{\\tau \\sim \\pi} \\left[ \\sum_{t=0}^T \\gamma^t R_t^{\\text{int}} \\right]\\] The key challenge lies in designing intrinsic reward functions \\(R_t^{\\text{int}}\\) that promote beneficial exploration without overwhelming or conflicting with task objectives. 15.2.1 Curiosity-Driven Learning One prominent approach bases intrinsic rewards on prediction errors. The intuition is that agents should be curious about aspects of the environment they cannot predict well, as these regions likely contain novel or important information. The prediction error curiosity model maintains a forward dynamics model \\(\\hat{f}\\) that attempts to predict next states given current states and actions: \\[\\hat{s}_{t+1} = \\hat{f}(s_t, a_t)\\] The intrinsic reward is proportional to the prediction error: \\[R_t^{\\text{int}} = \\frac{1}{2} \\|\\hat{f}(s_t, a_t) - s_{t+1}\\|^2\\] This approach encourages agents to explore regions where their world model is inaccurate, gradually improving understanding of environment dynamics. However, raw prediction error can be misleading in stochastic environments or those with irrelevant but unpredictable elements. A more sophisticated approach uses the Intrinsic Curiosity Module (ICM) framework, which combines forward and inverse dynamics models. The ICM learns a feature representation \\(\\phi(s)\\) that captures task-relevant aspects of states while filtering out irrelevant details. The inverse dynamics model predicts actions from state transitions: \\[\\hat{a}_t = g(\\phi(s_t), \\phi(s_{t+1}))\\] The forward model predicts feature representations rather than raw states: \\[\\hat{\\phi}(s_{t+1}) = f(\\phi(s_t), a_t)\\] The intrinsic reward combines prediction errors from both models: \\[R_t^{\\text{int}} = \\frac{\\eta}{2} \\|\\hat{\\phi}(s_{t+1}) - \\phi(s_{t+1})\\|^2\\] where \\(\\eta\\) is a scaling factor. This formulation focuses curiosity on aspects of the environment that are both unpredictable and controllable by the agent’s actions. 15.2.2 Count-Based Exploration An alternative approach grounds intrinsic motivation in visitation statistics. States that have been visited infrequently receive higher intrinsic rewards, encouraging systematic exploration of the state space. The classical approach uses pseudo-counts based on density models. For a generative model \\(\\rho\\) that assigns probabilities to states, the pseudo-count for state \\(s\\) after \\(n\\) observations is: \\[N(s) = \\frac{\\rho(s)}{1 - \\rho(s)} \\cdot n\\] The intrinsic reward follows the form: \\[R_t^{\\text{int}} = \\frac{\\beta}{\\sqrt{N(s_t)}}\\] where \\(\\beta\\) controls the exploration bonus magnitude. This creates larger rewards for rarely visited states and smaller rewards for familiar regions. For high-dimensional state spaces, exact counting becomes impractical. The Random Network Distillation (RND) approach addresses this by training a neural network to predict the outputs of a fixed random network. The prediction error serves as a novelty measure: \\[R_t^{\\text{int}} = \\|f_{\\theta}(s_t) - \\hat{f}_{\\phi}(s_t)\\|^2\\] where \\(f_{\\theta}\\) is a randomly initialized fixed network and \\(\\hat{f}_{\\phi}\\) is a trainable predictor. States that are visited frequently will have smaller prediction errors as the predictor network learns to match the random network’s outputs. 15.3 Practical Implementation: Curiosity-Driven Grid World Navigation Consider a grid world environment where an agent must learn to navigate to a goal location. The environment provides rewards only upon reaching the goal, making exploration crucial for learning. # Curiosity-Driven Grid World Navigation library(ggplot2) library(reshape2) # Environment setup set.seed(123) grid_size &lt;- 8 goal_state &lt;- c(grid_size, grid_size) n_episodes &lt;- 500 max_steps &lt;- 100 # State encoding and action space encode_state &lt;- function(row, col) { (row - 1) * grid_size + col } decode_state &lt;- function(state_id) { row &lt;- floor((state_id - 1) / grid_size) + 1 col &lt;- (state_id - 1) %% grid_size + 1 c(row, col) } # Actions: 1=up, 2=right, 3=down, 4=left get_next_state &lt;- function(state, action) { pos &lt;- decode_state(state) row &lt;- pos[1] col &lt;- pos[2] if (action == 1) row &lt;- max(1, row - 1) # up else if (action == 2) col &lt;- min(grid_size, col + 1) # right else if (action == 3) row &lt;- min(grid_size, row + 1) # down else if (action == 4) col &lt;- max(1, col - 1) # left encode_state(row, col) } # Extrinsic reward function get_extrinsic_reward &lt;- function(state) { pos &lt;- decode_state(state) if (all(pos == goal_state)) return(10) return(0) } # Intrinsic Curiosity Module implementation n_states &lt;- grid_size^2 n_actions &lt;- 4 feature_dim &lt;- 4 # Initialize networks (simplified linear models for demonstration) # Forward model: predicts next state features from current features and action forward_model &lt;- array(runif(feature_dim * (feature_dim + n_actions), -0.1, 0.1), dim = c(feature_dim, feature_dim + n_actions)) # Inverse model: predicts action from state transition inverse_model &lt;- array(runif(n_actions * (2 * feature_dim), -0.1, 0.1), dim = c(n_actions, 2 * feature_dim)) # Feature encoder: maps states to feature representations feature_encoder &lt;- array(runif(feature_dim * 2, -0.1, 0.1), dim = c(feature_dim, 2)) # Helper functions for neural network operations get_features &lt;- function(state) { pos &lt;- decode_state(state) normalized_pos &lt;- (pos - 1) / (grid_size - 1) # normalize to [0,1] features &lt;- feature_encoder %*% normalized_pos tanh(features) # activation function } action_to_onehot &lt;- function(action) { onehot &lt;- rep(0, n_actions) onehot[action] &lt;- 1 onehot } # Compute intrinsic reward using ICM get_intrinsic_reward &lt;- function(state, action, next_state, alpha = 0.01) { features_t &lt;- get_features(state) features_t1 &lt;- get_features(next_state) action_onehot &lt;- action_to_onehot(action) # Forward model prediction forward_input &lt;- c(features_t, action_onehot) predicted_features &lt;- as.vector(forward_model %*% forward_input) # Prediction error as intrinsic reward prediction_error &lt;- sum((predicted_features - features_t1)^2) intrinsic_reward &lt;- alpha * prediction_error # Update forward model (simple gradient descent) # Gradient of loss w.r.t. weights: error * input error_gradient &lt;- 2 * (predicted_features - features_t1) # Update each row of the forward model for (i in 1:feature_dim) { forward_model[i, ] &lt;&lt;- forward_model[i, ] - 0.001 * error_gradient[i] * forward_input } intrinsic_reward } # Q-learning with intrinsic motivation Q &lt;- array(0, dim = c(n_states, n_actions)) visit_counts &lt;- array(0, dim = c(n_states)) learning_rate &lt;- 0.1 gamma &lt;- 0.99 epsilon &lt;- 0.3 intrinsic_weight &lt;- 0.5 # Training metrics episode_rewards &lt;- numeric(n_episodes) episode_lengths &lt;- numeric(n_episodes) intrinsic_rewards_history &lt;- list() cat(&quot;Training curiosity-driven agent...\\n&quot;) ## Training curiosity-driven agent... for (episode in 1:n_episodes) { state &lt;- encode_state(1, 1) # start at top-left episode_reward &lt;- 0 episode_intrinsic &lt;- numeric() for (step in 1:max_steps) { visit_counts[state] &lt;- visit_counts[state] + 1 # Epsilon-greedy action selection if (runif(1) &lt; epsilon) { action &lt;- sample(1:n_actions, 1) } else { action &lt;- which.max(Q[state, ]) } # Take action next_state &lt;- get_next_state(state, action) # Compute rewards extrinsic_reward &lt;- get_extrinsic_reward(next_state) intrinsic_reward &lt;- get_intrinsic_reward(state, action, next_state) total_reward &lt;- extrinsic_reward + intrinsic_weight * intrinsic_reward episode_intrinsic &lt;- c(episode_intrinsic, intrinsic_reward) episode_reward &lt;- episode_reward + extrinsic_reward # Q-learning update td_target &lt;- total_reward + gamma * max(Q[next_state, ]) td_error &lt;- td_target - Q[state, action] Q[state, action] &lt;- Q[state, action] + learning_rate * td_error # Check for goal if (extrinsic_reward &gt; 0) { episode_lengths[episode] &lt;- step break } state &lt;- next_state } if (episode_reward == 0) { episode_lengths[episode] &lt;- max_steps } episode_rewards[episode] &lt;- episode_reward intrinsic_rewards_history[[episode]] &lt;- episode_intrinsic # Decay exploration if (episode %% 50 == 0) { epsilon &lt;- max(0.05, epsilon * 0.95) avg_length &lt;- mean(episode_lengths[max(1, episode-49):episode]) success_rate &lt;- mean(episode_rewards[max(1, episode-49):episode] &gt; 0) cat(sprintf(&quot;Episode %d: Success rate %.2f, Avg length %.1f\\n&quot;, episode, success_rate, avg_length)) } } ## Episode 50: Success rate 0.00, Avg length 100.0 ## Episode 100: Success rate 0.00, Avg length 100.0 ## Episode 150: Success rate 0.00, Avg length 100.0 ## Episode 200: Success rate 0.00, Avg length 100.0 ## Episode 250: Success rate 0.00, Avg length 100.0 ## Episode 300: Success rate 0.00, Avg length 100.0 ## Episode 350: Success rate 0.00, Avg length 100.0 ## Episode 400: Success rate 0.00, Avg length 100.0 ## Episode 450: Success rate 0.00, Avg length 100.0 ## Episode 500: Success rate 0.00, Avg length 100.0 # Analyze learning progress success_episodes &lt;- which(episode_rewards &gt; 0) if (length(success_episodes) &gt; 0) { first_success &lt;- min(success_episodes) cat(sprintf(&quot;First successful episode: %d\\n&quot;, first_success)) cat(sprintf(&quot;Final success rate (last 100 episodes): %.2f\\n&quot;, mean(episode_rewards[(n_episodes-99):n_episodes] &gt; 0))) } else { first_success &lt;- n_episodes cat(&quot;No successful episodes found\\n&quot;) } ## No successful episodes found # Visualize learned policy learned_policy &lt;- array(0, dim = c(grid_size, grid_size)) for (row in 1:grid_size) { for (col in 1:grid_size) { state &lt;- encode_state(row, col) learned_policy[row, col] &lt;- which.max(Q[state, ]) } } # Create visualization data frame policy_df &lt;- expand.grid(row = 1:grid_size, col = 1:grid_size) policy_df$action &lt;- as.vector(learned_policy) policy_df$action_char &lt;- c(&quot;↑&quot;, &quot;→&quot;, &quot;↓&quot;, &quot;←&quot;)[policy_df$action] # Create heatmap of visit counts visit_matrix &lt;- matrix(visit_counts, nrow = grid_size, ncol = grid_size, byrow = TRUE) visit_df &lt;- expand.grid(row = 1:grid_size, col = 1:grid_size) visit_df$visits &lt;- as.vector(visit_matrix) # Plot learning curves learning_data &lt;- data.frame( episode = 1:n_episodes, success = as.numeric(episode_rewards &gt; 0), length = episode_lengths ) # Moving average for smoothing (handle edge cases) window_size &lt;- 50 if (n_episodes &gt;= window_size) { success_smooth &lt;- rep(NA, n_episodes) for (i in window_size:n_episodes) { success_smooth[i] &lt;- mean(learning_data$success[(i-window_size+1):i]) } learning_data$success_ma &lt;- success_smooth } else { learning_data$success_ma &lt;- cumsum(learning_data$success) / seq_along(learning_data$success) } # Create learning curve plot p1 &lt;- ggplot(learning_data, aes(x = episode)) + geom_line(aes(y = success_ma), color = &quot;blue&quot;, size = 1) + geom_smooth(aes(y = success), method = &quot;loess&quot;, span = 0.3, se = FALSE, color = &quot;red&quot;) + labs(title = &quot;Learning Progress&quot;, x = &quot;Episode&quot;, y = &quot;Success Rate&quot;, subtitle = &quot;Blue: Moving average, Red: Smoothed trend&quot;) + theme_minimal() ## Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0. ## ℹ Please use `linewidth` instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated. # Create policy visualization p2 &lt;- ggplot(policy_df, aes(x = col, y = grid_size + 1 - row)) + geom_tile(color = &quot;black&quot;, fill = &quot;white&quot;) + geom_text(aes(label = action_char), size = 6) + geom_tile(data = data.frame(col = grid_size, row = 1), aes(x = col, y = row), fill = &quot;gold&quot;, alpha = 0.7) + labs(title = &quot;Learned Policy&quot;, subtitle = &quot;Arrows show best action, Gold = Goal&quot;) + scale_x_continuous(breaks = 1:grid_size) + scale_y_continuous(breaks = 1:grid_size) + theme_minimal() + theme(aspect.ratio = 1) # Create visit count heatmap p3 &lt;- ggplot(visit_df, aes(x = col, y = grid_size + 1 - row, fill = visits)) + geom_tile() + scale_fill_gradient(low = &quot;white&quot;, high = &quot;darkblue&quot;) + labs(title = &quot;State Visitation Frequency&quot;, fill = &quot;Visit Count&quot;) + scale_x_continuous(breaks = 1:grid_size) + scale_y_continuous(breaks = 1:grid_size) + theme_minimal() + theme(aspect.ratio = 1) print(p1) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 49 rows containing missing values or values outside the scale range (`geom_line()`). print(p2) print(p3) cat(&quot;Learned policy (arrows show best action):\\n&quot;) ## Learned policy (arrows show best action): for (row in 1:grid_size) { cat(sprintf(&quot;Row %d: %s\\n&quot;, row, paste(c(&quot;↑&quot;, &quot;→&quot;, &quot;↓&quot;, &quot;←&quot;)[learned_policy[row, ]], collapse = &quot; &quot;))) } ## Row 1: → → → → → → → ← ## Row 2: ↑ ← ↑ ↑ ↑ ↑ ↑ ← ## Row 3: ↑ ↑ ↑ ↑ ↑ ← → ← ## Row 4: ↑ ↑ ↑ ↑ ← ← ← ↑ ## Row 5: ↑ ↑ ↑ ↑ ↑ ↑ ← ↑ ## Row 6: ↑ ↑ → ↑ ↑ ↑ ↑ ↑ ## Row 7: ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ## Row 8: ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ # Compare with baseline (random exploration) cat(&quot;\\nComparing with random baseline...\\n&quot;) ## ## Comparing with random baseline... Q_random &lt;- array(0, dim = c(n_states, n_actions)) random_success &lt;- numeric(n_episodes) for (episode in 1:n_episodes) { state &lt;- encode_state(1, 1) for (step in 1:max_steps) { if (runif(1) &lt; 0.3) { action &lt;- sample(1:n_actions, 1) } else { action &lt;- which.max(Q_random[state, ]) } next_state &lt;- get_next_state(state, action) extrinsic_reward &lt;- get_extrinsic_reward(next_state) # Standard Q-learning (no intrinsic reward) td_target &lt;- extrinsic_reward + gamma * max(Q_random[next_state, ]) td_error &lt;- td_target - Q_random[state, action] Q_random[state, action] &lt;- Q_random[state, action] + learning_rate * td_error if (extrinsic_reward &gt; 0) { random_success[episode] &lt;- 1 break } state &lt;- next_state } } random_first_success &lt;- which(random_success &gt; 0)[1] if (is.na(random_first_success)) random_first_success &lt;- n_episodes cat(sprintf(&quot;Curiosity agent first success: episode %d\\n&quot;, first_success)) ## Curiosity agent first success: episode 500 cat(sprintf(&quot;Random agent first success: episode %d\\n&quot;, random_first_success)) ## Random agent first success: episode 500 if (first_success &lt; n_episodes &amp;&amp; random_first_success &lt; n_episodes) { cat(sprintf(&quot;Improvement: %.1fx faster learning\\n&quot;, random_first_success / first_success)) } else if (first_success &lt; random_first_success) { cat(&quot;Curiosity agent learned faster\\n&quot;) } else { cat(&quot;Both agents had similar performance\\n&quot;) } ## Both agents had similar performance # Final performance comparison curiosity_final_success &lt;- mean(episode_rewards[(n_episodes-99):n_episodes] &gt; 0) random_final_success &lt;- mean(random_success[(n_episodes-99):n_episodes]) cat(sprintf(&quot;Final 100 episodes success rate:\\n&quot;)) ## Final 100 episodes success rate: cat(sprintf(&quot;Curiosity agent: %.2f\\n&quot;, curiosity_final_success)) ## Curiosity agent: 0.00 cat(sprintf(&quot;Random agent: %.2f\\n&quot;, random_final_success)) ## Random agent: 0.00 15.4 Advanced Intrinsic Motivation Mechanisms Beyond basic curiosity and count-based approaches, several sophisticated mechanisms have emerged to address specific challenges in intrinsic motivation. The Go-Explore algorithm addresses the problem of detachment, where agents may discover promising regions but fail to return to them reliably. The method maintains an archive of interesting states and uses goal-conditioned policies to return to promising regions for further exploration. Empowerment provides another perspective on intrinsic motivation, formalizing the intuition that agents should seek states where they have maximal influence over future outcomes. Mathematically, empowerment is defined as the mutual information between actions and future states: \\[\\text{Empowerment}(s) = I(A_{t:t+n}; S_{t+n+1} | S_t = s)\\] This quantity measures how much an agent’s action choices affect future state distributions, providing intrinsic motivation toward states with high controllability. Information Gain approaches reward agents for reducing uncertainty about environment parameters or dynamics. If the agent maintains a belief distribution \\(p(\\theta)\\) over environment parameters, the intrinsic reward for a transition \\((s, a, s&#39;)\\) becomes: \\[R_t^{\\text{int}} = H[p(\\theta | h_t)] - H[p(\\theta | h_t, s_t, a_t, s_{t+1})]\\] where \\(H[\\cdot]\\) denotes entropy and \\(h_t\\) represents the agent’s history up to time \\(t\\). 15.5 Implementation Considerations and Practical Guidelines Successful deployment of intrinsic motivation requires careful attention to several design choices. The balance between intrinsic and extrinsic motivation proves crucial, as excessive intrinsic rewards can prevent agents from focusing on task objectives. The weighting parameter \\(\\lambda\\) typically requires tuning for each domain, with common values ranging from 0.1 to 1.0. The choice of intrinsic mechanism depends on problem characteristics. Curiosity-driven approaches work well in deterministic environments with learnable dynamics, while count-based methods suit stochastic environments where prediction-based curiosity might be misled by irreducant randomness. Information gain approaches excel when the environment has hidden parameters that can be discovered through exploration. Computational overhead represents another important consideration. Some intrinsic motivation mechanisms require training additional neural networks or maintaining complex data structures, potentially doubling computational requirements. Efficient implementations often approximate theoretical ideals to maintain practical feasibility. 15.6 Theoretical Analysis and Convergence Properties The theoretical foundations of intrinsic motivation remain an active area of research. Unlike standard RL, where convergence guarantees are well-established, intrinsic motivation introduces additional complexity through time-varying reward functions that depend on the agent’s learning progress. For count-based approaches, theoretical analysis shows that under certain regularity conditions, the induced exploration behavior leads to polynomial sample complexity for discovering rewarding states. The key insight is that intrinsic rewards create a curriculum that guides agents toward systematic exploration rather than random wandering. Curiosity-driven approaches present greater theoretical challenges due to their adaptive nature. The forward model’s learning progress affects the intrinsic reward signal, creating a non-stationary optimization landscape. Recent work has established convergence guarantees under assumptions of bounded prediction error and sufficient exploration, but practical algorithms often deviate from theoretical requirements. 15.7 Applications and Empirical Results Intrinsic motivation has demonstrated significant improvements across diverse domains. In robotics, curiosity-driven learning enables robots to acquire motion skills without hand-crafted rewards, learning to manipulate objects through self-supervised exploration. The key advantage lies in avoiding the reward engineering bottleneck while still acquiring useful behaviors. Game-playing environments have provided compelling demonstrations of intrinsic motivation’s effectiveness. In exploration-heavy games like Montezuma’s Revenge, curiosity-driven agents achieve superhuman performance while baseline methods fail to make meaningful progress. The sparse reward structure of these games closely mirrors real-world challenges where feedback is delayed or infrequent. Scientific discovery represents an emerging application area where intrinsic motivation shows particular promise. Agents can learn to conduct experiments and form hypotheses about unknown systems purely through curiosity, potentially accelerating research in domains where human intuition provides limited guidance. 15.8 Limitations and Future Directions Despite significant advances, intrinsic motivation faces several fundamental limitations. The reward hacking problem occurs when agents discover ways to maximize intrinsic rewards without learning useful behaviors, such as finding screen regions with unpredictable visual noise in video games. The noisy TV problem illustrates how stochastic elements can mislead curiosity-based agents. If the environment contains sources of unpredictable but irrelevant variation, agents may focus excessive attention on these regions rather than exploring meaningful state space regions. Scale and transfer represent ongoing challenges. Most intrinsic motivation work has focused on relatively simple environments, and scaling to complex real-world domains remains difficult. Additionally, skills acquired through intrinsic motivation don’t always transfer effectively to new tasks or domains. Future research directions include developing more robust intrinsic reward functions that resist exploitation, creating hierarchical approaches that operate across multiple temporal scales, and establishing better theoretical foundations for understanding when and why intrinsic motivation succeeds or fails. The integration of intrinsic motivation with other RL advances, such as meta-learning and multi-task learning, offers promising avenues for creating more capable and autonomous agents. As environments become more complex and reward engineering becomes more challenging, intrinsic motivation mechanisms will likely play increasingly important roles in developing intelligent systems that can learn and adapt independently. "],["policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html", "Chapter 16 Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning 16.1 Introduction 16.2 Theoretical Framework 16.3 Implementation in R 16.4 Experimental Analysis 16.5 Discussion and Implementation Considerations 16.6 Conclusion", " Chapter 16 Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning 16.1 Introduction While value-based methods like Q-learning and Dyna focus on learning the worth of actions to derive optimal policies, they represent an indirect approach to the fundamental problem of reinforcement learning: selecting good actions. Policy gradient methods take a more direct path, explicitly parameterizing and optimizing the policy itself. This shift in perspective opens new possibilities for handling complex action spaces, stochastic optimal policies, and scenarios where the policy structure itself carries important information. The elegance of policy gradients lies in their conceptual clarity. Rather than learning values and hoping they translate into good action choices, we directly adjust the parameters that govern action selection. This approach proves particularly valuable when dealing with continuous action spaces where discrete value functions become unwieldy, or when the optimal policy is inherently stochastic and cannot be captured by deterministic value-based approaches. REINFORCE, introduced by Williams in 1992, stands as the foundational policy gradient algorithm. Despite its simplicity, it embodies the core principles that underpin modern policy optimization methods. By treating each episode as a sample from the policy’s trajectory distribution and using the policy gradient theorem to estimate improvement directions, REINFORCE establishes a direct link between observed rewards and parameter updates. The method’s relationship to supervised learning is illuminating. Where supervised learning adjusts parameters to match target outputs, policy gradients adjust parameters to increase the probability of actions that led to high rewards. This connection reveals why policy gradients can struggle with high variance: unlike supervised learning with clear targets, reinforcement learning must estimate the value of actions from noisy, delayed rewards. 16.2 Theoretical Framework 16.2.1 The Policy Gradient Theorem Policy gradient methods parameterize the policy as \\(\\pi(a|s, \\theta)\\), where \\(\\theta\\) represents learnable parameters. The objective is to maximize the expected return under this policy. For episodic tasks, we can express this as maximizing: \\[J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}[R(\\tau)]\\] where \\(\\tau\\) represents a trajectory \\((s_0, a_0, r_1, s_1, a_1, r_2, \\ldots, s_{T-1}, a_{T-1}, r_T, s_T)\\) and \\(R(\\tau) = \\sum_{t=0}^{T-1} \\gamma^t r_{t+1}\\) is the discounted return. The fundamental challenge lies in computing the gradient \\(\\nabla_\\theta J(\\theta)\\). The policy gradient theorem provides the solution: \\[\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi(a_t|s_t, \\theta) \\cdot R_t(\\tau)\\right]\\] where \\(R_t(\\tau) = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_{k+1}\\) is the return from time step \\(t\\) onward. This formulation reveals several key insights. The gradient is proportional to \\(\\nabla_\\theta \\log \\pi(a_t|s_t, \\theta)\\), which increases the probability of actions when the return is positive and decreases it when negative. The magnitude of adjustment scales with the return magnitude, creating a natural weighting mechanism that emphasizes consequential experiences. 16.2.2 REINFORCE Algorithm The REINFORCE algorithm directly implements the policy gradient theorem through Monte Carlo estimation: For each episode: 1. Generate trajectory \\(\\tau = (s_0, a_0, r_1, \\ldots, s_T)\\) following \\(\\pi(\\cdot|s, \\theta)\\) 2. For each time step \\(t\\) in the trajectory: - Compute return: \\(R_t = \\sum_{k=t}^{T-1} \\gamma^{k-t} r_{k+1}\\) - Compute gradient estimate: \\(\\widehat{g}_t = \\nabla_\\theta \\log \\pi(a_t|s_t, \\theta) \\cdot R_t\\) 3. Update parameters: \\(\\theta \\leftarrow \\theta + \\alpha \\sum_{t=0}^{T-1} \\widehat{g}_t\\) The algorithm’s elegance comes with a cost: high variance in gradient estimates. Since each trajectory provides only one sample of the policy’s behavior, the gradient estimates can vary dramatically between episodes, leading to unstable learning. This variance stems from two sources: the inherent randomness in trajectory generation and the use of full episode returns, which accumulate noise across multiple time steps. 16.2.3 Baseline Subtraction and Variance Reduction A crucial enhancement to basic REINFORCE involves subtracting a baseline from the returns without introducing bias. The modified gradient becomes: \\[\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[\\sum_{t=0}^{T-1} \\nabla_\\theta \\log \\pi(a_t|s_t, \\theta) \\cdot (R_t(\\tau) - b(s_t))\\right]\\] The baseline \\(b(s_t)\\) can be any function of the state that does not depend on the action. A natural choice is the state value function \\(V(s_t)\\), leading to the advantage function \\(A(s_t, a_t) = R_t(\\tau) - V(s_t)\\). This represents how much better the chosen action was compared to the average expected return from that state. When \\(V(s_t)\\) is approximated by a learned value function, the resulting algorithm becomes Actor-Critic, where the policy (actor) is updated using advantages estimated by the value function (critic). This combination leverages both the direct policy optimization of policy gradients and the sample efficiency of value-based methods. 16.2.4 Softmax Policy Parameterization For discrete action spaces, a common parameterization uses the softmax function: \\[\\pi(a|s, \\theta) = \\frac{e^{\\theta_a^T \\phi(s)}}{\\sum_{a&#39;} e^{\\theta_{a&#39;}^T \\phi(s)}}\\] where \\(\\phi(s)\\) represents state features and \\(\\theta_a\\) are the parameters for action \\(a\\). The log probability becomes: \\[\\log \\pi(a|s, \\theta) = \\theta_a^T \\phi(s) - \\log \\sum_{a&#39;} e^{\\theta_{a&#39;}^T \\phi(s)}\\] The gradient with respect to the parameters for action \\(a\\) is: \\[\\nabla_{\\theta_a} \\log \\pi(a|s, \\theta) = \\phi(s) - \\pi(a|s, \\theta) \\phi(s) = \\phi(s)(1 - \\pi(a|s, \\theta))\\] For actions not taken, the gradient is simply \\(-\\pi(a&#39;|s, \\theta) \\phi(s)\\), which decreases their probabilities proportionally. 16.3 Implementation in R Building upon our previous reinforcement learning implementations, we now develop a policy gradient framework that directly optimizes action selection policies. 16.3.1 Environment and Feature Representation We begin with the same 10-state environment used in our Dyna examples, but now we need to consider how to represent states for policy parameterization: # Environment parameters n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.95 terminal_state &lt;- n_states # Environment setup (consistent with previous examples) set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Environment interaction function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } # Feature extraction function extract_features &lt;- function(state) { # Simple one-hot encoding features &lt;- rep(0, n_states) features[state] &lt;- 1 return(features) } 16.3.2 Softmax Policy Implementation We implement a softmax policy that maps state features to action probabilities: softmax_policy &lt;- function(state, theta) { features &lt;- extract_features(state) # Compute logits for each action logits &lt;- rep(0, n_actions) for (a in 1:n_actions) { # Each action has its own parameter vector action_params &lt;- theta[((a-1) * n_states + 1):(a * n_states)] logits[a] &lt;- sum(action_params * features) } # Apply softmax exp_logits &lt;- exp(logits - max(logits)) # Subtract max for numerical stability probabilities &lt;- exp_logits / sum(exp_logits) return(probabilities) } # Sample action from policy sample_action &lt;- function(state, theta) { probs &lt;- softmax_policy(state, theta) action &lt;- sample(1:n_actions, 1, prob = probs) return(list(action = action, probability = probs[action])) } # Compute log probability of an action log_prob &lt;- function(state, action, theta) { probs &lt;- softmax_policy(state, theta) return(log(probs[action])) } # Compute gradient of log probability grad_log_prob &lt;- function(state, action, theta) { features &lt;- extract_features(state) probs &lt;- softmax_policy(state, theta) # Initialize gradient vector grad &lt;- rep(0, length(theta)) for (a in 1:n_actions) { param_indices &lt;- ((a-1) * n_states + 1):(a * n_states) if (a == action) { # Chosen action: gradient is features * (1 - probability) grad[param_indices] &lt;- features * (1 - probs[a]) } else { # Unchosen action: gradient is -features * probability grad[param_indices] &lt;- -features * probs[a] } } return(grad) } 16.3.3 REINFORCE Implementation The core REINFORCE algorithm generates episodes, computes returns, and updates policy parameters: reinforce &lt;- function(episodes = 1000, alpha = 0.01, baseline = FALSE) { # Initialize policy parameters n_params &lt;- n_states * n_actions theta &lt;- rnorm(n_params, 0, 0.1) # Storage for baseline if used if (baseline) { baseline_values &lt;- rep(0, n_states) baseline_counts &lt;- rep(0, n_states) } # Storage for performance tracking episode_returns &lt;- numeric(episodes) episode_lengths &lt;- numeric(episodes) for (ep in 1:episodes) { # Generate episode trajectory &lt;- list() s &lt;- 1 step &lt;- 0 while (s != terminal_state &amp;&amp; step &lt; 100) { # Prevent infinite episodes step &lt;- step + 1 # Sample action from current policy action_result &lt;- sample_action(s, theta) a &lt;- action_result$action # Take action and observe outcome outcome &lt;- sample_env(s, a) s_prime &lt;- outcome$s_prime r &lt;- outcome$reward # Store experience trajectory[[step]] &lt;- list( state = s, action = a, reward = r, log_prob = log_prob(s, a, theta) ) s &lt;- s_prime } # Compute returns for each time step T &lt;- length(trajectory) returns &lt;- numeric(T) G &lt;- 0 for (t in T:1) { G &lt;- trajectory[[t]]$reward + gamma * G returns[t] &lt;- G } # Update baseline if used if (baseline &amp;&amp; T &gt; 0) { for (t in 1:T) { state &lt;- trajectory[[t]]$state baseline_counts[state] &lt;- baseline_counts[state] + 1 # Running average update baseline_values[state] &lt;- baseline_values[state] + (returns[t] - baseline_values[state]) / baseline_counts[state] } } # Compute policy gradient and update parameters gradient &lt;- rep(0, n_params) for (t in 1:T) { state &lt;- trajectory[[t]]$state action &lt;- trajectory[[t]]$action # Advantage estimation advantage &lt;- returns[t] if (baseline) { advantage &lt;- advantage - baseline_values[state] } # Accumulate gradient grad_log_pi &lt;- grad_log_prob(state, action, theta) gradient &lt;- gradient + grad_log_pi * advantage } # Update parameters theta &lt;- theta + alpha * gradient # Record performance if (T &gt; 0) { episode_returns[ep] &lt;- returns[1] # Total episode return episode_lengths[ep] &lt;- T } } result &lt;- list( theta = theta, episode_returns = episode_returns, episode_lengths = episode_lengths ) if (baseline) { result$baseline_values &lt;- baseline_values } return(result) } 16.3.4 Actor-Critic Implementation Building upon REINFORCE, we implement a basic Actor-Critic method that learns both a policy and a value function: actor_critic &lt;- function(episodes = 1000, alpha_actor = 0.01, alpha_critic = 0.1) { # Initialize policy parameters (actor) n_params &lt;- n_states * n_actions theta &lt;- rnorm(n_params, 0, 0.1) # Initialize value function parameters (critic) v_weights &lt;- rnorm(n_states, 0, 0.1) # Performance tracking episode_returns &lt;- numeric(episodes) episode_lengths &lt;- numeric(episodes) td_errors &lt;- list() for (ep in 1:episodes) { s &lt;- 1 step &lt;- 0 episode_td_errors &lt;- numeric() while (s != terminal_state &amp;&amp; step &lt; 100) { step &lt;- step + 1 # Current state value v_s &lt;- v_weights[s] # Sample action from policy action_result &lt;- sample_action(s, theta) a &lt;- action_result$action # Take action outcome &lt;- sample_env(s, a) s_prime &lt;- outcome$s_prime r &lt;- outcome$reward # Next state value (0 if terminal) v_s_prime &lt;- ifelse(s_prime == terminal_state, 0, v_weights[s_prime]) # TD error (this is our advantage estimate) td_error &lt;- r + gamma * v_s_prime - v_s episode_td_errors &lt;- c(episode_td_errors, td_error) # Update critic (value function) v_weights[s] &lt;- v_weights[s] + alpha_critic * td_error # Update actor (policy) grad_log_pi &lt;- grad_log_prob(s, a, theta) theta &lt;- theta + alpha_actor * grad_log_pi * td_error s &lt;- s_prime } # Record episode statistics episode_lengths[ep] &lt;- step td_errors[[ep]] &lt;- episode_td_errors # Estimate episode return by running a test episode if (ep %% 50 == 0) { test_return &lt;- evaluate_policy(theta) episode_returns[ep] &lt;- test_return } } return(list( theta = theta, v_weights = v_weights, episode_returns = episode_returns, episode_lengths = episode_lengths, td_errors = td_errors )) } # Helper function to evaluate current policy evaluate_policy &lt;- function(theta, n_episodes = 10) { total_return &lt;- 0 for (i in 1:n_episodes) { s &lt;- 1 episode_return &lt;- 0 step &lt;- 0 while (s != terminal_state &amp;&amp; step &lt; 100) { step &lt;- step + 1 action_result &lt;- sample_action(s, theta) outcome &lt;- sample_env(s, action_result$action) episode_return &lt;- episode_return + outcome$reward * (gamma ^ (step - 1)) s &lt;- outcome$s_prime } total_return &lt;- total_return + episode_return } return(total_return / n_episodes) } 16.4 Experimental Analysis 16.4.1 Comparison of Policy Gradient Variants We compare REINFORCE with and without baselines, and Actor-Critic to understand their relative performance characteristics: comparative_experiment &lt;- function() { set.seed(123) # Run different variants reinforce_basic &lt;- reinforce(episodes = 1000, alpha = 0.005, baseline = FALSE) reinforce_baseline &lt;- reinforce(episodes = 1000, alpha = 0.005, baseline = TRUE) ac_result &lt;- actor_critic(episodes = 1000, alpha_actor = 0.005, alpha_critic = 0.01) # Create comparison data frame episodes_seq &lt;- 1:1000 # For fair comparison, evaluate all policies at the same intervals eval_episodes &lt;- seq(50, 1000, by = 50) basic_returns &lt;- numeric(1000) baseline_returns &lt;- numeric(1000) ac_returns &lt;- ac_result$episode_returns # Evaluate REINFORCE variants at specified intervals for (i in eval_episodes) { if (i &lt;= length(reinforce_basic$episode_returns)) { # Use stored episode returns instead of re-evaluating basic_returns[i] &lt;- mean(reinforce_basic$episode_returns[max(1, i-10):i], na.rm = TRUE) baseline_returns[i] &lt;- mean(reinforce_baseline$episode_returns[max(1, i-10):i], na.rm = TRUE) } } # Interpolate for smoother plotting for (i in 1:1000) { if (basic_returns[i] == 0 &amp;&amp; i &gt; 1) { basic_returns[i] &lt;- basic_returns[i-1] baseline_returns[i] &lt;- baseline_returns[i-1] } if (ac_returns[i] == 0 &amp;&amp; i &gt; 1) { ac_returns[i] &lt;- ac_returns[i-1] } } comparison_data &lt;- data.frame( episode = rep(episodes_seq, 3), algorithm = rep(c(&quot;REINFORCE&quot;, &quot;REINFORCE + Baseline&quot;, &quot;Actor-Critic&quot;), each = 1000), return = c(basic_returns, baseline_returns, ac_returns) ) return(list( data = comparison_data, basic_result = reinforce_basic, baseline_result = reinforce_baseline, ac_result = ac_result )) } 16.4.1.1 Visualizing Policy Gradient Performance Let’s run the comparative experiment and visualize the learning curves: if (!require(&quot;ggplot2&quot;, quietly = TRUE)) install.packages(&quot;ggplot2&quot;) if (!require(&quot;dplyr&quot;, quietly = TRUE)) install.packages(&quot;dplyr&quot;) ## ## Attaching package: &#39;dplyr&#39; ## The following object is masked from &#39;package:gridExtra&#39;: ## ## combine ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union library(ggplot2) library(dplyr) # Run comparative experiment experiment_results &lt;- comparative_experiment() plot_data &lt;- experiment_results$data # Filter out zero values for cleaner visualization plot_data &lt;- plot_data %&gt;% filter(return != 0) %&gt;% group_by(algorithm) %&gt;% arrange(episode) %&gt;% # Apply some smoothing for visualization mutate(smoothed_return = zoo::rollmean(return, k = 3, fill = NA, align = &quot;right&quot;)) ggplot(plot_data, aes(x = episode, y = smoothed_return, color = algorithm)) + geom_line(size = 1.2, alpha = 0.8) + geom_smooth(method = &quot;loess&quot;, se = FALSE, span = 0.3, size = 0.8) + labs( title = &quot;Policy Gradient Methods Comparison&quot;, subtitle = &quot;Learning curves showing convergence behavior of different policy gradient variants&quot;, x = &quot;Episode&quot;, y = &quot;Average Return&quot;, color = &quot;Algorithm&quot; ) + theme_minimal(base_size = 14) + scale_color_manual(values = c( &quot;REINFORCE&quot; = &quot;#e31a1c&quot;, &quot;REINFORCE + Baseline&quot; = &quot;#1f78b4&quot;, &quot;Actor-Critic&quot; = &quot;#33a02c&quot; )) + theme( legend.position = &quot;bottom&quot;, plot.title = element_text(face = &quot;bold&quot;), plot.subtitle = element_text(color = &quot;grey30&quot;) ) ## `geom_smooth()` using formula = &#39;y ~ x&#39; ## Warning: Removed 6 rows containing non-finite outside the scale range (`stat_smooth()`). ## Warning: Removed 6 rows containing missing values or values outside the scale range (`geom_line()`). 16.4.2 Learning Rate Sensitivity Analysis Policy gradients are particularly sensitive to learning rates due to their high variance. Let’s analyze this relationship: learning_rate_experiment &lt;- function() { learning_rates &lt;- c(0.001, 0.005, 0.01, 0.05, 0.1) results &lt;- list() for (i in seq_along(learning_rates)) { set.seed(42) # Consistent starting conditions lr &lt;- learning_rates[i] result &lt;- reinforce(episodes = 500, alpha = lr, baseline = TRUE) # Calculate final performance (average of last 50 episodes) final_episodes &lt;- max(1, length(result$episode_returns) - 49):length(result$episode_returns) final_performance &lt;- mean(result$episode_returns[final_episodes], na.rm = TRUE) # Calculate convergence speed (episode where performance first exceeds threshold) threshold &lt;- 0.5 # Arbitrary threshold for &quot;good&quot; performance convergence_episode &lt;- which(result$episode_returns &gt; threshold)[1] if (is.na(convergence_episode)) convergence_episode &lt;- 500 results[[i]] &lt;- list( learning_rate = lr, final_performance = final_performance, convergence_episode = convergence_episode, episode_returns = result$episode_returns ) } return(results) } # Visualization function for learning rate analysis plot_learning_rate_analysis &lt;- function(lr_results) { # Extract summary statistics lr_values &lt;- sapply(lr_results, function(x) x$learning_rate) final_perfs &lt;- sapply(lr_results, function(x) x$final_performance) convergence_eps &lt;- sapply(lr_results, function(x) x$convergence_episode) par(mfrow = c(1, 2), mar = c(5, 4, 4, 2) + 0.1) # Plot 1: Final performance vs learning rate plot(lr_values, final_perfs, type = &quot;b&quot;, pch = 16, col = &quot;darkblue&quot;, xlab = &quot;Learning Rate&quot;, ylab = &quot;Final Average Return&quot;, main = &quot;Final Performance vs Learning Rate&quot;, log = &quot;x&quot;) # Log scale for learning rate grid(lty = 1, col = &quot;gray90&quot;) # Plot 2: Convergence speed vs learning rate plot(lr_values, convergence_eps, type = &quot;b&quot;, pch = 16, col = &quot;darkred&quot;, xlab = &quot;Learning Rate&quot;, ylab = &quot;Episodes to Convergence&quot;, main = &quot;Convergence Speed vs Learning Rate&quot;, log = &quot;x&quot;) grid(lty = 1, col = &quot;gray90&quot;) par(mfrow = c(1, 1)) } 16.4.2.1 Running the Learning Rate Analysis # Run the analysis lr_results &lt;- learning_rate_experiment() # Create summary table lr_summary &lt;- data.frame( learning_rate = sapply(lr_results, function(x) x$learning_rate), final_performance = sapply(lr_results, function(x) round(x$final_performance, 3)), convergence_episode = sapply(lr_results, function(x) x$convergence_episode) ) if (!require(&quot;knitr&quot;, quietly = TRUE)) install.packages(&quot;knitr&quot;) knitr::kable(lr_summary, caption = &quot;Learning Rate Sensitivity Analysis Results&quot;, col.names = c(&quot;Learning Rate&quot;, &quot;Final Performance&quot;, &quot;Episodes to Convergence&quot;), align = &#39;c&#39;) Table 16.1: Learning Rate Sensitivity Analysis Results Learning Rate Final Performance Episodes to Convergence 0.001 0.869 1 0.005 0.890 1 0.010 0.887 1 0.050 0.946 1 0.100 0.964 1 # Generate the plots plot_learning_rate_analysis(lr_results) 16.4.3 Variance Analysis of Gradient Estimates One of the key challenges in policy gradients is the high variance of gradient estimates. Let’s analyze this: variance_analysis &lt;- function() { set.seed(123) # Run REINFORCE and track gradient magnitudes n_params &lt;- n_states * n_actions theta &lt;- rnorm(n_params, 0, 0.1) gradient_magnitudes &lt;- numeric() gradient_variances &lt;- list() episode_returns &lt;- numeric() for (ep in 1:100) { # Shorter run for detailed analysis # Generate episode trajectory &lt;- list() s &lt;- 1 step &lt;- 0 while (s != terminal_state &amp;&amp; step &lt; 50) { step &lt;- step + 1 action_result &lt;- sample_action(s, theta) outcome &lt;- sample_env(s, action_result$action) trajectory[[step]] &lt;- list( state = s, action = action_result$action, reward = outcome$reward ) s &lt;- outcome$s_prime } if (step &gt; 0) { # Compute returns T &lt;- length(trajectory) returns &lt;- numeric(T) G &lt;- 0 for (t in T:1) { G &lt;- trajectory[[t]]$reward + gamma * G returns[t] &lt;- G } episode_returns[ep] &lt;- returns[1] # Compute gradient for this episode gradient &lt;- rep(0, n_params) step_gradients &lt;- list() for (t in 1:T) { state &lt;- trajectory[[t]]$state action &lt;- trajectory[[t]]$action grad_log_pi &lt;- grad_log_prob(state, action, theta) step_gradient &lt;- grad_log_pi * returns[t] gradient &lt;- gradient + step_gradient step_gradients[[t]] &lt;- step_gradient } gradient_magnitudes[ep] &lt;- sqrt(sum(gradient^2)) # Store individual step gradients for variance analysis if (length(step_gradients) &gt; 1) { step_gradient_matrix &lt;- do.call(rbind, step_gradients) gradient_variances[[ep]] &lt;- apply(step_gradient_matrix, 2, var) } } } return(list( gradient_magnitudes = gradient_magnitudes, gradient_variances = gradient_variances, episode_returns = episode_returns )) } # Visualization of variance patterns plot_variance_analysis &lt;- function() { var_data &lt;- variance_analysis() episodes &lt;- 1:length(var_data$gradient_magnitudes) par(mfrow = c(2, 1), mar = c(4, 4, 3, 2)) # Plot gradient magnitudes over episodes plot(episodes, var_data$gradient_magnitudes, type = &quot;l&quot;, xlab = &quot;Episode&quot;, ylab = &quot;Gradient Magnitude&quot;, main = &quot;Gradient Magnitude Evolution&quot;, col = &quot;blue&quot;, lwd = 2) grid(lty = 1, col = &quot;gray90&quot;) # Plot relationship between returns and gradient magnitudes valid_indices &lt;- !is.na(var_data$episode_returns) &amp; !is.na(var_data$gradient_magnitudes) plot(var_data$episode_returns[valid_indices], var_data$gradient_magnitudes[valid_indices], xlab = &quot;Episode Return&quot;, ylab = &quot;Gradient Magnitude&quot;, main = &quot;Return vs Gradient Magnitude&quot;, pch = 16, col = &quot;red&quot;) # Add trend line if (sum(valid_indices) &gt; 2) { lm_fit &lt;- lm(var_data$gradient_magnitudes[valid_indices] ~ var_data$episode_returns[valid_indices]) abline(lm_fit, col = &quot;darkred&quot;, lwd = 2) } grid(lty = 1, col = &quot;gray90&quot;) par(mfrow = c(1, 1)) } 16.4.3.1 Example: Analyzing Gradient Variance plot_variance_analysis() 16.5 Discussion and Implementation Considerations Policy gradient methods represent a fundamental shift in how we approach reinforcement learning problems. Rather than indirectly deriving policies from value estimates, they optimize the policy directly, offering both conceptual clarity and practical advantages in specific domains. This directness comes with trade-offs that illuminate deeper principles about learning and optimization in stochastic environments. The high variance inherent in policy gradient estimates reflects a fundamental challenge in reinforcement learning: the credit assignment problem. When an episode yields high rewards, which actions deserve credit? REINFORCE’s approach of crediting all actions in proportion to the total return is statistically unbiased but creates substantial noise in the learning signal. Each episode provides only one sample of the policy’s behavior, and the full episode return conflates the effects of all decisions made during that trajectory. Baseline subtraction addresses this variance problem through a clever insight: we can subtract any state-dependent function from the returns without introducing bias, since the expectation of the gradient of the log probability times the baseline is zero. The optimal baseline minimizes variance and turns out to be the value function itself, leading naturally to Actor-Critic methods. This connection reveals why Actor-Critic algorithms often outperform pure policy gradient approaches—they leverage the variance reduction benefits of value function learning while maintaining the policy optimization focus. The choice of policy parameterization profoundly influences both the optimization landscape and the final solution quality. Softmax policies naturally handle discrete action spaces and provide smooth gradients, but they impose limitations in multi-modal or complex action selection scenarios. The parameterization also determines which policies the algorithm can represent—a critical consideration often overlooked in implementation. Linear parameterizations restrict the policy to relatively simple decision boundaries, while neural network parameterizations enable more complex behaviors at the cost of increased optimization difficulty. Learning rate selection in policy gradients requires more care than in supervised learning due to the non-stationary nature of the optimization problem. As the policy changes, the distribution of states and actions encountered shifts, creating a moving target for the gradient estimates. Too high a learning rate leads to destructive updates that can completely destabilize the policy, while too low a rate results in painfully slow learning. The relationship between learning rate, variance, and convergence speed creates a delicate balancing act that often requires extensive tuning. Implementation in practice demands attention to numerical stability, particularly in the softmax computation and gradient calculations. The log-sum-exp trick prevents overflow in the softmax calculation, while careful handling of log probabilities avoids underflow issues that can lead to infinite gradients. These seemingly minor implementation details can dramatically affect algorithm performance and reliability. The temporal structure of episodes introduces additional complexity absent from supervised learning. Early actions in an episode receive credit for all subsequent rewards, even those that may be largely independent of the early decisions. This temporal credit assignment problem becomes more severe in longer episodes, where the connection between individual actions and final outcomes grows increasingly tenuous. Techniques like eligibility traces and n-step returns attempt to address this by providing more immediate feedback, but they introduce additional hyperparameters and computational overhead. Modern policy gradient methods have evolved sophisticated variance reduction techniques beyond simple baselines. Natural policy gradients account for the geometry of the policy space by incorporating the Fisher information matrix, leading to more stable updates. Trust region methods like TRPO constrain update sizes to prevent catastrophic policy changes. PPO simplifies the trust region approach through clever objective clipping. These advances highlight an important lesson: the basic policy gradient theorem provides a foundation, but practical algorithms require extensive engineering to handle the challenges of high-dimensional, stochastic optimization. The connection between policy gradients and evolutionary strategies offers an interesting perspective on exploration and optimization. Both approaches treat the policy as a whole object to be improved, but evolutionary methods use population-based search rather than gradient ascent. In high-noise environments or with discontinuous reward functions, evolutionary approaches sometimes outperform gradient-based methods, suggesting that the choice between them depends critically on problem structure. Policy gradients excel in domains where the optimal policy is inherently stochastic, such as partially observable environments or games requiring mixed strategies. Value-based methods struggle in these scenarios because they typically converge to deterministic policies. The ability to learn stochastic policies also proves valuable in multi-agent settings where unpredictability can be advantageous, and in continuous control problems where exploration through action noise is natural. The sample efficiency limitations of policy gradients compared to value-based methods reflect the different information each update requires. Q-learning can improve its estimates from any transition, regardless of the policy that generated it. Policy gradients, by contrast, can only learn from on-policy data generated by the current policy. This constraint means that policy gradients often require more environment interactions to achieve the same performance level, though they may converge to better final policies in stochastic or continuous domains. Debugging policy gradient algorithms presents unique challenges because failure modes can be subtle. Unlike supervised learning where prediction errors are immediately obvious, policy gradient failures may manifest as slow learning, instability, or convergence to suboptimal policies. The stochastic nature of both the environment and the policy makes it difficult to distinguish between bad luck and algorithmic problems. Careful tracking of gradient magnitudes, policy entropy, and learning curves becomes essential for diagnosing issues. The relationship between exploration and exploitation in policy gradients differs fundamentally from value-based methods. Rather than using explicit exploration strategies like epsilon-greedy, policy gradients explore naturally through the stochasticity of the learned policy. This approach can be more sample efficient when the policy uncertainty aligns with value uncertainty, but it can also lead to insufficient exploration when the policy becomes overly confident too quickly. Modern implementations often benefit from techniques borrowed from deep learning: gradient clipping prevents explosive updates, batch normalization stabilizes learning, and adaptive learning rates help navigate varying gradient scales across parameters. The integration of policy gradients with deep neural networks has enabled remarkable achievements in complex domains, but it has also introduced new challenges around generalization, catastrophic forgetting, and computational efficiency. 16.6 Conclusion Policy gradient methods represent one of the most theoretically elegant approaches to reinforcement learning, directly optimizing the fundamental object of interest: the policy itself. The journey from the basic policy gradient theorem to modern sophisticated algorithms illustrates how simple mathematical insights can evolve into powerful practical tools through careful engineering and theoretical refinement. The core contribution of policy gradients lies not just in their ability to handle continuous action spaces or learn stochastic policies, but in their conceptual clarity about the learning objective. By framing reinforcement learning as direct policy optimization, these methods provide a natural bridge between classical control theory and modern machine learning. This perspective has proven particularly valuable as the field has moved toward more complex, high-dimensional problems where the clarity of the optimization objective becomes crucial. The evolution from REINFORCE to Actor-Critic to modern methods like PPO and SAC demonstrates how foundational algorithms serve as stepping stones to more sophisticated approaches. Each advancement addresses specific limitations while preserving the core insights that make policy gradients effective. This progressive refinement exemplifies how algorithmic development in reinforcement learning often proceeds: starting with clear theoretical principles and gradually adding practical enhancements to handle real-world complexities. The variance challenge that characterizes policy gradients has driven much innovation in the field. The development of baseline subtraction, control variates, and more sophisticated variance reduction techniques has advanced not only policy gradient methods but also our broader understanding of how to learn from delayed, noisy feedback. These techniques have found applications well beyond reinforcement learning, influencing areas like variational inference and neural architecture search. Looking forward, the principles underlying policy gradients remain relevant even as the field explores new frontiers. The idea of directly optimizing parameterized policies appears in modern approaches to meta-learning, where algorithms learn to adapt their policies quickly to new tasks. The policy gradient theorem’s insights about the relationship between policy changes and performance improvements inform newer methods that aim to learn more efficiently from limited data. The integration of policy gradients with other learning paradigms continues to yield insights. Combining them with model-based methods addresses sample efficiency concerns while preserving the benefits of direct policy optimization. Integration with imitation learning enables more effective learning from demonstrations. These hybrid approaches suggest that the future of reinforcement learning may lie not in choosing between different paradigms but in thoughtfully combining their complementary strengths. Perhaps most importantly, policy gradients have helped establish reinforcement learning as a principled approach to sequential decision making under uncertainty. By providing clear mathematical foundations for policy improvement, they have enabled the field to move beyond heuristic approaches toward systematic algorithms with theoretical guarantees. This mathematical rigor has been essential for the field’s growth and its increasing influence in areas ranging from robotics to finance to game playing. The simplicity of the basic policy gradient update—increase the probability of actions that led to good outcomes—belies the sophisticated mathematical machinery required to make this principle work reliably in practice. This tension between conceptual simplicity and implementation complexity characterizes many successful machine learning algorithms. Policy gradients remind us that the most powerful ideas often have elegant theoretical foundations even when their practical realization requires careful attention to numerous technical details. As reinforcement learning continues to tackle increasingly complex real-world problems, the direct approach of policy gradients—optimizing exactly what we care about rather than hoping proxy objectives lead to good policies—becomes ever more valuable. The clarity of this approach provides a strong foundation for future developments, whether they involve new architectures, better optimization techniques, or novel applications to emerging domains. "],["actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html", "Chapter 17 Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning 17.1 Theoretical Framework 17.2 Implementation and Comparative Analysis 17.3 Computational and Convergence Considerations 17.4 Conclusion", " Chapter 17 Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning The trajectory from pure policy gradient methods to Actor-Critic algorithms represents one of the most elegant theoretical developments in reinforcement learning. While REINFORCE provides a direct path to policy optimization, its high variance and sample inefficiency reveal fundamental limitations that Actor-Critic methods address through a sophisticated marriage of policy and value learning. This synthesis creates algorithms that retain the policy optimization benefits of gradient methods while leveraging the sample efficiency advantages of temporal difference learning. The conceptual leap from REINFORCE to Actor-Critic emerges from a deeper understanding of variance sources in policy gradients. When REINFORCE uses entire episode returns to weight policy updates, it conflates the value of individual actions with the accumulated randomness of complete trajectories. Actor-Critic methods resolve this by decomposing the learning problem into two interacting components: an actor that learns the policy and a critic that estimates state or action values to provide more immediate feedback. This decomposition transforms the high-variance Monte Carlo returns of REINFORCE into lower-variance temporal difference estimates, dramatically improving learning stability and speed. The mathematical foundation underlying this transformation rests on the policy gradient theorem, but with a crucial insight about baseline subtraction. Recall that we can subtract any state-dependent baseline from policy gradient returns without introducing bias. The optimal baseline, in terms of variance minimization, turns out to be the state value function itself. This observation naturally leads to using advantage functions \\(A(s,a) = Q(s,a) - V(s)\\) as the weighting terms in policy updates. Since we typically don’t have access to the true advantage function, Actor-Critic methods learn approximations to both the policy and the value function simultaneously, creating a feedback loop where each component improves the other. The elegance of this approach extends beyond variance reduction. While REINFORCE must wait until episode completion to update its policy, Actor-Critic methods can learn from individual transitions, enabling continuous adaptation throughout each episode. This online learning capability proves particularly valuable in continuing tasks or environments with very long episodes, where the delay inherent in Monte Carlo methods becomes prohibitive. The critic provides immediate feedback about action quality, allowing the actor to adjust its behavior based on short-term consequences rather than distant, noisy episode outcomes. 17.1 Theoretical Framework The Actor-Critic architecture decomposes the policy optimization problem into two parallel learning tasks. The actor maintains a parameterized policy \\(\\pi(a|s, \\theta)\\) and updates its parameters \\(\\theta\\) to maximize expected rewards. The critic maintains a value function approximation, typically the state value function \\(V(s, w)\\) parameterized by weights \\(w\\), though some variants use action-value functions or advantage functions directly. The critic’s role extends beyond simple value estimation. By learning to predict returns from each state, it provides the actor with a baseline for evaluating action quality. When the critic estimates that a state has high value, actions leading to rewards in that state receive less credit than they would under the raw return. Conversely, actions that generate rewards from states the critic considers poor receive amplified credit. This relative weighting scheme helps the actor focus on genuinely surprising outcomes rather than expected rewards. The temporal difference error forms the bridge between actor and critic updates. For each transition \\((s_t, a_t, r_{t+1}, s_{t+1})\\), we compute the TD error: \\[\\delta_t = r_{t+1} + \\gamma V(s_{t+1}, w) - V(s_t, w)\\] This error serves dual purposes: it provides the critic with a learning signal for updating its value estimates, and it gives the actor an estimate of the advantage of action \\(a_t\\) in state \\(s_t\\). The TD error represents how much better or worse the immediate outcome was compared to the critic’s expectation, making it a natural measure of action quality. The actor update follows the standard policy gradient form but uses the TD error as the advantage estimate: \\[\\theta \\leftarrow \\theta + \\alpha_\\theta \\nabla_\\theta \\log \\pi(a_t|s_t, \\theta) \\cdot \\delta_t\\] The critic update uses the same TD error to improve its value predictions: \\[w \\leftarrow w + \\alpha_w \\delta_t \\nabla_w V(s_t, w)\\] The learning rates \\(\\alpha_\\theta\\) and \\(\\alpha_w\\) are typically set independently, allowing fine-tuned control over the relative adaptation speeds of the two components. This separation proves crucial because the actor and critic operate on different timescales and have different convergence properties. The theoretical guarantees for Actor-Critic methods require careful analysis because the algorithm involves two coupled learning processes. The critic’s value function estimates change as the policy evolves, while the policy updates depend on the critic’s current estimates. This interdependence creates a non-stationary learning problem where standard convergence proofs don’t immediately apply. However, under appropriate conditions on learning rates and function approximation, convergence results exist that guarantee both components will improve over time. 17.2 Implementation and Comparative Analysis Building upon our previous policy gradient implementations, we can construct a comprehensive Actor-Critic framework that demonstrates both the theoretical elegance and practical advantages of this approach. The implementation reveals subtle aspects of the algorithm that theory alone cannot capture, particularly regarding the interaction between actor and critic learning dynamics. # Enhanced Actor-Critic implementation with detailed analysis actor_critic_enhanced &lt;- function(episodes = 1000, alpha_actor = 0.01, alpha_critic = 0.1, gamma = 0.95, lambda = 0.9) { # Initialize policy parameters (actor) n_states &lt;- 10 n_actions &lt;- 2 n_params &lt;- n_states * n_actions theta &lt;- rnorm(n_params, 0, 0.1) # Initialize value function parameters (critic) v_weights &lt;- rnorm(n_states, 0, 0.1) # Eligibility traces for both actor and critic theta_eligibility &lt;- rep(0, n_params) v_eligibility &lt;- rep(0, n_states) # Tracking variables for analysis td_errors_history &lt;- list() value_estimates_history &lt;- list() policy_entropy_history &lt;- numeric(episodes) gradient_magnitudes &lt;- numeric(episodes) for (ep in 1:episodes) { # Reset eligibility traces for new episode theta_eligibility &lt;- rep(0, n_params) v_eligibility &lt;- rep(0, n_states) s &lt;- 1 step &lt;- 0 episode_td_errors &lt;- numeric() episode_values &lt;- numeric() while (s != terminal_state &amp;&amp; step &lt; 100) { step &lt;- step + 1 # Store current value estimate v_current &lt;- sum(v_weights * extract_features(s)) episode_values &lt;- c(episode_values, v_current) # Sample action from current policy action_probs &lt;- softmax_policy(s, theta) action &lt;- sample(1:n_actions, 1, prob = action_probs) # Take action and observe outcome outcome &lt;- sample_env(s, action) s_prime &lt;- outcome$s_prime reward &lt;- outcome$reward # Compute TD error if (s_prime == terminal_state) { td_error &lt;- reward - v_current } else { v_next &lt;- sum(v_weights * extract_features(s_prime)) td_error &lt;- reward + gamma * v_next - v_current } episode_td_errors &lt;- c(episode_td_errors, td_error) # Update eligibility traces grad_log_pi &lt;- grad_log_prob(s, action, theta) theta_eligibility &lt;- gamma * lambda * theta_eligibility + grad_log_pi state_features &lt;- extract_features(s) v_eligibility &lt;- gamma * lambda * v_eligibility + state_features # Update actor and critic using eligibility traces theta &lt;- theta + alpha_actor * td_error * theta_eligibility v_weights &lt;- v_weights + alpha_critic * td_error * v_eligibility s &lt;- s_prime } # Calculate policy entropy for this episode entropy &lt;- 0 for (state in 1:(n_states-1)) { probs &lt;- softmax_policy(state, theta) entropy &lt;- entropy - sum(probs * log(probs + 1e-8)) } policy_entropy_history[ep] &lt;- entropy / (n_states - 1) # Store episode statistics td_errors_history[[ep]] &lt;- episode_td_errors value_estimates_history[[ep]] &lt;- episode_values # Compute gradient magnitude for analysis if (length(episode_td_errors) &gt; 0) { avg_td_error &lt;- mean(episode_td_errors) gradient_mag &lt;- sqrt(sum((alpha_actor * avg_td_error * theta_eligibility)^2)) gradient_magnitudes[ep] &lt;- gradient_mag } } return(list( theta = theta, v_weights = v_weights, td_errors_history = td_errors_history, value_estimates_history = value_estimates_history, policy_entropy_history = policy_entropy_history, gradient_magnitudes = gradient_magnitudes )) } # Comparative evaluation function compare_methods &lt;- function(episodes = 800) { set.seed(42) # Run REINFORCE with baseline reinforce_result &lt;- reinforce(episodes = episodes, alpha = 0.005, baseline = TRUE) # Run basic Actor-Critic ac_basic &lt;- actor_critic(episodes = episodes, alpha_actor = 0.005, alpha_critic = 0.02) # Run enhanced Actor-Critic with eligibility traces ac_enhanced &lt;- actor_critic_enhanced(episodes = episodes, alpha_actor = 0.005, alpha_critic = 0.02, lambda = 0.9) # Evaluate final policies evaluation_episodes &lt;- 20 reinforce_performance &lt;- evaluate_policy(reinforce_result$theta, evaluation_episodes) ac_basic_performance &lt;- evaluate_policy(ac_basic$theta, evaluation_episodes) ac_enhanced_performance &lt;- evaluate_policy(ac_enhanced$theta, evaluation_episodes) return(list( reinforce = list(result = reinforce_result, performance = reinforce_performance), ac_basic = list(result = ac_basic, performance = ac_basic_performance), ac_enhanced = list(result = ac_enhanced, performance = ac_enhanced_performance) )) } The enhanced implementation incorporates eligibility traces, which extend the credit assignment mechanism of Actor-Critic methods. Rather than updating parameters based only on the current transition, eligibility traces create a decaying memory of recent state-action pairs, allowing TD errors to update multiple previous estimates simultaneously. This mechanism addresses the temporal credit assignment problem more effectively than basic one-step updates, particularly in environments where the consequences of actions unfold over multiple time steps. The inclusion of policy entropy tracking provides insights into the exploration-exploitation dynamics of Actor-Critic learning. As the algorithm progresses, we expect the policy entropy to decrease as the policy becomes more deterministic around optimal actions. However, premature entropy collapse can indicate insufficient exploration, while persistently high entropy might suggest convergence problems. Monitoring this quantity helps diagnose learning difficulties and guide hyperparameter selection. 17.2.1 Variance Analysis and Learning Dynamics The fundamental advantage of Actor-Critic methods lies in their variance reduction compared to pure policy gradient approaches. To understand this improvement quantitatively, we need to examine how the different components contribute to gradient estimate variance and how the critic’s learning affects overall stability. # Detailed variance analysis function analyze_learning_dynamics &lt;- function() { comparison_results &lt;- compare_methods(episodes = 600) # Extract TD errors from Actor-Critic results ac_td_errors &lt;- comparison_results$ac_enhanced$result$td_errors_history # Compute variance statistics episode_td_variances &lt;- sapply(ac_td_errors, function(errors) { if (length(errors) &gt; 1) var(errors) else 0 }) episode_td_means &lt;- sapply(ac_td_errors, function(errors) { if (length(errors) &gt; 0) mean(errors) else 0 }) # Analyze value function learning value_histories &lt;- comparison_results$ac_enhanced$result$value_estimates_history # Compute how value estimates change over time for each state value_evolution &lt;- matrix(0, nrow = 600, ncol = 9) # Excluding terminal state for (ep in 1:min(600, length(value_histories))) { if (length(value_histories[[ep]]) &gt; 0) { # Take first value estimate of episode (from starting state) value_evolution[ep, 1] &lt;- value_histories[[ep]][1] } } return(list( td_variances = episode_td_variances, td_means = episode_td_means, value_evolution = value_evolution, policy_entropy = comparison_results$ac_enhanced$result$policy_entropy_history, gradient_magnitudes = comparison_results$ac_enhanced$result$gradient_magnitudes )) } # Visualization function for learning dynamics plot_learning_dynamics &lt;- function() { dynamics_data &lt;- analyze_learning_dynamics() par(mfrow = c(2, 2), mar = c(4, 4, 3, 2)) # Plot 1: TD error variance over episodes episodes &lt;- 1:length(dynamics_data$td_variances) plot(episodes, dynamics_data$td_variances, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, xlab = &quot;Episode&quot;, ylab = &quot;TD Error Variance&quot;, main = &quot;TD Error Variance Evolution&quot;) # Add smoothed trend line if (length(dynamics_data$td_variances) &gt; 10) { smoothed_var &lt;- zoo::rollmean(dynamics_data$td_variances, k = 20, fill = NA) lines(episodes, smoothed_var, col = &quot;red&quot;, lwd = 2, lty = 2) } grid(col = &quot;gray90&quot;) # Plot 2: Policy entropy evolution plot(episodes, dynamics_data$policy_entropy[1:length(episodes)], type = &quot;l&quot;, col = &quot;darkgreen&quot;, lwd = 2, xlab = &quot;Episode&quot;, ylab = &quot;Policy Entropy&quot;, main = &quot;Policy Entropy Evolution&quot;) grid(col = &quot;gray90&quot;) # Plot 3: Value function learning for starting state plot(episodes, dynamics_data$value_evolution[1:length(episodes), 1], type = &quot;l&quot;, col = &quot;purple&quot;, lwd = 2, xlab = &quot;Episode&quot;, ylab = &quot;Value Estimate&quot;, main = &quot;Value Function Learning (Start State)&quot;) grid(col = &quot;gray90&quot;) # Plot 4: Gradient magnitude evolution plot(episodes, dynamics_data$gradient_magnitudes[1:length(episodes)], type = &quot;l&quot;, col = &quot;orange&quot;, lwd = 2, xlab = &quot;Episode&quot;, ylab = &quot;Gradient Magnitude&quot;, main = &quot;Policy Gradient Magnitudes&quot;) grid(col = &quot;gray90&quot;) par(mfrow = c(1, 1)) } The variance analysis reveals several key insights about Actor-Critic learning dynamics. Initially, TD errors exhibit high variance as both the policy and value function are poorly calibrated. As learning progresses, we typically observe a reduction in TD error variance, indicating that the critic is becoming better at predicting returns and providing more stable feedback to the actor. However, this trend isn’t monotonic—periods of increased variance often correspond to significant policy changes as the actor explores new behaviors. The relationship between policy entropy and learning progress provides another window into algorithm behavior. Healthy Actor-Critic learning typically shows a gradual decline in entropy as the policy concentrates probability mass on better actions. Sudden entropy drops might indicate premature convergence to suboptimal policies, while persistently high entropy could suggest learning difficulties or inappropriate hyperparameters. The entropy evolution also reveals the exploration-exploitation balance inherent in the algorithm’s design. # Execute the analysis plot_learning_dynamics() 17.2.2 Algorithmic Variants and Extensions The basic Actor-Critic framework admits numerous variations, each addressing specific limitations or targeting particular problem domains. These variants illustrate the flexibility of the core concept while revealing the subtle design choices that affect algorithm behavior. One significant variant involves the choice of critic architecture. While our implementation uses state value functions, action-value critics that learn \\(Q(s,a,w)\\) provide richer information about individual action values. The advantage estimation becomes more direct since \\(A(s,a) = Q(s,a) - \\max_{a&#39;} Q(s,a&#39;)\\), but the learning problem becomes more complex as the critic must learn values for all state-action pairs rather than just states. Advantage Actor-Critic (A2C) methods represent another important direction, explicitly learning advantage functions rather than deriving them from value estimates. This approach can reduce bias in advantage estimates but requires careful handling of the advantage function’s inherent constraints—advantages must sum to zero across actions for any given state. The temporal scope of updates offers another dimension for algorithmic variation. Our implementation uses one-step TD errors, but n-step returns provide a natural interpolation between the low-variance, high-bias one-step estimates and the high-variance, low-bias Monte Carlo returns of REINFORCE. The parameter \\(\\lambda\\) in TD(\\(\\lambda\\)) methods controls this interpolation, with \\(\\lambda = 0\\) corresponding to pure TD learning and \\(\\lambda = 1\\) approximating Monte Carlo methods. # N-step Actor-Critic implementation n_step_actor_critic &lt;- function(episodes = 500, n_steps = 5, alpha_actor = 0.01, alpha_critic = 0.1) { # Initialize parameters n_states &lt;- 10 n_actions &lt;- 2 n_params &lt;- n_states * n_actions theta &lt;- rnorm(n_params, 0, 0.1) v_weights &lt;- rnorm(n_states, 0, 0.1) episode_returns &lt;- numeric(episodes) for (ep in 1:episodes) { # Store trajectory for n-step updates trajectory &lt;- list() s &lt;- 1 step &lt;- 0 while (s != terminal_state &amp;&amp; step &lt; 100) { step &lt;- step + 1 # Store current state and value current_value &lt;- sum(v_weights * extract_features(s)) # Sample action action_probs &lt;- softmax_policy(s, theta) action &lt;- sample(1:n_actions, 1, prob = action_probs) # Take action outcome &lt;- sample_env(s, action) # Store experience trajectory[[step]] &lt;- list( state = s, action = action, reward = outcome$reward, value = current_value ) s &lt;- outcome$s_prime # Perform n-step update if we have enough steps if (length(trajectory) &gt;= n_steps) { update_idx &lt;- length(trajectory) - n_steps + 1 # Compute n-step return n_step_return &lt;- 0 for (i in 0:(n_steps-1)) { if (update_idx + i &lt;= length(trajectory)) { n_step_return &lt;- n_step_return + (gamma^i) * trajectory[[update_idx + i]]$reward } } # Add bootstrapped value if not terminal if (s != terminal_state) { bootstrap_value &lt;- sum(v_weights * extract_features(s)) n_step_return &lt;- n_step_return + (gamma^n_steps) * bootstrap_value } # Compute advantage advantage &lt;- n_step_return - trajectory[[update_idx]]$value # Update actor update_state &lt;- trajectory[[update_idx]]$state update_action &lt;- trajectory[[update_idx]]$action grad_log_pi &lt;- grad_log_prob(update_state, update_action, theta) theta &lt;- theta + alpha_actor * advantage * grad_log_pi # Update critic state_features &lt;- extract_features(update_state) v_weights &lt;- v_weights + alpha_critic * advantage * state_features } } # Final updates for remaining trajectory while (length(trajectory) &gt; 0) { update_idx &lt;- 1 remaining_steps &lt;- min(n_steps, length(trajectory)) # Compute return for remaining steps remaining_return &lt;- 0 for (i in 0:(remaining_steps-1)) { remaining_return &lt;- remaining_return + (gamma^i) * trajectory[[update_idx + i]]$reward } advantage &lt;- remaining_return - trajectory[[update_idx]]$value # Update parameters update_state &lt;- trajectory[[update_idx]]$state update_action &lt;- trajectory[[update_idx]]$action grad_log_pi &lt;- grad_log_prob(update_state, update_action, theta) theta &lt;- theta + alpha_actor * advantage * grad_log_pi state_features &lt;- extract_features(update_state) v_weights &lt;- v_weights + alpha_critic * advantage * state_features trajectory &lt;- trajectory[-1] # Remove processed step } # Evaluate episode performance periodically if (ep %% 50 == 0) { episode_returns[ep] &lt;- evaluate_policy(theta, n_episodes = 5) } } return(list( theta = theta, v_weights = v_weights, episode_returns = episode_returns )) } The n-step variant demonstrates how Actor-Critic methods can be positioned along the bias-variance spectrum. Larger values of n reduce bias by incorporating more actual rewards before bootstrapping with value estimates, but they increase variance by accumulating more stochastic transitions. The optimal choice depends on the environment’s characteristics and the quality of value function approximation. 17.3 Computational and Convergence Considerations The practical success of Actor-Critic methods depends critically on the relative learning rates of actor and critic components. This relationship affects both convergence speed and final solution quality in ways that theory alone cannot fully predict. The critic must learn accurate value estimates to provide useful feedback to the actor, but the actor’s changing policy continuously shifts the target that the critic is trying to learn. This creates a non-stationary learning problem that requires careful balancing. Empirically, critics typically require faster learning rates than actors because value function learning resembles supervised learning with clear targets, while policy learning must navigate a more complex optimization landscape. However, if the critic learns too quickly relative to the actor, it may overfit to the current policy’s behavior and provide misleading advantage estimates. Conversely, a critic that learns too slowly provides noisy, outdated feedback that can destabilize actor learning. The choice of function approximation architecture significantly influences both components’ behavior. Linear function approximation provides theoretical guarantees but limits representational capacity, while neural network approximation enables complex policies and value functions but introduces optimization challenges. The interaction between actor and critic function approximation creates additional complexity—errors in one component can compound in the other, leading to divergent behavior even when individual components would converge in isolation. Memory and computational requirements offer another practical consideration. Actor-Critic methods require maintaining and updating two sets of parameters, roughly doubling the memory overhead compared to pure policy gradient approaches. However, this cost is often offset by improved sample efficiency, as Actor-Critic methods typically require fewer environment interactions to achieve good performance. The online nature of Actor-Critic learning provides both advantages and challenges for practical implementation. The ability to learn from individual transitions enables continuous adaptation and makes these methods suitable for online learning scenarios. However, this same characteristic makes them sensitive to the sequence of experiences encountered during learning. Unlike batch methods that can smooth over bad experiences, Actor-Critic algorithms must cope with whatever sequence the current policy generates. 17.3.1 Comparative Performance Analysis To fully appreciate the strengths and limitations of Actor-Critic methods, we need systematic comparison with alternative approaches across multiple performance dimensions. Raw sample efficiency tells only part of the story—we must also consider learning stability, final performance quality, and computational efficiency. # Comprehensive comparison function comprehensive_comparison &lt;- function() { set.seed(123) episodes &lt;- 600 # Methods to compare methods &lt;- list( reinforce_basic = function() reinforce(episodes, alpha = 0.003, baseline = FALSE), reinforce_baseline = function() reinforce(episodes, alpha = 0.003, baseline = TRUE), actor_critic_basic = function() actor_critic(episodes, alpha_actor = 0.003, alpha_critic = 0.015), actor_critic_enhanced = function() actor_critic_enhanced(episodes, alpha_actor = 0.003, alpha_critic = 0.015, lambda = 0.8), n_step_ac = function() n_step_actor_critic(episodes, n_steps = 3, alpha_actor = 0.003, alpha_critic = 0.015) ) results &lt;- list() performance_metrics &lt;- data.frame( method = names(methods), final_performance = numeric(length(methods)), convergence_speed = numeric(length(methods)), learning_stability = numeric(length(methods)), stringsAsFactors = FALSE ) for (i in seq_along(methods)) { method_name &lt;- names(methods)[i] cat(&quot;Running&quot;, method_name, &quot;...\\n&quot;) result &lt;- methods[[i]]() results[[method_name]] &lt;- result # Evaluate final performance final_perf &lt;- evaluate_policy(result$theta, n_episodes = 20) performance_metrics$final_performance[i] &lt;- final_perf # Estimate convergence speed (episodes to reach 80% of final performance) if (&quot;episode_returns&quot; %in% names(result)) { returns &lt;- result$episode_returns[result$episode_returns &gt; 0] if (length(returns) &gt; 10) { target_perf &lt;- 0.8 * final_perf convergence_ep &lt;- which(returns &gt;= target_perf)[1] performance_metrics$convergence_speed[i] &lt;- ifelse(is.na(convergence_ep), episodes, convergence_ep) } } # Measure learning stability (coefficient of variation in later episodes) if (&quot;episode_returns&quot; %in% names(result)) { returns &lt;- result$episode_returns[result$episode_returns &gt; 0] if (length(returns) &gt; 50) { later_returns &lt;- tail(returns, 50) cv &lt;- sd(later_returns) / mean(later_returns) performance_metrics$learning_stability[i] &lt;- cv } } } return(list( results = results, metrics = performance_metrics )) } # Run comprehensive comparison comparison_results &lt;- comprehensive_comparison() print(comparison_results$metrics) The comprehensive comparison reveals that Actor-Critic methods generally achieve better sample efficiency than pure policy gradient approaches, typically converging faster and with greater stability. The enhanced Actor-Critic with eligibility traces often shows the best overall performance, combining rapid initial learning with stable convergence. However, the relative performance depends significantly on hyperparameter tuning, and the added complexity of Actor-Critic methods can make them more sensitive to parameter choices. The n-step variant demonstrates an interesting trade-off between the extremes of one-step TD and full Monte Carlo methods. With appropriate step sizes, it often achieves faster initial learning than basic Actor-Critic while maintaining better stability than REINFORCE. This flexibility makes n-step methods particularly attractive for practitioners who need to balance learning speed with stability requirements. Learning stability metrics reveal another advantage of Actor-Critic methods. The continuous value function updates provide a stabilizing influence on policy learning, reducing the wild oscillations that can plague pure policy gradient methods. This stability proves especially valuable in environments with sparse rewards, where the immediate feedback provided by the critic helps maintain learning progress even when episodes rarely reach rewarding states. 17.4 Conclusion The successful implementation of Actor-Critic methods requires attention to numerous subtle details that can dramatically affect performance. The initialization of both actor and critic parameters influences early learning dynamics, with poor initialization potentially leading to unstable or slow convergence. The critic should typically be initialized to provide reasonable value estimates for early policy evaluation, while the actor initialization should promote adequate exploration during initial learning. The coupling between actor and critic learning rates creates a multi-dimensional optimization problem for hyperparameter selection. Traditional grid search becomes prohibitively expensive, and the optimal ratio often depends on problem-specific characteristics. Adaptive learning rate methods can help, but they must account for the non-stationary nature of the optimization landscape as both components evolve. Numerical stability considerations become more complex in Actor-Critic methods because errors can propagate between components. Gradient clipping proves essential for both actor and critic updates, but the appropriate clipping thresholds may differ between components. The critic’s value predictions should be monitored for explosive growth, which can destabilize the entire algorithm. The choice of advantage estimation method significantly affects practical performance. While we’ve focused on TD error-based advantages, other approaches like Generalized Advantage Estimation (GAE) provide additional control over the bias-variance trade-off. These methods use exponentially-weighted combinations of n-step advantages, offering fine-grained control over temporal credit assignment. Actor-Critic methods represent a sophisticated synthesis of the two major paradigms in reinforcement learning: policy optimization and value function learning. By combining these approaches, they address key limitations of pure methods while introducing new complexities that require careful management. The theoretical elegance of using learned value functions as variance-reducing baselines translates into practical algorithms that often outperform their constituent components. The journey from REINFORCE to Actor-Critic illustrates how algorithmic development in reinforcement learning often involves identifying and addressing specific failure modes through principled extensions. The high variance problem of policy gradients finds its solution not through abandoning the core approach but through augmenting it with complementary techniques. This pattern of incremental improvement built on solid theoretical foundations characterizes much of the progress in modern reinforcement learning. The flexibility of the Actor-Critic framework enables numerous variants, each targeting specific aspects of the learning problem. From eligibility traces to n-step returns to different critic architectures, these variations demonstrate how a strong conceptual foundation can support diverse practical implementations. The ability to tune the bias-variance trade-off through algorithmic choices rather than just hyperparameters provides practitioners with powerful tools for adapting methods to specific problem domains. The comparative analysis reveals that Actor-Critic methods generally provide superior sample efficiency and stability compared to pure policy gradient approaches, though at the cost of increased implementation complexity and hyperparameter sensitivity. This trade-off reflects a broader principle in machine learning: more sophisticated methods often achieve better performance but require more careful application. Looking forward, the Actor-Critic framework continues to influence modern reinforcement learning research. Advanced methods like PPO, A3C, and SAC all build upon the core insight that simultaneous policy and value learning can be more effective than either approach alone. The principles underlying Actor-Critic methods—variance reduction through baseline subtraction, temporal difference learning for immediate feedback, and the separation of exploration from exploitation—remain relevant as the field tackles increasingly complex problems. The implementation insights and practical considerations discussed here highlight the gap between theoretical understanding and successful application. While the mathematical foundations of Actor-Critic methods are well-established, achieving reliable performance requires careful attention to initialization, learning rates, numerical stability, and architectural choices. These practical aspects often determine success or failure in real applications, emphasizing the importance of implementation expertise alongside theoretical knowledge. Actor-Critic methods ultimately demonstrate that the most effective algorithmic approaches often involve combining complementary techniques rather than perfecting individual components. The synergy between policy optimization and value function learning creates capabilities that neither approach achieves alone, providing a template for algorithm development that continues to yield insights in contemporary reinforcement learning research. "],["appendix.html", "Chapter 18 Appendix 18.1 Comprehensive Reinforcement Learning Concepts Guide 18.2 Learning Mechanisms 18.3 Environment Properties 18.4 Learning Paradigms 18.5 Exploration Strategies 18.6 Key Algorithms &amp; Methods 18.7 Advanced Concepts 18.8 Fundamental Equations 18.9 Common Challenges &amp; Solutions", " Chapter 18 Appendix 18.1 Comprehensive Reinforcement Learning Concepts Guide Concept Description Mathematical Formalism Applications/Examples Key Properties Agent Autonomous learner/decision-maker that perceives environment and takes actions to maximize cumulative reward Agent: (π, V, Q) Autonomous vehicles, trading bots, game AI (AlphaGo), robotic controllers, chatbots Must balance exploration vs exploitation, learns from trial and error Environment External system that responds to agent actions with state transitions and rewards Markov Decision Process (MDP): ⟨S, A, P, R, γ⟩ OpenAI Gym environments, real-world robotics, financial markets, video games Can be deterministic or stochastic, fully or partially observable State (s) Complete information needed to make optimal decisions at current time s ∈ S, where S is state space Chess position (64 squares), robot pose (x,y,θ), market conditions, pixel arrays Markov property: future depends only on current state, not history Action (a) Discrete or continuous choices available to agent in given state a ∈ A(s), where A(s) ⊆ A Discrete: {up, down, left, right}, Continuous: steering angle [-30°, 30°] Action space can be finite, infinite, or parameterized Reward (r) Immediate scalar feedback signal indicating desirability of action r ∈ ℝ, often bounded: r ∈ [-R_max, R_max] Sparse: +1 goal, 0 elsewhere; Dense: distance-based; Shaped: intermediate milestones Defines objective, can be sparse, dense, or carefully shaped 18.2 Learning Mechanisms Concept Description Mathematical Formalism Applications/Examples Key Properties Return (G) Total discounted future reward from current time step G_t = Σ_{k=0}^∞ γ^k r_{t+k+1} Episode return in games, lifetime value in finance, trajectory rewards Discount factor γ ∈ [0,1] balances immediate vs future rewards Discount Factor (γ) Weighs importance of future vs immediate rewards γ ∈ [0,1], where γ=0 is myopic, γ=1 values all future equally γ=0.9 in games, γ=0.99 in continuous control, γ≈1 in finance Controls learning horizon and convergence properties Policy (π) Strategy mapping states to action probabilities or deterministic actions Stochastic: π(a|s) ∈ [0,1], Deterministic: π(s) → a ε-greedy, Boltzmann/softmax, Gaussian policies, neural networks Can be deterministic, stochastic, parameterized, or tabular Value Function (V) Expected return starting from state under policy π V^π(s) = 𝔼_π[G_t | S_t = s] State evaluation in chess, expected lifetime value Higher values indicate more promising states Q-Function (Q) Expected return from taking action a in state s, then following policy π Q^π(s,a) = 𝔼_π[G_t | S_t = s, A_t = a] Q-tables in tabular RL, neural Q-networks in DQN Enables action selection without environment model Advantage (A) How much better action a is compared to average in state s A^π(s,a) = Q^π(s,a) - V^π(s) Policy gradient variance reduction, actor-critic methods Positive advantage → above average action, zero → average 18.3 Environment Properties Concept Description Mathematical Formalism Applications/Examples Key Properties Model Agent’s internal representation of environment dynamics P(s’,r|s,a) or T(s,a,s’) and R(s,a) Forward models in planning, world models in Dyna-Q Enables planning and simulation of future trajectories Markov Property Future state depends only on current state and action, not history P(S_{t+1}|S_t, A_t, S_{t-1}, …) = P(S_{t+1}|S_t, A_t) Most RL algorithms assume this, POMDP when violated Simplifies learning by eliminating need to track full history Stationarity Environment dynamics don’t change over time P_t(s’,r|s,a) = P(s’,r|s,a) ∀t Stationary games vs non-stationary financial markets Non-stationary environments require adaptation mechanisms Observability Extent to which agent can perceive true environment state Fully: O_t = S_t, Partially: O_t = f(S_t) + noise Full: chess, Partial: poker (hidden cards), autonomous driving POMDPs require memory or state estimation techniques 18.4 Learning Paradigms Concept Description Mathematical Formalism Applications/Examples Key Properties On-Policy Learning from data generated by current policy being improved π_{behavior} = π_{target}, update π using its own experience SARSA, Policy Gradient, A2C, PPO, TRPO More stable but less sample efficient, safer updates Off-Policy Learning from data generated by different (often older) policy π_{behavior} ≠ π_{target}, importance sampling: ρ = π/μ Q-Learning, DQN, DDPG, SAC, experience replay More sample efficient but requires correction for distribution shift Model-Free Learn directly from experience without learning environment model Direct V/Q updates from (s,a,r,s’) tuples Q-Learning, Policy Gradient, Actor-Critic methods Simpler but may require more samples, works with unknown dynamics Model-Based Learn environment model first, then use for planning/learning Learn P(s’|s,a), then plan using model Dyna-Q, MCTS, MuZero, world models Sample efficient but model errors can compound 18.5 Exploration Strategies Concept Description Mathematical Formalism Applications/Examples Key Properties Exploration Trying suboptimal actions to discover potentially better policies Various strategies with different theoretical guarantees ε-greedy, UCB, Thompson sampling, curiosity-driven Essential for discovering optimal policies, balances with exploitation Exploitation Using current knowledge to maximize immediate reward π_{greedy}(s) = argmax_a Q(s,a) Greedy action selection after learning Must be balanced with exploration to avoid local optima ε-Greedy Choose random action with probability ε, best known action otherwise π(a|s) = ε/|A| + (1-ε)δ_{a,a} where a = argmax Q(s,a) Simple exploration in tabular RL, decaying ε schedules Simple but may explore inefficiently in large state spaces Upper Confidence Bound Choose actions with highest optimistic estimate a_t = argmax_a [Q_t(a) + c√(ln t / N_t(a))] Multi-armed bandits, MCTS (UCT), confidence-based exploration Theoretically principled, provides exploration guarantees Thompson Sampling Sample actions according to probability they are optimal Sample θ ~ posterior, choose a = argmax_a Q_θ(s,a) Bayesian bandits, Bayesian RL Naturally balances exploration/exploitation via uncertainty 18.6 Key Algorithms &amp; Methods Algorithm Type Key Innovation Mathematical Update Applications Q-Learning Off-policy, Model-free Learns optimal Q-function without following optimal policy Q(s,a) ← Q(s,a) + α[r + γ max_{a’} Q(s’,a’) - Q(s,a)] Tabular problems, foundation for DQN SARSA On-policy, Model-free Updates based on actual next action taken Q(s,a) ← Q(s,a) + α[r + γQ(s’,a’) - Q(s,a)] Safe exploration, tabular RL Policy Gradient On-policy, Model-free Direct policy optimization using gradient ascent ∇θ J(θ) = 𝔼[∇θ log π_θ(a|s) A^π(s,a)] Continuous action spaces, stochastic policies Actor-Critic On-policy, Model-free Combines value estimation with policy optimization Actor: ∇θ π, Critic: learns V^π or Q^π Reduces variance in policy gradients DQN Off-policy, Model-free Neural networks + experience replay + target networks Q-learning with neural network approximation High-dimensional state spaces, Atari games PPO On-policy, Model-free Clipped surrogate objective for stable policy updates L^{CLIP}(θ) = 𝔼[min(r_t(θ)A_t, clip(r_t(θ))A_t)] Stable policy optimization, continuous control 18.7 Advanced Concepts Concept Description Mathematical Formalism Applications/Examples Key Properties Function Approximation Using parameterized functions to represent V, Q, or π V_θ(s), Q_θ(s,a), π_θ(a|s) where θ are parameters Neural networks, linear functions, tile coding Enables scaling to large/continuous state spaces Experience Replay Storing and reusing past transitions to improve sample efficiency Sample (s,a,r,s’) from replay buffer D uniformly DQN, DDPG, rainbow improvements Breaks correlation in sequential data, enables off-policy learning Target Networks Using separate, slowly updated networks for stable learning θ^- ← τθ + (1-τ)θ^- where τ &lt;&lt; 1 DQN, DDPG for reducing moving target problem Stabilizes learning by reducing correlation between Q-values and targets Eligibility Traces Credit assignment mechanism that bridges temporal difference and Monte Carlo e_t(s) = γλe_{t-1}(s) + 1_{S_t=s} TD(λ), SARSA(λ) for faster learning Allows faster propagation of rewards to earlier states Multi-Agent RL Multiple agents learning simultaneously in shared environment Nash equilibria, correlated equilibria, social welfare Game theory, autonomous vehicle coordination, economics Requires handling non-stationarity from other learning agents 18.8 Fundamental Equations 18.8.1 Bellman Equations State Value: V^π(s) = Σ_a π(a|s) Σ_{s’,r} p(s’,r|s,a)[r + γV^π(s’)] Action Value: Q^π(s,a) = Σ_{s’,r} p(s’,r|s,a)[r + γ Σ_{a’} π(a’|s’)Q^π(s’,a’)] Optimality (State): V*(s) = max_a Σ_{s’,r} p(s’,r|s,a)[r + γV*(s’)] Optimality (Action): Q*(s,a) = Σ_{s’,r} p(s’,r|s,a)[r + γ max_{a’} Q*(s’,a’)] 18.8.2 Policy Gradient Theorem ∇_θ J(θ) = ∇_θ V^{π_θ}(s_0) = Σ_s μ^{π_θ}(s) Σ_a ∇_θ π_θ(a|s) Q^{π_θ}(s,a) where μ^{π_θ}(s) is the stationary distribution under policy π_θ. 18.8.3 Temporal Difference Error δ_t = r_{t+1} + γV(s_{t+1}) - V(s_t) This error drives learning in most RL algorithms and represents the difference between expected and actual returns. 18.9 Common Challenges &amp; Solutions Challenge Problem Solutions Examples Sample Efficiency RL often requires many environment interactions Model-based methods, experience replay, transfer learning Dyna-Q, DQN replay buffer, pre-training Exploration Discovering good policies in large state spaces Curiosity-driven exploration, count-based bonuses, UCB ICM, pseudo-counts, optimism under uncertainty Stability Function approximation can cause instability Target networks, experience replay, regularization DQN improvements, PPO clipping Sparse Rewards Learning with infrequent feedback signals Reward shaping, hierarchical RL, curriculum learning HER, Options framework, progressive tasks Partial Observability Agent cannot fully observe environment state Recurrent policies, belief state estimation, memory LSTM policies, particle filters, attention mechanisms "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
