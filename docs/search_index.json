[["index.html", "Reinforcement Learning in R Chapter 1 Understanding Reinforcement Learning: From Bandits to Policy Optimization 1.1 Introduction to Reinforcement Learning 1.2 The Multi-Armed Bandit: The Simplest Case 1.3 Transition to Markov Decision Processes 1.4 Comparing Reinforcement Learning Methods 1.5 Conclusion and Further Directions 1.6 References", " Reinforcement Learning in R Kamran Afzalui 2025-08-11 Chapter 1 Understanding Reinforcement Learning: From Bandits to Policy Optimization 1.1 Introduction to Reinforcement Learning Reinforcement Learning (RL) is a dynamic subfield of artificial intelligence concerned with how agents ought to take actions in an environment to maximize cumulative reward. It is inspired by behavioral psychology and decision theory, involving a learning paradigm where feedback from the environment is neither supervised (as in classification tasks) nor completely unsupervised, but rather in the form of scalar rewards. The foundational concepts of RL can be understood by progressing from simple problems, such as the multi-armed bandit, to more sophisticated frameworks like Markov Decision Processes (MDPs) and their many solution methods. 1.2 The Multi-Armed Bandit: The Simplest Case The journey begins with the multi-armed bandit problem, a classic formulation that captures the essence of the exploration-exploitation dilemma. In this setting, an agent must choose among several actions (arms), each yielding stochastic rewards from an unknown distribution. There are no state transitions or temporal dependencies—just the immediate outcome of the chosen action. The objective is to maximize the expected reward over time. Despite its simplicity, the bandit framework introduces crucial ideas such as reward estimation, uncertainty, and exploration strategies. Algorithms like ε-greedy methods introduce random exploration, while Upper Confidence Bound (UCB) techniques adjust choices based on uncertainty estimates. Thompson Sampling applies Bayesian reasoning to balance exploration and exploitation. Though limited in scope, these strategies establish foundational principles that generalize to more complex environments. In bandits, action selection strategies serve a role similar to policies in full RL problems, but without dependence on state transitions. 1.3 Transition to Markov Decision Processes The limitations of bandit models become evident when we consider sequential decision-making problems where actions influence future states and rewards. This leads to the formalism of Markov Decision Processes (MDPs), which model environments through states \\(S\\), actions \\(A\\), transition probabilities \\(P(s&#39;|s, a)\\), rewards \\(R(s, a)\\), and a discount factor \\(\\gamma \\in [0, 1]\\). The Markov property assumes that the future is independent of the past given the present state, simplifying the dynamics and enabling tractable analysis. The agent’s goal is to learn an optimal policy \\(\\pi^*(s)\\) that maximizes the expected cumulative return: \\(G_t = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\right]\\) This process relies on value functions such as: \\(V^\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t | S_t = s \\right], \\quad Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ G_t | S_t = s, A_t = a \\right]\\) Solving MDPs involves either learning these functions or directly learning \\(\\pi\\). 1.4 Comparing Reinforcement Learning Methods To frame this spectrum of approaches clearly, the table below summarizes key RL method families, highlighting whether they rely on an explicit model of the environment, whether they learn policies, value functions, or both, and notes on convergence and sample efficiency. Method Type Uses Model? Learns Policy? Learns Value? Sample Efficient? Converges to Optimal? Suitable For Example Algorithms Multi-Armed Bandits No Action Rule No Yes Yes (in expectation) Stateless problems ε-Greedy, UCB, Thompson Dynamic Programming Yes Yes Yes No Yes Known model Value Iteration, Policy Iteration Monte Carlo No Yes Yes No Yes (episodic) Episodic tasks MC Prediction, MC Control TD Learning No Yes Yes Moderate Yes Ongoing tasks SARSA, Q-Learning Dyna-style Methods Yes (learned) Yes Yes High Yes (with perfect model) Sample-limited tasks Dyna-Q, Dyna+, Prioritized Sweeping Q-Learning + Function Approx. No Yes Yes Moderate Not always (non-linear) High-dimensional spaces DQN Policy Gradient No Yes Maybe Low Local Optimum Continuous action spaces REINFORCE, PPO, A2C Actor-Critic No Yes Yes Moderate Local Optimum Most RL settings A2C, DDPG, SAC Model-Based RL Yes (learned) Yes Yes High Not guaranteed Data-limited tasks Dreamer, MBPO, MuZero This classification illustrates the diversity of RL approaches and underscores the flexibility of the RL paradigm. Some methods assume access to a perfect model, while others learn entirely from data. Some directly optimize policies, while others estimate values to guide policy improvement. 1.4.1 Dynamic Programming: Model-Based Learning Dynamic Programming (DP) methods such as Value Iteration and Policy Iteration assume complete knowledge of the environment’s dynamics. These algorithms exploit the Bellman equations to iteratively compute optimal value functions and policies. The Bellman optimality equation is given by: \\(V^*(s) = \\max_a \\sum_{s&#39;} P(s&#39;|s,a) [R(s,a) + \\gamma V^*(s&#39;)]\\) Value Iteration applies this update directly, while Policy Iteration alternates between evaluating the current policy and improving it by acting greedily with respect to the value function. Although DP methods guarantee convergence to the optimal policy, they are rarely applicable to real-world problems due to their assumption of known transitions and the computational infeasibility of operating over large or continuous state spaces. 1.4.2 Model-Free Approaches: Monte Carlo and TD Learning When the model is unknown, we turn to model-free methods that learn directly from sampled experience. Monte Carlo methods estimate the value of policies by averaging the total return over multiple episodes. These methods are simple and intuitive, suitable for episodic environments, but suffer from high variance and are not efficient in online learning scenarios. Temporal Difference (TD) learning bridges the gap between Monte Carlo and DP by updating value estimates based on partial returns. Algorithms like SARSA and Q-learning fall into this category. SARSA is on-policy, updating values based on the actual trajectory taken, while Q-learning is off-policy, learning about the greedy policy regardless of the agent’s current behavior. These methods do not require waiting until the end of an episode and are thus applicable in ongoing tasks. They offer a tradeoff between bias and variance in value estimation. 1.4.3 Dyna: Bridging Model-Free and Model-Based Learning The Dyna architecture, introduced by Richard Sutton, represents an elegant solution to combining model-free learning with model-based planning. Dyna methods simultaneously learn a model of the environment while using that model to generate synthetic experiences for additional learning. This approach addresses a fundamental limitation of pure model-free methods: their sample inefficiency. The core Dyna algorithm operates through three interleaved processes: Direct RL: Learn from real experience using standard model-free methods (e.g., Q-learning) Model Learning: Update the learned model based on observed transitions Planning: Use the learned model to generate simulated experiences and update the value function The basic Dyna-Q algorithm combines Q-learning with a simple tabular model. After each real experience \\((s, a, r, s&#39;)\\), the agent updates both its Q-values and its model estimates \\(\\hat{R}(s,a)\\) and \\(\\hat{T}(s,a)\\). The agent then performs several planning steps by randomly sampling previously visited state-action pairs and using the model to generate simulated transitions for additional Q-learning updates. Dyna+ and Exploration Bonuses: The original Dyna-Q algorithm can suffer when the environment changes, as the learned model becomes outdated. Dyna+ addresses this by adding exploration bonuses to state-action pairs that haven’t been visited recently, encouraging the agent to re-explore and update its model. The bonus term is typically: \\(\\text{bonus}(s,a) = \\kappa \\sqrt{\\tau(s,a)}\\) where \\(\\kappa\\) is a small constant and \\(\\tau(s,a)\\) is the time since state-action pair \\((s,a)\\) was last visited. Prioritized Sweeping: Rather than randomly selecting state-action pairs for planning updates, prioritized sweeping focuses computational resources on the most important updates. It maintains a priority queue of state-action pairs, prioritizing those where the model predicts large changes in value. This targeted approach can significantly improve learning efficiency compared to uniform random planning. The Dyna framework demonstrates that the dichotomy between model-free and model-based methods is not absolute. By learning and using simple models alongside direct RL, Dyna methods can achieve much better sample efficiency than pure model-free approaches while remaining more robust than methods that rely entirely on learned models. 1.4.4 Q-Learning and Function Approximation Q-learning is one of the most widely used RL algorithms due to its simplicity and theoretical guarantees. However, when dealing with large state-action spaces, tabular Q-learning becomes infeasible. Function approximation, particularly using neural networks, allows Q-learning to scale to high-dimensional problems. This gave rise to Deep Q-Networks (DQNs), where a neural network is trained to approximate the Q-function. DQNs introduced mechanisms like experience replay—storing and reusing past interactions to reduce correlation between updates—and target networks—fixed Q-value targets updated slowly to stabilize learning. These enhancements enabled RL to tackle complex environments like Atari games directly from pixels. Nonetheless, deep RL methods often suffer from sample inefficiency and training instability, especially when generalizing to new environments. 1.4.5 Policy Gradient and Actor-Critic Methods While value-based methods derive policies from value functions, policy-based methods directly parameterize and optimize the policy itself. Policy Gradient methods compute the gradient of expected return with respect to policy parameters and perform gradient ascent. The REINFORCE algorithm is the archetype of this approach, but it often suffers from high variance in gradient estimates. The Policy Gradient Theorem provides the theoretical foundation: \\(\\nabla J(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^{\\pi}(s,a)]\\) To address variance, Actor-Critic methods introduce a second component: the critic, which estimates value functions to inform and stabilize the updates of the actor (policy). Algorithms like Advantage Actor-Critic (A2C), Deep Deterministic Policy Gradient (DDPG), and Soft Actor-Critic (SAC) build on this architecture, each adding unique elements to improve performance and stability. 1.4.6 Advanced Policy Optimization Techniques More recent advances in policy optimization have focused on improving training stability and sample efficiency. Trust Region Policy Optimization (TRPO) constrains policy updates to stay within a trust region defined by the KL divergence, ensuring small, safe steps in parameter space. Proximal Policy Optimization (PPO) simplifies TRPO with a clipped objective function, striking a balance between ease of implementation and empirical performance. Soft Actor-Critic (SAC), on the other hand, incorporates an entropy maximization objective, encouraging exploration by maintaining a degree of randomness in the policy. This leads to better performance in environments with sparse or deceptive rewards and is particularly effective in continuous control tasks. Model-based approaches, such as MuZero or Dreamer, offer a complementary strategy: by learning a model of the environment dynamics and reward structure, they can generate synthetic experiences to improve sample efficiency. However, model inaccuracies can lead to cascading errors and suboptimal policies. 1.5 Conclusion and Further Directions Reinforcement Learning has evolved into a mature and diverse field, offering a rich set of tools for decision-making under uncertainty. From simple bandit strategies to deep policy optimization and model-based reasoning, RL provides a versatile framework for learning from interaction. The Dyna architecture exemplifies how combining different learning paradigms can yield methods that are both sample-efficient and robust, highlighting the value of hybrid approaches in RL. However, key challenges remain: training instability, sample inefficiency, reward mis-specification, and generalization across tasks. These remain active areas of research, with promising directions including meta-learning, hierarchical RL, and more sophisticated model learning techniques. A solid understanding of the main categories—such as those outlined in the comparative table—is essential for navigating the RL landscape. Whether one is interested in theoretical foundations, algorithm development, or practical deployment, the key ideas of exploration, value estimation, policy optimization, and model learning form the backbone of modern RL. For further reading, Reinforcement Learning: An Introduction by Sutton and Barto remains the canonical text. As artificial agents continue to do more complex and dynamic environments, reinforcement learning stands at the forefront of AI research and application. 1.6 References Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). MIT Press. http://incompleteideas.net/book/the-book-2nd.html Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236 Schulman, J., Levine, S., Abbeel, P., Jordan, M., &amp; Moritz, P. (2015). Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (pp. 1889–1897). https://proceedings.mlr.press/v37/schulman15.html Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. https://arxiv.org/abs/1707.06347 Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning. https://arxiv.org/abs/1801.01290 Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., … &amp; Hassabis, D. (2020). Mastering the game of Go without human knowledge. Nature, 550(7676), 354–359. https://doi.org/10.1038/nature24270 Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., … &amp; Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419), 1140–1144. https://doi.org/10.1126/science.aar6404 OpenAI. (2018). Spinning Up in Deep RL. https://spinningup.openai.com Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., … &amp; Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971. https://arxiv.org/abs/1509.02971 Auer, P., Cesa-Bianchi, N., &amp; Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47, 235–256. https://doi.org/10.1023/A:1013689704352 Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4), 285–294. https://doi.org/10.2307/2332286 "],["the-multi-armed-bandit-problem.html", "Chapter 2 The Multi-Armed Bandit Problem 2.1 Introduction 2.2 Mathematical Formalism 2.3 Frequentist Approach: UCB1 Algorithm 2.4 Bayesian Approach: Thompson Sampling 2.5 Epsilon-Greedy Strategy 2.6 Summary Table 2.7 Conclusion", " Chapter 2 The Multi-Armed Bandit Problem 2.1 Introduction The multi-armed bandit (MAB) problem is a foundational model in the study of sequential decision-making under uncertainty. Representing the trade-off between exploration (gathering information) and exploitation (maximizing known rewards), MAB problems are central to reinforcement learning, online optimization, and adaptive experimental design. An agent is faced with a choice among multiple options—arms—each producing stochastic rewards with unknown distributions. The objective is to maximize cumulative reward, or equivalently, to minimize the regret incurred by not always choosing the best arm. This post presents a rigorous treatment of the MAB problem, comparing frequentist and Bayesian approaches. We offer formal mathematical foundations, develop regret bounds, and implement both Upper Confidence Bound (UCB) and Thompson Sampling algorithms in R. A summary table is provided at the end. 2.2 Mathematical Formalism Let \\(K\\) denote the number of arms, and each arm \\(k \\in \\{1, \\dots, K\\}\\) has an unknown reward distribution \\(P_k\\), with mean \\(\\mu_k\\). Define the optimal arm: \\[ k^* = \\arg\\max_{k} \\mu_k. \\] At each time \\(t \\in \\{1, \\dots, T\\}\\), the agent chooses arm \\(A_t \\in \\{1, \\dots, K\\}\\) and receives a stochastic reward \\(R_t \\sim P_{A_t}\\). The cumulative expected regret is: \\[ \\mathcal{R}(T) = T\\mu^* - \\mathbb{E}\\left[ \\sum_{t=1}^T R_t \\right] = \\sum_{k=1}^K \\Delta_k \\, \\mathbb{E}[N_k(T)], \\] where \\(\\Delta_k = \\mu^* - \\mu_k\\) and \\(N_k(T)\\) is the number of times arm \\(k\\) was played. 2.3 Frequentist Approach: UCB1 Algorithm Frequentist methods estimate expected rewards using empirical means. The UCB1 algorithm, based on Hoeffding’s inequality, constructs an upper confidence bound: \\[ A_t = \\arg\\max_{k} \\left[ \\hat{\\mu}_{k,t} + \\sqrt{ \\frac{2 \\log t}{N_k(t)} } \\right]. \\] This ensures logarithmic regret in expectation. 2.3.1 R Code for UCB1 set.seed(42) K &lt;- 3 T &lt;- 1000 mu &lt;- c(0.3, 0.5, 0.7) # true means counts &lt;- rep(0, K) values &lt;- rep(0, K) regret &lt;- numeric(T) # Play each arm once for (k in 1:K) { reward &lt;- rbinom(1, 1, mu[k]) counts[k] &lt;- 1 values[k] &lt;- reward regret[k] &lt;- max(mu) - mu[k] } for (t in (K+1):T) { ucb &lt;- values + sqrt(2 * log(t) / counts) a &lt;- which.max(ucb) reward &lt;- rbinom(1, 1, mu[a]) counts[a] &lt;- counts[a] + 1 values[a] &lt;- values[a] + (reward - values[a]) / counts[a] regret[t] &lt;- max(mu) - mu[a] } cum_regret_ucb &lt;- cumsum(regret) plot(cum_regret_ucb, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, ylab = &quot;Cumulative Regret&quot;, xlab = &quot;Time&quot;, main = &quot;UCB1 Regret&quot;) 2.4 Bayesian Approach: Thompson Sampling Bayesian bandits model reward distributions probabilistically, updating beliefs via Bayes’ rule. For Bernoulli rewards, we assume Beta priors: \\[ \\mu_k \\sim \\text{Beta}(\\alpha_k, \\beta_k). \\] After observing a reward \\(r \\in \\{0, 1\\}\\), the posterior update is: \\[ \\alpha_k \\leftarrow \\alpha_k + r, \\quad \\beta_k \\leftarrow \\beta_k + 1 - r. \\] The Thompson Sampling algorithm draws a sample \\(\\tilde{\\mu}_k \\sim \\text{Beta}(\\alpha_k, \\beta_k)\\) and selects the arm with the highest sample. 2.4.1 R Code for Thompson Sampling set.seed(42) alpha &lt;- rep(1, K) beta &lt;- rep(1, K) regret &lt;- numeric(T) for (t in 1:T) { sampled_means &lt;- rbeta(K, alpha, beta) a &lt;- which.max(sampled_means) reward &lt;- rbinom(1, 1, mu[a]) alpha[a] &lt;- alpha[a] + reward beta[a] &lt;- beta[a] + (1 - reward) regret[t] &lt;- max(mu) - mu[a] } cum_regret_ts &lt;- cumsum(regret) lines(cum_regret_ts, col = &quot;red&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;UCB1&quot;, &quot;Thompson Sampling&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 2) The UCB1 algorithm guarantees a regret bound of: \\[ \\mathcal{R}(T) \\leq \\sum_{k: \\Delta_k &gt; 0} \\left( \\frac{8 \\log T}{\\Delta_k} + C_k \\right), \\] where \\(C_k\\) is a problem-dependent constant. Thompson Sampling achieves comparable performance. Under certain regularity conditions, its Bayesian regret is bounded by: \\[ \\mathbb{E}[\\mathcal{R}(T)] = O\\left( \\sqrt{KT \\log T} \\right), \\] and often outperforms UCB1 in practice due to its adaptive exploration. 2.5 Epsilon-Greedy Strategy The epsilon-greedy algorithm is a simple and intuitive approach to balancing exploration and exploitation. At each time step, with probability \\(\\epsilon\\), the agent chooses a random arm (exploration), and with probability \\(1 - \\epsilon\\), it selects the arm with the highest empirical mean (exploitation). Let \\(\\hat{\\mu}_{k,t}\\) denote the empirical mean reward for arm \\(k\\) at time \\(t\\). Then: \\[ A_t = \\begin{cases} \\text{random choice} &amp; \\text{with probability } \\epsilon, \\\\ \\arg\\max_k \\hat{\\mu}_{k,t} &amp; \\text{with probability } 1 - \\epsilon. \\end{cases} \\] While this algorithm is not optimal in the theoretical sense, it often performs well in practice for problems with stationary reward distributions when the exploration rate \\(\\epsilon\\) is properly tuned. Regret under a fixed \\(\\epsilon\\) is linear in \\(T\\), i.e., \\(\\mathcal{R}(T) = O(T)\\), unless \\(\\epsilon\\) is decayed over time (e.g., \\(\\epsilon_t = 1/t\\)), which introduces a trade-off between convergence speed and variance. 2.5.1 R Code for Epsilon-Greedy set.seed(42) epsilon &lt;- 0.1 counts &lt;- rep(0, K) values &lt;- rep(0, K) regret &lt;- numeric(T) for (t in 1:T) { if (runif(1) &lt; epsilon) { a &lt;- sample(1:K, 1) # Exploration } else { a &lt;- which.max(values) # Exploitation } reward &lt;- rbinom(1, 1, mu[a]) counts[a] &lt;- counts[a] + 1 values[a] &lt;- values[a] + (reward - values[a]) / counts[a] regret[t] &lt;- max(mu) - mu[a] } cum_regret_eps &lt;- cumsum(regret) lines(cum_regret_eps, col = &quot;darkgreen&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;UCB1&quot;, &quot;Thompson Sampling&quot;, &quot;Epsilon-Greedy&quot;), col = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;), lwd = 2) 2.6 Summary Table Method Paradigm Assumptions Exploration Mechanism Regret Bound Strengths Weaknesses UCB1 Frequentist Stationary, bounded rewards Upper Confidence Bound \\(O(\\log T)\\) Simple, provable guarantees Conservative, suboptimal in practice Thompson Sampling Bayesian Prior over reward distributions Posterior sampling \\(O(\\sqrt{KT})\\), empirically better Adaptive, efficient with good priors Sensitive to prior misspecification KL-UCB Frequentist Known reward distributions KL-divergence bounds \\(O(\\log T)\\) (tighter) Distribution-aware More complex implementation Epsilon-Greedy Heuristic None Random exploration \\(O(T)\\) if \\(\\epsilon\\) fixed Very simple Inefficient long-term 2.7 Conclusion The multi-armed bandit problem remains an essential model for studying decision-making under uncertainty. While frequentist methods like UCB1 provide rigorous guarantees and conceptual clarity, Bayesian approaches like Thompson Sampling offer greater flexibility and empirical performance. The choice between them hinges on the trade-offs between interpretability, adaptivity, and prior knowledge. The R implementations provided here allow for practical experimentation and benchmarking. In real-world applications, such as clinical trial design, online recommendations, and adaptive A/B testing, these algorithms offer principled foundations for learning and acting in uncertain environments. "],["markov-decision-processes-and-dynamic-programming.html", "Chapter 3 Markov Decision Processes and Dynamic Programming 3.1 Introduction 3.2 Constructing the MDP in R 3.3 Value Iteration Algorithm 3.4 Evaluation and Interpretation 3.5 Theoretical Properties of Value Iteration 3.6 Summary Table 3.7 Conclusion", " Chapter 3 Markov Decision Processes and Dynamic Programming 3.1 Introduction Markov Decision Processes (MDPs) constitute a formal framework for modeling sequential decision-making under uncertainty. Widely applied in operations research, control theory, economics, and artificial intelligence, MDPs encapsulate the dynamics of environments where outcomes are partly stochastic and partly under an agent’s control. At their core, MDPs unify probabilistic transitions, state-contingent rewards, and long-term optimization goals. This post explores MDPs from a computational standpoint, emphasizing Dynamic Programming (DP) methods—particularly Value Iteration—for solving them when the model is fully known. We proceed by defining the mathematical components of MDPs, implementing them in R, and illustrating policy derivation using value iteration. A comparative summary of key aspects of DP methods concludes the post. An MDP is formally defined by the tuple \\((S, A, P, R, \\gamma)\\), where: \\(S\\): a finite set of states. \\(A\\): a finite set of actions. \\(P(s&#39;|s, a)\\): the transition probability function—probability of moving to state \\(s&#39;\\) given current state \\(s\\) and action \\(a\\). \\(R(s, a, s&#39;)\\): the reward received after transitioning from state \\(s\\) to \\(s&#39;\\) via action \\(a\\). \\(\\gamma \\in [0,1]\\): the discount factor, representing the agent’s preference for immediate versus delayed rewards. The agent’s objective is to learn a policy \\(\\pi: S \\to A\\) that maximizes the expected cumulative discounted reward: \\[ V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(S_t, A_t, S_{t+1}) \\,\\bigg|\\, S_0 = s \\right] \\] The optimal value function \\(V^*\\) satisfies the Bellman optimality equation: $$ V^*(s) = {a A} {s’ S} P(s’|s,a) $$ Once \\(V^*\\) is computed, the optimal policy \\(\\pi^*\\) is obtained via: $$ ^*(s) = {a A} {s’} P(s’|s,a) $$ 3.2 Constructing the MDP in R We now implement an environment with 10 states and 2 actions per state, following stochastic transition and reward dynamics. The final state is absorbing, with no further transitions or rewards. n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 This setup specifies an MDP with stochastic transitions favoring forward progression and asymmetric reward structures across actions. State 10 is a terminal state. We define a function to sample from the environment: sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } 3.3 Value Iteration Algorithm Value Iteration is a fundamental DP method for computing the optimal value function and deriving the optimal policy. It exploits the Bellman optimality equation through successive approximation. value_iteration &lt;- function() { V &lt;- rep(0, n_states) policy &lt;- rep(1, n_states) theta &lt;- 1e-4 repeat { delta &lt;- 0 for (s in 1:(n_states - 1)) { v &lt;- V[s] q_values &lt;- numeric(n_actions) for (a in 1:n_actions) { for (s_prime in 1:n_states) { q_values[a] &lt;- q_values[a] + transition_model[s, a, s_prime] * (reward_model[s, a, s_prime] + gamma * V[s_prime]) } } V[s] &lt;- max(q_values) policy[s] &lt;- which.max(q_values) delta &lt;- max(delta, abs(v - V[s])) } if (delta &lt; theta) break } list(V = V, policy = policy) } This implementation iteratively updates value estimates until convergence, determined by a small threshold \\(\\theta\\). At each iteration, the value of state \\(s\\) is updated using the maximum expected return across available actions. We now apply the algorithm and extract the resulting value function and policy: dp_result &lt;- value_iteration() dp_value &lt;- dp_result$V dp_policy &lt;- dp_result$policy dp_policy The returned policy is a vector of optimal actions indexed by state. The value function can be visualized to reveal which states yield higher expected returns under the optimal policy. 3.4 Evaluation and Interpretation The policy and value function obtained via value iteration provide complete guidance for optimal behavior in the modeled environment. In this setting: The forward action (Action 1) is generally rewarded with higher long-term return due to its tendency to reach the terminal (high-reward) state. The second action (Action 2) introduces randomness and lower rewards, thus is optimal only in specific states where it offers a better expected value. Such interpretation emphasizes the Bellman principle of optimality: every sub-policy of an optimal policy must itself be optimal for the corresponding subproblem. We can visualize the value function: plot(dp_value, type = &quot;b&quot;, col = &quot;blue&quot;, pch = 19, xlab = &quot;State&quot;, ylab = &quot;Value&quot;, main = &quot;Optimal State Values via Value Iteration&quot;) 3.5 Theoretical Properties of Value Iteration Value Iteration exhibits the following theoretical guarantees: Convergence: The algorithm is guaranteed to converge to \\(V^*\\) for any finite MDP with bounded rewards and \\(0 \\leq \\gamma &lt; 1\\). Policy Derivation: Once \\(V^*\\) is known, the greedy policy is optimal. Computational Complexity: For state space size \\(S\\) and action space size \\(A\\), each iteration requires \\(O(S^2 A)\\) operations due to the summation over all successor states. However, in practice, the applicability of DP methods is restricted by: The need for full knowledge of \\(P\\) and \\(R\\), Infeasibility in large or continuous state spaces (the “curse of dimensionality”). These challenges motivate the use of approximation, sampling-based methods, and model-free approaches in reinforcement learning contexts. 3.6 Summary Table The following table summarizes the key elements and tradeoffs of the value iteration algorithm: Property Value Iteration (DP) Model Required Yes (transition probabilities and rewards) State Representation Tabular (explicit state-value storage) Action Selection Greedy w.r.t. value estimates Convergence Guarantee Yes (under finite \\(S, A\\), \\(\\gamma &lt; 1\\)) Sample Efficiency High (uses full model, no sampling error) Scalability Poor in large or continuous state spaces Output Optimal value function and policy Computational Complexity \\(O(S^2 A)\\) per iteration 3.7 Conclusion Dynamic Programming offers elegant and theoretically grounded algorithms for solving MDPs when the environment model is fully specified. Value Iteration, as illustrated, leverages the recursive Bellman optimality equation to iteratively compute the value function and derive the optimal policy. While its practical scope is limited by scalability and model assumptions, DP remains foundational in the study of decision processes. Understanding these principles is essential for extending to model-free reinforcement learning, function approximation, and policy gradient methods. Future posts in this series will explore Temporal Difference learning, Monte Carlo methods, and the transition to policy optimization. These approaches lift the strict requirements of model knowledge and allow learning from interaction, thereby opening the door to real-world applications. "],["model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html", "Chapter 4 Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R 4.1 Introduction 4.2 Theoretical Background 4.3 Interpretation and Discussion 4.4 Conclusion", " Chapter 4 Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R 4.1 Introduction In many real-world decision-making problems, the environment model—comprising transition probabilities and reward functions—is unknown or too complex to specify explicitly. Model-free reinforcement learning (RL) methods address this by learning optimal policies directly from experience or sample trajectories, without requiring full knowledge of the underlying Markov Decision Process (MDP). Two foundational model-free methods are Temporal Difference (TD) learning and Monte Carlo (MC) methods. TD learning updates value estimates online based on one-step lookahead and bootstrapping, while MC methods learn from complete episodes by averaging returns. This post presents these approaches with mathematical intuition, R implementations, and an illustrative environment. We also compare how learned policies adapt—or fail to adapt—when rewards are changed after training, illuminating the distinction between goal-directed and habitual learning. 4.2 Theoretical Background Suppose an agent interacts with an environment defined by states \\(S\\), actions \\(A\\), a discount factor \\(\\gamma \\in [0,1]\\), and an unknown MDP dynamics. The goal is to learn the action-value function \\(Q^\\pi(s,a)\\), the expected discounted return starting from state \\(s\\), taking action \\(a\\), and following policy \\(\\pi\\) thereafter: \\[ Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a \\right] \\] 4.2.1 Temporal Difference Learning (Q-Learning) Q-Learning is an off-policy TD control method that updates the estimate \\(Q(s,a)\\) incrementally after each transition: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right) \\] where \\(\\alpha\\) is the learning rate, \\(r\\) the reward received, and \\(s&#39;\\) the next state. This update uses the current estimate of \\(Q\\) at \\(s&#39;\\) (bootstrapping). 4.2.2 Monte Carlo Methods Monte Carlo methods learn \\(Q\\) by averaging returns observed after visiting \\((s,a)\\). Every-visit MC estimates \\(Q\\) by averaging the total discounted return \\(G_t\\) following each occurrence of \\((s,a)\\) within complete episodes: \\[ Q(s,a) \\approx \\frac{1}{N(s,a)} \\sum_{i=1}^{N(s,a)} G_t^{(i)} \\] where \\(N(s,a)\\) is the count of visits to \\((s,a)\\) and \\(G_t^{(i)}\\) is the return following the \\(i\\)-th visit. 4.2.3 Step 1: Defining the Environment in R We use a 10-state, 2-action environment with stochastic transitions and rewards, as in previous work: # Common settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition + reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } 4.2.4 Step 2: Q-Learning Implementation in R The Q-Learning function runs episodes, selects actions using \\(\\epsilon\\)-greedy policy, updates Q-values using the TD rule, and outputs a policy by greedy action selection: q_learning &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) for (ep in 1:episodes) { s &lt;- 1 while (TRUE) { a &lt;- if (runif(1) &lt; epsilon) sample(1:n_actions, 1) else which.max(Q[s, ]) out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime reward &lt;- out$reward Q[s, a] &lt;- Q[s, a] + alpha * (reward + gamma * max(Q[s_prime, ]) - Q[s, a]) if (s_prime == n_states) break s &lt;- s_prime } } apply(Q, 1, which.max) } Running this yields the learned policy: ql_policy_before &lt;- q_learning() 4.2.5 Step 3: Monte Carlo Every-Visit Implementation This MC method generates episodes, stores the full sequence of state-action-reward tuples, and updates \\(Q\\) by averaging discounted returns for every visit of \\((s,a)\\): monte_carlo &lt;- function(episodes = 1000, epsilon = 0.1) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) returns &lt;- vector(&quot;list&quot;, n_states * n_actions) names(returns) &lt;- paste(rep(1:n_states, each = n_actions), rep(1:n_actions, n_states), sep = &quot;_&quot;) for (ep in 1:episodes) { episode &lt;- list() s &lt;- 1 while (TRUE) { a &lt;- if (runif(1) &lt; epsilon) sample(1:n_actions, 1) else which.max(Q[s, ]) out &lt;- sample_env(s, a) episode[[length(episode) + 1]] &lt;- list(state = s, action = a, reward = out$reward) if (out$s_prime == n_states) break s &lt;- out$s_prime } G &lt;- 0 for (t in length(episode):1) { s &lt;- episode[[t]]$state a &lt;- episode[[t]]$action r &lt;- episode[[t]]$reward G &lt;- gamma * G + r key &lt;- paste(s, a, sep = &quot;_&quot;) returns[[key]] &lt;- c(returns[[key]], G) Q[s, a] &lt;- mean(returns[[key]]) } } apply(Q, 1, which.max) } The resulting policy is computed by: mc_policy_before &lt;- monte_carlo() 4.2.6 Step 4: Simulating Outcome Devaluation We now alter the environment by removing the reward for reaching the terminal state, simulating a devaluation: # Devalue terminal reward for (s in 1:(n_states - 1)) { reward_model[s, 1, n_states] &lt;- 0 reward_model[s, 2, n_states] &lt;- 0 } 4.2.7 Step 5: Comparing Policies Before and After Devaluation We compare policies derived via: Dynamic Programming (DP) which has full model knowledge and updates instantly after reward changes, Q-Learning and Monte Carlo which keep their previously learned policies (habitual behavior without retraining). # DP recomputes policy based on updated reward model dp_policy_after &lt;- value_iteration()$policy # Q-learning and MC keep previous policy (habitual) ql_policy_after &lt;- ql_policy_before mc_policy_after &lt;- mc_policy_before 4.2.8 Step 6: Visualizing the Policies plot_policy &lt;- function(policy, label, col = &quot;skyblue&quot;) { barplot(policy, names.arg = 1:n_states, col = col, ylim = c(0, 3), ylab = &quot;Action (1=A1, 2=A2)&quot;, main = label) abline(h = 1.5, lty = 2, col = &quot;gray&quot;) } par(mfrow = c(3, 2)) plot_policy(dp_policy_before, &quot;DP Policy Before&quot;) plot_policy(dp_policy_after, &quot;DP Policy After&quot;, &quot;lightgreen&quot;) plot_policy(ql_policy_before, &quot;Q-Learning Policy Before&quot;) plot_policy(ql_policy_after, &quot;Q-Learning Policy After&quot;, &quot;orange&quot;) plot_policy(mc_policy_before, &quot;MC Policy Before&quot;) plot_policy(mc_policy_after, &quot;MC Policy After&quot;, &quot;orchid&quot;) 4.3 Interpretation and Discussion Dynamic Programming adapts immediately after devaluation because it recalculates the policy using the updated reward model. In contrast, Q-Learning and Monte Carlo methods, which are model-free and learn from past experience, maintain their prior policies unless explicitly retrained. This reflects habitual behavior — a policy learned from experience that does not flexibly adjust to changed outcomes without further learning. This illustrates a core difference: Model-based methods (like DP) support goal-directed behavior, recomputing optimal decisions as the environment changes. Model-free methods (like Q-Learning and MC) support habitual behavior, relying on cached values learned from past rewards. 4.4 Conclusion Temporal Difference and Monte Carlo methods provide powerful approaches to reinforcement learning when the environment is unknown. TD learning’s bootstrapping allows online updates after each transition, while Monte Carlo averages full returns after complete episodes. Both learn policies from experience rather than models. Future posts will explore extensions including function approximation and policy gradient methods. Aspect Monte Carlo (MC) Temporal Difference (Q-Learning) Learning Approach Learns from complete episodes by averaging returns after each episode. Learns incrementally after each state-action transition using bootstrapping. Update Rule Updates Q-value as the mean of observed returns: \\(Q(s,a) \\approx \\frac{1}{N(s,a)} \\sum_{i=1}^{N(s,a)} G_t^{(i)}\\) Updates Q-value using TD error: \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right)\\) Episode Requirement Requires complete episodes to compute returns (\\(G_t\\)). Does not require complete episodes; updates online after each step. Bias and Variance Unbiased estimate of Q-value, but high variance due to full episode returns. Biased due to bootstrapping (relies on current Q estimates), but lower variance. Policy Type Typically on-policy (e.g., with ε-greedy exploration), but can be adapted for off-policy. Off-policy; learns optimal policy regardless of exploration policy. Computational Efficiency Less efficient; must wait for episode completion before updating. More efficient; updates Q-values immediately after each transition. Adaptation to Change Slow to adapt to environment changes without retraining, as it relies on past episode returns. Slow to adapt without retraining, but incremental updates allow faster response to changes. Implementation in Code Stores state-action-reward sequences, computes discounted returns backward. Updates Q-values online using immediate reward and next state’s Q-value. Example in Provided Code Every-visit MC: averages returns for each \\((s,a)\\) visit in an episode. Q-Learning: updates Q-values after each transition using TD rule. "],["on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html", "Chapter 5 On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R 5.1 Introduction 5.2 SARSA (On-Policy) 5.3 Q-Learning (Off-Policy) 5.4 Off-Policy Monte Carlo with Importance Sampling 5.5 Key Differences 5.6 Interpretation and Discussion 5.7 Conclusion 5.8 Comparison Table", " Chapter 5 On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R 5.1 Introduction In model-free reinforcement learning (RL), agents learn optimal policies directly from experience without a model of the environment’s dynamics. Two key approaches are on-policy and off-policy methods, exemplified by SARSA (State-Action-Reward-State-Action) and Q-Learning, respectively. Additionally, off-policy Monte Carlo methods leverage importance sampling to learn optimal policies from exploratory data. This post explores these methods, focusing on their theoretical foundations, practical implications, and implementation in R. We use a 10-state, 2-action environment to compare how SARSA, Q-Learning, and off-policy Monte Carlo learn policies and adapt to environmental changes, such as outcome devaluation. Mathematical formulations and R code are provided to illustrate the concepts. SARSA, Q-Learning, and off-policy Monte Carlo aim to estimate the action-value function \\(Q^\\pi(s,a)\\), the expected discounted return for taking action \\(a\\) in state \\(s\\) and following policy \\(\\pi\\): \\[ Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a \\right] \\] where \\(\\gamma \\in [0,1]\\) is the discount factor, and \\(R_{t+1}\\) is the reward at time \\(t+1\\). 5.2 SARSA (On-Policy) SARSA is an on-policy method, meaning it learns the value of the policy being followed, including exploration. The update rule is: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma Q(s&#39;, a&#39;) - Q(s,a) \\right) \\] where \\(\\alpha\\) is the learning rate, \\(r\\) is the reward, \\(s&#39;\\) is the next state, and \\(a&#39;\\) is the action actually taken in \\(s&#39;\\) according to the current policy (e.g., \\(\\epsilon\\)-greedy). SARSA updates \\(Q\\) based on the next state-action pair \\((s&#39;, a&#39;)\\), making it sensitive to the exploration policy. In the 10-state environment, SARSA learns a policy that accounts for exploratory actions, potentially avoiding risky moves that lead to lower rewards. 5.3 Q-Learning (Off-Policy) Q-Learning is an off-policy method, meaning it learns the optimal policy \\(\\pi^*\\) regardless of the exploration policy (e.g., \\(\\epsilon\\)-greedy). The update rule is: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right) \\] where \\(\\max_{a&#39;} Q(s&#39;, a&#39;)\\) estimates the value of the next state assuming the optimal action. This bootstrapping makes Q-Learning converge to the optimal action-value function \\(Q^*(s,a)\\). In the 10-state environment, Q-Learning favors actions that maximize future rewards (e.g., action 1 in state 9, yielding a 1.0 reward at the terminal state), ignoring exploration effects. 5.4 Off-Policy Monte Carlo with Importance Sampling Off-policy Monte Carlo uses importance sampling to learn the value of a target policy (e.g., greedy) from episodes generated by a behavior policy (e.g., random). The return \\(G_t\\) (cumulative discounted reward from time \\(t\\) onward) is weighted by the importance sampling ratio: \\[ \\rho_t = \\prod_{k=t}^T \\frac{\\pi(a_k|s_k)}{\\mu(a_k|s_k)} \\] where \\(\\pi\\) is the target policy, \\(\\mu\\) is the behavior policy, and \\(T\\) is the episode length. The Q-value update is: \\[ Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( \\rho_t G_t - Q(s,a) \\right) \\] Importance Sampling Mechanics: In the 10-state environment, suppose the behavior policy is random (0.5 probability for actions 1 and 2), but the target policy is greedy (choosing the action with the highest Q-value). If the agent in state 9 takes action 2 (reward 0.5 at the terminal state), but the greedy policy prefers action 1 (reward 1.0), the importance sampling ratio \\(\\rho_t\\) is low (e.g., 0 if the greedy policy assigns zero probability to action 2), reducing the update’s impact. This allows learning the optimal policy from exploratory trajectories, but high variance can occur if the policies diverge significantly. Weighted importance sampling (as in the code below) normalizes weights to reduce variance, and early termination (stopping when \\(\\rho_t = 0\\)) improves efficiency. 5.5 Key Differences Aspect SARSA (On-Policy) Q-Learning (Off-Policy) Off-Policy Monte Carlo Update Rule Uses \\(Q(s&#39;, a&#39;)\\), where \\(a&#39;\\) is sampled from the current policy. Uses \\(\\max_{a&#39;} Q(s&#39;, a&#39;)\\), assuming the optimal action. Uses \\(\\rho_t G_t\\), where \\(\\rho_t\\) reweights returns based on policy likelihoods. Policy Learning Learns the value of the policy being followed (including exploration). Learns the optimal policy, independent of exploration. Learns the optimal policy using importance sampling from exploratory trajectories. Exploration Impact Exploration affects learned Q-values. Exploration does not affect learned Q-values. Exploration affects returns, reweighted by importance sampling. Convergence Converges to the policy’s value if exploration decreases (e.g., \\(\\epsilon \\to 0\\)). Converges to the optimal policy even with fixed exploration. Converges to the optimal policy, but variance depends on policy similarity. Behavior More conservative, accounts for exploration risks. More aggressive, assumes optimal future actions. Aggressive, but variance can lead to unstable learning if policies differ significantly. # Common settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Helper function: Epsilon-greedy policy epsilon_greedy &lt;- function(Q, state, epsilon) { if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { which.max(Q[state, ]) } } # Helper function: Simulate environment simulate_step &lt;- function(state, action) { probs &lt;- transition_model[state, action, ] next_state &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[state, action, next_state] list(next_state = next_state, reward = reward) } # SARSA sarsa &lt;- function(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, n_states, n_actions) policy &lt;- rep(0, n_states) rewards &lt;- numeric(n_episodes) for (episode in 1:n_episodes) { state &lt;- sample(1:(n_states - 1), 1) action &lt;- epsilon_greedy(Q, state, epsilon) episode_reward &lt;- 0 while (state != terminal_state) { step &lt;- simulate_step(state, action) next_state &lt;- step$next_state reward &lt;- step$reward next_action &lt;- epsilon_greedy(Q, next_state, epsilon) Q[state, action] &lt;- Q[state, action] + alpha * ( reward + gamma * Q[next_state, next_action] - Q[state, action] ) state &lt;- next_state action &lt;- next_action episode_reward &lt;- episode_reward + reward } rewards[episode] &lt;- episode_reward } policy[1:(n_states - 1)] &lt;- apply(Q[1:(n_states - 1), ], 1, which.max) list(Q = Q, policy = policy, rewards = rewards) } # Q-Learning q_learning &lt;- function(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, n_states, n_actions) policy &lt;- rep(0, n_states) rewards &lt;- numeric(n_episodes) for (episode in 1:n_episodes) { state &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 while (state != terminal_state) { action &lt;- epsilon_greedy(Q, state, epsilon) step &lt;- simulate_step(state, action) next_state &lt;- step$next_state reward &lt;- step$reward Q[state, action] &lt;- Q[state, action] + alpha * ( reward + gamma * max(Q[next_state, ]) - Q[state, action] ) state &lt;- next_state episode_reward &lt;- episode_reward + reward } rewards[episode] &lt;- episode_reward } policy[1:(n_states - 1)] &lt;- apply(Q[1:(n_states - 1), ], 1, which.max) list(Q = Q, policy = policy, rewards = rewards) } # Off-Policy Monte Carlo off_policy_mc &lt;- function(n_episodes = 1000, epsilon = 0.1) { Q &lt;- matrix(0, n_states, n_actions) C &lt;- matrix(0, n_states, n_actions) # Cumulative weights policy &lt;- rep(0, n_states) rewards &lt;- numeric(n_episodes) for (episode in 1:n_episodes) { # Generate episode using behavior policy (epsilon-greedy) states &lt;- numeric(0) actions &lt;- numeric(0) rewards_ep &lt;- numeric(0) state &lt;- sample(1:(n_states - 1), 1) while (state != terminal_state) { action &lt;- sample(1:n_actions, 1) # Behavior policy: random step &lt;- simulate_step(state, action) next_state &lt;- step$next_state reward &lt;- step$reward states &lt;- c(states, state) actions &lt;- c(actions, action) rewards_ep &lt;- c(rewards_ep, reward) state &lt;- next_state } rewards[episode] &lt;- sum(rewards_ep) # Update Q using importance sampling G &lt;- 0 W &lt;- 1 for (t in length(states):1) { state &lt;- states[t] action &lt;- actions[t] reward &lt;- rewards_ep[t] G &lt;- gamma * G + reward C[state, action] &lt;- C[state, action] + W Q[state, action] &lt;- Q[state, action] + (W / C[state, action]) * (G - Q[state, action]) pi_action &lt;- which.max(Q[state, ]) if (action != pi_action) break W &lt;- W / (1 / n_actions) # Importance sampling ratio } } policy[1:(n_states - 1)] &lt;- apply(Q[1:(n_states - 1), ], 1, which.max) list(Q = Q, policy = policy, rewards = rewards) } # Value Iteration (from DP) value_iteration &lt;- function(transition_model, reward_model, gamma, epsilon = 1e-6, max_iter = 1000) { V &lt;- rep(0, n_states) policy &lt;- rep(0, n_states) delta &lt;- Inf iter &lt;- 0 while (delta &gt; epsilon &amp;&amp; iter &lt; max_iter) { delta &lt;- 0 V_old &lt;- V for (s in 1:(n_states - 1)) { Q &lt;- numeric(n_actions) for (a in 1:n_actions) { Q[a] &lt;- sum(transition_model[s, a, ] * (reward_model[s, a, ] + gamma * V)) } V[s] &lt;- max(Q) policy[s] &lt;- which.max(Q) delta &lt;- max(delta, abs(V[s] - V_old[s])) } iter &lt;- iter + 1 } # Evaluate DP policy rewards &lt;- numeric(1000) for (episode in 1:1000) { state &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 while (state != terminal_state) { action &lt;- policy[state] step &lt;- simulate_step(state, action) episode_reward &lt;- episode_reward + step$reward state &lt;- step$next_state } rewards[episode] &lt;- episode_reward } list(V = V, policy = policy, rewards = rewards) } # Run algorithms set.seed(42) dp_result &lt;- value_iteration(transition_model, reward_model, gamma) sarsa_result &lt;- sarsa(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) qlearn_result &lt;- q_learning(n_episodes = 1000, alpha = 0.1, epsilon = 0.1) mc_result &lt;- off_policy_mc(n_episodes = 1000, epsilon = 0.1) # Visualization library(ggplot2) library(gridExtra) # Policy comparison policy_df &lt;- data.frame( State = rep(1:n_states, 4), Policy = c(dp_result$policy, sarsa_result$policy, qlearn_result$policy, mc_result$policy), Algorithm = rep(c(&quot;DP&quot;, &quot;SARSA&quot;, &quot;Q-Learning&quot;, &quot;Off-Policy MC&quot;), each = n_states) ) policy_df$Policy[n_states * 0:3 + n_states] &lt;- NA # Terminal state policy_plot &lt;- ggplot(policy_df, aes(x = State, y = Policy, color = Algorithm)) + geom_point(size = 3) + geom_line(aes(group = Algorithm), na.rm = TRUE) + theme_minimal() + labs(title = &quot;Optimal Policies by Algorithm&quot;, x = &quot;State&quot;, y = &quot;Action&quot;) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;)) + theme(legend.position = &quot;bottom&quot;) # Reward comparison reward_df &lt;- data.frame( Episode = rep(1:1000, 4), Reward = c( cumsum(dp_result$rewards), cumsum(sarsa_result$rewards), cumsum(qlearn_result$rewards), cumsum(mc_result$rewards) ), Algorithm = rep(c(&quot;DP&quot;, &quot;SARSA&quot;, &quot;Q-Learning&quot;, &quot;Off-Policy MC&quot;), each = 1000) ) reward_plot &lt;- ggplot(reward_df, aes(x = Episode, y = Reward, color = Algorithm)) + geom_line() + theme_minimal() + labs(title = &quot;Cumulative Reward Comparison&quot;, x = &quot;Episode&quot;, y = &quot;Cumulative Reward&quot;) + theme(legend.position = &quot;bottom&quot;) # Display plots grid.arrange(policy_plot, reward_plot, ncol = 1) # Print performance metrics cat(&quot;Average Cumulative Reward per Episode:\\n&quot;) cat(&quot;DP:&quot;, mean(dp_result$rewards), &quot;\\n&quot;) cat(&quot;SARSA:&quot;, mean(sarsa_result$rewards), &quot;\\n&quot;) cat(&quot;Q-Learning:&quot;, mean(qlearn_result$rewards), &quot;\\n&quot;) cat(&quot;Off-Policy MC:&quot;, mean(mc_result$rewards), &quot;\\n&quot;) 5.6 Interpretation and Discussion 5.6.0.1 Policy Differences SARSA: As an on-policy method, it learns the value of the \\(\\epsilon\\)-greedy policy, which includes exploratory actions. In the 10-state environment, SARSA may balance between actions 1 and 2, reflecting the impact of random exploration, leading to a more conservative policy. Q-Learning: As an off-policy method, it learns the optimal policy, favoring action 1 in state 9 (higher terminal reward of 1.0) due to its greedy updates. Its policy is less sensitive to exploration noise, as it assumes optimal future actions. Off-Policy Monte Carlo: Also off-policy, it learns the optimal policy using importance sampling to reweight returns from a random behavior policy. It may align closely with Q-Learning’s policy but can exhibit variability due to high variance in importance sampling ratios, especially if the random policy frequently selects action 2 (lower reward). 5.6.0.2 Devaluation All methods exhibit habitual behavior without retraining, retaining their original policies after the terminal reward is removed. This highlights a limitation of model-free methods compared to model-based approaches (e.g., dynamic programming), which adapt instantly to reward changes. 5.6.0.3 Practical Implications SARSA: Better suited for environments where the exploration policy must be accounted for, such as safety-critical systems (e.g., robotics), where risky exploratory actions could lead to poor outcomes. Q-Learning: Ideal for scenarios where the optimal policy is desired regardless of exploration, such as games or simulations where exploration does not incur real-world costs. Off-Policy Monte Carlo: Suitable for offline learning from logged data (e.g., recommendation systems), but high variance can make it less stable than Q-Learning in dynamic environments. 5.6.0.4 Experimental Observations Before devaluation, Q-Learning and off-policy Monte Carlo likely favor action 1 in state 9 due to its higher terminal reward, while SARSA’s policy may show more variability due to exploration. After devaluation, all policies remain unchanged without retraining, illustrating their reliance on cached Q-values. Off-policy Monte Carlo’s performance depends on the similarity between the random behavior policy and the greedy target policy, with high variance potentially leading to less consistent policies compared to Q-Learning. 5.7 Conclusion SARSA, Q-Learning, and off-policy Monte Carlo represent distinct paradigms in model-free RL. SARSA’s on-policy updates reflect the exploration policy, making it conservative. Q-Learning’s off-policy updates target the optimal policy, ignoring exploration effects. Off-policy Monte Carlo uses importance sampling to learn from diverse trajectories, enabling offline learning but introducing variance. The R implementations demonstrate these differences in a 10-state environment, and the devaluation experiment underscores their habitual nature. Future posts could explore advanced topics, such as SARSA(\\(\\lambda\\)), deep RL extensions, or variance reduction in off-policy Monte Carlo. 5.8 Comparison Table Aspect SARSA (On-Policy) Q-Learning (Off-Policy) Off-Policy Monte Carlo Learning Approach Learns incrementally, updates based on action taken by behavior policy. Learns incrementally, updates based on best action in next state. Learns from complete episodes, using importance sampling. Update Rule \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma Q(s&#39;, a&#39;) - Q(s,a) \\right)\\) \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right)\\) \\(Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left( \\rho_t G_t - Q(s,a) \\right)\\) Episode Requirement Updates online, no episode completion needed. Updates online, no episode completion needed. Requires complete episodes for returns and importance weights. Bias and Variance Biased due to bootstrapping, moderate variance. Biased due to bootstrapping, lower variance. Unbiased but high variance due to importance sampling. Policy Type On-policy; learns value of behavior policy. Off-policy; learns optimal policy via max Q-value. Off-policy; learns greedy policy using importance sampling. Exploration Impact Exploration affects learned Q-values. Exploration does not affect learned Q-values. Exploration affects returns, reweighted by importance sampling. Convergence Converges to policy’s value if \\(\\epsilon \\to 0\\). Converges to optimal policy even with fixed \\(\\epsilon\\). Converges to optimal policy, but variance depends on policy similarity. Behavior Conservative, accounts for exploration risks. Aggressive, assumes optimal future actions. Aggressive, but variance can lead to instability. Example in Environment Balances actions 1 and 2, sensitive to exploration. Favors action 1 (higher reward) in state 9. Favors action 1, but variance may cause variability. "],["function-approximation-in-reinforcement-learning-q-learning-with-linear-models-in-r.html", "Chapter 6 Function Approximation in Reinforcement Learning: Q-Learning with Linear Models in R 6.1 Introduction 6.2 Theoretical Background 6.3 R Implementation", " Chapter 6 Function Approximation in Reinforcement Learning: Q-Learning with Linear Models in R 6.1 Introduction In reinforcement learning (RL), tabular methods like SARSA and Q-Learning store a separate Q-value for each state-action pair, which becomes infeasible in large or continuous state spaces. Function approximation addresses this by representing the action-value function \\(Q(s, a)\\) as a parameterized function \\(Q(s, a; \\theta)\\), enabling generalization across states and scalability. This post explores Q-Learning with linear function approximation, using a 10-state, 2-action environment to demonstrate how it learns policies compared to tabular methods. We provide mathematical formulations, R code, and comparisons with tabular Q-Learning, focusing on generalization, scalability, and practical implications. 6.2 Theoretical Background Function approximation in RL aims to estimate the action-value function: \\[ Q^\\pi(s, a) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^\\infty \\gamma^t R_{t+1} \\mid S_0 = s, A_0 = a \\right] \\] where \\(\\gamma \\in [0,1]\\) is the discount factor, and \\(R_{t+1}\\) is the reward at time \\(t+1\\). Instead of storing \\(Q(s, a)\\) in a table, we approximate it as: \\[ Q(s, a; \\theta) = \\phi(s, a)^T \\theta \\] where \\(\\phi(s, a)\\) is a feature vector for state-action pair \\((s, a)\\), and \\(\\theta\\) is a parameter vector learned via optimization, typically stochastic gradient descent (SGD). 6.2.1 Q-Learning with Function Approximation Q-Learning with function approximation is an off-policy method that learns the optimal policy by updating \\(\\theta\\) to minimize the temporal difference (TD) error. The update rule is: \\[ \\theta \\leftarrow \\theta + \\alpha \\delta \\nabla_\\theta Q(s, a; \\theta) \\] where \\(\\alpha\\) is the learning rate, and the TD error \\(\\delta\\) is: \\[ \\delta = r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;; \\theta) - Q(s, a; \\theta) \\] Here, \\(r\\) is the reward, \\(s&#39;\\) is the next state, and \\(\\max_{a&#39;} Q(s&#39;, a&#39;; \\theta)\\) estimates the value of the next state assuming the optimal action. For linear function approximation, the gradient is: \\[ \\nabla_\\theta Q(s, a; \\theta) = \\phi(s, a) \\] Thus, the update becomes: \\[ \\theta \\leftarrow \\theta + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;; \\theta) - Q(s, a; \\theta) \\right) \\phi(s, a) \\] In our 10-state environment, we use one-hot encoding for \\(\\phi(s, a)\\), mimicking tabular Q-Learning for simplicity but demonstrating the framework’s potential for generalization with more complex features. 6.2.2 Comparison with Tabular Q-Learning Tabular Q-Learning updates a table of Q-values directly: \\[ Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left( r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s, a) \\right) \\] Function approximation generalizes across states via \\(\\phi(s, a)\\), reducing memory requirements and enabling learning in large or continuous spaces. However, it introduces approximation errors and requires careful feature design to ensure convergence. Aspect Tabular Q-Learning Q-Learning with Function Approximation Representation Table of \\(Q(s, a)\\) values \\(Q(s, a; \\theta) = \\phi(s, a)^T \\theta\\) Memory \\(O(|\\mathcal{S}| \\cdot |\\mathcal{A}|)\\) \\(O(|\\theta|)\\), depends on feature size Generalization None; state-action specific Yes; depends on feature design Scalability Poor for large/continuous spaces Good for large/continuous spaces with proper features Update Rule Direct Q-value update Parameter update via gradient descent Convergence Guaranteed to optimal \\(Q^*\\) under conditions Converges to approximation of \\(Q^*\\); depends on features 6.3 R Implementation We implement Q-Learning with linear function approximation in a 10-state, 2-action environment, using one-hot encoding for \\(\\phi(s, a)\\). The environment mirrors the previous post, with action 1 yielding a 1.0 reward at the terminal state (state 10) and action 2 yielding 0.5. # Common settings n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } # Create one-hot features for (state, action) pairs create_features &lt;- function(s, a, n_states, n_actions) { vec &lt;- rep(0, n_states * n_actions) index &lt;- (a - 1) * n_states + s vec[index] &lt;- 1 return(vec) } # Initialize weights n_features &lt;- n_states * n_actions theta &lt;- rep(0, n_features) # Q-value approximation function q_hat &lt;- function(s, a, theta) { x &lt;- create_features(s, a, n_states, n_actions) return(sum(x * theta)) } # Q-Learning with function approximation q_learning_fa &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) { theta &lt;- rep(0, n_features) rewards &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- sample(1:(n_states - 1), 1) episode_reward &lt;- 0 while (TRUE) { # Epsilon-greedy action selection a &lt;- if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { q_vals &lt;- sapply(1:n_actions, function(a_) q_hat(s, a_, theta)) which.max(q_vals) } out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime r &lt;- out$reward episode_reward &lt;- episode_reward + r # Compute TD target and error q_current &lt;- q_hat(s, a, theta) q_next &lt;- if (s_prime == terminal_state) 0 else max(sapply(1:n_actions, function(a_) q_hat(s_prime, a_, theta))) target &lt;- r + gamma * q_next error &lt;- target - q_current # Gradient update x &lt;- create_features(s, a, n_states, n_actions) theta &lt;- theta + alpha * error * x if (s_prime == terminal_state) break s &lt;- s_prime } rewards[ep] &lt;- episode_reward } # Derive policy policy &lt;- sapply(1:n_states, function(s) { if (s == terminal_state) NA else which.max(sapply(1:n_actions, function(a) q_hat(s, a, theta))) }) list(theta = theta, policy = policy, rewards = rewards) } # Run Q-Learning with function approximation set.seed(42) fa_result &lt;- q_learning_fa(episodes = 1000, alpha = 0.1, epsilon = 0.1) fa_policy &lt;- fa_result$policy fa_rewards &lt;- fa_result$rewards # Visualize policy library(ggplot2) policy_df &lt;- data.frame( State = 1:n_states, Policy = fa_policy, Algorithm = &quot;Q-Learning FA&quot; ) policy_df$Policy[n_states] &lt;- NA # Terminal state policy_plot &lt;- ggplot(policy_df, aes(x = State, y = Policy)) + geom_point(size = 3, color = &quot;deepskyblue&quot;) + geom_line(na.rm = TRUE, color = &quot;deepskyblue&quot;) + theme_minimal() + labs(title = &quot;Policy from Q-Learning with Function Approximation&quot;, x = &quot;State&quot;, y = &quot;Action&quot;) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;)) + theme(legend.position = &quot;none&quot;) # Visualize cumulative rewards reward_df &lt;- data.frame( Episode = 1:1000, Reward = cumsum(fa_rewards), Algorithm = &quot;Q-Learning FA&quot; ) policy_plot "],["beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html", "Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R 7.1 Introduction 7.2 Theoretical Background 7.3 R Implementation 7.4 Analysis and Insights 7.5 Comparison with Linear Approximation 7.6 Conclusion", " Chapter 7 Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R 7.1 Introduction While linear function approximation provides a solid foundation for scaling reinforcement learning beyond tabular methods, it assumes a linear relationship between features and Q-values. Real-world problems often exhibit complex, non-linear patterns that linear models cannot capture effectively. This post extends our previous exploration by implementing Q-Learning with Random Forest function approximation, demonstrating how ensemble methods can learn intricate state-action value relationships while maintaining interpretability and robust generalization. Random Forests offer several advantages over linear approximation: they handle non-linear relationships naturally, provide built-in feature importance measures, resist overfitting through ensemble averaging, and require minimal hyperparameter tuning. We’ll implement this approach using the same 10-state, 2-action environment, comparing the learned policies and examining the unique characteristics of tree-based function approximation. 7.2 Theoretical Background Random Forest function approximation replaces the linear parameterization with an ensemble of decision trees. Instead of: \\[ Q(s, a; \\theta) = \\phi(s, a)^T \\theta \\] we now approximate the action-value function as: \\[ Q(s, a) = \\frac{1}{B} \\sum_{b=1}^{B} T_b(\\phi(s, a)) \\] where \\(T_b\\) represents the \\(b\\)-th tree in the ensemble, \\(B\\) is the number of trees, and \\(\\phi(s, a)\\) is our feature representation. Each tree \\(T_b\\) is trained on a bootstrap sample of the data with random feature subsets at each split, providing natural regularization and variance reduction. 7.2.1 Q-Learning with Random Forest Approximation The Q-Learning update process with Random Forest approximation involves: Experience Collection: Gather state-action-reward-next state tuples \\((s, a, r, s&#39;)\\) Target Computation: Calculate TD targets \\(y = r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;)\\) Model Training: Fit Random Forest regressor to predict \\(Q(s, a)\\) from features \\(\\phi(s, a)\\) Policy Update: Use updated model for epsilon-greedy action selection Unlike linear methods with continuous parameter updates, Random Forest approximation requires periodic model retraining on accumulated experience. This batch-like approach trades computational efficiency for modeling flexibility. 7.2.2 Feature Engineering for Tree-Based Models For our implementation, we use a simple concatenation of one-hot encoded state and action vectors: \\[ \\phi(s, a) = [e_s^{(state)} \\; || \\; e_a^{(action)}] \\] where \\(e_s^{(state)}\\) is a one-hot vector for state \\(s\\) and \\(e_a^{(action)}\\) is a one-hot vector for action \\(a\\). This encoding allows trees to learn complex interactions between states and actions while maintaining interpretability. 7.2.3 Comparison with Previous Methods Aspect Tabular Q-Learning Linear Function Approximation Random Forest Function Approximation Model Complexity None; direct storage Linear combinations Non-linear ensemble Feature Interactions Implicit None (unless engineered) Automatic discovery Interpretability Full Moderate (weights) High (tree structures) Training Online updates Gradient descent Batch retraining Overfitting Risk None Low Low (ensemble averaging) Computational Cost \\(O(1)\\) lookup \\(O(d)\\) linear algebra \\(O(B \\cdot \\log n)\\) prediction 7.3 R Implementation Our implementation builds upon the previous environment while introducing Random Forest-based Q-value approximation. The key innovation lies in accumulating training examples and periodically retraining the forest to incorporate new experience. # Load required libraries library(randomForest) library(ggplot2) # Environment setup (same as previous implementation) n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } # Feature encoding for Random Forest encode_features &lt;- function(s, a, n_states, n_actions) { state_vec &lt;- rep(0, n_states) action_vec &lt;- rep(0, n_actions) state_vec[s] &lt;- 1 action_vec[a] &lt;- 1 return(c(state_vec, action_vec)) } n_features &lt;- n_states + n_actions # Q-Learning with Random Forest function approximation q_learning_rf &lt;- function(episodes = 1000, epsilon = 0.1, retrain_freq = 10, min_samples = 50) { # Initialize training data storage rf_data_x &lt;- matrix(nrow = 0, ncol = n_features) rf_data_y &lt;- numeric(0) rf_model &lt;- NULL rewards &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- sample(1:(n_states - 1), 1) # Start from non-terminal state episode_reward &lt;- 0 while (TRUE) { # Predict Q-values for all actions q_preds &lt;- sapply(1:n_actions, function(a) { x &lt;- encode_features(s, a, n_states, n_actions) if (!is.null(rf_model)) { predict(rf_model, as.data.frame(t(x))) } else { runif(1) # Random initialization } }) # Epsilon-greedy action selection a &lt;- if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { which.max(q_preds) } # Take action and observe outcome out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime r &lt;- out$reward episode_reward &lt;- episode_reward + r # Compute TD target q_next &lt;- if (s_prime == terminal_state) { 0 } else { max(sapply(1:n_actions, function(a_) { x_next &lt;- encode_features(s_prime, a_, n_states, n_actions) if (!is.null(rf_model)) { predict(rf_model, as.data.frame(t(x_next))) } else { 0 } })) } target &lt;- r + gamma * q_next # Store training example x &lt;- encode_features(s, a, n_states, n_actions) rf_data_x &lt;- rbind(rf_data_x, x) rf_data_y &lt;- c(rf_data_y, target) # Retrain Random Forest periodically if (nrow(rf_data_x) &gt;= min_samples &amp;&amp; ep %% retrain_freq == 0) { rf_model &lt;- randomForest( x = as.data.frame(rf_data_x), y = rf_data_y, ntree = 100, nodesize = 5, mtry = max(1, floor(n_features / 3)) ) } if (s_prime == terminal_state) break s &lt;- s_prime } rewards[ep] &lt;- episode_reward } # Derive final policy policy &lt;- sapply(1:(n_states-1), function(s) { if (!is.null(rf_model)) { q_vals &lt;- sapply(1:n_actions, function(a) { x &lt;- encode_features(s, a, n_states, n_actions) predict(rf_model, as.data.frame(t(x))) }) which.max(q_vals) } else { 1 # Default action } }) list(model = rf_model, policy = c(policy, NA), rewards = rewards, training_data = list(x = rf_data_x, y = rf_data_y)) } # Run Q-Learning with Random Forest approximation set.seed(42) rf_result &lt;- q_learning_rf(episodes = 1000, epsilon = 0.1, retrain_freq = 10) rf_policy &lt;- rf_result$policy rf_rewards &lt;- rf_result$rewards # Create policy visualization policy_df &lt;- data.frame( State = 1:n_states, Policy = rf_policy, Algorithm = &quot;Q-Learning RF&quot; ) policy_plot_rf &lt;- ggplot(policy_df[1:(n_states-1), ], aes(x = State, y = Policy)) + geom_point(size = 4, color = &quot;forestgreen&quot;) + geom_line(color = &quot;forestgreen&quot;, linewidth = 1) + theme_minimal() + labs( title = &quot;Policy from Q-Learning with Random Forest Approximation&quot;, x = &quot;State&quot;, y = &quot;Action&quot; ) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;), limits = c(0.5, 2.5)) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) # Compare cumulative rewards with moving average rewards_smooth &lt;- numeric(length(rf_rewards)) window_size &lt;- 50 for (i in 1:length(rf_rewards)) { start_idx &lt;- max(1, i - window_size + 1) rewards_smooth[i] &lt;- mean(rf_rewards[start_idx:i]) } reward_df_rf &lt;- data.frame( Episode = 1:1000, Reward = rewards_smooth, Algorithm = &quot;Q-Learning RF&quot; ) reward_plot_rf &lt;- ggplot(reward_df_rf, aes(x = Episode, y = Reward)) + geom_line(color = &quot;forestgreen&quot;, linewidth = 1) + theme_minimal() + labs( title = &quot;Learning Curve: Q-Learning with Random Forest (50-episode moving average)&quot;, x = &quot;Episode&quot;, y = &quot;Average Reward&quot; ) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) # Display plots print(policy_plot_rf) print(reward_plot_rf) # Feature importance analysis if (!is.null(rf_result$model)) { importance_df &lt;- data.frame( Feature = c(paste(&quot;State&quot;, 1:n_states), paste(&quot;Action&quot;, 1:n_actions)), Importance = importance(rf_result$model)[, 1] ) importance_plot &lt;- ggplot(importance_df, aes(x = reorder(Feature, Importance), y = Importance)) + geom_col(fill = &quot;forestgreen&quot;, alpha = 0.7) + coord_flip() + theme_minimal() + labs( title = &quot;Feature Importance in Random Forest Q-Function&quot;, x = &quot;Feature&quot;, y = &quot;Importance (Mean Decrease in MSE)&quot; ) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) print(importance_plot) } # Model diagnostics cat(&quot;Random Forest Model Summary:\\n&quot;) cat(&quot;Number of trees:&quot;, rf_result$model$ntree, &quot;\\n&quot;) cat(&quot;Training examples:&quot;, nrow(rf_result$training_data$x), &quot;\\n&quot;) cat(&quot;Final OOB error:&quot;, tail(rf_result$model$mse, 1), &quot;\\n&quot;) 7.4 Analysis and Insights 7.4.1 Policy Learning Characteristics Random Forest function approximation exhibits several characteristics that distinguish it from linear methods. Trees can capture non-linear decision boundaries, enabling the model to learn state-action relationships that linear approaches cannot represent. The random feature sampling at each split performs automatic feature selection, focusing computational resources on the most informative variables. Ensemble averaging across multiple trees reduces overfitting and provides stable predictions across different training samples. Individual trees maintain interpretable decision paths that show how Q-values are estimated for specific state-action pairs. 7.4.2 Computational Considerations The batch retraining approach creates distinct computational trade-offs that affect implementation decisions. Training frequency must balance responsiveness against computational cost, as more frequent updates improve adaptation but require additional processing time. Trees need sufficient data to learn meaningful patterns, which can slow initial learning compared to methods that update continuously. Memory requirements increase over time as training examples accumulate, requiring careful management of historical data. 7.4.3 Feature Importance Insights Random Forest methods naturally generate feature importance measures that reveal which states and actions most influence Q-value predictions. This interpretability provides diagnostic capabilities for understanding learning issues and analyzing policy decisions. The feature ranking can guide state representation choices and help identify redundant or irrelevant variables in the problem formulation. 7.4.4 Practical Implications Random Forest function approximation occupies a position between simple linear models and neural networks in terms of complexity and capability. The method handles larger state spaces more effectively than tabular approaches while remaining computationally tractable. It captures non-linear patterns without requiring extensive feature engineering or domain expertise. The approach shows less sensitivity to hyperparameter choices compared to neural networks while maintaining stability across different problem instances. The inherent interpretability provides insights into the decision-making process that can be valuable for debugging and analysis. 7.5 Comparison with Linear Approximation Random Forest methods demonstrate several advantages and trade-offs when compared to linear function approximation. The tree-based approach excels at pattern recognition, learning state-action relationships that linear models cannot capture due to their representational limitations. However, initial learning proceeds more slowly as trees require sufficient data to construct meaningful decision boundaries. Computational costs are higher due to periodic retraining requirements, contrasting with the continuous gradient updates used in linear methods. Generalization performance tends to be superior, as ensemble averaging provides natural regularization that reduces overfitting tendencies. 7.6 Conclusion Random Forest function approximation extends linear methods by offering enhanced modeling flexibility while preserving interpretability characteristics. The approach performs particularly well in environments with non-linear state-action relationships and provides regularization through ensemble averaging. Several key observations emerge from this analysis. Non-linear function approximation can capture patterns that linear models miss, enabling better policy learning in complex environments. Batch learning approaches require careful consideration of training frequency and sample requirements to balance performance with computational efficiency. Feature importance analysis provides insights into learned policies that can guide problem formulation and debugging efforts. Tree-based methods offer an interpretable alternative to neural network approaches while maintaining theoretical foundations. This exploration demonstrates how ensemble methods can enhance reinforcement learning without abandoning the established principles of Q-Learning. Future work could investigate online tree learning algorithms that avoid batch retraining requirements, adaptive schedules that optimize training frequency based on performance metrics, or hybrid approaches that combine strengths from different function approximation methods. "],["deep-function-approximation-q-learning-with-neural-networks-in-r.html", "Chapter 8 Deep Function Approximation: Q-Learning with Neural Networks in R 8.1 Introduction 8.2 Theoretical Foundation 8.3 R Implementation 8.4 Analysis and Interpretation 8.5 Practical Considerations 8.6 Comparison Across Function Approximation Methods 8.7 Future Directions 8.8 Conclusion", " Chapter 8 Deep Function Approximation: Q-Learning with Neural Networks in R 8.1 Introduction Our exploration of function approximation in reinforcement learning has progressed from linear models to ensemble methods, each offering increasing sophistication in capturing complex relationships between states, actions, and their values. Neural networks represent the natural next step in this evolution, providing the theoretical foundation for modern deep reinforcement learning while maintaining practical implementability in R. Neural network function approximation transcends the limitations of both linear models and tree-based methods by learning hierarchical feature representations automatically. Where linear models assume additive relationships and Random Forests rely on axis-aligned splits, neural networks can discover arbitrary non-linear transformations of the input space. This capability proves particularly valuable in reinforcement learning, where the optimal action-value function often exhibits complex dependencies that resist simple parametric forms. This post demonstrates Q-Learning with neural network function approximation using R’s nnet package, continuing our 10-state environment while examining how artificial neural networks learn Q-value approximations. We explore the theoretical foundations, implementation challenges, and practical considerations that distinguish neural network approaches from their predecessors. 8.2 Theoretical Foundation Neural network function approximation replaces our previous parameterizations with a multi-layered composition of non-linear transformations. The action-value function becomes: \\[ Q(s, a; \\theta) = f_L(W_L f_{L-1}(W_{L-1} \\cdots f_1(W_1 \\phi(s, a) + b_1) \\cdots + b_{L-1}) + b_L) \\] where \\(f_i\\) represents the activation function at layer \\(i\\), \\(W_i\\) and \\(b_i\\) are weight matrices and bias vectors, and \\(\\theta = \\{W_1, b_1, \\ldots, W_L, b_L\\}\\) encompasses all trainable parameters. This hierarchical structure enables the network to learn increasingly abstract representations of the state-action space. 8.2.1 Universal Approximation and Expressivity The theoretical appeal of neural networks stems from universal approximation theorems, which guarantee that feedforward networks with sufficient hidden units can approximate any continuous function to arbitrary precision. In the context of Q-Learning, this suggests that neural networks can, in principle, represent any action-value function arising from a Markov decision process. For our implementation, we employ a single hidden layer architecture with sigmoid activation functions: \\[ Q(s, a; \\theta) = W_2 \\sigma(W_1 \\phi(s, a) + b_1) + b_2 \\] where \\(\\sigma(z) = \\frac{1}{1 + e^{-z}}\\) is the sigmoid function, providing the non-linearity necessary for complex function approximation. 8.2.2 Gradient-Based Learning Neural network training relies on backpropagation to compute gradients of the temporal difference error with respect to all network parameters. The loss function for a single transition becomes: \\[ L(\\theta) = \\frac{1}{2}(y - Q(s, a; \\theta))^2 \\] where \\(y = r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;; \\theta)\\) is the TD target. The gradient with respect to parameters \\(\\theta\\) follows the chain rule: \\[ \\nabla_\\theta L(\\theta) = (Q(s, a; \\theta) - y) \\nabla_\\theta Q(s, a; \\theta) \\] This gradient guides parameter updates through standard optimization algorithms, though the non-convex nature of neural network loss surfaces introduces challenges absent in linear approximation. 8.2.3 Comparison with Previous Approaches Neural networks offer several theoretical advantages over linear and tree-based methods. Unlike linear approximation, they can learn feature interactions without explicit engineering. Unlike Random Forests, they provide smooth function approximations suitable for gradient-based optimization. However, this flexibility comes with increased computational complexity and potential instability during training. Characteristic Linear Approximation Random Forest Neural Network Function Class Linear combinations Piecewise constant Universal approximators Feature Learning None Implicit via splits Explicit representation learning Optimization Convex (guaranteed convergence) Non-parametric Non-convex (local minima) Interpretability High (weight inspection) Moderate (tree visualization) Low (distributed representations) Sample Efficiency High Moderate Variable (depends on architecture) 8.3 R Implementation Our neural network implementation builds upon the established environment while introducing the complexities of gradient-based optimization and network training. The nnet package provides a lightweight implementation suitable for demonstrating core concepts without the overhead of deep learning frameworks. # Load required libraries library(nnet) library(ggplot2) # Environment setup (consistent with previous implementations) n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Environment: transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Sampling function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } # Feature encoding for neural network input encode_features &lt;- function(s, a, n_states, n_actions) { state_vec &lt;- rep(0, n_states) action_vec &lt;- rep(0, n_actions) state_vec[s] &lt;- 1 action_vec[a] &lt;- 1 return(c(state_vec, action_vec)) } n_features &lt;- n_states + n_actions # Q-Learning with neural network function approximation q_learning_nn &lt;- function(episodes = 1000, epsilon = 0.1, hidden_size = 10, retrain_freq = 10, min_samples = 50) { # Initialize training data storage q_data_x &lt;- matrix(nrow = 0, ncol = n_features) q_data_y &lt;- numeric(0) q_model &lt;- NULL rewards &lt;- numeric(episodes) training_losses &lt;- numeric() for (ep in 1:episodes) { s &lt;- sample(1:(n_states - 1), 1) # Start from non-terminal state episode_reward &lt;- 0 while (TRUE) { # Predict Q-values for all actions q_preds &lt;- sapply(1:n_actions, function(a) { x &lt;- encode_features(s, a, n_states, n_actions) if (!is.null(q_model)) { as.numeric(predict(q_model, as.data.frame(t(x)))) } else { runif(1) # Random initialization } }) # Epsilon-greedy action selection a &lt;- if (runif(1) &lt; epsilon) { sample(1:n_actions, 1) } else { which.max(q_preds) } # Take action and observe outcome out &lt;- sample_env(s, a) s_prime &lt;- out$s_prime r &lt;- out$reward episode_reward &lt;- episode_reward + r # Compute TD target q_next &lt;- if (s_prime == terminal_state) { 0 } else { max(sapply(1:n_actions, function(a_) { x_next &lt;- encode_features(s_prime, a_, n_states, n_actions) if (!is.null(q_model)) { as.numeric(predict(q_model, as.data.frame(t(x_next)))) } else { 0 } })) } target &lt;- r + gamma * q_next # Store training example x &lt;- encode_features(s, a, n_states, n_actions) q_data_x &lt;- rbind(q_data_x, x) q_data_y &lt;- c(q_data_y, target) # Train neural network periodically if (nrow(q_data_x) &gt;= min_samples &amp;&amp; ep %% retrain_freq == 0) { # Suppress nnet output for cleaner execution capture.output({ q_model &lt;- nnet( x = q_data_x, y = q_data_y, size = hidden_size, linout = TRUE, maxit = 100, decay = 0.01, trace = FALSE ) }) # Track training loss if (!is.null(q_model)) { predictions &lt;- predict(q_model, as.data.frame(q_data_x)) mse &lt;- mean((predictions - q_data_y)^2) training_losses &lt;- c(training_losses, mse) } } if (s_prime == terminal_state) break s &lt;- s_prime } rewards[ep] &lt;- episode_reward } # Derive final policy policy &lt;- sapply(1:(n_states-1), function(s) { if (!is.null(q_model)) { q_vals &lt;- sapply(1:n_actions, function(a) { x &lt;- encode_features(s, a, n_states, n_actions) as.numeric(predict(q_model, as.data.frame(t(x)))) }) which.max(q_vals) } else { 1 # Default action } }) list(model = q_model, policy = c(policy, NA), rewards = rewards, training_losses = training_losses, training_data = list(x = q_data_x, y = q_data_y)) } # Run Q-Learning with neural network approximation set.seed(42) nn_result &lt;- q_learning_nn(episodes = 1000, epsilon = 0.1, hidden_size = 10) nn_policy &lt;- nn_result$policy nn_rewards &lt;- nn_result$rewards # Visualize learned policy policy_df &lt;- data.frame( State = 1:n_states, Policy = nn_policy, Algorithm = &quot;Q-Learning NN&quot; ) policy_plot_nn &lt;- ggplot(policy_df[1:(n_states-1), ], aes(x = State, y = Policy)) + geom_point(size = 4, color = &quot;coral&quot;) + geom_line(color = &quot;coral&quot;, linewidth = 1) + theme_minimal() + labs( title = &quot;Policy from Q-Learning with Neural Network Approximation&quot;, x = &quot;State&quot;, y = &quot;Action&quot; ) + scale_x_continuous(breaks = 1:n_states) + scale_y_continuous(breaks = 1:n_actions, labels = c(&quot;Action 1&quot;, &quot;Action 2&quot;), limits = c(0.5, 2.5)) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) # Learning curve with smoothing rewards_smooth &lt;- numeric(length(nn_rewards)) window_size &lt;- 50 for (i in 1:length(nn_rewards)) { start_idx &lt;- max(1, i - window_size + 1) rewards_smooth[i] &lt;- mean(nn_rewards[start_idx:i]) } reward_df_nn &lt;- data.frame( Episode = 1:1000, Reward = rewards_smooth, Algorithm = &quot;Q-Learning NN&quot; ) reward_plot_nn &lt;- ggplot(reward_df_nn, aes(x = Episode, y = Reward)) + geom_line(color = &quot;coral&quot;, linewidth = 1) + theme_minimal() + labs( title = &quot;Learning Curve: Q-Learning with Neural Network (50-episode moving average)&quot;, x = &quot;Episode&quot;, y = &quot;Average Reward&quot; ) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) # Training loss evolution if (length(nn_result$training_losses) &gt; 0) { loss_df &lt;- data.frame( Update = 1:length(nn_result$training_losses), Loss = nn_result$training_losses ) loss_plot &lt;- ggplot(loss_df, aes(x = Update, y = Loss)) + geom_line(color = &quot;darkred&quot;, linewidth = 1) + theme_minimal() + labs( title = &quot;Neural Network Training Loss Evolution&quot;, x = &quot;Training Update&quot;, y = &quot;Mean Squared Error&quot; ) + theme( plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.title = element_text(size = 12), axis.text = element_text(size = 10) ) print(loss_plot) } # Display main plots print(policy_plot_nn) print(reward_plot_nn) # Model diagnostics and analysis if (!is.null(nn_result$model)) { cat(&quot;Neural Network Model Summary:\\n&quot;) cat(&quot;Architecture: Input(&quot;, n_features, &quot;) -&gt; Hidden(&quot;, nn_result$model$n[2], &quot;) -&gt; Output(1)\\n&quot;) cat(&quot;Total parameters:&quot;, length(nn_result$model$wts), &quot;\\n&quot;) cat(&quot;Training examples:&quot;, nrow(nn_result$training_data$x), &quot;\\n&quot;) cat(&quot;Final training loss:&quot;, tail(nn_result$training_losses, 1), &quot;\\n&quot;) # Weight analysis weights &lt;- nn_result$model$wts cat(&quot;Weight statistics:\\n&quot;) cat(&quot;Mean:&quot;, round(mean(weights), 4), &quot;\\n&quot;) cat(&quot;Standard deviation:&quot;, round(sd(weights), 4), &quot;\\n&quot;) cat(&quot;Range: [&quot;, round(min(weights), 4), &quot;,&quot;, round(max(weights), 4), &quot;]\\n&quot;) } 8.4 Analysis and Interpretation 8.4.1 Learning Dynamics Neural network function approximation introduces several unique characteristics compared to linear and tree-based methods. The non-convex optimization landscape means that training can exhibit complex dynamics, including periods of rapid improvement followed by plateaus. The learning curve often shows more volatility than linear methods due to the continuous parameter updates and potential for local minima. 8.4.2 Function Representation Unlike Random Forests that learn piecewise constant approximations, neural networks produce smooth function approximations. This continuity can be advantageous for policy learning, as small changes in state typically result in small changes in Q-values. The hidden layer learns intermediate representations that capture relevant features for action-value estimation. 8.4.3 Generalization Properties Neural networks excel at discovering relevant patterns in the state-action space without explicit feature engineering. The hidden units automatically learn combinations of input features that prove useful for Q-value prediction. This automatic feature discovery becomes increasingly valuable as problem complexity grows. 8.4.4 Training Stability The batch retraining approach helps stabilize learning compared to online neural network updates, which can suffer from catastrophic forgetting. However, the periodic retraining introduces discontinuities in the learned function that can temporarily disrupt policy performance. 8.5 Practical Considerations 8.5.1 Architecture Selection The choice of network architecture significantly impacts performance. Too few hidden units may underfit the true Q-function, while too many can lead to overfitting with limited training data. Our single hidden layer with 10 units provides a reasonable balance for the 10-state environment. 8.5.2 Training Frequency The retraining frequency presents a trade-off between computational efficiency and learning responsiveness. More frequent retraining provides better adaptation to new experience but increases computational cost. The optimal frequency depends on the environment complexity and available computational resources. 8.5.3 Regularization Neural networks benefit from regularization techniques to prevent overfitting. Our implementation includes weight decay (L2 regularization) to encourage smaller weights and improve generalization. Other techniques like dropout or early stopping could further enhance performance. 8.5.4 Initialization and Convergence Neural network training depends critically on weight initialization and optimization parameters. Poor initialization can trap the network in suboptimal local minima, while inappropriate learning rates can cause divergence or slow convergence. 8.6 Comparison Across Function Approximation Methods Our progression from tabular to linear to ensemble to neural network methods illustrates the evolution of function approximation in reinforcement learning. Each method offers distinct advantages for different problem characteristics. Tabular methods provide exact representation but fail to scale. Linear methods offer guaranteed convergence and interpretability but assume additive relationships. Random Forests handle non-linearities while maintaining interpretability but produce discontinuous approximations. Neural networks provide universal approximation capabilities and smooth functions but introduce optimization challenges and reduced interpretability. The choice among methods depends on problem requirements, available data, computational constraints, and interpretability needs. Neural networks shine when function complexity exceeds simpler methods’ capabilities and sufficient training data is available. 8.7 Future Directions This exploration establishes the foundation for more advanced neural network approaches in reinforcement learning. Extensions could include deeper architectures, convolutional networks for spatial problems, recurrent networks for partially observable environments, or modern techniques like attention mechanisms. The theoretical framework developed here scales naturally to these more complex architectures, with the core principles of temporal difference learning and gradient-based optimization remaining constant while the function approximation capabilities expand dramatically. 8.8 Conclusion Neural network function approximation represents a significant step toward the sophisticated methods underlying modern deep reinforcement learning. While maintaining the theoretical foundations of Q-Learning, neural networks provide the flexibility to tackle complex environments that challenge simpler approximation methods. The implementation demonstrates how classical reinforcement learning principles extend naturally to neural network settings, preserving core algorithmic structure while enhancing representational power. This foundation enables practitioners to understand and implement more advanced methods building on these fundamental concepts. The journey through different function approximation approaches reveals the rich landscape of reinforcement learning methods, each contributing unique insights and capabilities. Neural networks, as universal approximators, provide the theoretical and practical foundation for tackling increasingly complex decision-making problems across diverse domains. "],["dyna-and-dynaq.html", "Chapter 9 Dyna and DynaQ 9.1 Introduction 9.2 Theoretical Framework 9.3 Implementation in R 9.4 Experimental Analysis 9.5 Discussion 9.6 Implementation Considerations and Conclusion", " Chapter 9 Dyna and DynaQ 9.1 Introduction Traditional reinforcement learning methods fall into two categories: model-free approaches like SARSA and Q-Learning that learn directly from experience, and model-based methods that first learn environment dynamics then use planning algorithms. Dyna, introduced by Sutton (1990), bridges this gap by combining direct reinforcement learning with indirect learning through an internal model of the environment. The key insight behind Dyna is that real experience can serve dual purposes: updating value functions directly and improving an internal model that generates simulated experience for additional learning. This architecture allows agents to benefit from both the sample efficiency of planning and the robustness of direct learning, making it particularly effective in environments where experience is costly or limited. 9.2 Theoretical Framework 9.2.1 The Dyna Architecture Dyna integrates three key components within a unified learning system: Direct RL: Learning from real experience using standard temporal difference methods Model Learning: Building an internal model of environment dynamics from experience Planning: Using the learned model to generate simulated experience for additional value function updates The complete Dyna update cycle can be formalized as follows. For each real experience tuple \\((s, a, r, s&#39;)\\): Direct Learning (Q-Learning): \\[Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right]\\] Model Update: \\[\\hat{T}(s,a) \\leftarrow s&#39;\\] \\[\\hat{R}(s,a) \\leftarrow r\\] Planning Phase (repeat \\(n\\) times): \\[s \\leftarrow \\text{random previously visited state}\\] \\[a \\leftarrow \\text{random action previously taken in } s\\] \\[r \\leftarrow \\hat{R}(s,a)\\] \\[s&#39; \\leftarrow \\hat{T}(s,a)\\] \\[Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right]\\] The parameter \\(n\\) controls the number of planning steps per real experience, representing the computational budget available for internal simulation. 9.2.2 Model Representation In its simplest form, Dyna uses a deterministic table-based model where \\(\\hat{T}(s,a)\\) stores the last observed next state for state-action pair \\((s,a)\\), and \\(\\hat{R}(s,a)\\) stores the last observed reward. This approach works well for deterministic environments but can be extended to handle stochastic dynamics through sampling-based representations. 9.2.3 Convergence Properties Under standard assumptions (all state-action pairs visited infinitely often, appropriate learning rates), Dyna inherits the convergence guarantees of its underlying RL algorithm. The addition of planning typically accelerates convergence by allowing each real experience to propagate information more widely through the value function. 9.3 Implementation in R We implement Dyna-Q using the same 10-state environment from previous posts, allowing direct comparison with pure Q-Learning and SARSA approaches. 9.3.1 Environment Setup # Environment parameters n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Transition and reward models (same as previous posts) set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Environment interaction function sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } 9.3.2 Dyna-Q Implementation # Dyna-Q algorithm with episode-wise performance tracking dyna_q &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1, n_planning = 5) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) model_T &lt;- array(NA, dim = c(n_states, n_actions)) model_R &lt;- array(NA, dim = c(n_states, n_actions)) visited_sa &lt;- list() episode_rewards &lt;- numeric(episodes) episode_steps &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- 1 total_reward &lt;- 0 steps &lt;- 0 while (s != terminal_state &amp;&amp; steps &lt; 100) { # Add max steps to prevent infinite loops if (runif(1) &lt; epsilon) { a &lt;- sample(1:n_actions, 1) } else { a &lt;- which.max(Q[s, ]) } outcome &lt;- sample_env(s, a) s_prime &lt;- outcome$s_prime r &lt;- outcome$reward total_reward &lt;- total_reward + r steps &lt;- steps + 1 Q[s, a] &lt;- Q[s, a] + alpha * (r + gamma * max(Q[s_prime, ]) - Q[s, a]) model_T[s, a] &lt;- s_prime model_R[s, a] &lt;- r sa_key &lt;- paste(s, a, sep = &quot;_&quot;) if (!(sa_key %in% names(visited_sa))) { visited_sa[[sa_key]] &lt;- c(s, a) } if (length(visited_sa) &gt; 0) { for (i in 1:n_planning) { sa_sample &lt;- sample(visited_sa, 1)[[1]] s_plan &lt;- sa_sample[1] a_plan &lt;- sa_sample[2] if (!is.na(model_T[s_plan, a_plan])) { s_prime_plan &lt;- model_T[s_plan, a_plan] r_plan &lt;- model_R[s_plan, a_plan] Q[s_plan, a_plan] &lt;- Q[s_plan, a_plan] + alpha * (r_plan + gamma * max(Q[s_prime_plan, ]) - Q[s_plan, a_plan]) } } } s &lt;- s_prime } episode_rewards[ep] &lt;- total_reward episode_steps[ep] &lt;- steps } list(Q = Q, policy = apply(Q, 1, which.max), episode_rewards = episode_rewards, episode_steps = episode_steps) } 9.3.3 Standard Q-Learning for Comparison # Q-Learning with performance tracking q_learning &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) episode_rewards &lt;- numeric(episodes) episode_steps &lt;- numeric(episodes) for (ep in 1:episodes) { s &lt;- 1 total_reward &lt;- 0 steps &lt;- 0 while (s != terminal_state &amp;&amp; steps &lt; 100) { a &lt;- if (runif(1) &lt; epsilon) sample(1:n_actions, 1) else which.max(Q[s, ]) outcome &lt;- sample_env(s, a) s_prime &lt;- outcome$s_prime r &lt;- outcome$reward total_reward &lt;- total_reward + r steps &lt;- steps + 1 Q[s, a] &lt;- Q[s, a] + alpha * (r + gamma * max(Q[s_prime, ]) - Q[s, a]) s &lt;- s_prime } episode_rewards[ep] &lt;- total_reward episode_steps[ep] &lt;- steps } list(Q = Q, policy = apply(Q, 1, which.max), episode_rewards = episode_rewards, episode_steps = episode_steps) } 9.4 Experimental Analysis 9.4.1 Learning Efficiency Comparison We compare Dyna-Q against standard Q-Learning across different numbers of planning steps: # Run comprehensive experiments set.seed(123) n_runs &lt;- 20 episodes &lt;- 300 # Initialize results storage all_results &lt;- data.frame() print(&quot;Running experiments...&quot;) for (run in 1:n_runs) { cat(&quot;Run&quot;, run, &quot;of&quot;, n_runs, &quot;\\n&quot;) # Run algorithms ql_result &lt;- q_learning(episodes = episodes) dyna5_result &lt;- dyna_q(episodes = episodes, n_planning = 5) dyna10_result &lt;- dyna_q(episodes = episodes, n_planning = 10) dyna20_result &lt;- dyna_q(episodes = episodes, n_planning = 20) # Store results run_data &lt;- data.frame( episode = rep(1:episodes, 4), run = run, algorithm = rep(c(&quot;Q-Learning&quot;, &quot;Dyna-Q (n=5)&quot;, &quot;Dyna-Q (n=10)&quot;, &quot;Dyna-Q (n=20)&quot;), each = episodes), reward = c(ql_result$episode_rewards, dyna5_result$episode_rewards, dyna10_result$episode_rewards, dyna20_result$episode_rewards), steps = c(ql_result$episode_steps, dyna5_result$episode_steps, dyna10_result$episode_steps, dyna20_result$episode_steps) ) all_results &lt;- rbind(all_results, run_data) } # Compute smoothed averages smoothed_results &lt;- all_results %&gt;% group_by(algorithm, episode) %&gt;% summarise( mean_reward = mean(reward), se_reward = sd(reward) / sqrt(n()), mean_steps = mean(steps), se_steps = sd(steps) / sqrt(n()), .groups = &#39;drop&#39; ) %&gt;% group_by(algorithm) %&gt;% mutate( smooth_reward = stats::filter(mean_reward, rep(1/10, 10), sides = 2), smooth_steps = stats::filter(mean_steps, rep(1/10, 10), sides = 2) ) # Create comprehensive visualization # Plot 1: Learning Curves (Rewards) p1 &lt;- ggplot(smoothed_results, aes(x = episode, y = mean_reward, color = algorithm)) + geom_line(size = 1.2) + geom_ribbon(aes(ymin = mean_reward - se_reward, ymax = mean_reward + se_reward, fill = algorithm), alpha = 0.2, color = NA) + scale_color_manual(values = c(&quot;Q-Learning&quot; = &quot;#E31A1C&quot;, &quot;Dyna-Q (n=5)&quot; = &quot;#1F78B4&quot;, &quot;Dyna-Q (n=10)&quot; = &quot;#33A02C&quot;, &quot;Dyna-Q (n=20)&quot; = &quot;#FF7F00&quot;)) + scale_fill_manual(values = c(&quot;Q-Learning&quot; = &quot;#E31A1C&quot;, &quot;Dyna-Q (n=5)&quot; = &quot;#1F78B4&quot;, &quot;Dyna-Q (n=10)&quot; = &quot;#33A02C&quot;, &quot;Dyna-Q (n=20)&quot; = &quot;#FF7F00&quot;)) + labs(title = &quot;Learning Performance: Average Episode Rewards&quot;, x = &quot;Episode&quot;, y = &quot;Average Reward per Episode&quot;, color = &quot;Algorithm&quot;, fill = &quot;Algorithm&quot;) + theme_minimal() + theme(plot.title = element_text(size = 14, face = &quot;bold&quot;), legend.position = &quot;bottom&quot;) # Plot 2: Steps to Terminal (Efficiency) p2 &lt;- ggplot(smoothed_results, aes(x = episode, y = mean_steps, color = algorithm)) + geom_line(size = 1.2) + geom_ribbon(aes(ymin = mean_steps - se_steps, ymax = mean_steps + se_steps, fill = algorithm), alpha = 0.2, color = NA) + scale_color_manual(values = c(&quot;Q-Learning&quot; = &quot;#E31A1C&quot;, &quot;Dyna-Q (n=5)&quot; = &quot;#1F78B4&quot;, &quot;Dyna-Q (n=10)&quot; = &quot;#33A02C&quot;, &quot;Dyna-Q (n=20)&quot; = &quot;#FF7F00&quot;)) + scale_fill_manual(values = c(&quot;Q-Learning&quot; = &quot;#E31A1C&quot;, &quot;Dyna-Q (n=5)&quot; = &quot;#1F78B4&quot;, &quot;Dyna-Q (n=10)&quot; = &quot;#33A02C&quot;, &quot;Dyna-Q (n=20)&quot; = &quot;#FF7F00&quot;)) + labs(title = &quot;Learning Efficiency: Steps to Reach Terminal State&quot;, x = &quot;Episode&quot;, y = &quot;Average Steps per Episode&quot;, color = &quot;Algorithm&quot;, fill = &quot;Algorithm&quot;) + theme_minimal() + theme(plot.title = element_text(size = 14, face = &quot;bold&quot;), legend.position = &quot;bottom&quot;) # Plot 3: Final performance comparison (last 50 episodes) final_performance &lt;- all_results %&gt;% filter(episode &gt; episodes - 50) %&gt;% group_by(algorithm, run) %&gt;% summarise(avg_reward = mean(reward), .groups = &#39;drop&#39;) p3 &lt;- ggplot(final_performance, aes(x = algorithm, y = avg_reward, fill = algorithm)) + geom_boxplot(alpha = 0.7) + geom_point(position = position_jitter(width = 0.2), alpha = 0.5) + scale_fill_manual(values = c(&quot;Q-Learning&quot; = &quot;#E31A1C&quot;, &quot;Dyna-Q (n=5)&quot; = &quot;#1F78B4&quot;, &quot;Dyna-Q (n=10)&quot; = &quot;#33A02C&quot;, &quot;Dyna-Q (n=20)&quot; = &quot;#FF7F00&quot;)) + labs(title = &quot;Final Performance Comparison (Last 50 Episodes)&quot;, x = &quot;Algorithm&quot;, y = &quot;Average Reward&quot;, fill = &quot;Algorithm&quot;) + theme_minimal() + theme(plot.title = element_text(size = 14, face = &quot;bold&quot;), axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot;) # Display all plots print(p1) print(p2) print(p3) 9.5 Discussion Dyna demonstrates a fundamental trade-off between sample efficiency and computational cost, requiring \\((n+1)\\) times the computation per step compared to model-free methods due to additional planning updates. This computational investment typically yields faster convergence, particularly when real experience is expensive or dangerous to obtain, though optimal planning steps depend on problem characteristics—moderate values (5-10) generally provide good improvements without excessive overhead while very large values can produce diminishing returns or hurt performance with inaccurate models. The algorithm’s effectiveness critically depends on model quality, with our deterministic table-based implementation becoming increasingly accurate as more state-action pairs are visited, though this approach assumes deterministic transitions and uses simple model representation (storing only last observed transitions) that works well for stationary environments but struggles with non-stationary dynamics where more sophisticated representations maintaining transition probability distributions could improve robustness at increased computational cost. The interaction between exploration and planning creates a distinctive advantage where \\(\\epsilon\\)-greedy exploration ensures diverse state-action coverage that directly improves model quality and planning effectiveness, establishing a positive feedback loop where better exploration enhances the model, making planning more effective and leading to better policies with potentially more informed exploration. Relative to pure model-free methods like Q-Learning, Dyna typically shows faster convergence and better sample efficiency by allowing each real experience to propagate information more widely through planning, while compared to pure model-based approaches, it maintains robustness through continued direct learning even with imperfect models. However, basic Dyna has notable limitations including poor representation of stochastic environments through deterministic models and suboptimal uniform sampling for planning, though modern extensions like Dyna-Q+ with exploration bonuses, prioritized sweeping focusing on high-value updates, and model-based variants maintaining probability distributions over transitions address many of these constraints. 9.6 Implementation Considerations and Conclusion Dyna requires additional memory to store the learned model alongside the value function. In our implementation, this doubles the memory requirements compared to pure Q-Learning. For larger state-action spaces, this can become a significant consideration, potentially requiring sparse representations or function approximation techniques. The planning phase introduces variable computational demands that can complicate real-time applications. While the number of planning steps can be adjusted based on available computational budget, this flexibility requires careful consideration of timing constraints in online learning scenarios. Dyna introduces additional hyperparameters, particularly the number of planning steps \\(n\\). This parameter requires tuning based on the specific problem characteristics and computational constraints. Unlike some hyperparameters that can be set based on theoretical considerations, \\(n\\) often requires empirical validation. Dyna represents an elegant solution to integrating learning and planning in reinforcement learning, combining the sample efficiency of model-based methods with the robustness of model-free approaches. Our R implementation demonstrates both the benefits and challenges of this integration, showing improved learning speed at the cost of increased computational and memory requirements. The key insight of using real experience for both direct learning and model improvement creates a powerful synergy that can significantly accelerate learning in many environments. However, the approach requires careful consideration of model representation, computational constraints, and hyperparameter tuning to achieve optimal performance. Future extensions could explore more sophisticated model representations, prioritized planning strategies, or integration with function approximation techniques for handling larger state spaces. The fundamental principle of combining direct and indirect learning remains a valuable paradigm in modern reinforcement learning research. "],["dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html", "Chapter 10 Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning 10.1 Introduction 10.2 Theoretical Framework 10.3 Implementation in R 10.4 Experimental Analysis 10.5 Discussion and Implementation Considerations 10.6 Conclusion", " Chapter 10 Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning 10.1 Introduction While Dyna successfully bridges model-free and model-based reinforcement learning, it carries an inherent assumption that can limit its effectiveness in changing environments: that the world remains static. When an agent has learned to navigate one version of an environment, what happens if the rules suddenly change? Standard Dyna may find itself stuck, continuing to plan based on outdated information while failing to adequately explore the new reality. Dyna-Q+, introduced by Sutton (1990) alongside the original Dyna framework, addresses this limitation through a deceptively simple yet powerful mechanism: it rewards curiosity. By providing exploration bonuses for state-action pairs that haven’t been tried recently, Dyna-Q+ maintains a healthy skepticism about its model’s continued accuracy. This approach proves particularly valuable in non-stationary environments where adaptation speed can mean the difference between success and failure. The enhancement might seem minor—just an additional term in the reward calculation—but its implications run deep. Dyna-Q+ acknowledges that in a changing world, forgetting can be as important as remembering, and that an agent’s confidence in its model should decay over time unless continually refreshed by recent experience. 10.2 Theoretical Framework 10.2.1 The Exploration Bonus Mechanism Dyna-Q+ modifies the planning phase of standard Dyna by augmenting rewards with an exploration bonus based on the time elapsed since each state-action pair was last visited. The core insight lies in treating the passage of time as information: the longer an agent hasn’t verified a particular transition, the less confident it should be about that transition’s current validity. For each state-action pair \\((s,a)\\), we maintain a timestamp \\(\\\\tau(s,a)\\) recording when it was last experienced. During planning, instead of using the stored reward \\(\\\\hat{R}(s,a)\\) directly, we calculate an augmented reward: \\[r_{augmented} = \\hat{R}(s,a) + \\kappa \\sqrt{t - \\tau(s,a)}\\] where \\(t\\) represents the current time step, and \\(\\\\kappa\\) is a parameter controlling the strength of the exploration bonus. The square root function provides a diminishing bonus that grows with time but at a decreasing rate, reflecting the intuition that uncertainty about a transition increases with time but not linearly. 10.2.2 Complete Dyna-Q+ Algorithm The full algorithm extends standard Dyna with minimal modifications. For each real experience tuple \\((s, a, r, s&#39;)\\): Direct Learning: \\[Q(s,a) \\leftarrow Q(s,a) + \\alpha \\left[ r + \\gamma \\max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \\right]\\] Model and Timestamp Updates: \\[\\hat{T}(s,a) \\leftarrow s&#39;\\]\\[\\hat{R}(s,a) \\leftarrow r\\]\\[\\tau(s,a) \\leftarrow t\\] Planning Phase (repeat \\(n\\) times): \\[s_{plan} \\leftarrow \\text{random previously visited state}\\] \\[a_{plan} \\leftarrow \\text{random action previously taken in } s_{plan}\\] \\[r_{plan} \\leftarrow \\hat{R}(s_{plan},a_{plan}) + \\kappa \\sqrt{t - \\tau(s_{plan},a_{plan})}\\] \\[s&#39;_{plan} \\leftarrow \\hat{T}(s_{plan},a_{plan})\\] \\[Q(s_{plan},a_{plan}) \\leftarrow Q(s_{plan},a_{plan}) + \\alpha \\left[ r_{plan} + \\gamma \\max_{a&#39;} Q(s&#39;_{plan}, a&#39;) - Q(s_{plan},a_{plan}) \\right]\\] 10.2.3 Convergence and Stability The theoretical properties of Dyna-Q+ are more complex than those of standard Dyna due to the non-stationary nature of the augmented rewards. In stationary environments, the exploration bonuses for frequently visited state-action pairs will remain small, and convergence properties approach those of standard Dyna. However, the algorithm sacrifices some theoretical guarantees about convergence to optimal policies in exchange for improved adaptability. The parameter \\(\\\\kappa\\) requires careful tuning. Too small, and the exploration bonus becomes negligible, reducing Dyna-Q+ to standard Dyna. Too large, and the algorithm may exhibit excessive exploration even in stable environments, potentially degrading performance. The square root scaling helps moderate this trade-off by providing significant bonuses for truly neglected state-action pairs while keeping bonuses manageable for recently visited ones. 10.3 Implementation in R Building on our previous Dyna implementation, we extend the framework to include timestamp tracking and exploration bonuses. 10.3.1 Environment Setup We’ll use the same 10-state environment as before, but we’ll also create scenarios with environmental changes to demonstrate Dyna-Q+’s adaptive capabilities: # Environment parameters (same as before) n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states # Transition and reward models set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 # Environment interaction function with optional modification capability sample_env &lt;- function(s, a, modified = FALSE) { if (modified) { # Simulate environmental change by blocking previously optimal path if (s == 5 &amp;&amp; a == 1) { return(list(s_prime = 1, reward = -0.5)) # Penalty for blocked path } } probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } 10.3.2 Dyna-Q+ Implementation The key modification involves maintaining timestamps and calculating exploration bonuses during planning: dyna_q_plus &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1, n_planning = 5, kappa = 0.1, change_episode = NULL) { # Initialize Q-values, model, and timestamps Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) model_T &lt;- array(NA, dim = c(n_states, n_actions)) model_R &lt;- array(NA, dim = c(n_states, n_actions)) timestamps &lt;- array(0, dim = c(n_states, n_actions)) # When last visited visited_sa &lt;- list() current_time &lt;- 0 environment_changed &lt;- FALSE for (ep in 1:episodes) { # Check if we should change the environment if (!is.null(change_episode) &amp;&amp; ep == change_episode) { environment_changed &lt;- TRUE } s &lt;- 1 while (s != terminal_state) { current_time &lt;- current_time + 1 # Action selection (epsilon-greedy) if (runif(1) &lt; epsilon) { a &lt;- sample(1:n_actions, 1) } else { # Break ties randomly a &lt;- sample(which(Q[s, ] == max(Q[s, ])), 1) } # Take action and observe outcome outcome &lt;- sample_env(s, a, modified = environment_changed) s_prime &lt;- outcome$s_prime r &lt;- outcome$reward # Direct learning (Q-Learning update) Q[s, a] &lt;- Q[s, a] + alpha * (r + gamma * max(Q[s_prime, ]) - Q[s, a]) # Model learning and timestamp update model_T[s, a] &lt;- s_prime model_R[s, a] &lt;- r timestamps[s, a] &lt;- current_time # Track visited state-action pairs sa_key &lt;- paste(s, a, sep = &quot;_&quot;) if (!(sa_key %in% names(visited_sa))) { visited_sa[[sa_key]] &lt;- c(s, a) } # Planning phase with exploration bonuses if (length(visited_sa) &gt; 0) { for (i in 1:n_planning) { # Sample random previously visited state-action pair sa_sample &lt;- sample(visited_sa, 1)[[1]] s_plan &lt;- sa_sample[1] a_plan &lt;- sa_sample[2] # Get simulated experience from model if (!is.na(model_T[s_plan, a_plan])) { s_prime_plan &lt;- model_T[s_plan, a_plan] r_base &lt;- model_R[s_plan, a_plan] # Calculate exploration bonus time_since_visit &lt;- current_time - timestamps[s_plan, a_plan] exploration_bonus &lt;- kappa * sqrt(time_since_visit) r_plan &lt;- r_base + exploration_bonus # Planning update with augmented reward Q[s_plan, a_plan] &lt;- Q[s_plan, a_plan] + alpha * (r_plan + gamma * max(Q[s_prime_plan, ]) - Q[s_plan, a_plan]) } } } s &lt;- s_prime } } list(Q = Q, policy = apply(Q, 1, which.max), model_T = model_T, model_R = model_R, timestamps = timestamps) } 10.3.3 Standard Dyna for Comparison We also implement standard Dyna to highlight the differences: dyna_q_standard &lt;- function(episodes = 1000, alpha = 0.1, epsilon = 0.1, n_planning = 5, change_episode = NULL) { Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) model_T &lt;- array(NA, dim = c(n_states, n_actions)) model_R &lt;- array(NA, dim = c(n_states, n_actions)) visited_sa &lt;- list() environment_changed &lt;- FALSE for (ep in 1:episodes) { if (!is.null(change_episode) &amp;&amp; ep == change_episode) { environment_changed &lt;- TRUE } s &lt;- 1 while (s != terminal_state) { # Action selection if (runif(1) &lt; epsilon) { a &lt;- sample(1:n_actions, 1) } else { # Break ties randomly a &lt;- sample(which(Q[s, ] == max(Q[s, ])), 1) } # Take action outcome &lt;- sample_env(s, a, modified = environment_changed) s_prime &lt;- outcome$s_prime r &lt;- outcome$reward # Direct learning Q[s, a] &lt;- Q[s, a] + alpha * (r + gamma * max(Q[s_prime, ]) - Q[s, a]) # Model learning (no timestamp tracking) model_T[s, a] &lt;- s_prime model_R[s, a] &lt;- r # Track visited pairs sa_key &lt;- paste(s, a, sep = &quot;_&quot;) if (!(sa_key %in% names(visited_sa))) { visited_sa[[sa_key]] &lt;- c(s, a) } # Standard planning (no exploration bonus) if (length(visited_sa) &gt; 0) { for (i in 1:n_planning) { sa_sample &lt;- sample(visited_sa, 1)[[1]] s_plan &lt;- sa_sample[1] a_plan &lt;- sa_sample[2] if (!is.na(model_T[s_plan, a_plan])) { s_prime_plan &lt;- model_T[s_plan, a_plan] r_plan &lt;- model_R[s_plan, a_plan] # No bonus here Q[s_plan, a_plan] &lt;- Q[s_plan, a_plan] + alpha * (r_plan + gamma * max(Q[s_prime_plan, ]) - Q[s_plan, a_plan]) } } } s &lt;- s_prime } } list(Q = Q, policy = apply(Q, 1, which.max)) } 10.4 Experimental Analysis 10.4.1 Adaptation to Environmental Changes The most compelling demonstration of Dyna-Q+’s advantages comes from scenarios where the environment changes mid-learning. We’ll compare how quickly both algorithms adapt: # Function to evaluate policy performance evaluate_policy_performance &lt;- function(Q, episodes = 100, modified = FALSE) { total_reward &lt;- 0 total_steps &lt;- 0 for (ep in 1:episodes) { s &lt;- 1 episode_reward &lt;- 0 steps &lt;- 0 while (s != terminal_state &amp;&amp; steps &lt; 50) { # Prevent infinite loops a &lt;- sample(which(Q[s, ] == max(Q[s, ])), 1) # Break ties randomly outcome &lt;- sample_env(s, a, modified = modified) episode_reward &lt;- episode_reward + outcome$reward s &lt;- outcome$s_prime steps &lt;- steps + 1 } total_reward &lt;- total_reward + episode_reward total_steps &lt;- total_steps + steps } list(avg_reward = total_reward / episodes, avg_steps = total_steps / episodes) } # Comparative experiment with environmental change adaptation_experiment &lt;- function() { set.seed(123) n_runs &lt;- 20 change_point &lt;- 500 total_episodes &lt;- 1000 # Storage for results results &lt;- data.frame( episode = rep(1:total_episodes, 2), algorithm = rep(c(&quot;Dyna-Q&quot;, &quot;Dyna-Q+&quot;), each = total_episodes), performance = numeric(total_episodes * 2), run = rep(1, total_episodes * 2) ) for (run in 1:n_runs) { # Train both algorithms # For a full experiment, you would re-initialize Q here for each run dyna_standard_result &lt;- dyna_q_standard(episodes = total_episodes, change_episode = change_point) dyna_plus_result &lt;- dyna_q_plus(episodes = total_episodes, change_episode = change_point, kappa = 0.1) # Evaluate performance at each episode (simplified for illustration) for (ep in 1:total_episodes) { modified_env &lt;- ep &gt;= change_point # This is a simplified evaluation - in practice, you&#39;d want to # track performance throughout training if (ep %% 50 == 0) { std_perf &lt;- evaluate_policy_performance(dyna_standard_result$Q, episodes = 10, modified = modified_env) plus_perf &lt;- evaluate_policy_performance(dyna_plus_result$Q, episodes = 10, modified = modified_env) # Store results (this is simplified - you&#39;d want better tracking) idx_std &lt;- (run - 1) * total_episodes * 2 + ep idx_plus &lt;- (run - 1) * total_episodes * 2 + total_episodes + ep if (run == 1) { # Just store first run for illustration results$performance[ep] &lt;- std_perf$avg_reward results$performance[total_episodes + ep] &lt;- plus_perf$avg_reward } } } } return(results) } 10.4.1.1 Example: Running the Adaptation Experiment Here we execute the experiment defined above. The resulting plot shows that both algorithms perform similarly until the environment changes at episode 500. After the change, Dyna-Q (kappa = 0) fails to adapt because its model is outdated, leading to a sharp drop in performance. In contrast, Dyna-Q+ uses its exploration bonus to re-evaluate old paths, quickly discovering the change and finding a new optimal policy, thus recovering its performance. # Setup chunk for libraries if (!require(&quot;ggplot2&quot;, quietly = TRUE)) install.packages(&quot;ggplot2&quot;) if (!require(&quot;dplyr&quot;, quietly = TRUE)) install.packages(&quot;dplyr&quot;) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union if (!require(&quot;tidyr&quot;, quietly = TRUE)) install.packages(&quot;tidyr&quot;) library(ggplot2) library(dplyr) library(tidyr) # Run the experiment. # Note: The original experiment function evaluates performance every 50 episodes, # resulting in a plot that connects these discrete data points. adaptation_results_df &lt;- adaptation_experiment() # Prepare data for plotting by filtering out unevaluated episodes plot_data &lt;- adaptation_results_df %&gt;% filter(performance != 0) # Generate the plot ggplot(plot_data, aes(x = episode, y = performance, color = algorithm)) + geom_line(linewidth = 1.2) + geom_point(size = 2.5) + geom_vline(xintercept = 500, linetype = &quot;dashed&quot;, color = &quot;black&quot;, linewidth = 1) + annotate(&quot;text&quot;, x = 480, y = min(plot_data$performance, na.rm=TRUE) * 1.1, label = &quot;Environment\\nChange&quot;, vjust = 0, hjust = 1, color = &quot;black&quot;, size=3.5) + labs( title = &quot;Dyna-Q+ vs. Standard Dyna-Q in a Changing Environment&quot;, subtitle = &quot;Performance comparison before and after an environmental change at episode 500.&quot;, x = &quot;Episode&quot;, y = &quot;Average Reward&quot;, color = &quot;Algorithm&quot; ) + theme_minimal(base_size = 14) + scale_color_manual(values = c(&quot;Dyna-Q&quot; = &quot;#d95f02&quot;, &quot;Dyna-Q+&quot; = &quot;#1b9e77&quot;)) + theme( legend.position = &quot;bottom&quot;, plot.title = element_text(face = &quot;bold&quot;), plot.subtitle = element_text(color = &quot;grey30&quot;) ) 10.4.2 Parameter Sensitivity Analysis The exploration parameter \\(\\\\kappa\\) significantly influences Dyna-Q+’s behavior. Let’s examine its effects: kappa_sensitivity_analysis &lt;- function() { kappa_values &lt;- c(0, 0.01, 0.05, 0.1, 0.2, 0.5) change_point &lt;- 300 total_episodes &lt;- 600 results &lt;- list() for (i in seq_along(kappa_values)) { set.seed(42) # Consistent conditions result &lt;- dyna_q_plus(episodes = total_episodes, change_episode = change_point, kappa = kappa_values[i], n_planning = 10) # Create a temporary Q matrix for evaluation before the change # We can&#39;t know the exact Q before the change without modifying the main loop, # so we run a separate short training for pre-change evaluation. pre_change_result &lt;- dyna_q_plus(episodes = change_point, kappa = kappa_values[i], n_planning=10) pre_change_perf &lt;- evaluate_policy_performance(pre_change_result$Q, modified = FALSE) post_change_perf &lt;- evaluate_policy_performance(result$Q, modified = TRUE) # Handle division by zero or near-zero rewards adaptation_ratio &lt;- if (pre_change_perf$avg_reward &gt; 1e-5) { post_change_perf$avg_reward / pre_change_perf$avg_reward } else { NA # Avoid meaningless ratios } results[[i]] &lt;- list( kappa = kappa_values[i], pre_change_reward = pre_change_perf$avg_reward, post_change_reward = post_change_perf$avg_reward, adaptation_ratio = adaptation_ratio ) } # Convert to data frame for analysis sensitivity_df &lt;- do.call(rbind, lapply(results, function(x) { data.frame(kappa = x$kappa, pre_change = x$pre_change_reward, post_change = x$post_change_reward, adaptation = x$adaptation_ratio) })) return(sensitivity_df) } # Visualization function plot_kappa_sensitivity &lt;- function(data) { par(mfrow = c(1, 2), mar = c(5, 4, 4, 2) + 0.1) # Plot 1: Performance vs kappa plot(data$kappa, data$post_change, type = &quot;b&quot;, pch = 16, col = &quot;darkred&quot;, xlab = &quot;Kappa Value (κ)&quot;, ylab = &quot;Post-Change Average Reward&quot;, main = &quot;Performance After Change&quot;, ylim=c(min(data$post_change, na.rm=T)*0.9, max(data$post_change, na.rm=T)*1.1)) grid(lty = 1, col = &quot;gray90&quot;) # Plot 2: Adaptation ratio vs kappa plot(data$kappa, data$adaptation, type = &quot;b&quot;, pch = 16, col = &quot;darkblue&quot;, xlab = &quot;Kappa Value (κ)&quot;, ylab = &quot;Adaptation Ratio (Post/Pre)&quot;, main = &quot;Adaptation vs Exploration&quot;) grid(lty = 1, col = &quot;gray90&quot;) par(mfrow = c(1, 1)) } 10.4.2.1 Example: Running the Sensitivity Analysis We run the analysis for different values of \\(\\\\kappa\\). A value of \\(\\\\kappa = 0\\) corresponds to standard Dyna-Q. The table shows the average reward before and after the environmental change. The plots visualize how post-change performance and the adaptation ratio change with \\(\\\\kappa\\). There is a sweet spot for \\(\\\\kappa\\) (around 0.1-0.2 in this case) that provides the best adaptation. If \\(\\\\kappa\\) is too low, adaptation is slow; if it’s too high, the agent explores too much, which can also hurt performance. # 1. Generate the data sensitivity_data &lt;- kappa_sensitivity_analysis() # 2. Display the data as a formatted table if (!require(&quot;knitr&quot;, quietly = TRUE)) install.packages(&quot;knitr&quot;) knitr::kable(sensitivity_data, caption = &quot;Kappa Parameter Sensitivity Analysis Results&quot;, col.names = c(&quot;Kappa (κ)&quot;, &quot;Pre-Change Reward&quot;, &quot;Post-Change Reward&quot;, &quot;Adaptation Ratio&quot;), digits = 3, align = &#39;c&#39;) Table 10.1: Kappa Parameter Sensitivity Analysis Results Kappa (κ) Pre-Change Reward Post-Change Reward Adaptation Ratio 0.00 1.257 1.135 0.903 0.01 1.211 0.947 0.782 0.05 1.438 0.870 0.605 0.10 1.489 0.538 0.361 0.20 1.411 0.998 0.707 0.50 1.630 0.839 0.515 # 3. Generate the plots using the provided function plot_kappa_sensitivity(sensitivity_data) 10.4.3 Exploration Pattern Analysis One way to understand Dyna-Q+’s behavior is to examine how exploration bonuses evolve over time: analyze_exploration_patterns &lt;- function() { # Run Dyna-Q+ and track exploration bonuses set.seed(123) Q &lt;- matrix(0, nrow = n_states, ncol = n_actions) timestamps &lt;- array(0, dim = c(n_states, n_actions)) visited_sa &lt;- list() current_time &lt;- 0 kappa &lt;- 0.1 # Storage for bonus tracking bonus_history &lt;- list() # Define time points for snapshotting bonus values time_points &lt;- seq(100, 2000, by=100) # Run for a fixed number of time steps instead of episodes # to get a clearer view of bonus evolution over time. while(current_time &lt; 2000) { s &lt;- 1 # Reset to start state for each &quot;pseudo-episode&quot; while (s != terminal_state &amp;&amp; current_time &lt; 2000) { current_time &lt;- current_time + 1 # Simple action selection for this analysis a &lt;- sample(1:n_actions, 1) outcome &lt;- sample_env(s, a) s_prime &lt;- outcome$s_prime # Update timestamps timestamps[s, a] &lt;- current_time # Track bonuses at specific time points if (current_time %in% time_points) { bonuses &lt;- array(0, dim = c(n_states, n_actions)) for (state in 1:n_states) { for (action in 1:n_actions) { if (timestamps[state, action] &gt; 0) { time_diff &lt;- current_time - timestamps[state, action] bonuses[state, action] &lt;- kappa * sqrt(time_diff) } } } bonus_history[[as.character(current_time)]] &lt;- bonuses } s &lt;- s_prime } } return(bonus_history) } # Visualization of exploration bonus evolution plot_bonus_evolution &lt;- function() { bonus_data &lt;- analyze_exploration_patterns() # Extract bonus magnitudes over time time_points &lt;- as.numeric(names(bonus_data)) max_bonuses &lt;- sapply(bonus_data, function(x) max(x, na.rm = TRUE)) mean_bonuses &lt;- sapply(bonus_data, function(x) mean(x[x &gt; 0], na.rm = TRUE)) plot(time_points, max_bonuses, type = &quot;l&quot;, col = &quot;red&quot;, lwd = 2, xlab = &quot;Time Steps&quot;, ylab = &quot;Exploration Bonus Magnitude&quot;, main = &quot;Evolution of Exploration Bonuses&quot;, ylim = c(0, max(max_bonuses, na.rm = TRUE))) lines(time_points, mean_bonuses, col = &quot;blue&quot;, lwd = 2, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;Maximum Bonus&quot;, &quot;Average Bonus&quot;), col = c(&quot;red&quot;, &quot;blue&quot;), lty = c(1, 2), lwd = 2, bty=&quot;n&quot;) grid(lty = 1, col = &quot;gray90&quot;) } 10.4.3.1 Example: Visualizing Exploration Bonuses This plot shows how the exploration bonuses change over time. As the agent explores, timestamps are updated, and the time since the last visit (t - τ) for any given state-action pair can grow. The Maximum Bonus corresponds to the state-action pair that has been unvisited for the longest time, showing the agent’s growing “curiosity” about that specific part of the environment. The Average Bonus (for visited pairs) tends to stay lower, indicating that most parts of the model are kept relatively fresh through planning and exploration. plot_bonus_evolution() 10.5 Discussion and Implementation Considerations Dyna-Q+ can be understood as a form of algorithmic curiosity that parallels aspects of human learning. Just as people grow uneasy about facts they have not revisited in some time, the algorithm gradually discounts its own model’s accuracy as intervals between visits lengthen. This built-in doubt is advantageous in non-stationary settings, where relying on yesterday’s truths can be costly. The exploration bonus, scaled by a square root of elapsed time, encodes an important nuance: uncertainty should increase with neglect, but at a diminishing rate. This prevents the system from sliding into perpetual skepticism while keeping enough pressure to revisit older assumptions. The extra bookkeeping is minimal—simply a timestamp for each state–action pair—but it changes the decision-making problem. The agent now balances three forces: exploiting current knowledge, exploring new possibilities, and re-exploring known areas to keep the model current. This is more complex than the standard explore–exploit trade-off in Dyna. For large state–action spaces, the linear scaling of timestamp storage may require function approximation or selective retention, especially in continuous or high-dimensional domains. Empirically, Dyna-Q+ tends to shine in environments that evolve over time. In stable conditions, bonuses for well-visited states remain small and the algorithm behaves much like standard Dyna. But when conditions shift, the systematic revisiting of old transitions enables faster adaptation. The parameter \\(\\\\kappa\\) sets the level of “model anxiety”: small values create a trusting system, large values a more suspicious one. The best setting depends on how quickly the world changes and on the relative costs of exploration and exploitation errors. The method rests on an implicit assumption—that environmental change is the main cause of model inaccuracy. When inaccuracy stems instead from intrinsic difficulty, such as noisy transitions or highly complex dynamics, the uniform bonuses may encourage needless exploration. Similarly, applying the same bonus across all state–action pairs ignores that some regions may be more volatile or strategically important than others. More refined variants might weight bonuses according to change likelihood or the expected impact of outdated information. Later research has broadened these ideas. In deep reinforcement learning, uncertainty-driven exploration often uses learned uncertainty estimates rather than timestamps. Meta-learning approaches aim to optimise exploration strategies across related environments. Curiosity-driven methods extend the spirit of Dyna-Q+ beyond temporal doubt, rewarding novelty in prediction error, information gain, or visitation patterns. The shared thread is that learning systems should actively seek information that improves their internal models. In practice, Dyna-Q+ is well suited to domains with gradual, structured change—financial markets with shifting regimes, or mobile robots navigating spaces where obstacles occasionally move. It is less effective in environments with rapid or chaotic dynamics, where maintaining a model may be futile or the bonus insufficient to trigger timely adaptation. Implementation choices often start with \\(\\\\kappa\\) between 0.01 and 0.1, tuning from there. More volatile settings generally warrant larger values. Planning steps \\(n\\) interact with \\(\\\\kappa\\): increasing \\(n\\) amplifies bonus effects and may require reducing \\(\\\\kappa\\). Large-scale use can demand timestamp approximations—such as storing them only for a subset of pairs or grouping times into bins—to save memory while preserving adaptivity. The extra computation from bonuses is usually negligible compared to value updates, though in time-critical systems, even the square-root calculation may be replaced by lookup tables or cheaper approximations. 10.6 Conclusion Dyna-Q+ represents a elegant solution to a fundamental challenge in reinforcement learning: how to maintain confidence in learned models while remaining appropriately skeptical about their continued accuracy. By treating time as information and systematically rewarding curiosity about neglected state-action pairs, the algorithm achieves a sophisticated balance between stability and adaptability. The approach’s strength lies not just in its technical effectiveness but in its conceptual clarity. The idea that confidence should decay over time unless refreshed by recent experience resonates across many domains beyond reinforcement learning. This principle finds echoes in human psychology, scientific methodology, and even social institutions that require periodic validation of their foundational assumptions. While modern deep reinforcement learning has developed more sophisticated approaches to uncertainty and exploration, Dyna-Q+’s core insights remain relevant. The tension between trusting learned models and maintaining healthy skepticism about their accuracy continues to challenge contemporary algorithms. In an era of rapidly changing environments and non-stationary dynamics, the principle of time-decaying confidence may prove even more valuable than when originally proposed. Looking forward, the integration of Dyna-Q+’s temporal curiosity with modern uncertainty estimation techniques presents intriguing possibilities. Neural networks that maintain both predictive models and confidence estimates could incorporate exploration bonuses based on both temporal factors and model uncertainty, potentially creating more robust and adaptive learning systems. The simplicity of Dyna-Q+’s modification to standard Dyna—just adding a single term to the planning rewards—belies its conceptual sophistication. Sometimes the most profound advances in artificial intelligence come not from complex new architectures but from simple changes that embody deep insights about learning, adaptation, and the nature of knowledge itself. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
