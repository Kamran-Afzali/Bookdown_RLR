[["index.html", "Reinforcement Learning in R Chapter 1 Understanding Reinforcement Learning: From Bandits to Policy Optimization 1.1 Introduction to Reinforcement Learning 1.2 The Multi-Armed Bandit: The Simplest Case 1.3 Transition to Markov Decision Processes 1.4 Comparing Reinforcement Learning Methods 1.5 Conclusion and Further Directions 1.6 References", " Reinforcement Learning in R Kamran Afzalui 2025-07-01 Chapter 1 Understanding Reinforcement Learning: From Bandits to Policy Optimization 1.1 Introduction to Reinforcement Learning Reinforcement Learning (RL) is a dynamic subfield of artificial intelligence concerned with how agents ought to take actions in an environment to maximize cumulative reward. It is inspired by behavioral psychology and decision theory, involving a learning paradigm where feedback from the environment is neither supervised (as in classification tasks) nor completely unsupervised, but rather in the form of scalar rewards. The foundational concepts of RL can be understood by progressing from simple problems, such as the multi-armed bandit, to more sophisticated frameworks like Markov Decision Processes (MDPs) and their many solution methods. 1.2 The Multi-Armed Bandit: The Simplest Case The journey begins with the multi-armed bandit problem, a classic formulation that captures the essence of the exploration-exploitation dilemma. In this setting, an agent must choose among several actions (arms), each yielding stochastic rewards from an unknown distribution. There are no state transitions or temporal dependencies—just the immediate outcome of the chosen action. The objective is to maximize the expected reward over time. Despite its simplicity, the bandit framework introduces crucial ideas such as reward estimation, uncertainty, and exploration strategies. Algorithms like ε-greedy methods introduce random exploration, while Upper Confidence Bound (UCB) techniques adjust choices based on uncertainty estimates. Thompson Sampling applies Bayesian reasoning to balance exploration and exploitation. Though limited in scope, these strategies establish foundational principles that generalize to more complex environments. In bandits, action selection strategies serve a role similar to policies in full RL problems, but without dependence on state transitions. 1.3 Transition to Markov Decision Processes The limitations of bandit models become evident when we consider sequential decision-making problems where actions influence future states and rewards. This leads to the formalism of Markov Decision Processes (MDPs), which model environments through states \\(S\\), actions \\(A\\), transition probabilities \\(P(s&#39;|s, a)\\), rewards \\(R(s, a)\\), and a discount factor \\(\\gamma \\in [0, 1]\\). The Markov property assumes that the future is independent of the past given the present state, simplifying the dynamics and enabling tractable analysis. The agent’s goal is to learn an optimal policy \\(\\pi^*(s)\\) that maximizes the expected cumulative return: \\(G_t = \\mathbb{E}_\\pi \\left[ \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1} \\right]\\) This process relies on value functions such as: \\(V^\\pi(s) = \\mathbb{E}_\\pi \\left[ G_t | S_t = s \\right], \\quad Q^\\pi(s,a) = \\mathbb{E}_\\pi \\left[ G_t | S_t = s, A_t = a \\right]\\) Solving MDPs involves either learning these functions or directly learning \\(\\pi\\). 1.4 Comparing Reinforcement Learning Methods To frame this spectrum of approaches clearly, the table below summarizes key RL method families, highlighting whether they rely on an explicit model of the environment, whether they learn policies, value functions, or both, and notes on convergence and sample efficiency. Method Type Uses Model? Learns Policy? Learns Value? Sample Efficient? Converges to Optimal? Suitable For Example Algorithms Multi-Armed Bandits No Action Rule No Yes Yes (in expectation) Stateless problems ε-Greedy, UCB, Thompson Dynamic Programming Yes Yes Yes No Yes Known model Value Iteration, Policy Iteration Monte Carlo No Yes Yes No Yes (episodic) Episodic tasks MC Prediction, MC Control TD Learning No Yes Yes Moderate Yes Ongoing tasks SARSA, Q-Learning Q-Learning + Function Approx. No Yes Yes Moderate Not always (non-linear) High-dimensional spaces DQN Policy Gradient No Yes Maybe Low Local Optimum Continuous action spaces REINFORCE, PPO, A2C Actor-Critic No Yes Yes Moderate Local Optimum Most RL settings A2C, DDPG, SAC Model-Based RL Yes (learned) Yes Yes High Not guaranteed Data-limited tasks Dreamer, MBPO, MuZero This classification illustrates the diversity of RL approaches and underscores the flexibility of the RL paradigm. Some methods assume access to a perfect model, while others learn entirely from data. Some directly optimize policies, while others estimate values to guide policy improvement. 1.4.1 Dynamic Programming: Model-Based Learning Dynamic Programming (DP) methods such as Value Iteration and Policy Iteration assume complete knowledge of the environment’s dynamics. These algorithms exploit the Bellman equations to iteratively compute optimal value functions and policies. The Bellman optimality equation is given by: \\(V^*(s) = \\max_a \\sum_{s&#39;} P(s&#39;|s,a) [R(s,a) + \\gamma V^*(s&#39;)]\\) Value Iteration applies this update directly, while Policy Iteration alternates between evaluating the current policy and improving it by acting greedily with respect to the value function. Although DP methods guarantee convergence to the optimal policy, they are rarely applicable to real-world problems due to their assumption of known transitions and the computational infeasibility of operating over large or continuous state spaces. 1.4.2 Model-Free Approaches: Monte Carlo and TD Learning When the model is unknown, we turn to model-free methods that learn directly from sampled experience. Monte Carlo methods estimate the value of policies by averaging the total return over multiple episodes. These methods are simple and intuitive, suitable for episodic environments, but suffer from high variance and are not efficient in online learning scenarios. Temporal Difference (TD) learning bridges the gap between Monte Carlo and DP by updating value estimates based on partial returns. Algorithms like SARSA and Q-learning fall into this category. SARSA is on-policy, updating values based on the actual trajectory taken, while Q-learning is off-policy, learning about the greedy policy regardless of the agent’s current behavior. These methods do not require waiting until the end of an episode and are thus applicable in ongoing tasks. They offer a tradeoff between bias and variance in value estimation. 1.4.3 Q-Learning and Function Approximation Q-learning is one of the most widely used RL algorithms due to its simplicity and theoretical guarantees. However, when dealing with large state-action spaces, tabular Q-learning becomes infeasible. Function approximation, particularly using neural networks, allows Q-learning to scale to high-dimensional problems. This gave rise to Deep Q-Networks (DQNs), where a neural network is trained to approximate the Q-function. DQNs introduced mechanisms like experience replay—storing and reusing past interactions to reduce correlation between updates—and target networks—fixed Q-value targets updated slowly to stabilize learning. These enhancements enabled RL to tackle complex environments like Atari games directly from pixels. Nonetheless, deep RL methods often suffer from sample inefficiency and training instability, especially when generalizing to new environments. 1.4.4 Policy Gradient and Actor-Critic Methods While value-based methods derive policies from value functions, policy-based methods directly parameterize and optimize the policy itself. Policy Gradient methods compute the gradient of expected return with respect to policy parameters and perform gradient ascent. The REINFORCE algorithm is the archetype of this approach, but it often suffers from high variance in gradient estimates. The Policy Gradient Theorem provides the theoretical foundation: \\(\\nabla J(\\theta) = \\mathbb{E}_{\\pi_\\theta} [\\nabla_\\theta \\log \\pi_\\theta(a|s) Q^{\\pi}(s,a)]\\) To address variance, Actor-Critic methods introduce a second component: the critic, which estimates value functions to inform and stabilize the updates of the actor (policy). Algorithms like Advantage Actor-Critic (A2C), Deep Deterministic Policy Gradient (DDPG), and Soft Actor-Critic (SAC) build on this architecture, each adding unique elements to improve performance and stability. 1.4.5 Advanced Policy Optimization Techniques More recent advances in policy optimization have focused on improving training stability and sample efficiency. Trust Region Policy Optimization (TRPO) constrains policy updates to stay within a trust region defined by the KL divergence, ensuring small, safe steps in parameter space. Proximal Policy Optimization (PPO) simplifies TRPO with a clipped objective function, striking a balance between ease of implementation and empirical performance. Soft Actor-Critic (SAC), on the other hand, incorporates an entropy maximization objective, encouraging exploration by maintaining a degree of randomness in the policy. This leads to better performance in environments with sparse or deceptive rewards and is particularly effective in continuous control tasks. Model-based approaches, such as MuZero or Dreamer, offer a complementary strategy: by learning a model of the environment dynamics and reward structure, they can generate synthetic experiences to improve sample efficiency. However, model inaccuracies can lead to cascading errors and suboptimal policies. 1.5 Conclusion and Further Directions Reinforcement Learning has evolved into a mature and diverse field, offering a rich set of tools for decision-making under uncertainty. From simple bandit strategies to deep policy optimization and model-based reasoning, RL provides a versatile framework for learning from interaction. However, key challenges remain: training instability, sample inefficiency, reward mis-specification, and generalization across tasks. These remain active areas of research. A solid understanding of the main categories—such as those outlined in the comparative table—is essential for navigating the RL landscape. Whether one is interested in theoretical foundations, algorithm development, or practical deployment, the key ideas of exploration, value estimation, policy optimization, and model learning form the backbone of modern RL. For further reading, Reinforcement Learning: An Introduction by Sutton and Barto remains the canonical text. Online resources like OpenAI’s Spinning Up, DeepMind’s technical blog, and repositories of papers on arXiv are excellent for staying current with the latest advancements. As artificial agents continue to tackle more complex and dynamic environments, reinforcement learning stands at the forefront of AI research and application. 1.6 References Sutton, R. S., &amp; Barto, A. G. (2018). Reinforcement learning: An introduction (2nd ed.). MIT Press. http://incompleteideas.net/book/the-book-2nd.html Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A., Antonoglou, I., Wierstra, D., &amp; Riedmiller, M. (2015). Human-level control through deep reinforcement learning. Nature, 518(7540), 529–533. https://doi.org/10.1038/nature14236 Schulman, J., Levine, S., Abbeel, P., Jordan, M., &amp; Moritz, P. (2015). Trust region policy optimization. In Proceedings of the 32nd International Conference on Machine Learning (pp. 1889–1897). https://proceedings.mlr.press/v37/schulman15.html Schulman, J., Wolski, F., Dhariwal, P., Radford, A., &amp; Klimov, O. (2017). Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. https://arxiv.org/abs/1707.06347 Haarnoja, T., Zhou, A., Abbeel, P., &amp; Levine, S. (2018). Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In Proceedings of the 35th International Conference on Machine Learning. https://arxiv.org/abs/1801.01290 Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., … &amp; Hassabis, D. (2020). Mastering the game of Go without human knowledge. Nature, 550(7676), 354–359. https://doi.org/10.1038/nature24270 Silver, D., Hubert, T., Schrittwieser, J., Antonoglou, I., Lai, M., Guez, A., … &amp; Hassabis, D. (2018). A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play. Science, 362(6419), 1140–1144. https://doi.org/10.1126/science.aar6404 OpenAI. (2018). Spinning Up in Deep RL. https://spinningup.openai.com Lillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez, T., Tassa, Y., … &amp; Wierstra, D. (2015). Continuous control with deep reinforcement learning. arXiv preprint arXiv:1509.02971. https://arxiv.org/abs/1509.02971 Auer, P., Cesa-Bianchi, N., &amp; Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine Learning, 47, 235–256. https://doi.org/10.1023/A:1013689704352 Thompson, W. R. (1933). On the likelihood that one unknown probability exceeds another in view of the evidence of two samples. Biometrika, 25(3/4), 285–294. https://doi.org/10.2307/2332286 "],["the-multi-armed-bandit-problem.html", "Chapter 2 The Multi-Armed Bandit Problem 2.1 Introduction 2.2 Mathematical Formalism 2.3 Frequentist Approach: UCB1 Algorithm 2.4 Bayesian Approach: Thompson Sampling 2.5 Epsilon-Greedy Strategy 2.6 Summary Table 2.7 Conclusion", " Chapter 2 The Multi-Armed Bandit Problem 2.1 Introduction The multi-armed bandit (MAB) problem is a foundational model in the study of sequential decision-making under uncertainty. Representing the trade-off between exploration (gathering information) and exploitation (maximizing known rewards), MAB problems are central to reinforcement learning, online optimization, and adaptive experimental design. An agent is faced with a choice among multiple options—arms—each producing stochastic rewards with unknown distributions. The objective is to maximize cumulative reward, or equivalently, to minimize the regret incurred by not always choosing the best arm. This post presents a rigorous treatment of the MAB problem, comparing frequentist and Bayesian approaches. We offer formal mathematical foundations, develop regret bounds, and implement both Upper Confidence Bound (UCB) and Thompson Sampling algorithms in R. A summary table is provided at the end. 2.2 Mathematical Formalism Let \\(K\\) denote the number of arms, and each arm \\(k \\in \\{1, \\dots, K\\}\\) has an unknown reward distribution \\(P_k\\), with mean \\(\\mu_k\\). Define the optimal arm: \\[ k^* = \\arg\\max_{k} \\mu_k. \\] At each time \\(t \\in \\{1, \\dots, T\\}\\), the agent chooses arm \\(A_t \\in \\{1, \\dots, K\\}\\) and receives a stochastic reward \\(R_t \\sim P_{A_t}\\). The cumulative expected regret is: \\[ \\mathcal{R}(T) = T\\mu^* - \\mathbb{E}\\left[ \\sum_{t=1}^T R_t \\right] = \\sum_{k=1}^K \\Delta_k \\, \\mathbb{E}[N_k(T)], \\] where \\(\\Delta_k = \\mu^* - \\mu_k\\) and \\(N_k(T)\\) is the number of times arm \\(k\\) was played. 2.3 Frequentist Approach: UCB1 Algorithm Frequentist methods estimate expected rewards using empirical means. The UCB1 algorithm, based on Hoeffding’s inequality, constructs an upper confidence bound: \\[ A_t = \\arg\\max_{k} \\left[ \\hat{\\mu}_{k,t} + \\sqrt{ \\frac{2 \\log t}{N_k(t)} } \\right]. \\] This ensures logarithmic regret in expectation. 2.3.1 R Code for UCB1 set.seed(42) K &lt;- 3 T &lt;- 1000 mu &lt;- c(0.3, 0.5, 0.7) # true means counts &lt;- rep(0, K) values &lt;- rep(0, K) regret &lt;- numeric(T) # Play each arm once for (k in 1:K) { reward &lt;- rbinom(1, 1, mu[k]) counts[k] &lt;- 1 values[k] &lt;- reward regret[k] &lt;- max(mu) - mu[k] } for (t in (K+1):T) { ucb &lt;- values + sqrt(2 * log(t) / counts) a &lt;- which.max(ucb) reward &lt;- rbinom(1, 1, mu[a]) counts[a] &lt;- counts[a] + 1 values[a] &lt;- values[a] + (reward - values[a]) / counts[a] regret[t] &lt;- max(mu) - mu[a] } cum_regret_ucb &lt;- cumsum(regret) plot(cum_regret_ucb, type = &quot;l&quot;, col = &quot;blue&quot;, lwd = 2, ylab = &quot;Cumulative Regret&quot;, xlab = &quot;Time&quot;, main = &quot;UCB1 Regret&quot;) 2.4 Bayesian Approach: Thompson Sampling Bayesian bandits model reward distributions probabilistically, updating beliefs via Bayes’ rule. For Bernoulli rewards, we assume Beta priors: \\[ \\mu_k \\sim \\text{Beta}(\\alpha_k, \\beta_k). \\] After observing a reward \\(r \\in \\{0, 1\\}\\), the posterior update is: \\[ \\alpha_k \\leftarrow \\alpha_k + r, \\quad \\beta_k \\leftarrow \\beta_k + 1 - r. \\] The Thompson Sampling algorithm draws a sample \\(\\tilde{\\mu}_k \\sim \\text{Beta}(\\alpha_k, \\beta_k)\\) and selects the arm with the highest sample. 2.4.1 R Code for Thompson Sampling set.seed(42) alpha &lt;- rep(1, K) beta &lt;- rep(1, K) regret &lt;- numeric(T) for (t in 1:T) { sampled_means &lt;- rbeta(K, alpha, beta) a &lt;- which.max(sampled_means) reward &lt;- rbinom(1, 1, mu[a]) alpha[a] &lt;- alpha[a] + reward beta[a] &lt;- beta[a] + (1 - reward) regret[t] &lt;- max(mu) - mu[a] } cum_regret_ts &lt;- cumsum(regret) lines(cum_regret_ts, col = &quot;red&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;UCB1&quot;, &quot;Thompson Sampling&quot;), col = c(&quot;blue&quot;, &quot;red&quot;), lwd = 2) The UCB1 algorithm guarantees a regret bound of: \\[ \\mathcal{R}(T) \\leq \\sum_{k: \\Delta_k &gt; 0} \\left( \\frac{8 \\log T}{\\Delta_k} + C_k \\right), \\] where \\(C_k\\) is a problem-dependent constant. Thompson Sampling achieves comparable performance. Under certain regularity conditions, its Bayesian regret is bounded by: \\[ \\mathbb{E}[\\mathcal{R}(T)] = O\\left( \\sqrt{KT \\log T} \\right), \\] and often outperforms UCB1 in practice due to its adaptive exploration. 2.5 Epsilon-Greedy Strategy The epsilon-greedy algorithm is a simple and intuitive approach to balancing exploration and exploitation. At each time step, with probability \\(\\epsilon\\), the agent chooses a random arm (exploration), and with probability \\(1 - \\epsilon\\), it selects the arm with the highest empirical mean (exploitation). Let \\(\\hat{\\mu}_{k,t}\\) denote the empirical mean reward for arm \\(k\\) at time \\(t\\). Then: \\[ A_t = \\begin{cases} \\text{random choice} &amp; \\text{with probability } \\epsilon, \\\\ \\arg\\max_k \\hat{\\mu}_{k,t} &amp; \\text{with probability } 1 - \\epsilon. \\end{cases} \\] While this algorithm is not optimal in the theoretical sense, it often performs well in practice for problems with stationary reward distributions when the exploration rate \\(\\epsilon\\) is properly tuned. Regret under a fixed \\(\\epsilon\\) is linear in \\(T\\), i.e., \\(\\mathcal{R}(T) = O(T)\\), unless \\(\\epsilon\\) is decayed over time (e.g., \\(\\epsilon_t = 1/t\\)), which introduces a trade-off between convergence speed and variance. 2.5.1 R Code for Epsilon-Greedy set.seed(42) epsilon &lt;- 0.1 counts &lt;- rep(0, K) values &lt;- rep(0, K) regret &lt;- numeric(T) for (t in 1:T) { if (runif(1) &lt; epsilon) { a &lt;- sample(1:K, 1) # Exploration } else { a &lt;- which.max(values) # Exploitation } reward &lt;- rbinom(1, 1, mu[a]) counts[a] &lt;- counts[a] + 1 values[a] &lt;- values[a] + (reward - values[a]) / counts[a] regret[t] &lt;- max(mu) - mu[a] } cum_regret_eps &lt;- cumsum(regret) lines(cum_regret_eps, col = &quot;darkgreen&quot;, lwd = 2) legend(&quot;topright&quot;, legend = c(&quot;UCB1&quot;, &quot;Thompson Sampling&quot;, &quot;Epsilon-Greedy&quot;), col = c(&quot;blue&quot;, &quot;red&quot;, &quot;darkgreen&quot;), lwd = 2) 2.6 Summary Table Method Paradigm Assumptions Exploration Mechanism Regret Bound Strengths Weaknesses UCB1 Frequentist Stationary, bounded rewards Upper Confidence Bound \\(O(\\log T)\\) Simple, provable guarantees Conservative, suboptimal in practice Thompson Sampling Bayesian Prior over reward distributions Posterior sampling \\(O(\\sqrt{KT})\\), empirically better Adaptive, efficient with good priors Sensitive to prior misspecification KL-UCB Frequentist Known reward distributions KL-divergence bounds \\(O(\\log T)\\) (tighter) Distribution-aware More complex implementation Epsilon-Greedy Heuristic None Random exploration \\(O(T)\\) if \\(\\epsilon\\) fixed Very simple Inefficient long-term 2.7 Conclusion The multi-armed bandit problem remains an essential model for studying decision-making under uncertainty. While frequentist methods like UCB1 provide rigorous guarantees and conceptual clarity, Bayesian approaches like Thompson Sampling offer greater flexibility and empirical performance. The choice between them hinges on the trade-offs between interpretability, adaptivity, and prior knowledge. The R implementations provided here allow for practical experimentation and benchmarking. In real-world applications, such as clinical trial design, online recommendations, and adaptive A/B testing, these algorithms offer principled foundations for learning and acting in uncertain environments. "],["markov-decision-processes-and-dynamic-programming.html", "Chapter 3 Markov Decision Processes and Dynamic Programming 3.1 Introduction 3.2 Constructing the MDP in R 3.3 Value Iteration Algorithm 3.4 Evaluation and Interpretation 3.5 Theoretical Properties of Value Iteration 3.6 Summary Table 3.7 Conclusion", " Chapter 3 Markov Decision Processes and Dynamic Programming 3.1 Introduction Markov Decision Processes (MDPs) constitute a formal framework for modeling sequential decision-making under uncertainty. Widely applied in operations research, control theory, economics, and artificial intelligence, MDPs encapsulate the dynamics of environments where outcomes are partly stochastic and partly under an agent’s control. At their core, MDPs unify probabilistic transitions, state-contingent rewards, and long-term optimization goals. This post explores MDPs from a computational standpoint, emphasizing Dynamic Programming (DP) methods—particularly Value Iteration—for solving them when the model is fully known. We proceed by defining the mathematical components of MDPs, implementing them in R, and illustrating policy derivation using value iteration. A comparative summary of key aspects of DP methods concludes the post. An MDP is formally defined by the tuple \\((S, A, P, R, \\gamma)\\), where: \\(S\\): a finite set of states. \\(A\\): a finite set of actions. \\(P(s&#39;|s, a)\\): the transition probability function—probability of moving to state \\(s&#39;\\) given current state \\(s\\) and action \\(a\\). \\(R(s, a, s&#39;)\\): the reward received after transitioning from state \\(s\\) to \\(s&#39;\\) via action \\(a\\). \\(\\gamma \\in [0,1]\\): the discount factor, representing the agent’s preference for immediate versus delayed rewards. The agent’s objective is to learn a policy \\(\\pi: S \\to A\\) that maximizes the expected cumulative discounted reward: \\[ V^\\pi(s) = \\mathbb{E}_\\pi \\left[ \\sum_{t=0}^{\\infty} \\gamma^t R(S_t, A_t, S_{t+1}) \\,\\bigg|\\, S_0 = s \\right] \\] The optimal value function \\(V^*\\) satisfies the Bellman optimality equation: $$ V^*(s) = {a A} {s’ S} P(s’|s,a) $$ Once \\(V^*\\) is computed, the optimal policy \\(\\pi^*\\) is obtained via: $$ ^*(s) = {a A} {s’} P(s’|s,a) $$ 3.2 Constructing the MDP in R We now implement an environment with 10 states and 2 actions per state, following stochastic transition and reward dynamics. The final state is absorbing, with no further transitions or rewards. n_states &lt;- 10 n_actions &lt;- 2 gamma &lt;- 0.9 terminal_state &lt;- n_states set.seed(42) transition_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) reward_model &lt;- array(0, dim = c(n_states, n_actions, n_states)) for (s in 1:(n_states - 1)) { transition_model[s, 1, s + 1] &lt;- 0.9 transition_model[s, 1, sample(1:n_states, 1)] &lt;- 0.1 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.8 transition_model[s, 2, sample(1:n_states, 1)] &lt;- 0.2 for (s_prime in 1:n_states) { reward_model[s, 1, s_prime] &lt;- ifelse(s_prime == n_states, 1.0, 0.1 * runif(1)) reward_model[s, 2, s_prime] &lt;- ifelse(s_prime == n_states, 0.5, 0.05 * runif(1)) } } transition_model[n_states, , ] &lt;- 0 reward_model[n_states, , ] &lt;- 0 This setup specifies an MDP with stochastic transitions favoring forward progression and asymmetric reward structures across actions. State 10 is a terminal state. We define a function to sample from the environment: sample_env &lt;- function(s, a) { probs &lt;- transition_model[s, a, ] s_prime &lt;- sample(1:n_states, 1, prob = probs) reward &lt;- reward_model[s, a, s_prime] list(s_prime = s_prime, reward = reward) } 3.3 Value Iteration Algorithm Value Iteration is a fundamental DP method for computing the optimal value function and deriving the optimal policy. It exploits the Bellman optimality equation through successive approximation. value_iteration &lt;- function() { V &lt;- rep(0, n_states) policy &lt;- rep(1, n_states) theta &lt;- 1e-4 repeat { delta &lt;- 0 for (s in 1:(n_states - 1)) { v &lt;- V[s] q_values &lt;- numeric(n_actions) for (a in 1:n_actions) { for (s_prime in 1:n_states) { q_values[a] &lt;- q_values[a] + transition_model[s, a, s_prime] * (reward_model[s, a, s_prime] + gamma * V[s_prime]) } } V[s] &lt;- max(q_values) policy[s] &lt;- which.max(q_values) delta &lt;- max(delta, abs(v - V[s])) } if (delta &lt; theta) break } list(V = V, policy = policy) } This implementation iteratively updates value estimates until convergence, determined by a small threshold \\(\\theta\\). At each iteration, the value of state \\(s\\) is updated using the maximum expected return across available actions. We now apply the algorithm and extract the resulting value function and policy: dp_result &lt;- value_iteration() dp_value &lt;- dp_result$V dp_policy &lt;- dp_result$policy dp_policy The returned policy is a vector of optimal actions indexed by state. The value function can be visualized to reveal which states yield higher expected returns under the optimal policy. 3.4 Evaluation and Interpretation The policy and value function obtained via value iteration provide complete guidance for optimal behavior in the modeled environment. In this setting: The forward action (Action 1) is generally rewarded with higher long-term return due to its tendency to reach the terminal (high-reward) state. The second action (Action 2) introduces randomness and lower rewards, thus is optimal only in specific states where it offers a better expected value. Such interpretation emphasizes the Bellman principle of optimality: every sub-policy of an optimal policy must itself be optimal for the corresponding subproblem. We can visualize the value function: plot(dp_value, type = &quot;b&quot;, col = &quot;blue&quot;, pch = 19, xlab = &quot;State&quot;, ylab = &quot;Value&quot;, main = &quot;Optimal State Values via Value Iteration&quot;) 3.5 Theoretical Properties of Value Iteration Value Iteration exhibits the following theoretical guarantees: Convergence: The algorithm is guaranteed to converge to \\(V^*\\) for any finite MDP with bounded rewards and \\(0 \\leq \\gamma &lt; 1\\). Policy Derivation: Once \\(V^*\\) is known, the greedy policy is optimal. Computational Complexity: For state space size \\(S\\) and action space size \\(A\\), each iteration requires \\(O(S^2 A)\\) operations due to the summation over all successor states. However, in practice, the applicability of DP methods is restricted by: The need for full knowledge of \\(P\\) and \\(R\\), Infeasibility in large or continuous state spaces (the “curse of dimensionality”). These challenges motivate the use of approximation, sampling-based methods, and model-free approaches in reinforcement learning contexts. 3.6 Summary Table The following table summarizes the key elements and tradeoffs of the value iteration algorithm: Property Value Iteration (DP) Model Required Yes (transition probabilities and rewards) State Representation Tabular (explicit state-value storage) Action Selection Greedy w.r.t. value estimates Convergence Guarantee Yes (under finite \\(S, A\\), \\(\\gamma &lt; 1\\)) Sample Efficiency High (uses full model, no sampling error) Scalability Poor in large or continuous state spaces Output Optimal value function and policy Computational Complexity \\(O(S^2 A)\\) per iteration 3.7 Conclusion Dynamic Programming offers elegant and theoretically grounded algorithms for solving MDPs when the environment model is fully specified. Value Iteration, as illustrated, leverages the recursive Bellman optimality equation to iteratively compute the value function and derive the optimal policy. While its practical scope is limited by scalability and model assumptions, DP remains foundational in the study of decision processes. Understanding these principles is essential for extending to model-free reinforcement learning, function approximation, and policy gradient methods. Future posts in this series will explore Temporal Difference learning, Monte Carlo methods, and the transition to policy optimization. These approaches lift the strict requirements of model knowledge and allow learning from interaction, thereby opening the door to real-world applications. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
