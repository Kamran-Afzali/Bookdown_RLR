<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning | Reinforcement Learning in R</title>
  <meta name="description" content="Chapter 10 Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning | Reinforcement Learning in R" />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning | Reinforcement Learning in R" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning | Reinforcement Learning in R" />
  
  
  

<meta name="author" content="Kamran Afzalui" />


<meta name="date" content="2025-12-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="dyna-and-dynaq.html"/>
<link rel="next" href="function-approximation-and-feature-engineering.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Reinforcement Learning in R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> <strong>Understanding Reinforcement Learning: From Bandits to Policy Optimization</strong></a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#introduction-to-reinforcement-learning"><i class="fa fa-check"></i><b>1.1</b> Introduction to Reinforcement Learning</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#the-multi-armed-bandit-the-simplest-case"><i class="fa fa-check"></i><b>1.2</b> The Multi-Armed Bandit: The Simplest Case</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#transition-to-markov-decision-processes"><i class="fa fa-check"></i><b>1.3</b> Transition to Markov Decision Processes</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#comparing-reinforcement-learning-methods"><i class="fa fa-check"></i><b>1.4</b> Comparing Reinforcement Learning Methods</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="index.html"><a href="index.html#dynamic-programming-model-based-learning"><i class="fa fa-check"></i><b>1.4.1</b> Dynamic Programming: Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.2" data-path="index.html"><a href="index.html#model-free-approaches-monte-carlo-and-td-learning"><i class="fa fa-check"></i><b>1.4.2</b> Model-Free Approaches: Monte Carlo and TD Learning</a></li>
<li class="chapter" data-level="1.4.3" data-path="index.html"><a href="index.html#dyna-bridging-model-free-and-model-based-learning"><i class="fa fa-check"></i><b>1.4.3</b> Dyna: Bridging Model-Free and Model-Based Learning</a></li>
<li class="chapter" data-level="1.4.4" data-path="index.html"><a href="index.html#q-learning-and-function-approximation"><i class="fa fa-check"></i><b>1.4.4</b> Q-Learning and Function Approximation</a></li>
<li class="chapter" data-level="1.4.5" data-path="index.html"><a href="index.html#policy-gradient-and-actor-critic-methods"><i class="fa fa-check"></i><b>1.4.5</b> Policy Gradient and Actor-Critic Methods</a></li>
<li class="chapter" data-level="1.4.6" data-path="index.html"><a href="index.html#advanced-policy-optimization-techniques"><i class="fa fa-check"></i><b>1.4.6</b> Advanced Policy Optimization Techniques</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#further-directions"><i class="fa fa-check"></i><b>1.5</b> Further Directions</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#references"><i class="fa fa-check"></i><b>1.6</b> References</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html"><i class="fa fa-check"></i><b>2</b> The Multi-Armed Bandit Problem</a>
<ul>
<li class="chapter" data-level="2.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#introduction"><i class="fa fa-check"></i><b>2.1</b> Introduction</a></li>
<li class="chapter" data-level="2.2" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#mathematical-formalism"><i class="fa fa-check"></i><b>2.2</b> Mathematical Formalism</a></li>
<li class="chapter" data-level="2.3" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#frequentist-approach-ucb1-algorithm"><i class="fa fa-check"></i><b>2.3</b> Frequentist Approach: UCB1 Algorithm</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-ucb1"><i class="fa fa-check"></i><b>2.3.1</b> R Code for UCB1</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#bayesian-approach-thompson-sampling"><i class="fa fa-check"></i><b>2.4</b> Bayesian Approach: Thompson Sampling</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-thompson-sampling"><i class="fa fa-check"></i><b>2.4.1</b> R Code for Thompson Sampling</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#epsilon-greedy-strategy"><i class="fa fa-check"></i><b>2.5</b> Epsilon-Greedy Strategy</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#r-code-for-epsilon-greedy"><i class="fa fa-check"></i><b>2.5.1</b> R Code for Epsilon-Greedy</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#summary-table"><i class="fa fa-check"></i><b>2.6</b> Summary Table</a></li>
<li class="chapter" data-level="2.7" data-path="the-multi-armed-bandit-problem.html"><a href="the-multi-armed-bandit-problem.html#conclusion"><i class="fa fa-check"></i><b>2.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html"><i class="fa fa-check"></i><b>3</b> Markov Decision Processes and Dynamic Programming</a>
<ul>
<li class="chapter" data-level="3.1" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#introduction-1"><i class="fa fa-check"></i><b>3.1</b> Introduction</a></li>
<li class="chapter" data-level="3.2" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#constructing-the-mdp-in-r"><i class="fa fa-check"></i><b>3.2</b> Constructing the MDP in R</a></li>
<li class="chapter" data-level="3.3" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#value-iteration-algorithm"><i class="fa fa-check"></i><b>3.3</b> Value Iteration Algorithm</a></li>
<li class="chapter" data-level="3.4" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#evaluation-and-interpretation"><i class="fa fa-check"></i><b>3.4</b> Evaluation and Interpretation</a></li>
<li class="chapter" data-level="3.5" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties"><i class="fa fa-check"></i><b>3.5</b> Theoretical Properties</a></li>
<li class="chapter" data-level="3.6" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-1"><i class="fa fa-check"></i><b>3.6</b> Summary Table</a></li>
<li class="chapter" data-level="3.7" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-1"><i class="fa fa-check"></i><b>3.7</b> Conclusion</a></li>
<li class="chapter" data-level="3.8" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#theoretical-properties-of-value-iteration"><i class="fa fa-check"></i><b>3.8</b> Theoretical Properties of Value Iteration</a></li>
<li class="chapter" data-level="3.9" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#summary-table-2"><i class="fa fa-check"></i><b>3.9</b> Summary Table</a></li>
<li class="chapter" data-level="3.10" data-path="markov-decision-processes-and-dynamic-programming.html"><a href="markov-decision-processes-and-dynamic-programming.html#conclusion-2"><i class="fa fa-check"></i><b>3.10</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><i class="fa fa-check"></i><b>4</b> Model-Free Reinforcement Learning: Temporal Difference and Monte Carlo Methods in R</a>
<ul>
<li class="chapter" data-level="4.1" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#introduction-2"><i class="fa fa-check"></i><b>4.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#implementation"><i class="fa fa-check"></i><b>4.2</b> Implementation</a></li>
<li class="chapter" data-level="4.3" data-path="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html"><a href="model-free-reinforcement-learning-temporal-difference-and-monte-carlo-methods-in-r.html#interpretation-and-discussion"><i class="fa fa-check"></i><b>4.3</b> Interpretation and Discussion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><i class="fa fa-check"></i><b>5</b> On-Policy vs Off-Policy Reinforcement Learning: SARSA, Q-Learning, and Monte Carlo in R</a>
<ul>
<li class="chapter" data-level="5.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#introduction-3"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-on-policy"><i class="fa fa-check"></i><b>5.2</b> SARSA (On-Policy)</a></li>
<li class="chapter" data-level="5.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-off-policy"><i class="fa fa-check"></i><b>5.3</b> Q-Learning (Off-Policy)</a></li>
<li class="chapter" data-level="5.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-with-importance-sampling"><i class="fa fa-check"></i><b>5.4</b> Off-Policy Monte Carlo with Importance Sampling</a></li>
<li class="chapter" data-level="5.5" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#environment-and-common-r-components"><i class="fa fa-check"></i><b>5.5</b> Environment and Common R Components</a></li>
<li class="chapter" data-level="5.6" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#sarsa-in-r"><i class="fa fa-check"></i><b>5.6</b> SARSA in R</a></li>
<li class="chapter" data-level="5.7" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#q-learning-in-r"><i class="fa fa-check"></i><b>5.7</b> Q-Learning in R</a></li>
<li class="chapter" data-level="5.8" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#off-policy-monte-carlo-in-r"><i class="fa fa-check"></i><b>5.8</b> Off-Policy Monte Carlo in R</a></li>
<li class="chapter" data-level="5.9" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#value-iteration-in-r"><i class="fa fa-check"></i><b>5.9</b> Value Iteration in R</a></li>
<li class="chapter" data-level="5.10" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#running-and-visualizing-the-algorithms-in-r"><i class="fa fa-check"></i><b>5.10</b> Running and Visualizing the Algorithms in R</a></li>
<li class="chapter" data-level="5.11" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#interpretation-and-discussion-1"><i class="fa fa-check"></i><b>5.11</b> Interpretation and Discussion</a>
<ul>
<li class="chapter" data-level="5.11.1" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#policy-differences"><i class="fa fa-check"></i><b>5.11.1</b> Policy differences</a></li>
<li class="chapter" data-level="5.11.2" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#devaluation"><i class="fa fa-check"></i><b>5.11.2</b> Devaluation</a></li>
<li class="chapter" data-level="5.11.3" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#practical-implications"><i class="fa fa-check"></i><b>5.11.3</b> Practical implications</a></li>
<li class="chapter" data-level="5.11.4" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#experimental-observations"><i class="fa fa-check"></i><b>5.11.4</b> Experimental observations</a></li>
</ul></li>
<li class="chapter" data-level="5.12" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#conclusion-3"><i class="fa fa-check"></i><b>5.12</b> Conclusion</a></li>
<li class="chapter" data-level="5.13" data-path="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html"><a href="on-policy-vs-off-policy-reinforcement-learning-sarsa-q-learning-and-monte-carlo-in-r.html#comparison-table"><i class="fa fa-check"></i><b>5.13</b> Comparison Table</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html"><i class="fa fa-check"></i><b>6</b> Function Approximation Q-Learning with Linear Models</a>
<ul>
<li class="chapter" data-level="6.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#introduction-4"><i class="fa fa-check"></i><b>6.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#q-learning-with-function-approximation"><i class="fa fa-check"></i><b>6.1.1</b> Q-Learning with Function Approximation</a></li>
<li class="chapter" data-level="6.1.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#comparison-with-tabular-q-learning"><i class="fa fa-check"></i><b>6.1.2</b> Comparison with Tabular Q-Learning</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="function-approximation-q-learning-with-linear-models.html"><a href="function-approximation-q-learning-with-linear-models.html#r-implementation"><i class="fa fa-check"></i><b>6.2</b> R Implementation</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><i class="fa fa-check"></i><b>7</b> Beyond Linear Models: Q-Learning with Random Forest Function Approximation in R</a>
<ul>
<li class="chapter" data-level="7.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#introduction-5"><i class="fa fa-check"></i><b>7.1</b> Introduction</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#q-learning-with-random-forest-approximation"><i class="fa fa-check"></i><b>7.1.1</b> Q-Learning with Random Forest Approximation</a></li>
<li class="chapter" data-level="7.1.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#comparison-with-previous-methods"><i class="fa fa-check"></i><b>7.1.2</b> Comparison with Previous Methods</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#r-implementation-1"><i class="fa fa-check"></i><b>7.2</b> R Implementation</a></li>
<li class="chapter" data-level="7.3" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#analysis-and-insights"><i class="fa fa-check"></i><b>7.3</b> Analysis and Insights</a></li>
<li class="chapter" data-level="7.4" data-path="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html"><a href="beyond-linear-models-q-learning-with-random-forest-function-approximation-in-r.html#conclusion-4"><i class="fa fa-check"></i><b>7.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><i class="fa fa-check"></i><b>8</b> Deep Function Approximation: Q-Learning with Neural Networks in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#introduction-6"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#theoretical-foundation"><i class="fa fa-check"></i><b>8.2</b> Theoretical Foundation</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#universal-approximation-and-expressivity"><i class="fa fa-check"></i><b>8.2.1</b> Universal Approximation and Expressivity</a></li>
<li class="chapter" data-level="8.2.2" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#gradient-based-learning"><i class="fa fa-check"></i><b>8.2.2</b> Gradient-Based Learning</a></li>
<li class="chapter" data-level="8.2.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#comparison-with-previous-approaches"><i class="fa fa-check"></i><b>8.2.3</b> Comparison with Previous Approaches</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#r-implementation-2"><i class="fa fa-check"></i><b>8.3</b> R Implementation</a></li>
<li class="chapter" data-level="8.4" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#analysis-and-interpretation"><i class="fa fa-check"></i><b>8.4</b> Analysis and Interpretation</a></li>
<li class="chapter" data-level="8.5" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#future-directions"><i class="fa fa-check"></i><b>8.5</b> Future Directions</a></li>
<li class="chapter" data-level="8.6" data-path="deep-function-approximation-q-learning-with-neural-networks-in-r.html"><a href="deep-function-approximation-q-learning-with-neural-networks-in-r.html#conclusion-5"><i class="fa fa-check"></i><b>8.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html"><i class="fa fa-check"></i><b>9</b> Dyna and DynaQ</a>
<ul>
<li class="chapter" data-level="9.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#introduction-7"><i class="fa fa-check"></i><b>9.1</b> Introduction</a></li>
<li class="chapter" data-level="9.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#theoretical-framework"><i class="fa fa-check"></i><b>9.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#the-dyna-architecture"><i class="fa fa-check"></i><b>9.2.1</b> The Dyna Architecture</a></li>
<li class="chapter" data-level="9.2.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#model-representation"><i class="fa fa-check"></i><b>9.2.2</b> Model Representation</a></li>
<li class="chapter" data-level="9.2.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#convergence-properties"><i class="fa fa-check"></i><b>9.2.3</b> Convergence Properties</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-in-r"><i class="fa fa-check"></i><b>9.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="9.3.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#environment-setup"><i class="fa fa-check"></i><b>9.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="9.3.2" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#dyna-q-implementation"><i class="fa fa-check"></i><b>9.3.2</b> Dyna-Q Implementation</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#experimental-analysis"><i class="fa fa-check"></i><b>9.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="9.4.1" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#learning-efficiency-comparison"><i class="fa fa-check"></i><b>9.4.1</b> Learning Efficiency Comparison</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#discussion"><i class="fa fa-check"></i><b>9.5</b> Discussion</a></li>
<li class="chapter" data-level="9.6" data-path="dyna-and-dynaq.html"><a href="dyna-and-dynaq.html#implementation-considerations-and-conclusion"><i class="fa fa-check"></i><b>9.6</b> Implementation Considerations and Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><i class="fa fa-check"></i><b>10</b> Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#introduction-8"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#theoretical-framework-1"><i class="fa fa-check"></i><b>10.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="10.2.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#the-exploration-bonus-mechanism"><i class="fa fa-check"></i><b>10.2.1</b> The Exploration Bonus Mechanism</a></li>
<li class="chapter" data-level="10.2.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#complete-dyna-q-algorithm"><i class="fa fa-check"></i><b>10.2.2</b> Complete Dyna-Q+ Algorithm</a></li>
<li class="chapter" data-level="10.2.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#convergence-and-stability"><i class="fa fa-check"></i><b>10.2.3</b> Convergence and Stability</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#implementation-in-r-1"><i class="fa fa-check"></i><b>10.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#environment-setup-1"><i class="fa fa-check"></i><b>10.3.1</b> Environment Setup</a></li>
<li class="chapter" data-level="10.3.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-implementation-1"><i class="fa fa-check"></i><b>10.3.2</b> Dyna-Q+ Implementation</a></li>
<li class="chapter" data-level="10.3.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#standard-dyna-for-comparison"><i class="fa fa-check"></i><b>10.3.3</b> Standard Dyna for Comparison</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#experimental-analysis-1"><i class="fa fa-check"></i><b>10.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="10.4.1" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#adaptation-to-environmental-changes"><i class="fa fa-check"></i><b>10.4.1</b> Adaptation to Environmental Changes</a></li>
<li class="chapter" data-level="10.4.2" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#parameter-sensitivity-analysis"><i class="fa fa-check"></i><b>10.4.2</b> Parameter Sensitivity Analysis</a></li>
<li class="chapter" data-level="10.4.3" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#exploration-pattern-analysis"><i class="fa fa-check"></i><b>10.4.3</b> Exploration Pattern Analysis</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#discussion-and-implementation-considerations"><i class="fa fa-check"></i><b>10.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="10.6" data-path="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#conclusion-6"><i class="fa fa-check"></i><b>10.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html"><i class="fa fa-check"></i><b>11</b> Function Approximation And Feature Engineering</a>
<ul>
<li class="chapter" data-level="11.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#feature-engineering-and-state-representation"><i class="fa fa-check"></i><b>11.1</b> Feature Engineering and State Representation</a>
<ul>
<li class="chapter" data-level="11.1.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-discrimination-vs.-generalization-tradeoff"><i class="fa fa-check"></i><b>11.1.1</b> The Discrimination vs.Â Generalization Tradeoff</a></li>
<li class="chapter" data-level="11.1.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#principles-of-effective-feature-design"><i class="fa fa-check"></i><b>11.1.2</b> Principles of Effective Feature Design</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#mathematical-foundations-of-linear-function-approximation"><i class="fa fa-check"></i><b>11.2</b> Mathematical Foundations of Linear Function Approximation</a>
<ul>
<li class="chapter" data-level="11.2.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#linear-value-functions-and-their-properties"><i class="fa fa-check"></i><b>11.2.1</b> Linear Value Functions and Their Properties</a></li>
<li class="chapter" data-level="11.2.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#temporal-difference-learning-with-linear-approximation"><i class="fa fa-check"></i><b>11.2.2</b> Temporal Difference Learning with Linear Approximation</a></li>
<li class="chapter" data-level="11.2.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#the-deadly-triad-and-stability-concerns"><i class="fa fa-check"></i><b>11.2.3</b> The Deadly Triad and Stability Concerns</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#classical-basis-function-methods"><i class="fa fa-check"></i><b>11.3</b> Classical Basis Function Methods</a>
<ul>
<li class="chapter" data-level="11.3.1" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#coarse-coding-overlapping-receptive-fields"><i class="fa fa-check"></i><b>11.3.1</b> Coarse Coding: Overlapping Receptive Fields</a></li>
<li class="chapter" data-level="11.3.2" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#tile-coding-structured-overlapping-grids"><i class="fa fa-check"></i><b>11.3.2</b> Tile Coding: Structured Overlapping Grids</a></li>
<li class="chapter" data-level="11.3.3" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#radial-basis-functions-smooth-continuous-features"><i class="fa fa-check"></i><b>11.3.3</b> Radial Basis Functions: Smooth Continuous Features</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#comparative-analysis-and-practical-considerations"><i class="fa fa-check"></i><b>11.4</b> Comparative Analysis and Practical Considerations</a></li>
<li class="chapter" data-level="11.5" data-path="function-approximation-and-feature-engineering.html"><a href="function-approximation-and-feature-engineering.html#bridging-classical-and-modern-approaches"><i class="fa fa-check"></i><b>11.5</b> Bridging Classical and Modern Approaches</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html"><i class="fa fa-check"></i><b>12</b> Learning Policies versus Learning Values:</a>
<ul>
<li class="chapter" data-level="12.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#the-two-paradigms-of-reinforcement-learning"><i class="fa fa-check"></i><b>12.1</b> The Two Paradigms of Reinforcement Learning</a></li>
<li class="chapter" data-level="12.2" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#value-based-methods-learning-worth-before-action"><i class="fa fa-check"></i><b>12.2</b> Value-Based Methods: Learning Worth Before Action</a></li>
<li class="chapter" data-level="12.3" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#policy-based-methods-direct-optimization-of-behavior"><i class="fa fa-check"></i><b>12.3</b> Policy-Based Methods: Direct Optimization of Behavior</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#the-policy-gradient-theorem"><i class="fa fa-check"></i><b>12.3.1</b> The Policy Gradient Theorem</a></li>
<li class="chapter" data-level="12.3.2" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#variance-reduction-through-baselines"><i class="fa fa-check"></i><b>12.3.2</b> Variance Reduction Through Baselines</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="learning-policies-versus-learning-values.html"><a href="learning-policies-versus-learning-values.html#conclusion-7"><i class="fa fa-check"></i><b>12.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><i class="fa fa-check"></i><b>13</b> Average Reward in Reinforcement Learning: A Comprehensive Guide</a>
<ul>
<li class="chapter" data-level="13.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-limitations-of-discounted-reward-formulations"><i class="fa fa-check"></i><b>13.1</b> The Limitations of Discounted Reward Formulations</a></li>
<li class="chapter" data-level="13.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#the-average-reward-alternative-theoretical-foundations"><i class="fa fa-check"></i><b>13.2</b> The Average Reward Alternative: Theoretical Foundations</a></li>
<li class="chapter" data-level="13.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#value-functions-in-the-average-reward-setting"><i class="fa fa-check"></i><b>13.3</b> Value Functions in the Average Reward Setting</a></li>
<li class="chapter" data-level="13.4" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-theory-for-average-reward"><i class="fa fa-check"></i><b>13.4</b> Optimality Theory for Average Reward</a></li>
<li class="chapter" data-level="13.5" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#learning-algorithms-for-average-reward"><i class="fa fa-check"></i><b>13.5</b> Learning Algorithms for Average Reward</a></li>
<li class="chapter" data-level="13.6" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#practical-implementation-patient-triage-in-emergency-department"><i class="fa fa-check"></i><b>13.6</b> Practical Implementation: Patient Triage in Emergency Department</a></li>
<li class="chapter" data-level="13.7" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#implementation-considerations"><i class="fa fa-check"></i><b>13.7</b> Implementation Considerations</a>
<ul>
<li class="chapter" data-level="13.7.1" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#convergence-of-average-reward-td-learning"><i class="fa fa-check"></i><b>13.7.1</b> Convergence of Average Reward TD Learning</a></li>
<li class="chapter" data-level="13.7.2" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#policy-gradient-theorem-for-average-reward"><i class="fa fa-check"></i><b>13.7.2</b> Policy Gradient Theorem for Average Reward</a></li>
<li class="chapter" data-level="13.7.3" data-path="average-reward-in-reinforcement-learning-a-comprehensive-guide.html"><a href="average-reward-in-reinforcement-learning-a-comprehensive-guide.html#optimality-equations-derivation"><i class="fa fa-check"></i><b>13.7.3</b> Optimality Equations Derivation</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="eligibility-traces.html"><a href="eligibility-traces.html"><i class="fa fa-check"></i><b>14</b> Eligibility Traces</a>
<ul>
<li class="chapter" data-level="14.1" data-path="eligibility-traces.html"><a href="eligibility-traces.html#from-one-step-to-multi-step-learning"><i class="fa fa-check"></i><b>14.1</b> From One-Step to Multi-Step Learning</a></li>
<li class="chapter" data-level="14.2" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-mechanics-of-eligibility-traces-a-backward-view"><i class="fa fa-check"></i><b>14.2</b> The Mechanics of Eligibility Traces: A Backward View</a></li>
<li class="chapter" data-level="14.3" data-path="eligibility-traces.html"><a href="eligibility-traces.html#the-tdlambda-algorithm-for-prediction"><i class="fa fa-check"></i><b>14.3</b> The TD(<span class="math inline">\(`\lambda`\)</span>) Algorithm for Prediction</a></li>
<li class="chapter" data-level="14.4" data-path="eligibility-traces.html"><a href="eligibility-traces.html#control-with-eligibility-traces-average-reward-sarsalambda"><i class="fa fa-check"></i><b>14.4</b> Control with Eligibility Traces: Average Reward Sarsa(<span class="math inline">\(`\lambda`\)</span>)</a></li>
<li class="chapter" data-level="14.5" data-path="eligibility-traces.html"><a href="eligibility-traces.html#practical-implementation-server-load-balancing-with-sarsalambda"><i class="fa fa-check"></i><b>14.5</b> Practical Implementation: Server Load Balancing with Sarsa(<span class="math inline">\(`\lambda`\)</span>)</a></li>
<li class="chapter" data-level="14.6" data-path="eligibility-traces.html"><a href="eligibility-traces.html#computational-and-performance-considerations"><i class="fa fa-check"></i><b>14.6</b> Computational and Performance Considerations</a></li>
<li class="chapter" data-level="14.7" data-path="eligibility-traces.html"><a href="eligibility-traces.html#conclusion-8"><i class="fa fa-check"></i><b>14.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html"><i class="fa fa-check"></i><b>15</b> Intrinsic Rewards</a>
<ul>
<li class="chapter" data-level="15.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#the-sparse-reward-problem-and-its-implications"><i class="fa fa-check"></i><b>15.1</b> The Sparse Reward Problem and Its Implications</a></li>
<li class="chapter" data-level="15.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#mathematical-foundations-of-intrinsic-motivation"><i class="fa fa-check"></i><b>15.2</b> Mathematical Foundations of Intrinsic Motivation</a>
<ul>
<li class="chapter" data-level="15.2.1" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#curiosity-driven-learning"><i class="fa fa-check"></i><b>15.2.1</b> Curiosity-Driven Learning</a></li>
<li class="chapter" data-level="15.2.2" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#count-based-exploration"><i class="fa fa-check"></i><b>15.2.2</b> Count-Based Exploration</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#practical-implementation-curiosity-driven-grid-world-navigation"><i class="fa fa-check"></i><b>15.3</b> Practical Implementation: Curiosity-Driven Grid World Navigation</a></li>
<li class="chapter" data-level="15.4" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#advanced-intrinsic-motivation-mechanisms"><i class="fa fa-check"></i><b>15.4</b> Advanced Intrinsic Motivation Mechanisms</a></li>
<li class="chapter" data-level="15.5" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#implementation-considerations-and-practical-guidelines"><i class="fa fa-check"></i><b>15.5</b> Implementation Considerations and Practical Guidelines</a></li>
<li class="chapter" data-level="15.6" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#theoretical-analysis-and-convergence-properties"><i class="fa fa-check"></i><b>15.6</b> Theoretical Analysis and Convergence Properties</a></li>
<li class="chapter" data-level="15.7" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#applications-and-empirical-results"><i class="fa fa-check"></i><b>15.7</b> Applications and Empirical Results</a></li>
<li class="chapter" data-level="15.8" data-path="intrinsic-rewards.html"><a href="intrinsic-rewards.html#limitations-and-future-directions"><i class="fa fa-check"></i><b>15.8</b> Limitations and Future Directions</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>16</b> Policy Gradients: Direct Optimization of Action Selection in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="16.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#introduction-9"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#theoretical-framework-2"><i class="fa fa-check"></i><b>16.2</b> Theoretical Framework</a>
<ul>
<li class="chapter" data-level="16.2.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#the-policy-gradient-theorem-1"><i class="fa fa-check"></i><b>16.2.1</b> The Policy Gradient Theorem</a></li>
<li class="chapter" data-level="16.2.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-algorithm"><i class="fa fa-check"></i><b>16.2.2</b> REINFORCE Algorithm</a></li>
<li class="chapter" data-level="16.2.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#baseline-subtraction-and-variance-reduction"><i class="fa fa-check"></i><b>16.2.3</b> Baseline Subtraction and Variance Reduction</a></li>
<li class="chapter" data-level="16.2.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-parameterization"><i class="fa fa-check"></i><b>16.2.4</b> Softmax Policy Parameterization</a></li>
</ul></li>
<li class="chapter" data-level="16.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#implementation-in-r-2"><i class="fa fa-check"></i><b>16.3</b> Implementation in R</a>
<ul>
<li class="chapter" data-level="16.3.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#environment-and-feature-representation"><i class="fa fa-check"></i><b>16.3.1</b> Environment and Feature Representation</a></li>
<li class="chapter" data-level="16.3.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#softmax-policy-implementation"><i class="fa fa-check"></i><b>16.3.2</b> Softmax Policy Implementation</a></li>
<li class="chapter" data-level="16.3.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#reinforce-implementation"><i class="fa fa-check"></i><b>16.3.3</b> REINFORCE Implementation</a></li>
<li class="chapter" data-level="16.3.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#actor-critic-implementation"><i class="fa fa-check"></i><b>16.3.4</b> Actor-Critic Implementation</a></li>
</ul></li>
<li class="chapter" data-level="16.4" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#experimental-analysis-2"><i class="fa fa-check"></i><b>16.4</b> Experimental Analysis</a>
<ul>
<li class="chapter" data-level="16.4.1" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#comparison-of-policy-gradient-variants"><i class="fa fa-check"></i><b>16.4.1</b> Comparison of Policy Gradient Variants</a></li>
<li class="chapter" data-level="16.4.2" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#learning-rate-sensitivity-analysis"><i class="fa fa-check"></i><b>16.4.2</b> Learning Rate Sensitivity Analysis</a></li>
<li class="chapter" data-level="16.4.3" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#variance-analysis-of-gradient-estimates"><i class="fa fa-check"></i><b>16.4.3</b> Variance Analysis of Gradient Estimates</a></li>
</ul></li>
<li class="chapter" data-level="16.5" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#discussion-and-implementation-considerations-1"><i class="fa fa-check"></i><b>16.5</b> Discussion and Implementation Considerations</a></li>
<li class="chapter" data-level="16.6" data-path="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html"><a href="policy-gradients-direct-optimization-of-action-selection-in-reinforcement-learning.html#conclusion-9"><i class="fa fa-check"></i><b>16.6</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="17" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><i class="fa fa-check"></i><b>17</b> Actor-Critic Methods: Bridging Policy and Value Learning in Reinforcement Learning</a>
<ul>
<li class="chapter" data-level="17.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#theoretical-framework-3"><i class="fa fa-check"></i><b>17.1</b> Theoretical Framework</a></li>
<li class="chapter" data-level="17.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#implementation-and-comparative-analysis"><i class="fa fa-check"></i><b>17.2</b> Implementation and Comparative Analysis</a>
<ul>
<li class="chapter" data-level="17.2.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#variance-analysis-and-learning-dynamics"><i class="fa fa-check"></i><b>17.2.1</b> Variance Analysis and Learning Dynamics</a></li>
<li class="chapter" data-level="17.2.2" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#algorithmic-variants-and-extensions"><i class="fa fa-check"></i><b>17.2.2</b> Algorithmic Variants and Extensions</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#computational-and-convergence-considerations"><i class="fa fa-check"></i><b>17.3</b> Computational and Convergence Considerations</a>
<ul>
<li class="chapter" data-level="17.3.1" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#comparative-performance-analysis"><i class="fa fa-check"></i><b>17.3.1</b> Comparative Performance Analysis</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html"><a href="actor-critic-methods-bridging-policy-and-value-learning-in-reinforcement-learning.html#conclusion-10"><i class="fa fa-check"></i><b>17.4</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>18</b> Appendix</a>
<ul>
<li class="chapter" data-level="18.1" data-path="appendix.html"><a href="appendix.html#comprehensive-reinforcement-learning-concepts-guide"><i class="fa fa-check"></i><b>18.1</b> Comprehensive Reinforcement Learning Concepts Guide</a></li>
<li class="chapter" data-level="18.2" data-path="appendix.html"><a href="appendix.html#learning-mechanisms"><i class="fa fa-check"></i><b>18.2</b> Learning Mechanisms</a></li>
<li class="chapter" data-level="18.3" data-path="appendix.html"><a href="appendix.html#environment-properties"><i class="fa fa-check"></i><b>18.3</b> Environment Properties</a></li>
<li class="chapter" data-level="18.4" data-path="appendix.html"><a href="appendix.html#learning-paradigms"><i class="fa fa-check"></i><b>18.4</b> Learning Paradigms</a></li>
<li class="chapter" data-level="18.5" data-path="appendix.html"><a href="appendix.html#exploration-strategies"><i class="fa fa-check"></i><b>18.5</b> Exploration Strategies</a></li>
<li class="chapter" data-level="18.6" data-path="appendix.html"><a href="appendix.html#key-algorithms-methods"><i class="fa fa-check"></i><b>18.6</b> Key Algorithms &amp; Methods</a></li>
<li class="chapter" data-level="18.7" data-path="appendix.html"><a href="appendix.html#advanced-concepts"><i class="fa fa-check"></i><b>18.7</b> Advanced Concepts</a></li>
<li class="chapter" data-level="18.8" data-path="appendix.html"><a href="appendix.html#fundamental-equations"><i class="fa fa-check"></i><b>18.8</b> Fundamental Equations</a>
<ul>
<li class="chapter" data-level="18.8.1" data-path="appendix.html"><a href="appendix.html#bellman-equations"><i class="fa fa-check"></i><b>18.8.1</b> <strong>Bellman Equations</strong></a></li>
<li class="chapter" data-level="18.8.2" data-path="appendix.html"><a href="appendix.html#policy-gradient-theorem"><i class="fa fa-check"></i><b>18.8.2</b> <strong>Policy Gradient Theorem</strong></a></li>
<li class="chapter" data-level="18.8.3" data-path="appendix.html"><a href="appendix.html#temporal-difference-error"><i class="fa fa-check"></i><b>18.8.3</b> <strong>Temporal Difference Error</strong></a></li>
</ul></li>
<li class="chapter" data-level="18.9" data-path="appendix.html"><a href="appendix.html#common-challenges-solutions"><i class="fa fa-check"></i><b>18.9</b> Common Challenges &amp; Solutions</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Reinforcement Learning in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="dyna-q-enhanced-exploration-in-integrated-learning-and-planning" class="section level1 hasAnchor" number="10">
<h1><span class="header-section-number">Chapter 10</span> Dyna-Q+: Enhanced Exploration in Integrated Learning and Planning<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-enhanced-exploration-in-integrated-learning-and-planning" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="introduction-8" class="section level2 hasAnchor" number="10.1">
<h2><span class="header-section-number">10.1</span> Introduction<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#introduction-8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>While Dyna successfully bridges model-free and model-based reinforcement learning, it carries an inherent assumption that can limit its effectiveness in changing environments: that the world remains static. When an agent has learned to navigate one version of an environment, what happens if the rules suddenly change? Standard Dyna may find itself stuck, continuing to plan based on outdated information while failing to adequately explore the new reality.</p>
<p>Dyna-Q+, introduced by Sutton (1990) alongside the original Dyna framework, addresses this limitation through a deceptively simple yet powerful mechanism: it rewards curiosity. By providing exploration bonuses for state-action pairs that havenât been tried recently, Dyna-Q+ maintains a healthy skepticism about its modelâs continued accuracy. This approach proves particularly valuable in non-stationary environments where adaptation speed can mean the difference between success and failure.</p>
<p>The enhancement might seem minorâjust an additional term in the reward calculationâbut its implications run deep. Dyna-Q+ acknowledges that in a changing world, forgetting can be as important as remembering, and that an agentâs confidence in its model should decay over time unless continually refreshed by recent experience.</p>
</div>
<div id="theoretical-framework-1" class="section level2 hasAnchor" number="10.2">
<h2><span class="header-section-number">10.2</span> Theoretical Framework<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#theoretical-framework-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="the-exploration-bonus-mechanism" class="section level3 hasAnchor" number="10.2.1">
<h3><span class="header-section-number">10.2.1</span> The Exploration Bonus Mechanism<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#the-exploration-bonus-mechanism" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Dyna-Q+ modifies the planning phase of standard Dyna by augmenting rewards with an exploration bonus based on the time elapsed since each state-action pair was last visited. The core insight lies in treating the passage of time as information: the longer an agent hasnât verified a particular transition, the less confident it should be about that transitionâs current validity.</p>
<p>For each state-action pair <span class="math inline">\((s,a)\)</span>, we maintain a timestamp <span class="math inline">\(\\tau(s,a)\)</span> recording when it was last experienced. During planning, instead of using the stored reward <span class="math inline">\(\\hat{R}(s,a)\)</span> directly, we calculate an augmented reward:</p>
<p><span class="math display">\[r_{augmented} = \hat{R}(s,a) + \kappa \sqrt{t - \tau(s,a)}\]</span></p>
<p>where <span class="math inline">\(t\)</span> represents the current time step, and <span class="math inline">\(\\kappa\)</span> is a parameter controlling the strength of the exploration bonus. The square root function provides a diminishing bonus that grows with time but at a decreasing rate, reflecting the intuition that uncertainty about a transition increases with time but not linearly.</p>
</div>
<div id="complete-dyna-q-algorithm" class="section level3 hasAnchor" number="10.2.2">
<h3><span class="header-section-number">10.2.2</span> Complete Dyna-Q+ Algorithm<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#complete-dyna-q-algorithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The full algorithm extends standard Dyna with minimal modifications. For each real experience tuple <span class="math inline">\((s, a, r, s&#39;)\)</span>:</p>
<p><strong>Direct Learning</strong>:
<span class="math display">\[Q(s,a) \leftarrow Q(s,a) + \alpha \left[ r + \gamma \max_{a&#39;} Q(s&#39;, a&#39;) - Q(s,a) \right]\]</span></p>
<p><strong>Model and Timestamp Updates</strong>:
<span class="math display">\[\hat{T}(s,a) \leftarrow s&#39;\]</span><span class="math display">\[\hat{R}(s,a) \leftarrow r\]</span><span class="math display">\[\tau(s,a) \leftarrow t\]</span></p>
<p><strong>Planning Phase</strong> (repeat <span class="math inline">\(n\)</span> times):
<span class="math display">\[s_{plan} \leftarrow \text{random previously visited state}\]</span></p>
<p><span class="math display">\[a_{plan} \leftarrow \text{random action previously taken in } s_{plan}\]</span></p>
<p><span class="math display">\[r_{plan} \leftarrow \hat{R}(s_{plan},a_{plan}) + \kappa \sqrt{t - \tau(s_{plan},a_{plan})}\]</span></p>
<p><span class="math display">\[s&#39;_{plan} \leftarrow \hat{T}(s_{plan},a_{plan})\]</span></p>
<p><span class="math display">\[Q(s_{plan},a_{plan}) \leftarrow Q(s_{plan},a_{plan}) + \alpha \left[ r_{plan} + \gamma \max_{a&#39;} Q(s&#39;_{plan}, a&#39;) - Q(s_{plan},a_{plan}) \right]\]</span></p>
</div>
<div id="convergence-and-stability" class="section level3 hasAnchor" number="10.2.3">
<h3><span class="header-section-number">10.2.3</span> Convergence and Stability<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#convergence-and-stability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The theoretical properties of Dyna-Q+ are more complex than those of standard Dyna due to the non-stationary nature of the augmented rewards. In stationary environments, the exploration bonuses for frequently visited state-action pairs will remain small, and convergence properties approach those of standard Dyna. However, the algorithm sacrifices some theoretical guarantees about convergence to optimal policies in exchange for improved adaptability.</p>
<p>The parameter <span class="math inline">\(\\kappa\)</span> requires careful tuning. Too small, and the exploration bonus becomes negligible, reducing Dyna-Q+ to standard Dyna. Too large, and the algorithm may exhibit excessive exploration even in stable environments, potentially degrading performance. The square root scaling helps moderate this trade-off by providing significant bonuses for truly neglected state-action pairs while keeping bonuses manageable for recently visited ones.</p>
</div>
</div>
<div id="implementation-in-r-1" class="section level2 hasAnchor" number="10.3">
<h2><span class="header-section-number">10.3</span> Implementation in R<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#implementation-in-r-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Building on our previous Dyna implementation, we extend the framework to include timestamp tracking and exploration bonuses.</p>
<div id="environment-setup-1" class="section level3 hasAnchor" number="10.3.1">
<h3><span class="header-section-number">10.3.1</span> Environment Setup<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#environment-setup-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Weâll use the same 10-state environment as before, but weâll also create scenarios with environmental changes to demonstrate Dyna-Q+âs adaptive capabilities:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb125-1"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-1" tabindex="-1"></a><span class="co"># Environment parameters (same as before)</span></span>
<span id="cb125-2"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-2" tabindex="-1"></a>n_states <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb125-3"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-3" tabindex="-1"></a>n_actions <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb125-4"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-4" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb125-5"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-5" tabindex="-1"></a>terminal_state <span class="ot">&lt;-</span> n_states</span>
<span id="cb125-6"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-6" tabindex="-1"></a></span>
<span id="cb125-7"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-7" tabindex="-1"></a><span class="co"># Transition and reward models</span></span>
<span id="cb125-8"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-8" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">42</span>)</span>
<span id="cb125-9"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-9" tabindex="-1"></a>transition_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb125-10"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-10" tabindex="-1"></a>reward_model <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions, n_states))</span>
<span id="cb125-11"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-11" tabindex="-1"></a></span>
<span id="cb125-12"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-12" tabindex="-1"></a><span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>(n_states <span class="sc">-</span> <span class="dv">1</span>)) {</span>
<span id="cb125-13"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-13" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, s <span class="sc">+</span> <span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fl">0.9</span></span>
<span id="cb125-14"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-14" tabindex="-1"></a>  transition_model[s, <span class="dv">1</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb125-15"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-15" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.8</span></span>
<span id="cb125-16"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-16" tabindex="-1"></a>  transition_model[s, <span class="dv">2</span>, <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>)] <span class="ot">&lt;-</span> <span class="fl">0.2</span></span>
<span id="cb125-17"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-17" tabindex="-1"></a>  </span>
<span id="cb125-18"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-18" tabindex="-1"></a>  <span class="cf">for</span> (s_prime <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_states) {</span>
<span id="cb125-19"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-19" tabindex="-1"></a>    reward_model[s, <span class="dv">1</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">1.0</span>, <span class="fl">0.1</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb125-20"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-20" tabindex="-1"></a>    reward_model[s, <span class="dv">2</span>, s_prime] <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(s_prime <span class="sc">==</span> n_states, <span class="fl">0.5</span>, <span class="fl">0.05</span> <span class="sc">*</span> <span class="fu">runif</span>(<span class="dv">1</span>))</span>
<span id="cb125-21"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-21" tabindex="-1"></a>  }</span>
<span id="cb125-22"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-22" tabindex="-1"></a>}</span>
<span id="cb125-23"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-23" tabindex="-1"></a></span>
<span id="cb125-24"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-24" tabindex="-1"></a>transition_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb125-25"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-25" tabindex="-1"></a>reward_model[n_states, , ] <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb125-26"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-26" tabindex="-1"></a></span>
<span id="cb125-27"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-27" tabindex="-1"></a><span class="co"># Environment interaction function with optional modification capability</span></span>
<span id="cb125-28"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-28" tabindex="-1"></a>sample_env <span class="ot">&lt;-</span> <span class="cf">function</span>(s, a, <span class="at">modified =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb125-29"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-29" tabindex="-1"></a>  <span class="cf">if</span> (modified) {</span>
<span id="cb125-30"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-30" tabindex="-1"></a>    <span class="co"># Simulate environmental change by blocking previously optimal path</span></span>
<span id="cb125-31"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-31" tabindex="-1"></a>    <span class="cf">if</span> (s <span class="sc">==</span> <span class="dv">5</span> <span class="sc">&amp;&amp;</span> a <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb125-32"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-32" tabindex="-1"></a>      <span class="fu">return</span>(<span class="fu">list</span>(<span class="at">s_prime =</span> <span class="dv">1</span>, <span class="at">reward =</span> <span class="sc">-</span><span class="fl">0.5</span>))  <span class="co"># Penalty for blocked path</span></span>
<span id="cb125-33"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-33" tabindex="-1"></a>    }</span>
<span id="cb125-34"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-34" tabindex="-1"></a>  }</span>
<span id="cb125-35"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-35" tabindex="-1"></a>  </span>
<span id="cb125-36"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-36" tabindex="-1"></a>  probs <span class="ot">&lt;-</span> transition_model[s, a, ]</span>
<span id="cb125-37"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-37" tabindex="-1"></a>  s_prime <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_states, <span class="dv">1</span>, <span class="at">prob =</span> probs)</span>
<span id="cb125-38"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-38" tabindex="-1"></a>  reward <span class="ot">&lt;-</span> reward_model[s, a, s_prime]</span>
<span id="cb125-39"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-39" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">s_prime =</span> s_prime, <span class="at">reward =</span> reward)</span>
<span id="cb125-40"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb125-40" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="dyna-q-implementation-1" class="section level3 hasAnchor" number="10.3.2">
<h3><span class="header-section-number">10.3.2</span> Dyna-Q+ Implementation<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#dyna-q-implementation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The key modification involves maintaining timestamps and calculating exploration bonuses during planning:</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb126-1"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-1" tabindex="-1"></a>dyna_q_plus <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>, </span>
<span id="cb126-2"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-2" tabindex="-1"></a>                        <span class="at">n_planning =</span> <span class="dv">5</span>, <span class="at">kappa =</span> <span class="fl">0.1</span>, <span class="at">change_episode =</span> <span class="cn">NULL</span>) {</span>
<span id="cb126-3"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-3" tabindex="-1"></a>  <span class="co"># Initialize Q-values, model, and timestamps</span></span>
<span id="cb126-4"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-4" tabindex="-1"></a>  Q <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> n_states, <span class="at">ncol =</span> n_actions)</span>
<span id="cb126-5"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-5" tabindex="-1"></a>  model_T <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions))</span>
<span id="cb126-6"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-6" tabindex="-1"></a>  model_R <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions))</span>
<span id="cb126-7"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-7" tabindex="-1"></a>  timestamps <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions))  <span class="co"># When last visited</span></span>
<span id="cb126-8"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-8" tabindex="-1"></a>  visited_sa <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb126-9"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-9" tabindex="-1"></a>  </span>
<span id="cb126-10"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-10" tabindex="-1"></a>  current_time <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb126-11"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-11" tabindex="-1"></a>  environment_changed <span class="ot">&lt;-</span> <span class="cn">FALSE</span></span>
<span id="cb126-12"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-12" tabindex="-1"></a>  </span>
<span id="cb126-13"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-13" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb126-14"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-14" tabindex="-1"></a>    <span class="co"># Check if we should change the environment</span></span>
<span id="cb126-15"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-15" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(change_episode) <span class="sc">&amp;&amp;</span> ep <span class="sc">==</span> change_episode) {</span>
<span id="cb126-16"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-16" tabindex="-1"></a>      environment_changed <span class="ot">&lt;-</span> <span class="cn">TRUE</span></span>
<span id="cb126-17"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-17" tabindex="-1"></a>    }</span>
<span id="cb126-18"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-18" tabindex="-1"></a>    </span>
<span id="cb126-19"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-19" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb126-20"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-20" tabindex="-1"></a>    </span>
<span id="cb126-21"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-21" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state) {</span>
<span id="cb126-22"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-22" tabindex="-1"></a>      current_time <span class="ot">&lt;-</span> current_time <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb126-23"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-23" tabindex="-1"></a>      </span>
<span id="cb126-24"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-24" tabindex="-1"></a>      <span class="co"># Action selection (epsilon-greedy)</span></span>
<span id="cb126-25"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-25" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb126-26"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-26" tabindex="-1"></a>        a <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb126-27"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-27" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb126-28"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-28" tabindex="-1"></a>        <span class="co"># Break ties randomly</span></span>
<span id="cb126-29"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-29" tabindex="-1"></a>        a <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">which</span>(Q[s, ] <span class="sc">==</span> <span class="fu">max</span>(Q[s, ])), <span class="dv">1</span>)</span>
<span id="cb126-30"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-30" tabindex="-1"></a>      }</span>
<span id="cb126-31"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-31" tabindex="-1"></a>      </span>
<span id="cb126-32"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-32" tabindex="-1"></a>      <span class="co"># Take action and observe outcome</span></span>
<span id="cb126-33"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-33" tabindex="-1"></a>      outcome <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a, <span class="at">modified =</span> environment_changed)</span>
<span id="cb126-34"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-34" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> outcome<span class="sc">$</span>s_prime</span>
<span id="cb126-35"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-35" tabindex="-1"></a>      r <span class="ot">&lt;-</span> outcome<span class="sc">$</span>reward</span>
<span id="cb126-36"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-36" tabindex="-1"></a>      </span>
<span id="cb126-37"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-37" tabindex="-1"></a>      <span class="co"># Direct learning (Q-Learning update)</span></span>
<span id="cb126-38"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-38" tabindex="-1"></a>      Q[s, a] <span class="ot">&lt;-</span> Q[s, a] <span class="sc">+</span> alpha <span class="sc">*</span> (r <span class="sc">+</span> gamma <span class="sc">*</span> <span class="fu">max</span>(Q[s_prime, ]) <span class="sc">-</span> Q[s, a])</span>
<span id="cb126-39"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-39" tabindex="-1"></a>      </span>
<span id="cb126-40"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-40" tabindex="-1"></a>      <span class="co"># Model learning and timestamp update</span></span>
<span id="cb126-41"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-41" tabindex="-1"></a>      model_T[s, a] <span class="ot">&lt;-</span> s_prime</span>
<span id="cb126-42"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-42" tabindex="-1"></a>      model_R[s, a] <span class="ot">&lt;-</span> r</span>
<span id="cb126-43"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-43" tabindex="-1"></a>      timestamps[s, a] <span class="ot">&lt;-</span> current_time</span>
<span id="cb126-44"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-44" tabindex="-1"></a>      </span>
<span id="cb126-45"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-45" tabindex="-1"></a>      <span class="co"># Track visited state-action pairs</span></span>
<span id="cb126-46"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-46" tabindex="-1"></a>      sa_key <span class="ot">&lt;-</span> <span class="fu">paste</span>(s, a, <span class="at">sep =</span> <span class="st">&quot;_&quot;</span>)</span>
<span id="cb126-47"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-47" tabindex="-1"></a>      <span class="cf">if</span> (<span class="sc">!</span>(sa_key <span class="sc">%in%</span> <span class="fu">names</span>(visited_sa))) {</span>
<span id="cb126-48"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-48" tabindex="-1"></a>        visited_sa[[sa_key]] <span class="ot">&lt;-</span> <span class="fu">c</span>(s, a)</span>
<span id="cb126-49"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-49" tabindex="-1"></a>      }</span>
<span id="cb126-50"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-50" tabindex="-1"></a>      </span>
<span id="cb126-51"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-51" tabindex="-1"></a>      <span class="co"># Planning phase with exploration bonuses</span></span>
<span id="cb126-52"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-52" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">length</span>(visited_sa) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb126-53"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-53" tabindex="-1"></a>        <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_planning) {</span>
<span id="cb126-54"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-54" tabindex="-1"></a>          <span class="co"># Sample random previously visited state-action pair</span></span>
<span id="cb126-55"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-55" tabindex="-1"></a>          sa_sample <span class="ot">&lt;-</span> <span class="fu">sample</span>(visited_sa, <span class="dv">1</span>)[[<span class="dv">1</span>]]</span>
<span id="cb126-56"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-56" tabindex="-1"></a>          s_plan <span class="ot">&lt;-</span> sa_sample[<span class="dv">1</span>]</span>
<span id="cb126-57"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-57" tabindex="-1"></a>          a_plan <span class="ot">&lt;-</span> sa_sample[<span class="dv">2</span>]</span>
<span id="cb126-58"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-58" tabindex="-1"></a>          </span>
<span id="cb126-59"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-59" tabindex="-1"></a>          <span class="co"># Get simulated experience from model</span></span>
<span id="cb126-60"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-60" tabindex="-1"></a>          <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.na</span>(model_T[s_plan, a_plan])) {</span>
<span id="cb126-61"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-61" tabindex="-1"></a>            s_prime_plan <span class="ot">&lt;-</span> model_T[s_plan, a_plan]</span>
<span id="cb126-62"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-62" tabindex="-1"></a>            r_base <span class="ot">&lt;-</span> model_R[s_plan, a_plan]</span>
<span id="cb126-63"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-63" tabindex="-1"></a>            </span>
<span id="cb126-64"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-64" tabindex="-1"></a>            <span class="co"># Calculate exploration bonus</span></span>
<span id="cb126-65"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-65" tabindex="-1"></a>            time_since_visit <span class="ot">&lt;-</span> current_time <span class="sc">-</span> timestamps[s_plan, a_plan]</span>
<span id="cb126-66"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-66" tabindex="-1"></a>            exploration_bonus <span class="ot">&lt;-</span> kappa <span class="sc">*</span> <span class="fu">sqrt</span>(time_since_visit)</span>
<span id="cb126-67"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-67" tabindex="-1"></a>            r_plan <span class="ot">&lt;-</span> r_base <span class="sc">+</span> exploration_bonus</span>
<span id="cb126-68"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-68" tabindex="-1"></a>            </span>
<span id="cb126-69"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-69" tabindex="-1"></a>            <span class="co"># Planning update with augmented reward</span></span>
<span id="cb126-70"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-70" tabindex="-1"></a>            Q[s_plan, a_plan] <span class="ot">&lt;-</span> Q[s_plan, a_plan] <span class="sc">+</span> </span>
<span id="cb126-71"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-71" tabindex="-1"></a>              alpha <span class="sc">*</span> (r_plan <span class="sc">+</span> gamma <span class="sc">*</span> <span class="fu">max</span>(Q[s_prime_plan, ]) <span class="sc">-</span> Q[s_plan, a_plan])</span>
<span id="cb126-72"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-72" tabindex="-1"></a>          }</span>
<span id="cb126-73"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-73" tabindex="-1"></a>        }</span>
<span id="cb126-74"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-74" tabindex="-1"></a>      }</span>
<span id="cb126-75"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-75" tabindex="-1"></a>      </span>
<span id="cb126-76"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-76" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb126-77"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-77" tabindex="-1"></a>    }</span>
<span id="cb126-78"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-78" tabindex="-1"></a>  }</span>
<span id="cb126-79"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-79" tabindex="-1"></a>  </span>
<span id="cb126-80"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-80" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">Q =</span> Q, <span class="at">policy =</span> <span class="fu">apply</span>(Q, <span class="dv">1</span>, which.max), </span>
<span id="cb126-81"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-81" tabindex="-1"></a>       <span class="at">model_T =</span> model_T, <span class="at">model_R =</span> model_R, <span class="at">timestamps =</span> timestamps)</span>
<span id="cb126-82"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb126-82" tabindex="-1"></a>}</span></code></pre></div>
</div>
<div id="standard-dyna-for-comparison" class="section level3 hasAnchor" number="10.3.3">
<h3><span class="header-section-number">10.3.3</span> Standard Dyna for Comparison<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#standard-dyna-for-comparison" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We also implement standard Dyna to highlight the differences:</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb127-1"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-1" tabindex="-1"></a>dyna_q_standard <span class="ot">&lt;-</span> <span class="cf">function</span>(<span class="at">episodes =</span> <span class="dv">1000</span>, <span class="at">alpha =</span> <span class="fl">0.1</span>, <span class="at">epsilon =</span> <span class="fl">0.1</span>, </span>
<span id="cb127-2"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-2" tabindex="-1"></a>                            <span class="at">n_planning =</span> <span class="dv">5</span>, <span class="at">change_episode =</span> <span class="cn">NULL</span>) {</span>
<span id="cb127-3"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-3" tabindex="-1"></a>  Q <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> n_states, <span class="at">ncol =</span> n_actions)</span>
<span id="cb127-4"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-4" tabindex="-1"></a>  model_T <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions))</span>
<span id="cb127-5"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-5" tabindex="-1"></a>  model_R <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="cn">NA</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions))</span>
<span id="cb127-6"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-6" tabindex="-1"></a>  visited_sa <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb127-7"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-7" tabindex="-1"></a>  </span>
<span id="cb127-8"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-8" tabindex="-1"></a>  environment_changed <span class="ot">&lt;-</span> <span class="cn">FALSE</span></span>
<span id="cb127-9"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-9" tabindex="-1"></a>  </span>
<span id="cb127-10"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-10" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb127-11"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-11" tabindex="-1"></a>    <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.null</span>(change_episode) <span class="sc">&amp;&amp;</span> ep <span class="sc">==</span> change_episode) {</span>
<span id="cb127-12"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-12" tabindex="-1"></a>      environment_changed <span class="ot">&lt;-</span> <span class="cn">TRUE</span></span>
<span id="cb127-13"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-13" tabindex="-1"></a>    }</span>
<span id="cb127-14"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-14" tabindex="-1"></a>    </span>
<span id="cb127-15"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-15" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb127-16"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-16" tabindex="-1"></a>    </span>
<span id="cb127-17"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-17" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state) {</span>
<span id="cb127-18"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-18" tabindex="-1"></a>      <span class="co"># Action selection</span></span>
<span id="cb127-19"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-19" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">runif</span>(<span class="dv">1</span>) <span class="sc">&lt;</span> epsilon) {</span>
<span id="cb127-20"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-20" tabindex="-1"></a>        a <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb127-21"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-21" tabindex="-1"></a>      } <span class="cf">else</span> {</span>
<span id="cb127-22"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-22" tabindex="-1"></a>        <span class="co"># Break ties randomly</span></span>
<span id="cb127-23"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-23" tabindex="-1"></a>        a <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">which</span>(Q[s, ] <span class="sc">==</span> <span class="fu">max</span>(Q[s, ])), <span class="dv">1</span>)</span>
<span id="cb127-24"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-24" tabindex="-1"></a>      }</span>
<span id="cb127-25"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-25" tabindex="-1"></a>      </span>
<span id="cb127-26"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-26" tabindex="-1"></a>      <span class="co"># Take action</span></span>
<span id="cb127-27"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-27" tabindex="-1"></a>      outcome <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a, <span class="at">modified =</span> environment_changed)</span>
<span id="cb127-28"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-28" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> outcome<span class="sc">$</span>s_prime</span>
<span id="cb127-29"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-29" tabindex="-1"></a>      r <span class="ot">&lt;-</span> outcome<span class="sc">$</span>reward</span>
<span id="cb127-30"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-30" tabindex="-1"></a>      </span>
<span id="cb127-31"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-31" tabindex="-1"></a>      <span class="co"># Direct learning</span></span>
<span id="cb127-32"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-32" tabindex="-1"></a>      Q[s, a] <span class="ot">&lt;-</span> Q[s, a] <span class="sc">+</span> alpha <span class="sc">*</span> (r <span class="sc">+</span> gamma <span class="sc">*</span> <span class="fu">max</span>(Q[s_prime, ]) <span class="sc">-</span> Q[s, a])</span>
<span id="cb127-33"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-33" tabindex="-1"></a>      </span>
<span id="cb127-34"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-34" tabindex="-1"></a>      <span class="co"># Model learning (no timestamp tracking)</span></span>
<span id="cb127-35"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-35" tabindex="-1"></a>      model_T[s, a] <span class="ot">&lt;-</span> s_prime</span>
<span id="cb127-36"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-36" tabindex="-1"></a>      model_R[s, a] <span class="ot">&lt;-</span> r</span>
<span id="cb127-37"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-37" tabindex="-1"></a>      </span>
<span id="cb127-38"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-38" tabindex="-1"></a>      <span class="co"># Track visited pairs</span></span>
<span id="cb127-39"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-39" tabindex="-1"></a>      sa_key <span class="ot">&lt;-</span> <span class="fu">paste</span>(s, a, <span class="at">sep =</span> <span class="st">&quot;_&quot;</span>)</span>
<span id="cb127-40"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-40" tabindex="-1"></a>      <span class="cf">if</span> (<span class="sc">!</span>(sa_key <span class="sc">%in%</span> <span class="fu">names</span>(visited_sa))) {</span>
<span id="cb127-41"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-41" tabindex="-1"></a>        visited_sa[[sa_key]] <span class="ot">&lt;-</span> <span class="fu">c</span>(s, a)</span>
<span id="cb127-42"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-42" tabindex="-1"></a>      }</span>
<span id="cb127-43"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-43" tabindex="-1"></a>      </span>
<span id="cb127-44"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-44" tabindex="-1"></a>      <span class="co"># Standard planning (no exploration bonus)</span></span>
<span id="cb127-45"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-45" tabindex="-1"></a>      <span class="cf">if</span> (<span class="fu">length</span>(visited_sa) <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb127-46"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-46" tabindex="-1"></a>        <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_planning) {</span>
<span id="cb127-47"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-47" tabindex="-1"></a>          sa_sample <span class="ot">&lt;-</span> <span class="fu">sample</span>(visited_sa, <span class="dv">1</span>)[[<span class="dv">1</span>]]</span>
<span id="cb127-48"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-48" tabindex="-1"></a>          s_plan <span class="ot">&lt;-</span> sa_sample[<span class="dv">1</span>]</span>
<span id="cb127-49"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-49" tabindex="-1"></a>          a_plan <span class="ot">&lt;-</span> sa_sample[<span class="dv">2</span>]</span>
<span id="cb127-50"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-50" tabindex="-1"></a>          </span>
<span id="cb127-51"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-51" tabindex="-1"></a>          <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.na</span>(model_T[s_plan, a_plan])) {</span>
<span id="cb127-52"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-52" tabindex="-1"></a>            s_prime_plan <span class="ot">&lt;-</span> model_T[s_plan, a_plan]</span>
<span id="cb127-53"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-53" tabindex="-1"></a>            r_plan <span class="ot">&lt;-</span> model_R[s_plan, a_plan]  <span class="co"># No bonus here</span></span>
<span id="cb127-54"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-54" tabindex="-1"></a>            </span>
<span id="cb127-55"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-55" tabindex="-1"></a>            Q[s_plan, a_plan] <span class="ot">&lt;-</span> Q[s_plan, a_plan] <span class="sc">+</span> </span>
<span id="cb127-56"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-56" tabindex="-1"></a>              alpha <span class="sc">*</span> (r_plan <span class="sc">+</span> gamma <span class="sc">*</span> <span class="fu">max</span>(Q[s_prime_plan, ]) <span class="sc">-</span> Q[s_plan, a_plan])</span>
<span id="cb127-57"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-57" tabindex="-1"></a>          }</span>
<span id="cb127-58"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-58" tabindex="-1"></a>        }</span>
<span id="cb127-59"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-59" tabindex="-1"></a>      }</span>
<span id="cb127-60"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-60" tabindex="-1"></a>      </span>
<span id="cb127-61"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-61" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb127-62"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-62" tabindex="-1"></a>    }</span>
<span id="cb127-63"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-63" tabindex="-1"></a>  }</span>
<span id="cb127-64"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-64" tabindex="-1"></a>  </span>
<span id="cb127-65"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-65" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">Q =</span> Q, <span class="at">policy =</span> <span class="fu">apply</span>(Q, <span class="dv">1</span>, which.max))</span>
<span id="cb127-66"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb127-66" tabindex="-1"></a>}</span></code></pre></div>
</div>
</div>
<div id="experimental-analysis-1" class="section level2 hasAnchor" number="10.4">
<h2><span class="header-section-number">10.4</span> Experimental Analysis<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#experimental-analysis-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="adaptation-to-environmental-changes" class="section level3 hasAnchor" number="10.4.1">
<h3><span class="header-section-number">10.4.1</span> Adaptation to Environmental Changes<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#adaptation-to-environmental-changes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The most compelling demonstration of Dyna-Q+âs advantages comes from scenarios where the environment changes mid-learning. Weâll compare how quickly both algorithms adapt:</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb128-1"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-1" tabindex="-1"></a><span class="co"># Function to evaluate policy performance</span></span>
<span id="cb128-2"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-2" tabindex="-1"></a>evaluate_policy_performance <span class="ot">&lt;-</span> <span class="cf">function</span>(Q, <span class="at">episodes =</span> <span class="dv">100</span>, <span class="at">modified =</span> <span class="cn">FALSE</span>) {</span>
<span id="cb128-3"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-3" tabindex="-1"></a>  total_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb128-4"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-4" tabindex="-1"></a>  total_steps <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb128-5"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-5" tabindex="-1"></a>  </span>
<span id="cb128-6"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-6" tabindex="-1"></a>  <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>episodes) {</span>
<span id="cb128-7"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-7" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb128-8"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-8" tabindex="-1"></a>    episode_reward <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb128-9"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-9" tabindex="-1"></a>    steps <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb128-10"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-10" tabindex="-1"></a>    </span>
<span id="cb128-11"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-11" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state <span class="sc">&amp;&amp;</span> steps <span class="sc">&lt;</span> <span class="dv">50</span>) {  <span class="co"># Prevent infinite loops</span></span>
<span id="cb128-12"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-12" tabindex="-1"></a>      a <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="fu">which</span>(Q[s, ] <span class="sc">==</span> <span class="fu">max</span>(Q[s, ])), <span class="dv">1</span>) <span class="co"># Break ties randomly</span></span>
<span id="cb128-13"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-13" tabindex="-1"></a>      outcome <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a, <span class="at">modified =</span> modified)</span>
<span id="cb128-14"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-14" tabindex="-1"></a>      episode_reward <span class="ot">&lt;-</span> episode_reward <span class="sc">+</span> outcome<span class="sc">$</span>reward</span>
<span id="cb128-15"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-15" tabindex="-1"></a>      s <span class="ot">&lt;-</span> outcome<span class="sc">$</span>s_prime</span>
<span id="cb128-16"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-16" tabindex="-1"></a>      steps <span class="ot">&lt;-</span> steps <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb128-17"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-17" tabindex="-1"></a>    }</span>
<span id="cb128-18"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-18" tabindex="-1"></a>    </span>
<span id="cb128-19"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-19" tabindex="-1"></a>    total_reward <span class="ot">&lt;-</span> total_reward <span class="sc">+</span> episode_reward</span>
<span id="cb128-20"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-20" tabindex="-1"></a>    total_steps <span class="ot">&lt;-</span> total_steps <span class="sc">+</span> steps</span>
<span id="cb128-21"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-21" tabindex="-1"></a>  }</span>
<span id="cb128-22"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-22" tabindex="-1"></a>  </span>
<span id="cb128-23"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-23" tabindex="-1"></a>  <span class="fu">list</span>(<span class="at">avg_reward =</span> total_reward <span class="sc">/</span> episodes, <span class="at">avg_steps =</span> total_steps <span class="sc">/</span> episodes)</span>
<span id="cb128-24"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-24" tabindex="-1"></a>}</span>
<span id="cb128-25"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-25" tabindex="-1"></a></span>
<span id="cb128-26"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-26" tabindex="-1"></a><span class="co"># Comparative experiment with environmental change</span></span>
<span id="cb128-27"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-27" tabindex="-1"></a>adaptation_experiment <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb128-28"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-28" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb128-29"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-29" tabindex="-1"></a>  n_runs <span class="ot">&lt;-</span> <span class="dv">20</span></span>
<span id="cb128-30"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-30" tabindex="-1"></a>  change_point <span class="ot">&lt;-</span> <span class="dv">500</span></span>
<span id="cb128-31"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-31" tabindex="-1"></a>  total_episodes <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb128-32"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-32" tabindex="-1"></a>  </span>
<span id="cb128-33"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-33" tabindex="-1"></a>  <span class="co"># Storage for results</span></span>
<span id="cb128-34"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-34" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb128-35"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-35" tabindex="-1"></a>    <span class="at">episode =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>total_episodes, <span class="dv">2</span>),</span>
<span id="cb128-36"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-36" tabindex="-1"></a>    <span class="at">algorithm =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">&quot;Dyna-Q&quot;</span>, <span class="st">&quot;Dyna-Q+&quot;</span>), <span class="at">each =</span> total_episodes),</span>
<span id="cb128-37"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-37" tabindex="-1"></a>    <span class="at">performance =</span> <span class="fu">numeric</span>(total_episodes <span class="sc">*</span> <span class="dv">2</span>),</span>
<span id="cb128-38"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-38" tabindex="-1"></a>    <span class="at">run =</span> <span class="fu">rep</span>(<span class="dv">1</span>, total_episodes <span class="sc">*</span> <span class="dv">2</span>)</span>
<span id="cb128-39"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-39" tabindex="-1"></a>  )</span>
<span id="cb128-40"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-40" tabindex="-1"></a>  </span>
<span id="cb128-41"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-41" tabindex="-1"></a>  <span class="cf">for</span> (run <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_runs) {</span>
<span id="cb128-42"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-42" tabindex="-1"></a>    <span class="co"># Train both algorithms</span></span>
<span id="cb128-43"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-43" tabindex="-1"></a>    <span class="co"># For a full experiment, you would re-initialize Q here for each run</span></span>
<span id="cb128-44"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-44" tabindex="-1"></a>    dyna_standard_result <span class="ot">&lt;-</span> <span class="fu">dyna_q_standard</span>(<span class="at">episodes =</span> total_episodes, </span>
<span id="cb128-45"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-45" tabindex="-1"></a>                                            <span class="at">change_episode =</span> change_point)</span>
<span id="cb128-46"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-46" tabindex="-1"></a>    dyna_plus_result <span class="ot">&lt;-</span> <span class="fu">dyna_q_plus</span>(<span class="at">episodes =</span> total_episodes, </span>
<span id="cb128-47"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-47" tabindex="-1"></a>                                    <span class="at">change_episode =</span> change_point,</span>
<span id="cb128-48"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-48" tabindex="-1"></a>                                    <span class="at">kappa =</span> <span class="fl">0.1</span>)</span>
<span id="cb128-49"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-49" tabindex="-1"></a>    </span>
<span id="cb128-50"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-50" tabindex="-1"></a>    <span class="co"># Evaluate performance at each episode (simplified for illustration)</span></span>
<span id="cb128-51"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-51" tabindex="-1"></a>    <span class="cf">for</span> (ep <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>total_episodes) {</span>
<span id="cb128-52"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-52" tabindex="-1"></a>      modified_env <span class="ot">&lt;-</span> ep <span class="sc">&gt;=</span> change_point</span>
<span id="cb128-53"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-53" tabindex="-1"></a>      </span>
<span id="cb128-54"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-54" tabindex="-1"></a>      <span class="co"># This is a simplified evaluation - in practice, you&#39;d want to</span></span>
<span id="cb128-55"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-55" tabindex="-1"></a>      <span class="co"># track performance throughout training</span></span>
<span id="cb128-56"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-56" tabindex="-1"></a>      <span class="cf">if</span> (ep <span class="sc">%%</span> <span class="dv">50</span> <span class="sc">==</span> <span class="dv">0</span>) {</span>
<span id="cb128-57"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-57" tabindex="-1"></a>        std_perf <span class="ot">&lt;-</span> <span class="fu">evaluate_policy_performance</span>(dyna_standard_result<span class="sc">$</span>Q, </span>
<span id="cb128-58"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-58" tabindex="-1"></a>                                                <span class="at">episodes =</span> <span class="dv">10</span>, </span>
<span id="cb128-59"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-59" tabindex="-1"></a>                                                <span class="at">modified =</span> modified_env)</span>
<span id="cb128-60"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-60" tabindex="-1"></a>        plus_perf <span class="ot">&lt;-</span> <span class="fu">evaluate_policy_performance</span>(dyna_plus_result<span class="sc">$</span>Q, </span>
<span id="cb128-61"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-61" tabindex="-1"></a>                                                 <span class="at">episodes =</span> <span class="dv">10</span>, </span>
<span id="cb128-62"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-62" tabindex="-1"></a>                                                 <span class="at">modified =</span> modified_env)</span>
<span id="cb128-63"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-63" tabindex="-1"></a>        </span>
<span id="cb128-64"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-64" tabindex="-1"></a>        <span class="co"># Store results (this is simplified - you&#39;d want better tracking)</span></span>
<span id="cb128-65"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-65" tabindex="-1"></a>        idx_std <span class="ot">&lt;-</span> (run <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> total_episodes <span class="sc">*</span> <span class="dv">2</span> <span class="sc">+</span> ep</span>
<span id="cb128-66"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-66" tabindex="-1"></a>        idx_plus <span class="ot">&lt;-</span> (run <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> total_episodes <span class="sc">*</span> <span class="dv">2</span> <span class="sc">+</span> total_episodes <span class="sc">+</span> ep</span>
<span id="cb128-67"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-67" tabindex="-1"></a>        </span>
<span id="cb128-68"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-68" tabindex="-1"></a>        <span class="cf">if</span> (run <span class="sc">==</span> <span class="dv">1</span>) {  <span class="co"># Just store first run for illustration</span></span>
<span id="cb128-69"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-69" tabindex="-1"></a>          results<span class="sc">$</span>performance[ep] <span class="ot">&lt;-</span> std_perf<span class="sc">$</span>avg_reward</span>
<span id="cb128-70"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-70" tabindex="-1"></a>          results<span class="sc">$</span>performance[total_episodes <span class="sc">+</span> ep] <span class="ot">&lt;-</span> plus_perf<span class="sc">$</span>avg_reward</span>
<span id="cb128-71"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-71" tabindex="-1"></a>        }</span>
<span id="cb128-72"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-72" tabindex="-1"></a>      }</span>
<span id="cb128-73"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-73" tabindex="-1"></a>    }</span>
<span id="cb128-74"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-74" tabindex="-1"></a>  }</span>
<span id="cb128-75"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-75" tabindex="-1"></a>  </span>
<span id="cb128-76"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-76" tabindex="-1"></a>  <span class="fu">return</span>(results)</span>
<span id="cb128-77"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb128-77" tabindex="-1"></a>}</span></code></pre></div>
<div id="example-running-the-adaptation-experiment" class="section level4 hasAnchor" number="10.4.1.1">
<h4><span class="header-section-number">10.4.1.1</span> Example: Running the Adaptation Experiment<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#example-running-the-adaptation-experiment" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Here we execute the experiment defined above. The resulting plot shows that both algorithms perform similarly until the environment changes at episode 500. After the change, Dyna-Q (<code>kappa = 0</code>) fails to adapt because its model is outdated, leading to a sharp drop in performance. In contrast, Dyna-Q+ uses its exploration bonus to re-evaluate old paths, quickly discovering the change and finding a new optimal policy, thus recovering its performance.</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb129-1"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-1" tabindex="-1"></a><span class="co"># Setup chunk for libraries</span></span>
<span id="cb129-2"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-2" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;ggplot2&quot;</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;ggplot2&quot;</span>)</span>
<span id="cb129-3"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-3" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;dplyr&quot;</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;dplyr&quot;</span>)</span>
<span id="cb129-4"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-4" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;tidyr&quot;</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;tidyr&quot;</span>)</span>
<span id="cb129-5"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-5" tabindex="-1"></a></span>
<span id="cb129-6"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-6" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb129-7"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-7" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb129-8"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-8" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb129-9"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-9" tabindex="-1"></a></span>
<span id="cb129-10"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-10" tabindex="-1"></a><span class="co"># Run the experiment.</span></span>
<span id="cb129-11"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-11" tabindex="-1"></a><span class="co"># Note: The original experiment function evaluates performance every 50 episodes,</span></span>
<span id="cb129-12"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-12" tabindex="-1"></a><span class="co"># resulting in a plot that connects these discrete data points.</span></span>
<span id="cb129-13"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-13" tabindex="-1"></a>adaptation_results_df <span class="ot">&lt;-</span> <span class="fu">adaptation_experiment</span>()</span>
<span id="cb129-14"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-14" tabindex="-1"></a></span>
<span id="cb129-15"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-15" tabindex="-1"></a><span class="co"># Prepare data for plotting by filtering out unevaluated episodes</span></span>
<span id="cb129-16"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-16" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> adaptation_results_df <span class="sc">%&gt;%</span></span>
<span id="cb129-17"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-17" tabindex="-1"></a>  <span class="fu">filter</span>(performance <span class="sc">!=</span> <span class="dv">0</span>)</span>
<span id="cb129-18"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-18" tabindex="-1"></a></span>
<span id="cb129-19"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-19" tabindex="-1"></a><span class="co"># Generate the plot</span></span>
<span id="cb129-20"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-20" tabindex="-1"></a><span class="fu">ggplot</span>(plot_data, <span class="fu">aes</span>(<span class="at">x =</span> episode, <span class="at">y =</span> performance, <span class="at">color =</span> algorithm)) <span class="sc">+</span></span>
<span id="cb129-21"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-21" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">linewidth =</span> <span class="fl">1.2</span>) <span class="sc">+</span></span>
<span id="cb129-22"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-22" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size =</span> <span class="fl">2.5</span>) <span class="sc">+</span></span>
<span id="cb129-23"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-23" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="dv">500</span>, <span class="at">linetype =</span> <span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb129-24"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-24" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">&quot;text&quot;</span>, <span class="at">x =</span> <span class="dv">480</span>, <span class="at">y =</span> <span class="fu">min</span>(plot_data<span class="sc">$</span>performance, <span class="at">na.rm=</span><span class="cn">TRUE</span>) <span class="sc">*</span> <span class="fl">1.1</span>, <span class="at">label =</span> <span class="st">&quot;Environment</span><span class="sc">\n</span><span class="st">Change&quot;</span>, <span class="at">vjust =</span> <span class="dv">0</span>, <span class="at">hjust =</span> <span class="dv">1</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>, <span class="at">size=</span><span class="fl">3.5</span>) <span class="sc">+</span></span>
<span id="cb129-25"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-25" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb129-26"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-26" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">&quot;Dyna-Q+ vs. Standard Dyna-Q in a Changing Environment&quot;</span>,</span>
<span id="cb129-27"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-27" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">&quot;Performance comparison before and after an environmental change at episode 500.&quot;</span>,</span>
<span id="cb129-28"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-28" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">&quot;Episode&quot;</span>,</span>
<span id="cb129-29"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-29" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">&quot;Average Reward&quot;</span>,</span>
<span id="cb129-30"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-30" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">&quot;Algorithm&quot;</span></span>
<span id="cb129-31"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-31" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb129-32"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-32" tabindex="-1"></a>  <span class="fu">theme_minimal</span>(<span class="at">base_size =</span> <span class="dv">14</span>) <span class="sc">+</span></span>
<span id="cb129-33"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-33" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;Dyna-Q&quot;</span> <span class="ot">=</span> <span class="st">&quot;#d95f02&quot;</span>, <span class="st">&quot;Dyna-Q+&quot;</span> <span class="ot">=</span> <span class="st">&quot;#1b9e77&quot;</span>)) <span class="sc">+</span></span>
<span id="cb129-34"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-34" tabindex="-1"></a>  <span class="fu">theme</span>(</span>
<span id="cb129-35"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-35" tabindex="-1"></a>    <span class="at">legend.position =</span> <span class="st">&quot;bottom&quot;</span>,</span>
<span id="cb129-36"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-36" tabindex="-1"></a>    <span class="at">plot.title =</span> <span class="fu">element_text</span>(<span class="at">face =</span> <span class="st">&quot;bold&quot;</span>),</span>
<span id="cb129-37"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-37" tabindex="-1"></a>    <span class="at">plot.subtitle =</span> <span class="fu">element_text</span>(<span class="at">color =</span> <span class="st">&quot;grey30&quot;</span>)</span>
<span id="cb129-38"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb129-38" tabindex="-1"></a>  )</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-37-1.png" width="672" /></p>
</div>
</div>
<div id="parameter-sensitivity-analysis" class="section level3 hasAnchor" number="10.4.2">
<h3><span class="header-section-number">10.4.2</span> Parameter Sensitivity Analysis<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#parameter-sensitivity-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The exploration parameter <span class="math inline">\(\\kappa\)</span> significantly influences Dyna-Q+âs behavior. Letâs examine its effects:</p>
<div class="sourceCode" id="cb130"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb130-1"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-1" tabindex="-1"></a>kappa_sensitivity_analysis <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb130-2"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-2" tabindex="-1"></a>  kappa_values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.1</span>, <span class="fl">0.2</span>, <span class="fl">0.5</span>)</span>
<span id="cb130-3"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-3" tabindex="-1"></a>  change_point <span class="ot">&lt;-</span> <span class="dv">300</span></span>
<span id="cb130-4"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-4" tabindex="-1"></a>  total_episodes <span class="ot">&lt;-</span> <span class="dv">600</span></span>
<span id="cb130-5"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-5" tabindex="-1"></a>  </span>
<span id="cb130-6"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-6" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb130-7"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-7" tabindex="-1"></a>  </span>
<span id="cb130-8"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-8" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(kappa_values)) {</span>
<span id="cb130-9"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-9" tabindex="-1"></a>    <span class="fu">set.seed</span>(<span class="dv">42</span>)  <span class="co"># Consistent conditions</span></span>
<span id="cb130-10"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-10" tabindex="-1"></a>    </span>
<span id="cb130-11"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-11" tabindex="-1"></a>    result <span class="ot">&lt;-</span> <span class="fu">dyna_q_plus</span>(<span class="at">episodes =</span> total_episodes,</span>
<span id="cb130-12"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-12" tabindex="-1"></a>                          <span class="at">change_episode =</span> change_point,</span>
<span id="cb130-13"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-13" tabindex="-1"></a>                          <span class="at">kappa =</span> kappa_values[i],</span>
<span id="cb130-14"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-14" tabindex="-1"></a>                          <span class="at">n_planning =</span> <span class="dv">10</span>)</span>
<span id="cb130-15"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-15" tabindex="-1"></a>    </span>
<span id="cb130-16"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-16" tabindex="-1"></a>    <span class="co"># Create a temporary Q matrix for evaluation before the change</span></span>
<span id="cb130-17"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-17" tabindex="-1"></a>    <span class="co"># We can&#39;t know the exact Q before the change without modifying the main loop,</span></span>
<span id="cb130-18"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-18" tabindex="-1"></a>    <span class="co"># so we run a separate short training for pre-change evaluation.</span></span>
<span id="cb130-19"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-19" tabindex="-1"></a>    pre_change_result <span class="ot">&lt;-</span> <span class="fu">dyna_q_plus</span>(<span class="at">episodes =</span> change_point, <span class="at">kappa =</span> kappa_values[i], <span class="at">n_planning=</span><span class="dv">10</span>)</span>
<span id="cb130-20"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-20" tabindex="-1"></a>    </span>
<span id="cb130-21"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-21" tabindex="-1"></a>    pre_change_perf <span class="ot">&lt;-</span> <span class="fu">evaluate_policy_performance</span>(pre_change_result<span class="sc">$</span>Q, <span class="at">modified =</span> <span class="cn">FALSE</span>)</span>
<span id="cb130-22"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-22" tabindex="-1"></a>    post_change_perf <span class="ot">&lt;-</span> <span class="fu">evaluate_policy_performance</span>(result<span class="sc">$</span>Q, <span class="at">modified =</span> <span class="cn">TRUE</span>)</span>
<span id="cb130-23"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-23" tabindex="-1"></a>    </span>
<span id="cb130-24"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-24" tabindex="-1"></a>    <span class="co"># Handle division by zero or near-zero rewards</span></span>
<span id="cb130-25"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-25" tabindex="-1"></a>    adaptation_ratio <span class="ot">&lt;-</span> <span class="cf">if</span> (pre_change_perf<span class="sc">$</span>avg_reward <span class="sc">&gt;</span> <span class="fl">1e-5</span>) {</span>
<span id="cb130-26"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-26" tabindex="-1"></a>      post_change_perf<span class="sc">$</span>avg_reward <span class="sc">/</span> pre_change_perf<span class="sc">$</span>avg_reward</span>
<span id="cb130-27"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-27" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb130-28"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-28" tabindex="-1"></a>      <span class="cn">NA</span> <span class="co"># Avoid meaningless ratios</span></span>
<span id="cb130-29"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-29" tabindex="-1"></a>    }</span>
<span id="cb130-30"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-30" tabindex="-1"></a></span>
<span id="cb130-31"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-31" tabindex="-1"></a>    results[[i]] <span class="ot">&lt;-</span> <span class="fu">list</span>(</span>
<span id="cb130-32"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-32" tabindex="-1"></a>      <span class="at">kappa =</span> kappa_values[i],</span>
<span id="cb130-33"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-33" tabindex="-1"></a>      <span class="at">pre_change_reward =</span> pre_change_perf<span class="sc">$</span>avg_reward,</span>
<span id="cb130-34"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-34" tabindex="-1"></a>      <span class="at">post_change_reward =</span> post_change_perf<span class="sc">$</span>avg_reward,</span>
<span id="cb130-35"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-35" tabindex="-1"></a>      <span class="at">adaptation_ratio =</span> adaptation_ratio</span>
<span id="cb130-36"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-36" tabindex="-1"></a>    )</span>
<span id="cb130-37"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-37" tabindex="-1"></a>  }</span>
<span id="cb130-38"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-38" tabindex="-1"></a>  </span>
<span id="cb130-39"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-39" tabindex="-1"></a>  <span class="co"># Convert to data frame for analysis</span></span>
<span id="cb130-40"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-40" tabindex="-1"></a>  sensitivity_df <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, <span class="fu">lapply</span>(results, <span class="cf">function</span>(x) {</span>
<span id="cb130-41"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-41" tabindex="-1"></a>    <span class="fu">data.frame</span>(<span class="at">kappa =</span> x<span class="sc">$</span>kappa,</span>
<span id="cb130-42"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-42" tabindex="-1"></a>               <span class="at">pre_change =</span> x<span class="sc">$</span>pre_change_reward,</span>
<span id="cb130-43"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-43" tabindex="-1"></a>               <span class="at">post_change =</span> x<span class="sc">$</span>post_change_reward,</span>
<span id="cb130-44"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-44" tabindex="-1"></a>               <span class="at">adaptation =</span> x<span class="sc">$</span>adaptation_ratio)</span>
<span id="cb130-45"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-45" tabindex="-1"></a>  }))</span>
<span id="cb130-46"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-46" tabindex="-1"></a>  </span>
<span id="cb130-47"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-47" tabindex="-1"></a>  <span class="fu">return</span>(sensitivity_df)</span>
<span id="cb130-48"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-48" tabindex="-1"></a>}</span>
<span id="cb130-49"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-49" tabindex="-1"></a></span>
<span id="cb130-50"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-50" tabindex="-1"></a><span class="co"># Visualization function</span></span>
<span id="cb130-51"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-51" tabindex="-1"></a>plot_kappa_sensitivity <span class="ot">&lt;-</span> <span class="cf">function</span>(data) {</span>
<span id="cb130-52"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-52" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">4</span>, <span class="dv">4</span>, <span class="dv">2</span>) <span class="sc">+</span> <span class="fl">0.1</span>)</span>
<span id="cb130-53"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-53" tabindex="-1"></a>  </span>
<span id="cb130-54"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-54" tabindex="-1"></a>  <span class="co"># Plot 1: Performance vs kappa</span></span>
<span id="cb130-55"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-55" tabindex="-1"></a>  <span class="fu">plot</span>(data<span class="sc">$</span>kappa, data<span class="sc">$</span>post_change, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&quot;darkred&quot;</span>,</span>
<span id="cb130-56"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-56" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Kappa Value (Îº)&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Post-Change Average Reward&quot;</span>,</span>
<span id="cb130-57"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-57" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Performance After Change&quot;</span>, <span class="at">ylim=</span><span class="fu">c</span>(<span class="fu">min</span>(data<span class="sc">$</span>post_change, <span class="at">na.rm=</span>T)<span class="sc">*</span><span class="fl">0.9</span>, <span class="fu">max</span>(data<span class="sc">$</span>post_change, <span class="at">na.rm=</span>T)<span class="sc">*</span><span class="fl">1.1</span>))</span>
<span id="cb130-58"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-58" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb130-59"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-59" tabindex="-1"></a>  </span>
<span id="cb130-60"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-60" tabindex="-1"></a>  <span class="co"># Plot 2: Adaptation ratio vs kappa</span></span>
<span id="cb130-61"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-61" tabindex="-1"></a>  <span class="fu">plot</span>(data<span class="sc">$</span>kappa, data<span class="sc">$</span>adaptation, <span class="at">type =</span> <span class="st">&quot;b&quot;</span>, <span class="at">pch =</span> <span class="dv">16</span>, <span class="at">col =</span> <span class="st">&quot;darkblue&quot;</span>,</span>
<span id="cb130-62"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-62" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Kappa Value (Îº)&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Adaptation Ratio (Post/Pre)&quot;</span>,</span>
<span id="cb130-63"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-63" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Adaptation vs Exploration&quot;</span>)</span>
<span id="cb130-64"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-64" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb130-65"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-65" tabindex="-1"></a>  </span>
<span id="cb130-66"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-66" tabindex="-1"></a>  <span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>))</span>
<span id="cb130-67"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb130-67" tabindex="-1"></a>}</span></code></pre></div>
<div id="example-running-the-sensitivity-analysis" class="section level4 hasAnchor" number="10.4.2.1">
<h4><span class="header-section-number">10.4.2.1</span> Example: Running the Sensitivity Analysis<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#example-running-the-sensitivity-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We run the analysis for different values of <span class="math inline">\(\\kappa\)</span>. A value of <span class="math inline">\(\\kappa = 0\)</span> corresponds to standard Dyna-Q. The table shows the average reward before and after the environmental change. The plots visualize how post-change performance and the adaptation ratio change with <span class="math inline">\(\\kappa\)</span>. There is a sweet spot for <span class="math inline">\(\\kappa\)</span> (around 0.1-0.2 in this case) that provides the best adaptation. If <span class="math inline">\(\\kappa\)</span> is too low, adaptation is slow; if itâs too high, the agent explores too much, which can also hurt performance.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb131-1"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb131-1" tabindex="-1"></a><span class="co"># 1. Generate the data</span></span>
<span id="cb131-2"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb131-2" tabindex="-1"></a>sensitivity_data <span class="ot">&lt;-</span> <span class="fu">kappa_sensitivity_analysis</span>()</span>
<span id="cb131-3"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb131-3" tabindex="-1"></a></span>
<span id="cb131-4"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb131-4" tabindex="-1"></a><span class="co"># 2. Display the data as a formatted table</span></span>
<span id="cb131-5"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb131-5" tabindex="-1"></a><span class="cf">if</span> (<span class="sc">!</span><span class="fu">require</span>(<span class="st">&quot;knitr&quot;</span>, <span class="at">quietly =</span> <span class="cn">TRUE</span>)) <span class="fu">install.packages</span>(<span class="st">&quot;knitr&quot;</span>)</span>
<span id="cb131-6"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb131-6" tabindex="-1"></a>knitr<span class="sc">::</span><span class="fu">kable</span>(sensitivity_data,</span>
<span id="cb131-7"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb131-7" tabindex="-1"></a>      <span class="at">caption =</span> <span class="st">&quot;Kappa Parameter Sensitivity Analysis Results&quot;</span>,</span>
<span id="cb131-8"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb131-8" tabindex="-1"></a>      <span class="at">col.names =</span> <span class="fu">c</span>(<span class="st">&quot;Kappa (Îº)&quot;</span>, <span class="st">&quot;Pre-Change Reward&quot;</span>, <span class="st">&quot;Post-Change Reward&quot;</span>, <span class="st">&quot;Adaptation Ratio&quot;</span>),</span>
<span id="cb131-9"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb131-9" tabindex="-1"></a>      <span class="at">digits =</span> <span class="dv">3</span>,</span>
<span id="cb131-10"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb131-10" tabindex="-1"></a>      <span class="at">align =</span> <span class="st">&#39;c&#39;</span>)</span></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-39">Table 10.1: </span>Kappa Parameter Sensitivity Analysis Results</caption>
<colgroup>
<col width="16%" />
<col width="27%" />
<col width="29%" />
<col width="26%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Kappa (Îº)</th>
<th align="center">Pre-Change Reward</th>
<th align="center">Post-Change Reward</th>
<th align="center">Adaptation Ratio</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">0.00</td>
<td align="center">1.257</td>
<td align="center">1.135</td>
<td align="center">0.903</td>
</tr>
<tr class="even">
<td align="center">0.01</td>
<td align="center">1.211</td>
<td align="center">0.947</td>
<td align="center">0.782</td>
</tr>
<tr class="odd">
<td align="center">0.05</td>
<td align="center">1.438</td>
<td align="center">0.870</td>
<td align="center">0.605</td>
</tr>
<tr class="even">
<td align="center">0.10</td>
<td align="center">1.489</td>
<td align="center">0.538</td>
<td align="center">0.361</td>
</tr>
<tr class="odd">
<td align="center">0.20</td>
<td align="center">1.411</td>
<td align="center">0.998</td>
<td align="center">0.707</td>
</tr>
<tr class="even">
<td align="center">0.50</td>
<td align="center">1.630</td>
<td align="center">0.839</td>
<td align="center">0.515</td>
</tr>
</tbody>
</table>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb132-1"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb132-1" tabindex="-1"></a><span class="co"># 3. Generate the plots using the provided function</span></span>
<span id="cb132-2"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb132-2" tabindex="-1"></a><span class="fu">plot_kappa_sensitivity</span>(sensitivity_data)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
</div>
</div>
<div id="exploration-pattern-analysis" class="section level3 hasAnchor" number="10.4.3">
<h3><span class="header-section-number">10.4.3</span> Exploration Pattern Analysis<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#exploration-pattern-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One way to understand Dyna-Q+âs behavior is to examine how exploration bonuses evolve over time:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb133-1"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-1" tabindex="-1"></a>analyze_exploration_patterns <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb133-2"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-2" tabindex="-1"></a>  <span class="co"># Run Dyna-Q+ and track exploration bonuses</span></span>
<span id="cb133-3"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-3" tabindex="-1"></a>  <span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb133-4"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-4" tabindex="-1"></a>  </span>
<span id="cb133-5"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-5" tabindex="-1"></a>  Q <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="dv">0</span>, <span class="at">nrow =</span> n_states, <span class="at">ncol =</span> n_actions)</span>
<span id="cb133-6"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-6" tabindex="-1"></a>  timestamps <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions))</span>
<span id="cb133-7"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-7" tabindex="-1"></a>  visited_sa <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb133-8"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-8" tabindex="-1"></a>  current_time <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb133-9"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-9" tabindex="-1"></a>  kappa <span class="ot">&lt;-</span> <span class="fl">0.1</span></span>
<span id="cb133-10"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-10" tabindex="-1"></a>  </span>
<span id="cb133-11"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-11" tabindex="-1"></a>  <span class="co"># Storage for bonus tracking</span></span>
<span id="cb133-12"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-12" tabindex="-1"></a>  bonus_history <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb133-13"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-13" tabindex="-1"></a>  <span class="co"># Define time points for snapshotting bonus values</span></span>
<span id="cb133-14"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-14" tabindex="-1"></a>  time_points <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">100</span>, <span class="dv">2000</span>, <span class="at">by=</span><span class="dv">100</span>)</span>
<span id="cb133-15"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-15" tabindex="-1"></a></span>
<span id="cb133-16"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-16" tabindex="-1"></a>  <span class="co"># Run for a fixed number of time steps instead of episodes</span></span>
<span id="cb133-17"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-17" tabindex="-1"></a>  <span class="co"># to get a clearer view of bonus evolution over time.</span></span>
<span id="cb133-18"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-18" tabindex="-1"></a>  <span class="cf">while</span>(current_time <span class="sc">&lt;</span> <span class="dv">2000</span>) {</span>
<span id="cb133-19"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-19" tabindex="-1"></a>    s <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="co"># Reset to start state for each &quot;pseudo-episode&quot;</span></span>
<span id="cb133-20"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-20" tabindex="-1"></a>    </span>
<span id="cb133-21"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-21" tabindex="-1"></a>    <span class="cf">while</span> (s <span class="sc">!=</span> terminal_state <span class="sc">&amp;&amp;</span> current_time <span class="sc">&lt;</span> <span class="dv">2000</span>) {</span>
<span id="cb133-22"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-22" tabindex="-1"></a>      current_time <span class="ot">&lt;-</span> current_time <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb133-23"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-23" tabindex="-1"></a>      </span>
<span id="cb133-24"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-24" tabindex="-1"></a>      <span class="co"># Simple action selection for this analysis</span></span>
<span id="cb133-25"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-25" tabindex="-1"></a>      a <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>n_actions, <span class="dv">1</span>)</span>
<span id="cb133-26"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-26" tabindex="-1"></a>      outcome <span class="ot">&lt;-</span> <span class="fu">sample_env</span>(s, a)</span>
<span id="cb133-27"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-27" tabindex="-1"></a>      s_prime <span class="ot">&lt;-</span> outcome<span class="sc">$</span>s_prime</span>
<span id="cb133-28"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-28" tabindex="-1"></a>      </span>
<span id="cb133-29"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-29" tabindex="-1"></a>      <span class="co"># Update timestamps</span></span>
<span id="cb133-30"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-30" tabindex="-1"></a>      timestamps[s, a] <span class="ot">&lt;-</span> current_time</span>
<span id="cb133-31"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-31" tabindex="-1"></a>      </span>
<span id="cb133-32"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-32" tabindex="-1"></a>      <span class="co"># Track bonuses at specific time points</span></span>
<span id="cb133-33"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-33" tabindex="-1"></a>      <span class="cf">if</span> (current_time <span class="sc">%in%</span> time_points) {</span>
<span id="cb133-34"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-34" tabindex="-1"></a>        bonuses <span class="ot">&lt;-</span> <span class="fu">array</span>(<span class="dv">0</span>, <span class="at">dim =</span> <span class="fu">c</span>(n_states, n_actions))</span>
<span id="cb133-35"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-35" tabindex="-1"></a>        <span class="cf">for</span> (state <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_states) {</span>
<span id="cb133-36"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-36" tabindex="-1"></a>          <span class="cf">for</span> (action <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_actions) {</span>
<span id="cb133-37"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-37" tabindex="-1"></a>            <span class="cf">if</span> (timestamps[state, action] <span class="sc">&gt;</span> <span class="dv">0</span>) {</span>
<span id="cb133-38"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-38" tabindex="-1"></a>              time_diff <span class="ot">&lt;-</span> current_time <span class="sc">-</span> timestamps[state, action]</span>
<span id="cb133-39"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-39" tabindex="-1"></a>              bonuses[state, action] <span class="ot">&lt;-</span> kappa <span class="sc">*</span> <span class="fu">sqrt</span>(time_diff)</span>
<span id="cb133-40"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-40" tabindex="-1"></a>            }</span>
<span id="cb133-41"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-41" tabindex="-1"></a>          }</span>
<span id="cb133-42"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-42" tabindex="-1"></a>        }</span>
<span id="cb133-43"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-43" tabindex="-1"></a>        </span>
<span id="cb133-44"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-44" tabindex="-1"></a>        bonus_history[[<span class="fu">as.character</span>(current_time)]] <span class="ot">&lt;-</span> bonuses</span>
<span id="cb133-45"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-45" tabindex="-1"></a>      }</span>
<span id="cb133-46"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-46" tabindex="-1"></a>      </span>
<span id="cb133-47"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-47" tabindex="-1"></a>      s <span class="ot">&lt;-</span> s_prime</span>
<span id="cb133-48"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-48" tabindex="-1"></a>    }</span>
<span id="cb133-49"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-49" tabindex="-1"></a>  }</span>
<span id="cb133-50"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-50" tabindex="-1"></a>  </span>
<span id="cb133-51"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-51" tabindex="-1"></a>  <span class="fu">return</span>(bonus_history)</span>
<span id="cb133-52"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-52" tabindex="-1"></a>}</span>
<span id="cb133-53"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-53" tabindex="-1"></a></span>
<span id="cb133-54"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-54" tabindex="-1"></a><span class="co"># Visualization of exploration bonus evolution</span></span>
<span id="cb133-55"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-55" tabindex="-1"></a>plot_bonus_evolution <span class="ot">&lt;-</span> <span class="cf">function</span>() {</span>
<span id="cb133-56"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-56" tabindex="-1"></a>  bonus_data <span class="ot">&lt;-</span> <span class="fu">analyze_exploration_patterns</span>()</span>
<span id="cb133-57"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-57" tabindex="-1"></a>  </span>
<span id="cb133-58"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-58" tabindex="-1"></a>  <span class="co"># Extract bonus magnitudes over time</span></span>
<span id="cb133-59"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-59" tabindex="-1"></a>  time_points <span class="ot">&lt;-</span> <span class="fu">as.numeric</span>(<span class="fu">names</span>(bonus_data))</span>
<span id="cb133-60"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-60" tabindex="-1"></a>  max_bonuses <span class="ot">&lt;-</span> <span class="fu">sapply</span>(bonus_data, <span class="cf">function</span>(x) <span class="fu">max</span>(x, <span class="at">na.rm =</span> <span class="cn">TRUE</span>))</span>
<span id="cb133-61"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-61" tabindex="-1"></a>  mean_bonuses <span class="ot">&lt;-</span> <span class="fu">sapply</span>(bonus_data, <span class="cf">function</span>(x) <span class="fu">mean</span>(x[x <span class="sc">&gt;</span> <span class="dv">0</span>], <span class="at">na.rm =</span> <span class="cn">TRUE</span>))</span>
<span id="cb133-62"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-62" tabindex="-1"></a>  </span>
<span id="cb133-63"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-63" tabindex="-1"></a>  <span class="fu">plot</span>(time_points, max_bonuses, <span class="at">type =</span> <span class="st">&quot;l&quot;</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>,</span>
<span id="cb133-64"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-64" tabindex="-1"></a>       <span class="at">xlab =</span> <span class="st">&quot;Time Steps&quot;</span>, <span class="at">ylab =</span> <span class="st">&quot;Exploration Bonus Magnitude&quot;</span>,</span>
<span id="cb133-65"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-65" tabindex="-1"></a>       <span class="at">main =</span> <span class="st">&quot;Evolution of Exploration Bonuses&quot;</span>,</span>
<span id="cb133-66"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-66" tabindex="-1"></a>       <span class="at">ylim =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="fu">max</span>(max_bonuses, <span class="at">na.rm =</span> <span class="cn">TRUE</span>)))</span>
<span id="cb133-67"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-67" tabindex="-1"></a>  </span>
<span id="cb133-68"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-68" tabindex="-1"></a>  <span class="fu">lines</span>(time_points, mean_bonuses, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb133-69"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-69" tabindex="-1"></a>  </span>
<span id="cb133-70"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-70" tabindex="-1"></a>  <span class="fu">legend</span>(<span class="st">&quot;topleft&quot;</span>, </span>
<span id="cb133-71"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-71" tabindex="-1"></a>         <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Maximum Bonus&quot;</span>, <span class="st">&quot;Average Bonus&quot;</span>),</span>
<span id="cb133-72"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-72" tabindex="-1"></a>         <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>), </span>
<span id="cb133-73"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-73" tabindex="-1"></a>         <span class="at">lty =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>), </span>
<span id="cb133-74"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-74" tabindex="-1"></a>         <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">bty=</span><span class="st">&quot;n&quot;</span>)</span>
<span id="cb133-75"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-75" tabindex="-1"></a>  </span>
<span id="cb133-76"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-76" tabindex="-1"></a>  <span class="fu">grid</span>(<span class="at">lty =</span> <span class="dv">1</span>, <span class="at">col =</span> <span class="st">&quot;gray90&quot;</span>)</span>
<span id="cb133-77"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb133-77" tabindex="-1"></a>}</span></code></pre></div>
<div id="example-visualizing-exploration-bonuses" class="section level4 hasAnchor" number="10.4.3.1">
<h4><span class="header-section-number">10.4.3.1</span> Example: Visualizing Exploration Bonuses<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#example-visualizing-exploration-bonuses" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>This plot shows how the exploration bonuses change over time. As the agent explores, timestamps are updated, and the time since the last visit (<code>t - Ï</code>) for any given state-action pair can grow. The <strong>Maximum Bonus</strong> corresponds to the state-action pair that has been unvisited for the longest time, showing the agentâs growing âcuriosityâ about that specific part of the environment. The <strong>Average Bonus</strong> (for visited pairs) tends to stay lower, indicating that most parts of the model are kept relatively fresh through planning and exploration.</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb134-1"><a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#cb134-1" tabindex="-1"></a><span class="fu">plot_bonus_evolution</span>()</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/unnamed-chunk-42-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="discussion-and-implementation-considerations" class="section level2 hasAnchor" number="10.5">
<h2><span class="header-section-number">10.5</span> Discussion and Implementation Considerations<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#discussion-and-implementation-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Dyna-Q+ can be understood as a form of algorithmic curiosity that parallels aspects of human learning. Just as people grow uneasy about facts they have not revisited in some time, the algorithm gradually discounts its own modelâs accuracy as intervals between visits lengthen. This built-in doubt is advantageous in non-stationary settings, where relying on yesterdayâs truths can be costly. The exploration bonus, scaled by a square root of elapsed time, encodes an important nuance: uncertainty should increase with neglect, but at a diminishing rate. This prevents the system from sliding into perpetual skepticism while keeping enough pressure to revisit older assumptions.</p>
<p>The extra bookkeeping is minimalâsimply a timestamp for each stateâaction pairâbut it changes the decision-making problem. The agent now balances three forces: exploiting current knowledge, exploring new possibilities, and re-exploring known areas to keep the model current. This is more complex than the standard exploreâexploit trade-off in Dyna. For large stateâaction spaces, the linear scaling of timestamp storage may require function approximation or selective retention, especially in continuous or high-dimensional domains.</p>
<p>Empirically, Dyna-Q+ tends to shine in environments that evolve over time. In stable conditions, bonuses for well-visited states remain small and the algorithm behaves much like standard Dyna. But when conditions shift, the systematic revisiting of old transitions enables faster adaptation. The parameter <span class="math inline">\(\\kappa\)</span> sets the level of âmodel anxietyâ: small values create a trusting system, large values a more suspicious one. The best setting depends on how quickly the world changes and on the relative costs of exploration and exploitation errors.</p>
<p>The method rests on an implicit assumptionâthat environmental change is the main cause of model inaccuracy. When inaccuracy stems instead from intrinsic difficulty, such as noisy transitions or highly complex dynamics, the uniform bonuses may encourage needless exploration. Similarly, applying the same bonus across all stateâaction pairs ignores that some regions may be more volatile or strategically important than others. More refined variants might weight bonuses according to change likelihood or the expected impact of outdated information.</p>
<p>Later research has broadened these ideas. In deep reinforcement learning, uncertainty-driven exploration often uses learned uncertainty estimates rather than timestamps. Meta-learning approaches aim to optimise exploration strategies across related environments. Curiosity-driven methods extend the spirit of Dyna-Q+ beyond temporal doubt, rewarding novelty in prediction error, information gain, or visitation patterns. The shared thread is that learning systems should actively seek information that improves their internal models.</p>
<p>In practice, Dyna-Q+ is well suited to domains with gradual, structured changeâfinancial markets with shifting regimes, or mobile robots navigating spaces where obstacles occasionally move. It is less effective in environments with rapid or chaotic dynamics, where maintaining a model may be futile or the bonus insufficient to trigger timely adaptation.</p>
<p>Implementation choices often start with <span class="math inline">\(\\kappa\)</span> between 0.01 and 0.1, tuning from there. More volatile settings generally warrant larger values. Planning steps <span class="math inline">\(n\)</span> interact with <span class="math inline">\(\\kappa\)</span>: increasing <span class="math inline">\(n\)</span> amplifies bonus effects and may require reducing <span class="math inline">\(\\kappa\)</span>. Large-scale use can demand timestamp approximationsâsuch as storing them only for a subset of pairs or grouping times into binsâto save memory while preserving adaptivity. The extra computation from bonuses is usually negligible compared to value updates, though in time-critical systems, even the square-root calculation may be replaced by lookup tables or cheaper approximations.</p>
</div>
<div id="conclusion-6" class="section level2 hasAnchor" number="10.6">
<h2><span class="header-section-number">10.6</span> Conclusion<a href="dyna-q-enhanced-exploration-in-integrated-learning-and-planning.html#conclusion-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Dyna-Q+ represents a elegant solution to a fundamental challenge in reinforcement learning: how to maintain confidence in learned models while remaining appropriately skeptical about their continued accuracy. By treating time as information and systematically rewarding curiosity about neglected state-action pairs, the algorithm achieves a sophisticated balance between stability and adaptability.</p>
<p>The approachâs strength lies not just in its technical effectiveness but in its conceptual clarity. The idea that confidence should decay over time unless refreshed by recent experience resonates across many domains beyond reinforcement learning. This principle finds echoes in human psychology, scientific methodology, and even social institutions that require periodic validation of their foundational assumptions.</p>
<p>While modern deep reinforcement learning has developed more sophisticated approaches to uncertainty and exploration, Dyna-Q+âs core insights remain relevant. The tension between trusting learned models and maintaining healthy skepticism about their accuracy continues to challenge contemporary algorithms. In an era of rapidly changing environments and non-stationary dynamics, the principle of time-decaying confidence may prove even more valuable than when originally proposed.</p>
<p>Looking forward, the integration of Dyna-Q+âs temporal curiosity with modern uncertainty estimation techniques presents intriguing possibilities. Neural networks that maintain both predictive models and confidence estimates could incorporate exploration bonuses based on both temporal factors and model uncertainty, potentially creating more robust and adaptive learning systems.</p>
<p>The simplicity of Dyna-Q+âs modification to standard Dynaâjust adding a single term to the planning rewardsâbelies its conceptual sophistication. Sometimes the most profound advances in artificial intelligence come not from complex new architectures but from simple changes that embody deep insights about learning, adaptation, and the nature of knowledge itself.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="dyna-and-dynaq.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="function-approximation-and-feature-engineering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans",
    "size": 2
  },
  "edit": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/edit/main/10-Q_Dyna_Plus.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": "https://github.com/Kamran-Afzali/Bookdown_RLR/blob/main/10-Q_Dyna_Plus.Rmd",
    "text": null
  },
  "download": null,
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "subsection"
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
